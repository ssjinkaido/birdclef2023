Date :04/10/2023, 13:38:40
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 256
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :04/10/2023, 13:50:21
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 256
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :04/10/2023, 15:02:09
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 256
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :04/10/2023, 15:15:06
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :04/10/2023, 15:16:25
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :04/11/2023, 02:47:03
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :04/11/2023, 02:48:07
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :04/11/2023, 02:55:55
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :04/11/2023, 04:29:48
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :04/11/2023, 04:48:43
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :04/11/2023, 04:49:52
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 10.6321, val loss: 8.2996
Model improve: 0.0000 -> 0.5300
Epoch: 2/100
Train loss: 9.2146, val loss: 6.0128
Model improve: 0.5300 -> 0.6300
Epoch: 3/100
Date :04/11/2023, 05:03:41
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 16
nmels: 128
fmax: 16386
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 10.5773, val loss: 8.4143
Model improve: 0.0000 -> 0.5266
Epoch: 2/100
Train loss: 9.1466, val loss: 6.1750
Model improve: 0.5266 -> 0.6236
Epoch: 3/100
Train loss: 8.1916, val loss: 5.3970
Model improve: 0.6236 -> 0.6705
Epoch: 4/100
Train loss: 7.7440, val loss: 4.6301
Model improve: 0.6705 -> 0.7083
Epoch: 5/100
Train loss: 7.3883, val loss: 4.3335
Model improve: 0.7083 -> 0.7296
Epoch: 6/100
Train loss: 7.1607, val loss: 4.1727
Model improve: 0.7296 -> 0.7457
Epoch: 7/100
Train loss: 6.7981, val loss: 4.0176
Model improve: 0.7457 -> 0.7572
Epoch: 8/100
Train loss: 6.7575, val loss: 3.7836
Model improve: 0.7572 -> 0.7640
Epoch: 9/100
Date :04/11/2023, 07:56:33
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 16
nmels: 128
fmax: 16386
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :04/11/2023, 08:08:10
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 16
nmels: 128
fmax: 16386
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 10.6360, val loss: 8.3409
Model improve: 0.0000 -> 0.5277
Epoch: 2/100
Date :04/11/2023, 08:20:28
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 16
nmels: 128
fmax: 16386
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 10.5727, val loss: 8.3101
Model improve: 0.0000 -> 0.5321
Epoch: 2/100
Date :04/11/2023, 08:26:43
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 16
nmels: 128
fmax: 16386
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :04/11/2023, 08:32:00
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 16
nmels: 128
fmax: 16386
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 10.2697, val loss: 7.3369
Model improve: 0.0000 -> 0.5666
Epoch: 2/100
Train loss: 8.1846, val loss: 5.4916
Model improve: 0.5666 -> 0.6541
Epoch: 3/100
Train loss: 6.9915, val loss: 4.6729
Model improve: 0.6541 -> 0.6994
Epoch: 4/100
Train loss: 6.4170, val loss: 4.2538
Model improve: 0.6994 -> 0.7339
Epoch: 5/100
Train loss: 6.0833, val loss: 4.0376
Model improve: 0.7339 -> 0.7557
Epoch: 6/100
Train loss: 5.7351, val loss: 3.8767
Model improve: 0.7557 -> 0.7742
Epoch: 7/100
Train loss: 5.2805, val loss: 3.7510
Model improve: 0.7742 -> 0.7802
Epoch: 8/100
Train loss: 5.1840, val loss: 3.4278
Model improve: 0.7802 -> 0.7924
Epoch: 9/100
Date :04/11/2023, 09:38:22
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 10.4608, val loss: 7.9505
Model improve: 0.0000 -> 0.5508
Epoch: 2/100
Train loss: 8.5893, val loss: 5.6527
Model improve: 0.5508 -> 0.6400
Epoch: 3/100
Train loss: 7.3523, val loss: 4.8554
Model improve: 0.6400 -> 0.6826
Epoch: 4/100
Date :04/11/2023, 11:14:41
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 16
nmels: 128
fmax: 16386
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 10.1031, val loss: 7.0811
Model improve: 0.0000 -> 0.5735
Epoch: 2/100
Train loss: 8.0436, val loss: 5.3791
Model improve: 0.5735 -> 0.6632
Epoch: 3/100
Train loss: 6.8374, val loss: 4.6700
Model improve: 0.6632 -> 0.7162
Epoch: 4/100
Date :04/11/2023, 11:43:29
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 10.7068, val loss: 8.8943
Model improve: 0.0000 -> 0.5065
Epoch: 2/100
Date :04/11/2023, 11:54:35
Duration: 5
Sample rate: 32000
nfft: 1024
fmin: 50
nmels: 128
fmax: 14000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 10.3945, val loss: 7.7493
Model improve: 0.0000 -> 0.5374
Epoch: 2/100
Date :04/11/2023, 12:04:56
Duration: 10
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 10.0619, val loss: 6.8885
Model improve: 0.0000 -> 0.5828
Epoch: 2/100
Train loss: 7.6341, val loss: 4.9540
Model improve: 0.5828 -> 0.6774
Epoch: 3/100
Train loss: 6.2711, val loss: 4.3541
Model improve: 0.6774 -> 0.7271
Epoch: 4/100
Train loss: 5.5387, val loss: 4.0249
Model improve: 0.7271 -> 0.7561
Epoch: 5/100
Train loss: 5.1641, val loss: 3.8200
Model improve: 0.7561 -> 0.7769
Epoch: 6/100
Train loss: 4.6821, val loss: 3.5578
Model improve: 0.7769 -> 0.7831
Epoch: 7/100
Train loss: 4.4754, val loss: 3.5538
Model improve: 0.7831 -> 0.7948
Epoch: 8/100
Train loss: 4.2476, val loss: 3.4292
Model improve: 0.7948 -> 0.8002
Epoch: 9/100
Train loss: 4.3127, val loss: 3.3194
Model improve: 0.8002 -> 0.8100
Epoch: 10/100
Train loss: 4.0129, val loss: 3.2911
Model improve: 0.8100 -> 0.8146
Epoch: 11/100
Train loss: 3.6959, val loss: 3.3891
Epoch: 12/100
Train loss: 3.7133, val loss: 3.2748
Epoch: 13/100
Date :04/11/2023, 13:54:22
Duration: 10
Sample rate: 32000
nfft: 1024
fmin: 50
nmels: 128
fmax: 14000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 320
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Date :04/11/2023, 13:55:06
Duration: 10
Sample rate: 32000
nfft: 1024
fmin: 50
nmels: 128
fmax: 14000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 320
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
60309
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :04/11/2023, 13:56:29
Duration: 10
Sample rate: 32000
nfft: 1024
fmin: 50
nmels: 128
fmax: 14000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 320
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
60309
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :04/11/2023, 13:57:30
Duration: 10
Sample rate: 32000
nfft: 1024
fmin: 50
nmels: 128
fmax: 14000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 320
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Date :04/11/2023, 13:59:55
Duration: 10
Sample rate: 32000
nfft: 2048
fmin: 16
nmels: 128
fmax: 16836
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 320
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :04/11/2023, 14:06:37
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 16
nmels: 128
fmax: 16836
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 320
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 10.2814, val loss: 7.4680
Model improve: 0.0000 -> 0.5617
Epoch: 2/100
Train loss: 8.1553, val loss: 5.4362
Model improve: 0.5617 -> 0.6466
Epoch: 3/100
Train loss: 7.0883, val loss: 4.8445
Model improve: 0.6466 -> 0.6985
Epoch: 4/100
Train loss: 6.6109, val loss: 4.3801
Model improve: 0.6985 -> 0.7251
Epoch: 5/100
Train loss: 6.0465, val loss: 4.0548
Model improve: 0.7251 -> 0.7496
Epoch: 6/100
Train loss: 5.7588, val loss: 3.8397
Model improve: 0.7496 -> 0.7706
Epoch: 7/100
Train loss: 5.4942, val loss: 3.7379
Model improve: 0.7706 -> 0.7764
Epoch: 8/100
Train loss: 5.4081, val loss: 3.6237
Model improve: 0.7764 -> 0.7963
Epoch: 9/100
Train loss: 5.0944, val loss: 3.3680
Model improve: 0.7963 -> 0.7994
Epoch: 10/100
Train loss: 5.1980, val loss: 3.5521
Model improve: 0.7994 -> 0.8014
Epoch: 11/100
Train loss: 4.9834, val loss: 3.4455
Model improve: 0.8014 -> 0.8092
Epoch: 12/100
Train loss: 4.7432, val loss: 3.2141
Model improve: 0.8092 -> 0.8138
Epoch: 13/100
Date :04/11/2023, 15:43:36
Duration: 10
Sample rate: 32000
nfft: 1024
fmin: 50
nmels: 128
fmax: 14000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 320
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Date :04/11/2023, 15:44:05
Duration: 10
Sample rate: 32000
nfft: 1024
fmin: 50
nmels: 128
fmax: 14000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 320
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Date :04/11/2023, 15:47:12
Duration: 10
Sample rate: 32000
nfft: 1024
fmin: 50
nmels: 128
fmax: 14000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 320
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
60309
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :04/11/2023, 15:49:29
Duration: 10
Sample rate: 32000
nfft: 1024
fmin: 50
nmels: 128
fmax: 14000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 320
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Date :04/11/2023, 15:50:00
Duration: 10
Sample rate: 32000
nfft: 1024
fmin: 50
nmels: 128
fmax: 14000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 320
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
60309
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :04/11/2023, 15:50:46
Duration: 10
Sample rate: 32000
nfft: 1024
fmin: 50
nmels: 128
fmax: 14000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 320
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
60309
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :04/11/2023, 15:52:00
Duration: 10
Sample rate: 32000
nfft: 1024
fmin: 50
nmels: 128
fmax: 14000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 320
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
60309
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :04/11/2023, 15:52:21
Duration: 10
Sample rate: 32000
nfft: 1024
fmin: 50
nmels: 128
fmax: 14000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 320
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
60309
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :04/11/2023, 15:52:40
Duration: 10
Sample rate: 32000
nfft: 1024
fmin: 50
nmels: 128
fmax: 14000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 320
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Date :04/11/2023, 15:53:30
Duration: 10
Sample rate: 32000
nfft: 1024
fmin: 50
nmels: 128
fmax: 14000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 320
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Date :04/11/2023, 15:53:57
Duration: 10
Sample rate: 32000
nfft: 1024
fmin: 50
nmels: 128
fmax: 14000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 320
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
60309
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :04/11/2023, 15:57:50
Duration: 5
Sample rate: 32000
nfft: 1024
fmin: 50
nmels: 128
fmax: 14000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 320
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
60309
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 10.5878, val loss: 6.7829
Model improve: 0.0000 -> 0.4743
Epoch: 2/100
Date :04/12/2023, 09:48:20
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 16
nmels: 128
fmax: 16836
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
Date :04/12/2023, 09:52:52
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 16
nmels: 128
fmax: 16836
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :04/12/2023, 09:58:17
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 16
nmels: 128
fmax: 16836
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Date :04/12/2023, 09:59:01
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 16
nmels: 128
fmax: 16836
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 9.9612, val loss: 6.8646
Model improve: 0.0000 -> 0.5982
Epoch: 2/100
Date :04/12/2023, 10:10:32
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 16
nmels: 128
fmax: 16836
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :04/12/2023, 10:11:00
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 16
nmels: 128
fmax: 16836
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :04/12/2023, 10:15:46
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 16
nmels: 128
fmax: 16836
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :04/12/2023, 10:16:08
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 16
nmels: 128
fmax: 16836
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :04/12/2023, 10:16:33
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 16
nmels: 128
fmax: 16836
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :04/12/2023, 10:17:00
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 16
nmels: 128
fmax: 16836
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :04/12/2023, 10:19:31
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 16
nmels: 128
fmax: 16836
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 9.9175, val loss: 6.9863
Model improve: 0.0000 -> 0.5956
Epoch: 2/100
Train loss: 7.9296, val loss: 5.4670
Model improve: 0.5956 -> 0.6773
Epoch: 3/100
Train loss: 7.0935, val loss: 4.7636
Model improve: 0.6773 -> 0.7242
Epoch: 4/100
Date :04/12/2023, 10:41:31
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 16
nmels: 128
fmax: 16836
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :04/12/2023, 10:42:19
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 16
nmels: 128
fmax: 16836
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 10.0256, val loss: 6.9000
Model improve: 0.0000 -> 0.5877
Epoch: 2/100
Train loss: 8.0710, val loss: 5.5986
Model improve: 0.5877 -> 0.6742
Epoch: 3/100
Train loss: 7.1776, val loss: 4.7409
Model improve: 0.6742 -> 0.7260
Epoch: 4/100
Train loss: 6.4996, val loss: 4.3549
Model improve: 0.7260 -> 0.7435
Epoch: 5/100
Train loss: 6.2153, val loss: 3.8686
Model improve: 0.7435 -> 0.7723
Epoch: 6/100
Train loss: 5.9369, val loss: 3.8317
Model improve: 0.7723 -> 0.7825
Epoch: 7/100
Train loss: 5.5893, val loss: 3.8031
Model improve: 0.7825 -> 0.7891
Epoch: 8/100
Train loss: 5.4338, val loss: 3.5845
Model improve: 0.7891 -> 0.7919
Epoch: 9/100
Train loss: 5.2518, val loss: 3.4200
Model improve: 0.7919 -> 0.8071
Epoch: 10/100
Train loss: 5.2585, val loss: 3.4578
Model improve: 0.8071 -> 0.8095
Epoch: 11/100
Train loss: 5.0310, val loss: 3.5486
Epoch: 12/100
Train loss: 4.9311, val loss: 3.3751
Model improve: 0.8095 -> 0.8168
Epoch: 13/100
Train loss: 4.8074, val loss: 3.2696
Epoch: 14/100
Train loss: 4.7519, val loss: 3.2188
Model improve: 0.8168 -> 0.8190
Epoch: 15/100
Train loss: 4.6060, val loss: 3.3982
Epoch: 16/100
Train loss: 4.5599, val loss: 3.1929
Model improve: 0.8190 -> 0.8215
Epoch: 17/100
Train loss: 4.3317, val loss: 3.2159
Epoch: 18/100
Train loss: 4.4508, val loss: 3.2359
Epoch: 19/100
Train loss: 4.4372, val loss: 2.9456
Model improve: 0.8215 -> 0.8347
Epoch: 20/100
Train loss: 4.3309, val loss: 2.9756
Model improve: 0.8347 -> 0.8349
Epoch: 21/100
Train loss: 4.2346, val loss: 3.0466
Epoch: 22/100
Train loss: 4.2714, val loss: 2.9717
Epoch: 23/100
Train loss: 4.1912, val loss: 3.0030
Model improve: 0.8349 -> 0.8361
Epoch: 24/100
Train loss: 4.1925, val loss: 2.8446
Model improve: 0.8361 -> 0.8436
Epoch: 25/100
Train loss: 4.0583, val loss: 2.9740
Epoch: 26/100
Train loss: 4.0545, val loss: 2.8702
Epoch: 27/100
Train loss: 3.9529, val loss: 3.0535
Epoch: 28/100
Train loss: 3.9166, val loss: 2.9497
Epoch: 29/100
Train loss: 4.0303, val loss: 2.8349
Model improve: 0.8436 -> 0.8447
Epoch: 30/100
Train loss: 3.9894, val loss: 2.8546
Epoch: 31/100
Train loss: 3.8300, val loss: 2.7819
Epoch: 32/100
Train loss: 3.8584, val loss: 2.7714
Model improve: 0.8447 -> 0.8467
Epoch: 33/100
Train loss: 3.9164, val loss: 2.8839
Epoch: 34/100
Train loss: 3.7419, val loss: 2.7874
Model improve: 0.8467 -> 0.8494
Epoch: 35/100
Train loss: 3.7464, val loss: 2.6963
Model improve: 0.8494 -> 0.8495
Epoch: 36/100
Train loss: 3.7520, val loss: 2.7974
Epoch: 37/100
Train loss: 3.6171, val loss: 2.7403
Epoch: 38/100
Train loss: 3.6594, val loss: 2.7382
Model improve: 0.8495 -> 0.8500
Epoch: 39/100
Train loss: 3.5818, val loss: 2.6993
Model improve: 0.8500 -> 0.8512
Epoch: 40/100
Train loss: 3.5954, val loss: 2.6756
Epoch: 41/100
Train loss: 3.6801, val loss: 2.6700
Epoch: 42/100
Train loss: 3.5707, val loss: 2.8236
Epoch: 43/100
Date :04/12/2023, 14:35:21
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 16
nmels: 128
fmax: 16836
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :04/12/2023, 14:37:29
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 16
nmels: 128
fmax: 16836
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :04/12/2023, 14:38:17
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 16
nmels: 128
fmax: 16836
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :04/12/2023, 14:38:36
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 16
nmels: 128
fmax: 16836
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :04/12/2023, 14:39:13
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 16
nmels: 128
fmax: 16836
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :04/12/2023, 14:39:54
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 16
nmels: 128
fmax: 16836
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :04/12/2023, 14:40:58
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 16
nmels: 128
fmax: 16836
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :04/12/2023, 14:44:59
Duration: 5
Sample rate: 32000
nfft: 1024
fmin: 50
nmels: 64
fmax: 14000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 320
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Date :04/12/2023, 14:46:53
Duration: 5
Sample rate: 32000
nfft: 1024
fmin: 50
nmels: 64
fmax: 14000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 320
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Date :04/12/2023, 14:48:30
Duration: 5
Sample rate: 32000
nfft: 1024
fmin: 50
nmels: 64
fmax: 14000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 320
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :04/12/2023, 14:49:10
Duration: 5
Sample rate: 32000
nfft: 1024
fmin: 50
nmels: 64
fmax: 14000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 320
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 10.1584, val loss: 7.1103
Model improve: 0.0000 -> 0.5748
Epoch: 2/100
Train loss: 8.2943, val loss: 5.4845
Model improve: 0.5748 -> 0.6658
Epoch: 3/100
Train loss: 7.2827, val loss: 4.8745
Model improve: 0.6658 -> 0.7109
Epoch: 4/100
Train loss: 6.6783, val loss: 4.5089
Model improve: 0.7109 -> 0.7355
Epoch: 5/100
Train loss: 6.3252, val loss: 4.2157
Model improve: 0.7355 -> 0.7575
Epoch: 6/100
Train loss: 5.9479, val loss: 3.9998
Model improve: 0.7575 -> 0.7781
Epoch: 7/100
Train loss: 5.7473, val loss: 3.8476
Model improve: 0.7781 -> 0.7820
Epoch: 8/100
Train loss: 5.5379, val loss: 3.7221
Model improve: 0.7820 -> 0.7885
Epoch: 9/100
Train loss: 5.3458, val loss: 3.6846
Model improve: 0.7885 -> 0.8038
Epoch: 10/100
Train loss: 5.1266, val loss: 3.5576
Epoch: 11/100
Train loss: 5.1409, val loss: 3.4661
Epoch: 12/100
Train loss: 4.9427, val loss: 3.4772
Model improve: 0.8038 -> 0.8091
Epoch: 13/100
Train loss: 4.8522, val loss: 3.3349
Model improve: 0.8091 -> 0.8212
Epoch: 14/100
Train loss: 4.7251, val loss: 3.6178
Epoch: 15/100
Train loss: 4.6533, val loss: 3.1830
Epoch: 16/100
Date :04/12/2023, 15:56:32
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 14000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 9.5730, val loss: 5.9987
Model improve: 0.0000 -> 0.6346
Epoch: 2/100
Train loss: 7.2928, val loss: 4.6244
Model improve: 0.6346 -> 0.7186
Epoch: 3/100
Train loss: 6.4119, val loss: 4.0729
Model improve: 0.7186 -> 0.7578
Epoch: 4/100
Train loss: 5.7430, val loss: 3.7530
Model improve: 0.7578 -> 0.7793
Epoch: 5/100
Train loss: 5.4930, val loss: 3.5712
Model improve: 0.7793 -> 0.7911
Epoch: 6/100
Train loss: 5.1665, val loss: 3.4053
Model improve: 0.7911 -> 0.8013
Epoch: 7/100
Train loss: 4.8785, val loss: 3.3218
Model improve: 0.8013 -> 0.8093
Epoch: 8/100
Train loss: 4.7503, val loss: 3.0914
Model improve: 0.8093 -> 0.8185
Epoch: 9/100
Train loss: 4.5589, val loss: 3.1589
Model improve: 0.8185 -> 0.8222
Epoch: 10/100
Train loss: 4.5796, val loss: 3.2784
Epoch: 11/100
Train loss: 4.3791, val loss: 3.0211
Model improve: 0.8222 -> 0.8234
Epoch: 12/100
Train loss: 4.2597, val loss: 3.0086
Model improve: 0.8234 -> 0.8332
Epoch: 13/100
Train loss: 4.1138, val loss: 2.9452
Epoch: 14/100
Train loss: 4.1036, val loss: 2.9766
Epoch: 15/100
Train loss: 4.0128, val loss: 3.0252
Epoch: 16/100
Train loss: 3.9571, val loss: 2.9084
Model improve: 0.8332 -> 0.8363
Epoch: 17/100
Train loss: 3.7305, val loss: 2.9580
Epoch: 18/100
Train loss: 3.8307, val loss: 2.9824
Model improve: 0.8363 -> 0.8383
Epoch: 19/100
Train loss: 3.7491, val loss: 2.8743
Model improve: 0.8383 -> 0.8387
Epoch: 20/100
Train loss: 3.7523, val loss: 2.7595
Model improve: 0.8387 -> 0.8437
Epoch: 21/100
Train loss: 3.6557, val loss: 2.8087
Epoch: 22/100
Train loss: 3.6932, val loss: 2.7166
Model improve: 0.8437 -> 0.8446
Epoch: 23/100
Train loss: 3.5937, val loss: 2.7866
Epoch: 24/100
Train loss: 3.6450, val loss: 2.7067
Model improve: 0.8446 -> 0.8533
Epoch: 25/100
Train loss: 3.5552, val loss: 2.7230
Epoch: 26/100
Train loss: 3.4897, val loss: 2.6409
Epoch: 27/100
Train loss: 3.3456, val loss: 2.8006
Epoch: 28/100
Train loss: 3.3691, val loss: 2.6666
Epoch: 29/100
Train loss: 3.4492, val loss: 2.6643
Epoch: 30/100
Train loss: 3.4270, val loss: 2.6194
Model improve: 0.8533 -> 0.8556
Epoch: 31/100
Train loss: 3.2954, val loss: 2.5836
Epoch: 32/100
Train loss: 3.3275, val loss: 2.6021
Epoch: 33/100
Train loss: 3.4021, val loss: 2.5982
Epoch: 34/100
Train loss: 3.2506, val loss: 2.6167
Epoch: 35/100
Train loss: 3.2641, val loss: 2.5578
Model improve: 0.8556 -> 0.8579
Epoch: 36/100
Train loss: 3.2054, val loss: 2.5849
Epoch: 37/100
Train loss: 3.0930, val loss: 2.5860
Epoch: 38/100
Train loss: 3.1520, val loss: 2.5554
Epoch: 39/100
Train loss: 3.0791, val loss: 2.5991
Epoch: 40/100
Train loss: 3.1050, val loss: 2.5077
Epoch: 41/100
Train loss: 3.2226, val loss: 2.5767
Epoch: 42/100
Train loss: 3.0393, val loss: 2.6435
Epoch: 43/100
Train loss: 2.9836, val loss: 2.6591
Epoch: 44/100
Train loss: 3.0586, val loss: 2.5203
Model improve: 0.8579 -> 0.8587
Epoch: 45/100
Train loss: 3.0157, val loss: 2.5015
Epoch: 46/100
Train loss: 2.8549, val loss: 2.4749
Model improve: 0.8587 -> 0.8588
Epoch: 47/100
Train loss: 2.9499, val loss: 2.5853
Epoch: 48/100
Train loss: 3.0137, val loss: 2.5653
Epoch: 49/100
Train loss: 2.8957, val loss: 2.4821
Model improve: 0.8588 -> 0.8598
Epoch: 50/100
Train loss: 2.9666, val loss: 2.4776
Epoch: 51/100
Train loss: 2.8069, val loss: 2.5539
Epoch: 52/100
Train loss: 3.0771, val loss: 2.4632
Epoch: 53/100
Train loss: 2.7816, val loss: 2.4951
Epoch: 54/100
Train loss: 2.8408, val loss: 2.5539
Model improve: 0.8598 -> 0.8602
Epoch: 55/100
Train loss: 2.8065, val loss: 2.5288
Epoch: 56/100
Train loss: 2.8245, val loss: 2.4586
Epoch: 57/100
Train loss: 2.7940, val loss: 2.5292
Epoch: 58/100
Date :04/12/2023, 20:57:00
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 14000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 9.9394, val loss: 6.1939
0.6176670011529803
Model improve: 0.0000 -> 0.6177
Epoch: 2/100
Train loss: 8.1791, val loss: 4.6974
0.7095296957482378
Model improve: 0.6177 -> 0.7095
Epoch: 3/100
Train loss: 7.3508, val loss: 4.0536
0.7537112692103839
Model improve: 0.7095 -> 0.7537
Epoch: 4/100
Train loss: 6.7218, val loss: 3.5833
0.7726277312707784
Model improve: 0.7537 -> 0.7726
Epoch: 5/100
Train loss: 6.5271, val loss: 3.3725
0.7960705640645606
Model improve: 0.7726 -> 0.7961
Epoch: 6/100
Train loss: 6.2266, val loss: 3.3328
0.8048709938061366
Model improve: 0.7961 -> 0.8049
Epoch: 7/100
Train loss: 5.9148, val loss: 3.1311
0.814114477546682
Model improve: 0.8049 -> 0.8141
Epoch: 8/100
Train loss: 5.8041, val loss: 2.9640
0.8247851529244119
Model improve: 0.8141 -> 0.8248
Epoch: 9/100
Train loss: 5.5953, val loss: 2.9074
0.8378806240501504
Model improve: 0.8248 -> 0.8379
Epoch: 10/100
Train loss: 5.6257, val loss: 2.9539
0.8389658091778307
Model improve: 0.8379 -> 0.8390
Epoch: 11/100
Train loss: 5.4683, val loss: 2.6651
0.8464424895181033
Model improve: 0.8390 -> 0.8464
Epoch: 12/100
Train loss: 5.3273, val loss: 2.7679
0.8528014723692178
Model improve: 0.8464 -> 0.8528
Epoch: 13/100
Train loss: 5.2095, val loss: 2.6166
0.8516777771830607
Epoch: 14/100
Train loss: 5.2390, val loss: 2.5078
0.850751733042676
Epoch: 15/100
Train loss: 5.1142, val loss: 2.6137
0.8540517506942769
Model improve: 0.8528 -> 0.8541
Epoch: 16/100
Train loss: 5.0639, val loss: 2.5715
0.8602220898906974
Model improve: 0.8541 -> 0.8602
Epoch: 17/100
Train loss: 4.8912, val loss: 2.4715
0.8612931756175083
Model improve: 0.8602 -> 0.8613
Epoch: 18/100
Train loss: 4.9604, val loss: 2.5546
0.8595329812366428
Epoch: 19/100
Train loss: 4.8727, val loss: 2.4295
0.8615670521448502
Model improve: 0.8613 -> 0.8616
Epoch: 20/100
Train loss: 4.8205, val loss: 2.3304
0.8676476824578107
Model improve: 0.8616 -> 0.8676
Epoch: 21/100
Train loss: 4.7936, val loss: 2.4059
0.8663225254786934
Epoch: 22/100
Train loss: 4.8243, val loss: 2.3149
0.8670833625477841
Epoch: 23/100
Train loss: 4.6710, val loss: 2.3286
0.8717083726823317
Model improve: 0.8676 -> 0.8717
Epoch: 24/100
Train loss: 4.7648, val loss: 2.2808
0.8731394265181514
Model improve: 0.8717 -> 0.8731
Epoch: 25/100
Train loss: 4.6744, val loss: 2.5123
0.872121594780265
Epoch: 26/100
Train loss: 4.6496, val loss: 2.3508
0.8670609534432506
Epoch: 27/100
Train loss: 4.5219, val loss: 2.3167
0.8731092615011976
Epoch: 28/100
Train loss: 4.4602, val loss: 2.2701
0.8728475022342483
Epoch: 29/100
Train loss: 4.5957, val loss: 2.3315
0.8717265971018444
Epoch: 30/100
Train loss: 4.5547, val loss: 2.2507
0.8690963723404141
Epoch: 31/100
Train loss: 4.4566, val loss: 2.2198
0.8764598733914184
Model improve: 0.8731 -> 0.8765
Epoch: 32/100
Train loss: 4.4790, val loss: 2.2062
0.8745696200493632
Epoch: 33/100
Train loss: 4.5037, val loss: 2.2187
0.878431477093512
Model improve: 0.8765 -> 0.8784
Epoch: 34/100
Train loss: 4.3438, val loss: 2.2587
0.8745360340364113
Epoch: 35/100
Train loss: 4.3546, val loss: 2.1177
0.8772723430288691
Epoch: 36/100
Train loss: 4.3786, val loss: 2.1330
0.8806094325576788
Model improve: 0.8784 -> 0.8806
Epoch: 37/100
Train loss: 4.2352, val loss: 2.1320
0.8751241323815714
Epoch: 38/100
Train loss: 4.2876, val loss: 2.1112
0.8779131024129799
Epoch: 39/100
Train loss: 4.1985, val loss: 2.1695
0.8742003059912334
Epoch: 40/100
Train loss: 4.2507, val loss: 2.1829
0.8772223256968066
Epoch: 41/100
Train loss: 4.3112, val loss: 2.1698
0.8753388233637975
Epoch: 42/100
Train loss: 4.2313, val loss: 2.2052
0.8722612910805903
Epoch: 43/100
Train loss: 4.1137, val loss: 2.2132
0.8735510197919243
Epoch: 44/100
Train loss: 4.2041, val loss: 2.1150
0.8795477552057106
Epoch: 45/100
Train loss: 4.1530, val loss: 2.0353
0.8809983364745227
Model improve: 0.8806 -> 0.8810
Epoch: 46/100
Train loss: 3.9916, val loss: 2.0855
0.8782887737443887
Epoch: 47/100
Train loss: 4.1515, val loss: 2.1245
0.8804537390070031
Epoch: 48/100
Train loss: 4.1651, val loss: 2.1482
0.8829385335849621
Model improve: 0.8810 -> 0.8829
Epoch: 49/100
Train loss: 4.0571, val loss: 2.0931
0.8857220399935386
Model improve: 0.8829 -> 0.8857
Epoch: 50/100
Train loss: 4.1297, val loss: 2.0120
0.88026967636093
Epoch: 51/100
Train loss: 3.9220, val loss: 2.1635
0.881872548551994
Epoch: 52/100
Train loss: 4.1902, val loss: 2.0489
0.8814115477168643
Epoch: 53/100
Train loss: 3.9734, val loss: 2.0479
0.8777608706125869
Epoch: 54/100
Train loss: 4.0056, val loss: 1.9985
0.8782667233215324
Epoch: 55/100
Train loss: 3.9168, val loss: 2.0794
0.8812922047176408
Epoch: 56/100
Train loss: 3.9860, val loss: 2.0507
0.880952902466162
Epoch: 57/100
Train loss: 3.9384, val loss: 2.0459
0.879344543170317
Epoch: 58/100
Train loss: 3.9414, val loss: 1.9849
0.8823132274804683
Epoch: 59/100
Train loss: 3.9146, val loss: 1.9861
0.8824001080336801
Epoch: 60/100
Train loss: 3.8320, val loss: 2.0835
0.8820466470340261
Epoch: 61/100
Train loss: 3.9387, val loss: 1.9490
0.8842145317126681
Epoch: 62/100
Train loss: 3.9764, val loss: 1.9950
0.8811029847295271
Epoch: 63/100
Train loss: 3.8757, val loss: 1.9687
0.8832640136423258
Epoch: 64/100
Train loss: 3.8489, val loss: 1.9975
0.8806489157458836
Epoch: 65/100
Train loss: 3.6886, val loss: 1.9769
0.8789514620282229
Epoch: 66/100
Train loss: 3.7940, val loss: 1.9529
0.8818513777878926
Epoch: 67/100
Train loss: 3.8057, val loss: 1.9976
0.8832549112762125
Epoch: 68/100
Train loss: 3.7984, val loss: 1.9556
0.8824242504218058
Epoch: 69/100
Train loss: 3.7116, val loss: 1.9812
0.8818225319742011
Epoch: 70/100
Train loss: 3.7171, val loss: 2.0218
0.8853438305518079
Epoch: 71/100
Train loss: 3.7469, val loss: 1.9127
0.8868023639436291
Model improve: 0.8857 -> 0.8868
Epoch: 72/100
Train loss: 3.6306, val loss: 1.9333
0.8857909276513367
Epoch: 73/100
Train loss: 3.6119, val loss: 1.9348
0.8839785655190111
Epoch: 74/100
Train loss: 3.6568, val loss: 1.8912
0.8851288970360742
Epoch: 75/100
Train loss: 3.6754, val loss: 1.9273
0.8836589936588343
Epoch: 76/100
Train loss: 3.8076, val loss: 1.9143
0.8845604047279929
Epoch: 77/100
Train loss: 3.5290, val loss: 1.9188
0.8861708428135606
Epoch: 78/100
Train loss: 3.6071, val loss: 1.8623
0.8886014766276037
Model improve: 0.8868 -> 0.8886
Epoch: 79/100
Train loss: 3.6206, val loss: 1.8621
0.8884463713805074
Epoch: 80/100
Train loss: 3.5655, val loss: 1.8972
0.8859438980078278
Epoch: 81/100
Train loss: 3.7844, val loss: 1.8872
0.8861001032737716
Epoch: 82/100
Train loss: 3.5832, val loss: 1.8832
0.8871101341601348
Epoch: 83/100
Train loss: 3.5277, val loss: 1.9076
0.8868165226608044
Epoch: 84/100
Train loss: 3.6478, val loss: 1.9005
0.8856542651423187
Epoch: 85/100
Train loss: 3.5844, val loss: 1.8994
0.8862271771697984
Epoch: 86/100
Train loss: 3.5740, val loss: 1.8828
0.8861783798476845
Epoch: 87/100
Train loss: 3.5892, val loss: 1.8791
0.8866900328837214
Epoch: 88/100
Train loss: 3.6047, val loss: 1.8988
0.8868241684857046
Epoch: 89/100
Train loss: 3.7235, val loss: 1.8940
0.8847892114926255
Epoch: 90/100
Train loss: 3.5324, val loss: 1.8707
0.8868279041571119
Epoch: 91/100
Train loss: 3.6233, val loss: 1.8762
0.8878764369041094
Epoch: 92/100
Train loss: 3.6213, val loss: 1.8951
0.8862214822676485
Epoch: 93/100
Train loss: 3.5691, val loss: 1.8530
0.8874670844390292
Epoch: 94/100
Train loss: 3.6217, val loss: 1.8632
0.8881522425539693
Epoch: 95/100
Train loss: 3.5163, val loss: 1.8803
0.8872688607872411
Epoch: 96/100
Train loss: 3.6015, val loss: 1.8759
0.8880091071456814
Epoch: 97/100
Train loss: 3.5861, val loss: 1.9194
0.8884398215995208
Epoch: 98/100
Train loss: 3.5195, val loss: 1.8770
0.8866520471608338
Epoch: 99/100
Train loss: 3.5355, val loss: 1.8756
0.8868139087254714
Epoch: 100/100
Train loss: 3.5761, val loss: 1.8731
0.8870273404805236
Date :04/13/2023, 06:34:59
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 14000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 9.4234, val loss: 5.6831
0.6539615167790559
Model improve: 0.0000 -> 0.6540
Epoch: 2/100
Train loss: 6.9368, val loss: 4.0471
0.7499719689499738
Model improve: 0.6540 -> 0.7500
Epoch: 3/100
Train loss: 5.9840, val loss: 3.5082
0.7917395968202472
Model improve: 0.7500 -> 0.7917
Epoch: 4/100
Train loss: 5.2757, val loss: 3.1513
0.8111504744065652
Model improve: 0.7917 -> 0.8112
Epoch: 5/100
Train loss: 5.0413, val loss: 2.9310
0.8264791468741239
Model improve: 0.8112 -> 0.8265
Epoch: 6/100
Train loss: 4.7231, val loss: 2.8509
0.8405538009074179
Model improve: 0.8265 -> 0.8406
Epoch: 7/100
Train loss: 4.3947, val loss: 2.8312
0.8421354478921119
Model improve: 0.8406 -> 0.8421
Epoch: 8/100
Train loss: 4.3014, val loss: 2.4903
0.8577687548903242
Model improve: 0.8421 -> 0.8578
Epoch: 9/100
Train loss: 4.0542, val loss: 2.4534
0.8600584691025392
Model improve: 0.8578 -> 0.8601
Epoch: 10/100
Train loss: 4.1649, val loss: 2.6732
0.8627051644834872
Model improve: 0.8601 -> 0.8627
Epoch: 11/100
Train loss: 3.9193, val loss: 2.4861
0.8607261558406369
Epoch: 12/100
Train loss: 3.8222, val loss: 2.4642
0.8682497148541299
Model improve: 0.8627 -> 0.8682
Epoch: 13/100
Train loss: 3.7034, val loss: 2.3425
0.8690305569443846
Model improve: 0.8682 -> 0.8690
Epoch: 14/100
Train loss: 3.6618, val loss: 2.2903
0.8702340652130865
Model improve: 0.8690 -> 0.8702
Epoch: 15/100
Train loss: 3.5293, val loss: 2.4089
0.8694273856020636
Epoch: 16/100
Train loss: 3.5376, val loss: 2.3508
0.8705527213369568
Model improve: 0.8702 -> 0.8706
Epoch: 17/100
Train loss: 3.2840, val loss: 2.2521
0.8797169456734469
Model improve: 0.8706 -> 0.8797
Epoch: 18/100
Train loss: 3.4052, val loss: 2.2934
0.8751707462630085
Epoch: 19/100
Train loss: 3.3598, val loss: 2.2643
0.8758737550905225
Epoch: 20/100
Train loss: 3.3417, val loss: 2.1105
0.8820305106299631
Model improve: 0.8797 -> 0.8820
Epoch: 21/100
Train loss: 3.2490, val loss: 2.1614
0.8790576293068018
Epoch: 22/100
Train loss: 3.2796, val loss: 2.0871
0.8836037148392718
Model improve: 0.8820 -> 0.8836
Epoch: 23/100
Train loss: 3.1714, val loss: 2.1779
0.8811738230064083
Epoch: 24/100
Train loss: 3.2421, val loss: 2.0888
0.8824795411872138
Epoch: 25/100
Train loss: 3.1317, val loss: 2.1998
0.8804540307898717
Epoch: 26/100
Train loss: 3.1050, val loss: 2.1319
0.8808063515599829
Epoch: 27/100
Train loss: 2.9865, val loss: 2.1428
0.8800724735207068
Epoch: 28/100
Train loss: 2.9788, val loss: 2.0854
0.8829281949936807
Epoch: 29/100
Train loss: 3.1069, val loss: 2.1861
0.8785652893186916
Epoch: 30/100
Train loss: 2.9881, val loss: 2.0835
0.8799118037125978
Epoch: 31/100
Train loss: 2.9007, val loss: 2.1289
0.8794497117075681
Epoch: 32/100
Train loss: 2.9197, val loss: 2.1033
0.8789221835679177
Epoch: 33/100
Train loss: 3.0032, val loss: 2.0633
0.8864615209609705
Model improve: 0.8836 -> 0.8865
Epoch: 34/100
Train loss: 2.8242, val loss: 2.1134
0.8864658682583572
Model improve: 0.8865 -> 0.8865
Epoch: 35/100
Train loss: 2.8389, val loss: 2.0314
0.8878722073624242
Model improve: 0.8865 -> 0.8879
Epoch: 36/100
Train loss: 2.8202, val loss: 2.0292
0.887476680324174
Epoch: 37/100
Train loss: 2.6841, val loss: 2.0262
0.8887481999136323
Model improve: 0.8879 -> 0.8887
Epoch: 38/100
Train loss: 2.7466, val loss: 1.9818
0.8884297341416384
Epoch: 39/100
Train loss: 2.6706, val loss: 2.0047
0.8891833025751301
Model improve: 0.8887 -> 0.8892
Epoch: 40/100
Train loss: 2.7196, val loss: 1.9787
0.8900184536635789
Model improve: 0.8892 -> 0.8900
Epoch: 41/100
Train loss: 2.7711, val loss: 2.0341
0.887403827200842
Epoch: 42/100
Train loss: 2.6737, val loss: 1.9928
0.8885241274926626
Epoch: 43/100
Train loss: 2.5930, val loss: 2.0064
0.8904395981773612
Model improve: 0.8900 -> 0.8904
Epoch: 44/100
Train loss: 2.6841, val loss: 1.8809
0.8957577894132979
Model improve: 0.8904 -> 0.8958
Epoch: 45/100
Train loss: 2.6233, val loss: 1.9154
0.8932264422842414
Epoch: 46/100
Train loss: 2.4525, val loss: 1.9269
0.8943929495152592
Epoch: 47/100
Train loss: 2.6007, val loss: 1.9432
0.8933995430482966
Epoch: 48/100
Train loss: 2.6293, val loss: 1.9353
0.8924209014984376
Epoch: 49/100
Train loss: 2.5232, val loss: 1.9361
0.8938386366456881
Epoch: 50/100
Train loss: 2.6126, val loss: 1.9355
0.8910791869149888
Epoch: 51/100
Train loss: 2.3972, val loss: 1.9679
0.8972059202300997
Model improve: 0.8958 -> 0.8972
Epoch: 52/100
Train loss: 2.6940, val loss: 1.9101
0.8937688973737595
Epoch: 53/100
Train loss: 2.4277, val loss: 1.8919
0.8922857123586047
Epoch: 54/100
Train loss: 2.4585, val loss: 1.9188
0.8924664364607269
Epoch: 55/100
Train loss: 2.4348, val loss: 1.9324
0.893237311976078
Epoch: 56/100
Train loss: 2.4679, val loss: 1.8927
0.89420171713638
Epoch: 57/100
Train loss: 2.4192, val loss: 1.8850
0.8930286820559802
Epoch: 58/100
Train loss: 2.4288, val loss: 1.8351
0.894988791703588
Epoch: 59/100
Train loss: 2.4067, val loss: 1.8749
0.8943117107101246
Epoch: 60/100
Train loss: 2.3520, val loss: 1.8363
0.8981376271641541
Model improve: 0.8972 -> 0.8981
Epoch: 61/100
Train loss: 2.4207, val loss: 1.8950
0.8925564415872189
Epoch: 62/100
Train loss: 2.4582, val loss: 1.8458
0.8954259244857966
Epoch: 63/100
Train loss: 2.3602, val loss: 1.8586
0.8963885837463526
Epoch: 64/100
Train loss: 2.3707, val loss: 1.8776
0.8936100019567202
Epoch: 65/100
Train loss: 2.1747, val loss: 1.8430
0.8955633893012545
Epoch: 66/100
Train loss: 2.3158, val loss: 1.8375
0.8978359459196167
Epoch: 67/100
Train loss: 2.2803, val loss: 1.8435
0.8974025387118791
Epoch: 68/100
Train loss: 2.2831, val loss: 1.8237
0.8970189490037488
Epoch: 69/100
Train loss: 2.2231, val loss: 1.8219
0.8985247535192922
Model improve: 0.8981 -> 0.8985
Epoch: 70/100
Train loss: 2.2076, val loss: 1.8121
0.9001297843849888
Model improve: 0.8985 -> 0.9001
Epoch: 71/100
Train loss: 2.2604, val loss: 1.8077
0.8980470610214513
Epoch: 72/100
Train loss: 2.1298, val loss: 1.8330
0.8954430542005033
Epoch: 73/100
Train loss: 2.1198, val loss: 1.8323
0.8991895338069936
Epoch: 74/100
Train loss: 2.1591, val loss: 1.8069
0.8974522583095756
Epoch: 75/100
Train loss: 2.1720, val loss: 1.8188
0.8972552274537404
Epoch: 76/100
Train loss: 2.3028, val loss: 1.8038
0.8974917028268053
Epoch: 77/100
Train loss: 2.0145, val loss: 1.8534
0.8988845241757903
Epoch: 78/100
Train loss: 2.0886, val loss: 1.8046
0.8996356593616859
Epoch: 79/100
Train loss: 2.1654, val loss: 1.7875
0.899731514340331
Epoch: 80/100
Train loss: 2.0790, val loss: 1.8142
0.89633011459682
Epoch: 81/100
Train loss: 2.2923, val loss: 1.8088
0.8977770694435948
Epoch: 82/100
Train loss: 2.1321, val loss: 1.7956
0.8985280509938794
Epoch: 83/100
Train loss: 1.9941, val loss: 1.7827
0.8993678129833231
Epoch: 84/100
Train loss: 2.1644, val loss: 1.7746
0.8997946417595197
Epoch: 85/100
Train loss: 2.0644, val loss: 1.8031
0.899147151436077
Epoch: 86/100
Train loss: 2.0541, val loss: 1.7857
0.899218539996639
Epoch: 87/100
Train loss: 2.1102, val loss: 1.7740
0.9012120780927148
Model improve: 0.9001 -> 0.9012
Epoch: 88/100
Train loss: 2.1057, val loss: 1.7589
0.9018904677119414
Model improve: 0.9012 -> 0.9019
Epoch: 89/100
Train loss: 2.2226, val loss: 1.7776
0.9002277258753613
Epoch: 90/100
Train loss: 2.0340, val loss: 1.7642
0.9011369558983526
Epoch: 91/100
Train loss: 2.1483, val loss: 1.7694
0.8998934062956013
Epoch: 92/100
Train loss: 2.0990, val loss: 1.7846
0.901146656423227
Epoch: 93/100
Train loss: 2.0591, val loss: 1.7681
0.8996671469712142
Epoch: 94/100
Train loss: 2.0908, val loss: 1.7565
0.9012703144345962
Epoch: 95/100
Train loss: 1.9930, val loss: 1.7759
0.9006491524507751
Epoch: 96/100
Train loss: 2.0678, val loss: 1.7650
0.9010985848685384
Epoch: 97/100
Train loss: 2.1028, val loss: 1.7855
0.9008060500664619
Epoch: 98/100
Train loss: 2.0296, val loss: 1.7606
0.9008425517222061
Epoch: 99/100
Train loss: 2.0813, val loss: 1.7775
0.9003999492216057
Epoch: 100/100
Train loss: 2.0745, val loss: 1.7624
0.9016164641672569
Date :04/13/2023, 15:50:00
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 14000
trainbs: 16
validbs: 64
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 9.3510, val loss: 5.5203
0.6736764285756659
Model improve: 0.0000 -> 0.6737
Epoch: 2/100
Train loss: 7.1096, val loss: 4.1793
0.7595388707566668
Model improve: 0.6737 -> 0.7595
Epoch: 3/100
Train loss: 6.2017, val loss: 3.5564
0.7962893393781403
Model improve: 0.7595 -> 0.7963
Epoch: 4/100
Train loss: 5.5843, val loss: 3.4881
0.8121630314334273
Model improve: 0.7963 -> 0.8122
Epoch: 5/100
Train loss: 5.2538, val loss: 3.2422
0.8221804296233265
Model improve: 0.8122 -> 0.8222
Epoch: 6/100
Date :04/13/2023, 16:32:08
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 14000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 100
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
30278
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 8.8762, val loss: 5.0620
0.7234763120119239
Model improve: 0.0000 -> 0.7235
Epoch: 2/100
Train loss: 6.1709, val loss: 3.7665
0.7894194161830407
Model improve: 0.7235 -> 0.7894
Epoch: 3/100
Train loss: 5.1624, val loss: 3.2505
0.8192534297968965
Model improve: 0.7894 -> 0.8193
Epoch: 4/100
Train loss: 4.6840, val loss: 3.0177
0.8332181028469258
Model improve: 0.8193 -> 0.8332
Epoch: 5/100
Train loss: 4.3836, val loss: 2.8073
0.8461716148749259
Model improve: 0.8332 -> 0.8462
Epoch: 6/100
Train loss: 4.0724, val loss: 2.8274
0.8513258636693237
Model improve: 0.8462 -> 0.8513
Epoch: 7/100
Train loss: 4.0513, val loss: 2.6544
0.8565871332473405
Model improve: 0.8513 -> 0.8566
Epoch: 8/100
Train loss: 3.7759, val loss: 2.5437
0.8540595417340022
Epoch: 9/100
Train loss: 3.6428, val loss: 2.5530
0.8593046852487448
Model improve: 0.8566 -> 0.8593
Epoch: 10/100
Train loss: 3.5365, val loss: 2.5148
0.8629916393524637
Model improve: 0.8593 -> 0.8630
Epoch: 11/100
Train loss: 3.3683, val loss: 2.4270
0.8667217865634245
Model improve: 0.8630 -> 0.8667
Epoch: 12/100
Train loss: 3.4076, val loss: 2.4379
0.8662377198860953
Epoch: 13/100
Train loss: 3.2490, val loss: 2.3482
0.8710644377066987
Model improve: 0.8667 -> 0.8711
Epoch: 14/100
Train loss: 3.3094, val loss: 2.4225
0.8660323164080673
Epoch: 15/100
Train loss: 3.2486, val loss: 2.4361
0.8707486642089479
Epoch: 16/100
Train loss: 3.2324, val loss: 2.3506
0.8738571825171674
Model improve: 0.8711 -> 0.8739
Epoch: 17/100
Train loss: 3.1012, val loss: 2.2732
0.8752366512082094
Model improve: 0.8739 -> 0.8752
Epoch: 18/100
Train loss: 3.0188, val loss: 2.1796
0.8754485380263759
Model improve: 0.8752 -> 0.8754
Epoch: 19/100
Train loss: 3.0601, val loss: 2.1560
0.8748967045608046
Epoch: 20/100
Date :04/13/2023, 19:00:05
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 14000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s_in21ft1k
mix_up: 0.1
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 10.0767, val loss: 6.3492
0.6065545231639181
Model improve: 0.0000 -> 0.6066
Epoch: 2/100
Date :04/13/2023, 19:10:48
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 14000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.1
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: True
use_drop_path: True
Fold: 0
19577
Date :04/13/2023, 19:14:57
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 14000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.1
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
Date :04/13/2023, 19:17:08
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 14000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.1
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 9.3116, val loss: 5.5696
0.6548146080897354
Model improve: 0.0000 -> 0.6548
Epoch: 2/100
Train loss: 6.6598, val loss: 4.0887
0.7540134321586242
Model improve: 0.6548 -> 0.7540
Epoch: 3/100
Train loss: 5.7345, val loss: 3.5123
0.7993637621558326
Model improve: 0.7540 -> 0.7994
Epoch: 4/100
Train loss: 4.9444, val loss: 3.2112
0.8110680681665742
Model improve: 0.7994 -> 0.8111
Epoch: 5/100
Train loss: 4.7236, val loss: 3.0465
0.8263013247659595
Model improve: 0.8111 -> 0.8263
Epoch: 6/100
Train loss: 4.3831, val loss: 2.7552
0.835298790062515
Model improve: 0.8263 -> 0.8353
Epoch: 7/100
Train loss: 4.1016, val loss: 2.7001
0.8461968889971423
Model improve: 0.8353 -> 0.8462
Epoch: 8/100
Train loss: 4.0115, val loss: 2.6027
0.854658023353104
Model improve: 0.8462 -> 0.8547
Epoch: 9/100
Train loss: 3.6893, val loss: 2.4883
0.8517439604978165
Epoch: 10/100
Train loss: 3.7137, val loss: 2.5944
0.8549789064154638
Model improve: 0.8547 -> 0.8550
Epoch: 11/100
Train loss: 3.6477, val loss: 2.4270
0.8601218828858187
Model improve: 0.8550 -> 0.8601
Epoch: 12/100
Train loss: 3.4522, val loss: 2.4210
0.8668846366674163
Model improve: 0.8601 -> 0.8669
Epoch: 13/100
Train loss: 3.2566, val loss: 2.2656
0.8733152281816728
Model improve: 0.8669 -> 0.8733
Epoch: 14/100
Train loss: 3.2473, val loss: 2.3155
0.8642739502949668
Epoch: 15/100
Train loss: 3.2363, val loss: 2.3492
0.8673385948023719
Epoch: 16/100
Train loss: 3.1523, val loss: 2.3936
0.867294753151006
Epoch: 17/100
Train loss: 2.9072, val loss: 2.1795
0.8747677180865622
Model improve: 0.8733 -> 0.8748
Epoch: 18/100
Train loss: 3.0643, val loss: 2.2275
0.8749949630738926
Model improve: 0.8748 -> 0.8750
Epoch: 19/100
Train loss: 3.0122, val loss: 2.2326
0.8743763754933942
Epoch: 20/100
Train loss: 2.9401, val loss: 2.1562
0.8794281853785311
Model improve: 0.8750 -> 0.8794
Epoch: 21/100
Train loss: 2.7661, val loss: 2.1776
0.8764253597615946
Epoch: 22/100
Train loss: 2.8498, val loss: 2.1001
0.8815343073730325
Model improve: 0.8794 -> 0.8815
Epoch: 23/100
Train loss: 2.8860, val loss: 2.1991
0.878251790255766
Epoch: 24/100
Train loss: 2.7004, val loss: 2.1354
0.8818189526047656
Model improve: 0.8815 -> 0.8818
Epoch: 25/100
Train loss: 2.7715, val loss: 2.0825
0.8839314405557481
Model improve: 0.8818 -> 0.8839
Epoch: 26/100
Train loss: 2.7072, val loss: 2.1801
0.8820090005339147
Epoch: 27/100
Train loss: 2.5679, val loss: 2.0668
0.8831579055951404
Epoch: 28/100
Train loss: 2.5810, val loss: 2.0766
0.8830838681961304
Epoch: 29/100
Train loss: 2.5871, val loss: 2.0823
0.8830366503463205
Epoch: 30/100
Train loss: 2.6176, val loss: 2.0935
0.8841016522581651
Model improve: 0.8839 -> 0.8841
Epoch: 31/100
Train loss: 2.6107, val loss: 2.1372
0.8799060472406488
Epoch: 32/100
Train loss: 2.5767, val loss: 2.0449
0.8854775583327227
Model improve: 0.8841 -> 0.8855
Epoch: 33/100
Train loss: 2.4789, val loss: 1.9628
0.8919211292379153
Model improve: 0.8855 -> 0.8919
Epoch: 34/100
Train loss: 2.4402, val loss: 2.0119
0.8907879330487681
Epoch: 35/100
Train loss: 2.6050, val loss: 2.0542
0.889898286269744
Epoch: 36/100
Train loss: 2.3128, val loss: 2.0315
0.8892470631997574
Epoch: 37/100
Train loss: 2.3170, val loss: 2.0582
0.8896113707316892
Epoch: 38/100
Train loss: 2.2801, val loss: 2.0000
0.891102850076554
Epoch: 39/100
Train loss: 2.3175, val loss: 1.9637
0.8907440556632169
Epoch: 40/100
Train loss: 2.3069, val loss: 1.9548
0.889835318457026
Epoch: 41/100
Train loss: 2.2888, val loss: 1.9215
0.8931020650924691
Model improve: 0.8919 -> 0.8931
Epoch: 42/100
Train loss: 2.3543, val loss: 1.9331
0.8939019719117184
Model improve: 0.8931 -> 0.8939
Epoch: 43/100
Train loss: 2.2314, val loss: 1.9459
0.8944591606587199
Model improve: 0.8939 -> 0.8945
Epoch: 44/100
Train loss: 2.1900, val loss: 1.9183
0.8941902451697975
Epoch: 45/100
Train loss: 2.2338, val loss: 1.9539
0.8878431365046109
Epoch: 46/100
Train loss: 2.2199, val loss: 1.9399
0.8934419556105876
Epoch: 47/100
Train loss: 2.0941, val loss: 1.9213
0.89239509779685
Epoch: 48/100
Train loss: 2.2585, val loss: 1.9593
0.8904527090645534
Epoch: 49/100
Train loss: 2.1748, val loss: 1.9523
0.8904054547199935
Epoch: 50/100
Train loss: 2.1259, val loss: 1.9423
0.8901457099832493
Epoch: 51/100
Train loss: 2.2215, val loss: 1.9640
0.8917904172704414
Epoch: 52/100
Train loss: 2.0402, val loss: 1.9091
0.8938965774461541
Epoch: 53/100
Train loss: 2.2593, val loss: 1.9059
0.8933527097503281
Epoch: 54/100
Date :04/14/2023, 00:01:27
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :04/14/2023, 00:04:09
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 9.4581, val loss: 5.7208
0.6448804895765388
Model improve: 0.0000 -> 0.6449
Epoch: 2/100
Train loss: 6.9918, val loss: 4.1118
0.7401746694192565
Model improve: 0.6449 -> 0.7402
Epoch: 3/100
Train loss: 6.0774, val loss: 3.5675
0.7848285814435415
Model improve: 0.7402 -> 0.7848
Epoch: 4/100
Train loss: 5.3120, val loss: 3.2356
0.7982073251231074
Model improve: 0.7848 -> 0.7982
Epoch: 5/100
Train loss: 5.1108, val loss: 3.0411
0.8176463427902602
Model improve: 0.7982 -> 0.8176
Epoch: 6/100
Train loss: 4.7669, val loss: 3.0250
0.8265776840073844
Model improve: 0.8176 -> 0.8266
Epoch: 7/100
Train loss: 4.4425, val loss: 2.9541
0.8341098316446439
Model improve: 0.8266 -> 0.8341
Epoch: 8/100
Train loss: 4.3491, val loss: 2.8705
0.8362053068235102
Model improve: 0.8341 -> 0.8362
Epoch: 9/100
Train loss: 4.1372, val loss: 2.7005
0.8410365706074753
Model improve: 0.8362 -> 0.8410
Epoch: 10/100
Train loss: 4.1822, val loss: 2.5891
0.8478623179342261
Model improve: 0.8410 -> 0.8479
Epoch: 11/100
Train loss: 3.9525, val loss: 2.5842
0.8542541052629703
Model improve: 0.8479 -> 0.8543
Epoch: 12/100
Train loss: 3.8242, val loss: 2.5022
0.8578709470058336
Model improve: 0.8543 -> 0.8579
Epoch: 13/100
Train loss: 3.7260, val loss: 2.3915
0.8583732191272201
Model improve: 0.8579 -> 0.8584
Epoch: 14/100
Train loss: 3.7072, val loss: 2.4945
0.8552217069699195
Epoch: 15/100
Train loss: 3.6045, val loss: 2.5323
0.8583851073109874
Model improve: 0.8584 -> 0.8584
Epoch: 16/100
Train loss: 3.5233, val loss: 2.3760
0.8598736707401414
Model improve: 0.8584 -> 0.8599
Epoch: 17/100
Train loss: 3.3551, val loss: 2.5028
0.8635740481705658
Model improve: 0.8599 -> 0.8636
Epoch: 18/100
Train loss: 3.3826, val loss: 2.5299
0.8591569927566213
Epoch: 19/100
Train loss: 3.3480, val loss: 2.2939
0.8623509226703893
Epoch: 20/100
Train loss: 3.2715, val loss: 2.3292
0.8611744824416901
Epoch: 21/100
Train loss: 3.3287, val loss: 2.2883
0.8665617213521277
Model improve: 0.8636 -> 0.8666
Epoch: 22/100
Train loss: 3.2972, val loss: 2.2763
0.8685906587257038
Model improve: 0.8666 -> 0.8686
Epoch: 23/100
Train loss: 3.1662, val loss: 2.2627
0.8678463963757693
Epoch: 24/100
Train loss: 3.2607, val loss: 2.3000
0.867646687060274
Epoch: 25/100
Train loss: 3.1229, val loss: 2.2832
0.8671414181029905
Epoch: 26/100
Train loss: 3.1379, val loss: 2.2611
0.8671725497164754
Epoch: 27/100
Train loss: 2.9157, val loss: 2.2155
0.8699102835070165
Model improve: 0.8686 -> 0.8699
Epoch: 28/100
Train loss: 3.0315, val loss: 2.3700
0.868278923157847
Epoch: 29/100
Train loss: 3.0172, val loss: 2.2350
0.8710723503190949
Model improve: 0.8699 -> 0.8711
Epoch: 30/100
Train loss: 2.9987, val loss: 2.1661
0.8696777064224229
Epoch: 31/100
Train loss: 2.9157, val loss: 2.1807
0.8695064583278579
Epoch: 32/100
Train loss: 2.9186, val loss: 2.1889
0.8705040218356481
Epoch: 33/100
Train loss: 3.0223, val loss: 2.3089
0.8710487299884911
Epoch: 34/100
Train loss: 2.8501, val loss: 2.0804
0.8739450340246508
Model improve: 0.8711 -> 0.8739
Epoch: 35/100
Train loss: 2.8699, val loss: 2.1382
0.8715485799198378
Epoch: 36/100
Train loss: 2.7668, val loss: 2.1637
0.8727960506676307
Epoch: 37/100
Train loss: 2.7495, val loss: 2.1758
0.873947351462831
Model improve: 0.8739 -> 0.8739
Epoch: 38/100
Train loss: 2.7055, val loss: 2.1491
0.8712988325410554
Epoch: 39/100
Train loss: 2.6972, val loss: 2.0879
0.8755941172797345
Model improve: 0.8739 -> 0.8756
Epoch: 40/100
Train loss: 2.6858, val loss: 2.0847
0.8765917505227183
Model improve: 0.8756 -> 0.8766
Epoch: 41/100
Train loss: 2.8296, val loss: 2.0831
0.8768728080308026
Model improve: 0.8766 -> 0.8769
Epoch: 42/100
Train loss: 2.6664, val loss: 2.1139
0.8746305986476945
Epoch: 43/100
Train loss: 2.6224, val loss: 2.0497
0.8765256014356692
Epoch: 44/100
Train loss: 2.6679, val loss: 2.0604
0.8755052917931435
Epoch: 45/100
Train loss: 2.5421, val loss: 2.0813
0.8769987836767531
Model improve: 0.8769 -> 0.8770
Epoch: 46/100
Train loss: 2.5310, val loss: 2.0671
0.8751115683156185
Epoch: 47/100
Train loss: 2.5800, val loss: 2.0555
0.8768510179747806
Epoch: 48/100
Train loss: 2.6210, val loss: 2.0566
0.8774851153227173
Model improve: 0.8770 -> 0.8775
Epoch: 49/100
Train loss: 2.5796, val loss: 2.0930
0.8793715667570852
Model improve: 0.8775 -> 0.8794
Epoch: 50/100
Train loss: 2.5858, val loss: 2.0780
0.8762950980238662
Epoch: 51/100
Train loss: 2.4887, val loss: 2.1180
0.8788921944197512
Epoch: 52/100
Train loss: 2.5581, val loss: 2.0357
0.8818964891043761
Model improve: 0.8794 -> 0.8819
Epoch: 53/100
Train loss: 2.4642, val loss: 2.0057
0.881032396494754
Epoch: 54/100
Train loss: 2.4497, val loss: 1.9872
0.8818450570391628
Epoch: 55/100
Train loss: 2.4410, val loss: 1.9546
0.883229796459099
Model improve: 0.8819 -> 0.8832
Epoch: 56/100
Train loss: 2.5226, val loss: 2.0148
0.8795410455580698
Epoch: 57/100
Train loss: 2.3662, val loss: 1.9795
0.8812138539150169
Epoch: 58/100
Train loss: 2.5248, val loss: 1.9397
0.8848291685968538
Model improve: 0.8832 -> 0.8848
Epoch: 59/100
Train loss: 2.3832, val loss: 1.9628
0.8810108494582483
Epoch: 60/100
Train loss: 2.3801, val loss: 2.0085
0.8799919362528531
Epoch: 61/100
Train loss: 2.4505, val loss: 1.9818
0.8853374614518597
Model improve: 0.8848 -> 0.8853
Epoch: 62/100
Train loss: 2.3931, val loss: 1.9761
0.8842653644628934
Epoch: 63/100
Train loss: 2.3204, val loss: 1.9602
0.8830143650588379
Epoch: 64/100
Train loss: 2.3452, val loss: 1.9781
0.8827781854146676
Epoch: 65/100
Train loss: 2.2712, val loss: 1.9807
0.8814810122693533
Epoch: 66/100
Train loss: 2.2983, val loss: 1.9569
0.8829152236999552
Epoch: 67/100
Train loss: 2.2689, val loss: 1.9556
0.884443495928254
Epoch: 68/100
Train loss: 2.2743, val loss: 1.9436
0.8849614286468866
Epoch: 69/100
Train loss: 2.2270, val loss: 1.9509
0.8856018885742214
Model improve: 0.8853 -> 0.8856
Epoch: 70/100
Train loss: 2.1664, val loss: 1.9600
0.885926260448179
Model improve: 0.8856 -> 0.8859
Epoch: 71/100
Train loss: 2.3082, val loss: 1.9660
0.8824400636890498
Epoch: 72/100
Train loss: 2.0923, val loss: 1.9373
0.8846262284428023
Epoch: 73/100
Train loss: 2.0626, val loss: 1.9247
0.8860161245895172
Model improve: 0.8859 -> 0.8860
Epoch: 74/100
Train loss: 2.1458, val loss: 1.9518
0.8838454699028808
Epoch: 75/100
Train loss: 2.2757, val loss: 1.9155
0.8855193342618795
Epoch: 76/100
Train loss: 2.2244, val loss: 1.9110
0.8872319194197591
Model improve: 0.8860 -> 0.8872
Epoch: 77/100
Train loss: 2.0459, val loss: 1.9023
0.8873013999976006
Model improve: 0.8872 -> 0.8873
Epoch: 78/100
Date :04/14/2023, 06:59:46
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 2
19479
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 9.3454, val loss: 5.8313
0.6548539553706593
Model improve: 0.0000 -> 0.6549
Epoch: 2/100
Train loss: 6.8918, val loss: 4.4289
0.7414255841508313
Model improve: 0.0000 -> 0.7414
Epoch: 3/100
Train loss: 5.9568, val loss: 3.6924
0.7802176960343841
Model improve: 0.0000 -> 0.7802
Epoch: 4/100
Train loss: 5.3073, val loss: 3.3188
0.8062036009411038
Model improve: 0.0000 -> 0.8062
Epoch: 5/100
Train loss: 5.0617, val loss: 3.1912
0.8165421638344451
Model improve: 0.0000 -> 0.8165
Epoch: 6/100
Train loss: 4.7185, val loss: 2.9596
0.8304585597452359
Model improve: 0.0000 -> 0.8305
Epoch: 7/100
Train loss: 4.3936, val loss: 2.9213
0.8351201810564619
Model improve: 0.0000 -> 0.8351
Epoch: 8/100
Date :04/14/2023, 07:36:58
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100Train loss: 6.9093, val loss: 4.1875
0.7391160456025984
Model improve: 0.0000 -> 0.7391
Epoch: 3/100
Train loss: 6.0239, val loss: 3.5858
0.7861244368481008
Model improve: 0.0000 -> 0.7861
Epoch: 4/100
Train loss: 5.2652, val loss: 3.1351
0.8039889625906735
Model improve: 0.0000 -> 0.8040
Epoch: 5/100
Train loss: 5.0843, val loss: 2.9648
0.8247486072783062
Model improve: 0.0000 -> 0.8247
Epoch: 6/100
Train loss: 4.7197, val loss: 2.8640
0.8326982967692238
Model improve: 0.0000 -> 0.8327
Epoch: 7/100
Train loss: 4.4075, val loss: 2.7760
0.8404997556101007
Model improve: 0.0000 -> 0.8405
Epoch: 8/100
Train loss: 4.3323, val loss: 2.8722
0.8378252007303119
Model improve: 0.0000 -> 0.8378
Epoch: 9/100
Date :04/14/2023, 08:25:10
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 9.3894, val loss: 5.4210
0.6563974827771444
Model improve: 0.0000 -> 0.6564
Epoch: 2/100
Train loss: 6.8934, val loss: 4.1864
0.743482838815118
Model improve: 0.6564 -> 0.7435
Epoch: 3/100
Train loss: 6.0040, val loss: 3.4570
0.7855674831846734
Model improve: 0.7435 -> 0.7856
Epoch: 4/100
Train loss: 5.2434, val loss: 3.2245
0.8062063037983551
Model improve: 0.7856 -> 0.8062
Epoch: 5/100
Train loss: 5.0562, val loss: 2.9957
0.8240107438118169
Model improve: 0.8062 -> 0.8240
Epoch: 6/100Train loss: 4.6922, val loss: 2.9577
0.8291313877094105
Model improve: 0.8240 -> 0.8291
Epoch: 7/100
Train loss: 4.3946, val loss: 2.8747
0.8341277274767888
Model improve: 0.8291 -> 0.8341
Epoch: 8/100
Train loss: 4.3145, val loss: 2.8458
0.8393731767574215
Model improve: 0.8341 -> 0.8394
Epoch: 9/100
Train loss: 4.0822, val loss: 2.6634
0.8422384400625177
Model improve: 0.8394 -> 0.8422
Epoch: 10/100
Train loss: 4.1441, val loss: 2.5776
0.8512456931109624
Model improve: 0.8422 -> 0.8512
Epoch: 11/100
Train loss: 3.9127, val loss: 2.5458
0.8531887891158859
Model improve: 0.8512 -> 0.8532
Epoch: 12/100
Train loss: 3.7788, val loss: 2.5918
0.8533399801718294
Model improve: 0.8532 -> 0.8533
Epoch: 13/100
Train loss: 3.7056, val loss: 2.4495
0.8558563567509064
Model improve: 0.8533 -> 0.8559
Epoch: 14/100
Train loss: 3.6747, val loss: 2.4384
0.8555917865202906
Epoch: 15/100
Train loss: 3.5675, val loss: 2.4082
0.8627428980015207
Model improve: 0.8559 -> 0.8627
Epoch: 16/100
Train loss: 3.4914, val loss: 2.3246
0.8628912718021202
Model improve: 0.8627 -> 0.8629
Epoch: 17/100
Train loss: 3.3300, val loss: 2.5742
0.8609888288224242
Epoch: 18/100
Train loss: 3.3599, val loss: 2.4388
0.8585381369674476
Epoch: 19/100
Train loss: 3.3209, val loss: 2.3911
0.857049822557505
Epoch: 20/100
Train loss: 3.2583, val loss: 2.3104
0.8598442243168509
Epoch: 21/100
Train loss: 3.3035, val loss: 2.2524
0.868412919067778
Model improve: 0.8629 -> 0.8684
Epoch: 22/100
Train loss: 3.2788, val loss: 2.2618
0.8688849089679999
Model improve: 0.8684 -> 0.8689
Epoch: 23/100
Train loss: 3.1520, val loss: 2.2726
0.868118293638702
Epoch: 24/100
Train loss: 3.2359, val loss: 2.3056
0.8680754138761042
Epoch: 25/100
Train loss: 3.1176, val loss: 2.3012
0.8688187163928222
Epoch: 26/100
Train loss: 3.1054, val loss: 2.2152
0.8737908508373003
Model improve: 0.8689 -> 0.8738
Epoch: 27/100
Train loss: 2.9031, val loss: 2.2203
0.8716044693763363
Epoch: 28/100
Train loss: 3.0145, val loss: 2.2997
0.8704439331270438
Epoch: 29/100
Train loss: 3.0012, val loss: 2.1930
0.8711229167324915
Epoch: 30/100
Train loss: 2.9930, val loss: 2.1938
0.8700322214647646
Epoch: 31/100
Train loss: 2.8923, val loss: 2.1858
0.8694177242956251
Epoch: 32/100
Train loss: 2.8965, val loss: 2.1855
0.8717302871876387
Epoch: 33/100
Train loss: 3.0221, val loss: 2.3166
0.8722435001538608
Epoch: 34/100
Train loss: 2.8239, val loss: 2.0770
0.8762512152928241
Model improve: 0.8738 -> 0.8763
Epoch: 35/100
Train loss: 2.8455, val loss: 2.1112
0.8759559230308425
Epoch: 36/100
Train loss: 2.7618, val loss: 2.2223
0.870119205887432
Epoch: 37/100
Train loss: 2.7467, val loss: 2.1958
0.8701013552618823
Epoch: 38/100
Train loss: 2.7062, val loss: 2.1556
0.8719075234136012
Epoch: 39/100
Train loss: 2.6686, val loss: 2.0901
0.8770885265307097
Model improve: 0.8763 -> 0.8771
Epoch: 40/100
Train loss: 2.6715, val loss: 2.1394
0.8735545898462569
Epoch: 41/100
Train loss: 2.8135, val loss: 2.1134
0.8722991658038959
Epoch: 42/100
Train loss: 2.6449, val loss: 2.1362
0.8723041849031102
Epoch: 43/100
Train loss: 2.6087, val loss: 2.0770
0.8769061505365552
Epoch: 44/100
Train loss: 2.6609, val loss: 2.0864
0.8769829643401863
Epoch: 45/100
Train loss: 2.5407, val loss: 2.0633
0.8771653367391298
Model improve: 0.8771 -> 0.8772
Epoch: 46/100
Train loss: 2.5268, val loss: 2.0686
0.8774891012183585
Model improve: 0.8772 -> 0.8775
Epoch: 47/100
Train loss: 2.5662, val loss: 2.0543
0.8783107111272421
Model improve: 0.8775 -> 0.8783
Epoch: 48/100
Train loss: 2.6004, val loss: 2.0605
0.877328930867657
Epoch: 49/100
Train loss: 2.5539, val loss: 2.0663
0.8803929080005664
Model improve: 0.8783 -> 0.8804
Epoch: 50/100
Train loss: 2.5691, val loss: 2.0365
0.8790718266511853
Epoch: 51/100
Train loss: 2.4845, val loss: 2.1460
0.8797577094473493
Epoch: 52/100
Train loss: 2.5434, val loss: 2.0529
0.8789831363185951
Epoch: 53/100
Train loss: 2.4560, val loss: 2.0136
0.8807868942330715
Model improve: 0.8804 -> 0.8808
Epoch: 54/100
Date :04/14/2023, 14:25:44
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :04/14/2023, 14:27:24
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :04/14/2023, 14:27:53
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :04/14/2023, 14:28:29
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 7.2521, val loss: 3.6666
0.6985574824365823
Model improve: 0.0000 -> 0.6986
Epoch: 2/100
Train loss: 4.4293, val loss: 2.8914
0.7752178204283692
Model improve: 0.6986 -> 0.7752
Epoch: 3/100
Train loss: 3.9673, val loss: 2.6443
0.8066081887000377
Model improve: 0.7752 -> 0.8066
Epoch: 4/100
Train loss: 3.5081, val loss: 2.3167
0.8313611700873766
Model improve: 0.8066 -> 0.8314
Epoch: 5/100
Date :04/14/2023, 14:50:55
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 7.2601, val loss: 3.5834
0.7084935860716972
Model improve: 0.0000 -> 0.7085
Epoch: 2/100
Train loss: 4.4261, val loss: 2.9781
0.7703455340214722
Model improve: 0.7085 -> 0.7703
Epoch: 3/100
Train loss: 3.9385, val loss: 2.5440
0.8189289302561803
Model improve: 0.7703 -> 0.8189
Epoch: 4/100
Date :04/14/2023, 15:09:33
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 7.2605, val loss: 3.6565
0.7022402923045724
Model improve: 0.0000 -> 0.7022
Epoch: 2/100
Train loss: 4.4200, val loss: 2.8661
0.781109353592169
Model improve: 0.7022 -> 0.7811
Epoch: 3/100
Train loss: 3.9330, val loss: 2.5831
0.8083993774436599
Model improve: 0.7811 -> 0.8084
Epoch: 4/100
Train loss: 3.4405, val loss: 2.2298
0.8352857444749061
Model improve: 0.8084 -> 0.8353
Epoch: 5/100
Train loss: 3.3045, val loss: 2.0898
0.8505491734894812
Model improve: 0.8353 -> 0.8505
Epoch: 6/100
Train loss: 3.0685, val loss: 2.0459
0.8617916418719112
Model improve: 0.8505 -> 0.8618
Epoch: 7/100
Train loss: 2.8239, val loss: 1.9292
0.8673216788557863
Model improve: 0.8618 -> 0.8673
Epoch: 8/100
Train loss: 2.7214, val loss: 1.8831
0.8707849262570886
Model improve: 0.8673 -> 0.8708
Epoch: 9/100
Train loss: 2.5188, val loss: 1.7820
0.8692520552241638
Epoch: 10/100
Train loss: 2.5431, val loss: 1.6707
0.8846203627014199
Model improve: 0.8708 -> 0.8846
Epoch: 11/100
Train loss: 2.3551, val loss: 1.6440
0.8866842534496348
Model improve: 0.8846 -> 0.8867
Epoch: 12/100
Train loss: 2.1991, val loss: 1.5790
0.8908207919304193
Model improve: 0.8867 -> 0.8908
Epoch: 13/100
Train loss: 2.1192, val loss: 1.4736
0.8940229053268705
Model improve: 0.8908 -> 0.8940
Epoch: 14/100
Train loss: 2.0350, val loss: 1.4707
0.8946716341104985
Model improve: 0.8940 -> 0.8947
Epoch: 15/100
Train loss: 1.9318, val loss: 1.4891
0.8957548563474245
Model improve: 0.8947 -> 0.8958
Epoch: 16/100
Train loss: 1.9215, val loss: 1.3785
0.8988661454003486
Model improve: 0.8958 -> 0.8989
Epoch: 17/100
Train loss: 1.7845, val loss: 1.4710
0.8993002073573745
Model improve: 0.8989 -> 0.8993
Epoch: 18/100
Train loss: 1.8041, val loss: 1.4468
0.898455700987272
Epoch: 19/100
Train loss: 1.8020, val loss: 1.3525
0.9007383936955239
Model improve: 0.8993 -> 0.9007
Epoch: 20/100
Train loss: 1.7808, val loss: 1.3674
0.8995422103234656
Epoch: 21/100
Train loss: 2.7108, val loss: 2.0094
0.8617464914166862
Epoch: 22/100
Train loss: 2.5715, val loss: 1.9141
0.8678935806498109
Epoch: 23/100
Train loss: 2.4510, val loss: 1.7809
0.8780189748576464
Epoch: 24/100
Train loss: 2.4169, val loss: 1.7333
0.8779346019877335
Epoch: 25/100
Train loss: 2.3094, val loss: 1.7468
0.877869819503703
Epoch: 26/100
Train loss: 2.2608, val loss: 1.6583
0.88245265917275
Epoch: 27/100
Train loss: 2.0799, val loss: 1.5866
0.8885333438589524
Epoch: 28/100
Train loss: 2.0952, val loss: 1.7143
0.8832128525009664
Epoch: 29/100
Train loss: 2.0424, val loss: 1.5463
0.8902438877633364
Epoch: 30/100
Date :04/14/2023, 18:07:33
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 2
19479
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :04/14/2023, 18:08:11
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 2
19479
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/20
Train loss: 7.2602, val loss: 3.8133
0.6852137251700715
Model improve: 0.0000 -> 0.6852
Epoch: 2/20
Train loss: 4.3865, val loss: 3.0191
0.7799281305582103
Model improve: 0.6852 -> 0.7799
Epoch: 3/20
Train loss: 3.8620, val loss: 2.5338
0.8154373048953184
Model improve: 0.7799 -> 0.8154
Epoch: 4/20
Train loss: 3.4556, val loss: 2.2432
0.8380131598805515
Model improve: 0.8154 -> 0.8380
Epoch: 5/20
Train loss: 3.2669, val loss: 2.2731
0.8461960009599088
Model improve: 0.8380 -> 0.8462
Epoch: 6/20
Train loss: 3.0277, val loss: 2.1027
0.8539686428968887
Model improve: 0.8462 -> 0.8540
Epoch: 7/20
Train loss: 2.7924, val loss: 1.8762
0.8701061918288421
Model improve: 0.8540 -> 0.8701
Epoch: 8/20
Train loss: 2.7230, val loss: 1.8736
0.8684649915722337
Epoch: 9/20
Train loss: 2.5439, val loss: 1.7713
0.8788844353111634
Model improve: 0.8701 -> 0.8789
Epoch: 10/20
Train loss: 2.5067, val loss: 1.7056
0.8790372425776297
Model improve: 0.8789 -> 0.8790
Epoch: 11/20
Train loss: 2.3827, val loss: 1.6651
0.8858711207343488
Model improve: 0.8790 -> 0.8859
Epoch: 12/20
Train loss: 2.1908, val loss: 1.6668
0.8881215133796992
Model improve: 0.8859 -> 0.8881
Epoch: 13/20
Train loss: 2.1346, val loss: 1.5703
0.893154809251609
Model improve: 0.8881 -> 0.8932
Epoch: 14/20
Train loss: 2.0437, val loss: 1.5007
0.8962483493143107
Model improve: 0.8932 -> 0.8962
Epoch: 15/20
Train loss: 1.9417, val loss: 1.4689
0.8969437293465956
Model improve: 0.8962 -> 0.8969
Epoch: 16/20
Train loss: 1.9012, val loss: 1.4614
0.8997076844091147
Model improve: 0.8969 -> 0.8997
Epoch: 17/20
Train loss: 1.7393, val loss: 1.4435
0.9000655218668948
Model improve: 0.8997 -> 0.9001
Epoch: 18/20
Train loss: 1.8244, val loss: 1.3789
0.9017078508054359
Model improve: 0.9001 -> 0.9017
Epoch: 19/20
Date :04/14/2023, 19:47:58
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 4
19574
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/20
Date :04/14/2023, 19:49:29
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 4
19574
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/20
Date :04/14/2023, 19:49:51
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 4
19574
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/20
Train loss: 7.1862, val loss: 3.8120
0.6818343767970366
Model improve: 0.0000 -> 0.6818
Epoch: 2/20
Train loss: 4.4755, val loss: 2.9923
0.7836914944077773
Model improve: 0.6818 -> 0.7837
Epoch: 3/20
Train loss: 3.9761, val loss: 2.6470
0.8155569125609161
Model improve: 0.7837 -> 0.8156
Epoch: 4/20
Train loss: 3.4911, val loss: 2.2978
0.8364344160138852
Model improve: 0.8156 -> 0.8364
Epoch: 5/20
Train loss: 3.3338, val loss: 2.0660
0.8570884335312973
Model improve: 0.8364 -> 0.8571
Epoch: 6/20
Train loss: 3.1010, val loss: 2.0947
0.8624282827383362
Model improve: 0.8571 -> 0.8624
Epoch: 7/20
Train loss: 2.8328, val loss: 2.0046
0.8657712004679107
Model improve: 0.8624 -> 0.8658
Epoch: 8/20
Train loss: 2.7386, val loss: 1.7283
0.8801875114808076
Model improve: 0.8658 -> 0.8802
Epoch: 9/20
Train loss: 2.5734, val loss: 1.7864
0.8815028168673804
Model improve: 0.8802 -> 0.8815
Epoch: 10/20
Train loss: 2.5404, val loss: 1.7644
0.8882565709292559
Model improve: 0.8815 -> 0.8883
Epoch: 11/20
Train loss: 2.3598, val loss: 1.5642
0.8944740816966056
Model improve: 0.8883 -> 0.8945
Epoch: 12/20
Train loss: 2.2457, val loss: 1.6014
0.897239558050254
Model improve: 0.8945 -> 0.8972
Epoch: 13/20
Train loss: 2.1266, val loss: 1.5520
0.8975385148680427
Model improve: 0.8972 -> 0.8975
Epoch: 14/20
Train loss: 2.0769, val loss: 1.4692
0.9022719711938946
Model improve: 0.8975 -> 0.9023
Epoch: 15/20
Train loss: 1.9439, val loss: 1.4652
0.9035709705654938
Model improve: 0.9023 -> 0.9036
Epoch: 16/20
Train loss: 1.9390, val loss: 1.4951
0.9044969466662744
Model improve: 0.9036 -> 0.9045
Epoch: 17/20
Train loss: 1.7492, val loss: 1.4430
0.9044553815653559
Epoch: 18/20
Train loss: 1.8484, val loss: 1.4181
0.9055713658875285
Model improve: 0.9045 -> 0.9056
Epoch: 19/20
Train loss: 1.8325, val loss: 1.4093
0.9057164999235066
Model improve: 0.9056 -> 0.9057
Epoch: 20/20
Train loss: 1.8419, val loss: 1.4119
0.9058640696823794
Model improve: 0.9057 -> 0.9059
Date :04/14/2023, 22:38:53
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/20
Date :04/14/2023, 22:39:13
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/20
Train loss: 8.3409, val loss: 4.7925
0.6494233282331978
Model improve: 0.0000 -> 0.6494
Epoch: 2/20
Train loss: 5.6466, val loss: 3.9753
0.7191687420629503
Model improve: 0.6494 -> 0.7192
Epoch: 3/20
Date :04/14/2023, 22:51:09
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 14000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 320
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/20
Train loss: 6.9517, val loss: 3.7227
0.6949223206450479
Model improve: 0.0000 -> 0.6949
Epoch: 2/20
Train loss: 4.5095, val loss: 2.9695
0.770204974621101
Model improve: 0.6949 -> 0.7702
Epoch: 3/20
Train loss: 3.9231, val loss: 2.6182
0.8073909727035946
Model improve: 0.7702 -> 0.8074
Epoch: 4/20
Date :04/14/2023, 23:06:37
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 0
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/20
Train loss: 6.9030, val loss: 3.7221
0.7059868964996628
Model improve: 0.0000 -> 0.7060
Epoch: 2/20
Train loss: 4.4771, val loss: 2.8569
0.784421636824022
Model improve: 0.7060 -> 0.7844
Epoch: 3/20
Train loss: 3.9445, val loss: 2.6258
0.812154913409445
Model improve: 0.7844 -> 0.8122
Epoch: 4/20
Train loss: 3.4941, val loss: 2.3130
0.8401026625140833
Model improve: 0.8122 -> 0.8401
Epoch: 5/20
Train loss: 3.3166, val loss: 1.9726
0.8601060529823065
Model improve: 0.8401 -> 0.8601
Epoch: 6/20
Train loss: 3.0895, val loss: 2.0414
0.8678513029294767
Model improve: 0.8601 -> 0.8679
Epoch: 7/20
Train loss: 2.8328, val loss: 1.9055
0.8758503381959544
Model improve: 0.8679 -> 0.8759
Epoch: 8/20
Train loss: 2.7382, val loss: 1.7944
0.881770139426385
Model improve: 0.8759 -> 0.8818
Epoch: 9/20
Train loss: 2.5630, val loss: 1.6348
0.8882489259311868
Model improve: 0.8818 -> 0.8882
Epoch: 10/20
Train loss: 2.5298, val loss: 1.6390
0.8981773237093251
Model improve: 0.8882 -> 0.8982
Epoch: 11/20
Train loss: 2.3581, val loss: 1.5079
0.8998908999617502
Model improve: 0.8982 -> 0.8999
Epoch: 12/20
Date :04/15/2023, 00:11:25
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 100
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
30278
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/20
Train loss: 6.3634, val loss: 3.4476
0.7527902431138894
Model improve: 0.0000 -> 0.7528
Epoch: 2/20
Train loss: 3.9957, val loss: 2.7212
0.8206774856247492
Model improve: 0.7528 -> 0.8207
Epoch: 3/20
Train loss: 3.3861, val loss: 2.3344
0.8454937220811253
Model improve: 0.8207 -> 0.8455
Epoch: 4/20
Train loss: 3.0817, val loss: 2.2862
0.8521986292221969
Model improve: 0.8455 -> 0.8522
Epoch: 5/20
Train loss: 2.8482, val loss: 2.0722
0.8709622433581998
Model improve: 0.8522 -> 0.8710
Epoch: 6/20
Train loss: 2.6203, val loss: 1.8524
0.8830280511704625
Model improve: 0.8710 -> 0.8830
Epoch: 7/20
Train loss: 2.5681, val loss: 1.7471
0.8893083874801218
Model improve: 0.8830 -> 0.8893
Epoch: 8/20
Train loss: 2.3363, val loss: 1.7986
0.8857850252938535
Epoch: 9/20
Train loss: 2.2431, val loss: 1.6582
0.8949745127592549
Model improve: 0.8893 -> 0.8950
Epoch: 10/20
Train loss: 2.0975, val loss: 1.5514
0.8973848080373471
Model improve: 0.8950 -> 0.8974
Epoch: 11/20
Train loss: 1.9658, val loss: 1.5225
0.8991817038339895
Model improve: 0.8974 -> 0.8992
Epoch: 12/20
Train loss: 1.9713, val loss: 1.4386
0.9024667227149559
Model improve: 0.8992 -> 0.9025
Epoch: 13/20
Train loss: 1.8259, val loss: 1.3997
0.91006658349601
Model improve: 0.9025 -> 0.9101
Epoch: 14/20
Train loss: 1.8406, val loss: 1.3944
0.9067761651237481
Epoch: 15/20
Train loss: 1.7604, val loss: 1.4505
0.9108359734846108
Model improve: 0.9101 -> 0.9108
Epoch: 16/20
Train loss: 1.7530, val loss: 1.3903
0.9102070645630272
Epoch: 17/20
Train loss: 1.6624, val loss: 1.3270
0.9114003325395309
Model improve: 0.9108 -> 0.9114
Epoch: 18/20
Train loss: 1.5937, val loss: 1.2711
0.9127414854538928
Model improve: 0.9114 -> 0.9127
Epoch: 19/20
Train loss: 1.6466, val loss: 1.2643
0.9138805242596222
Model improve: 0.9127 -> 0.9139
Epoch: 20/20
Train loss: 1.6449, val loss: 1.2767
0.9141964283668075
Model improve: 0.9139 -> 0.9142
Date :04/15/2023, 07:07:17
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 200
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
54783
Date :04/15/2023, 07:07:30
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 200
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
54783
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/20
Train loss: 5.2194, val loss: 2.9781
0.8091011061680348
Model improve: 0.0000 -> 0.8091
Epoch: 2/20
Train loss: 3.3107, val loss: 2.4719
0.8440414227813936
Model improve: 0.8091 -> 0.8440
Epoch: 3/20
Train loss: 2.8289, val loss: 2.1236
0.8574083255098032
Model improve: 0.8440 -> 0.8574
Epoch: 4/20
Train loss: 2.5603, val loss: 1.9109
0.8708818248999569
Model improve: 0.8574 -> 0.8709
Epoch: 5/20
Train loss: 2.3655, val loss: 1.8807
0.8735163808265894
Model improve: 0.8709 -> 0.8735
Epoch: 6/20
Train loss: 2.1672, val loss: 1.8296
0.8761925419128128
Model improve: 0.8735 -> 0.8762
Epoch: 7/20
Train loss: 2.0580, val loss: 1.6729
0.886880838595191
Model improve: 0.8762 -> 0.8869
Epoch: 8/20
Train loss: 2.0061, val loss: 1.7048
0.8900138615433998
Model improve: 0.8869 -> 0.8900
Epoch: 9/20
Train loss: 1.8893, val loss: 1.6775
0.8911896517706513
Model improve: 0.8900 -> 0.8912
Epoch: 10/20
Train loss: 1.7830, val loss: 1.6146
0.8926124586295157
Model improve: 0.8912 -> 0.8926
Epoch: 11/20
Train loss: 1.7532, val loss: 1.5551
0.8962934802495834
Model improve: 0.8926 -> 0.8963
Epoch: 12/20
Train loss: 1.7140, val loss: 1.5245
0.899484555435032
Model improve: 0.8963 -> 0.8995
Epoch: 13/20
Date :04/15/2023, 10:01:29
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 200
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
54783
Date :04/15/2023, 10:02:00
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 200
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
54783
Date :04/15/2023, 10:03:16
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 200
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
54783
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/20
Date :04/15/2023, 10:04:32
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/20
Train loss: 7.0895, val loss: 2.2718
0.8403218380331631
Model improve: 0.0000 -> 0.8403
Epoch: 2/20
Train loss: 3.2356, val loss: 1.9106
0.8703433619010098
Model improve: 0.8403 -> 0.8703
Epoch: 3/20
Train loss: 2.8899, val loss: 1.6651
0.8883295884757435
Model improve: 0.8703 -> 0.8883
Epoch: 4/20
Train loss: 2.5696, val loss: 1.6476
0.8946941962524795
Model improve: 0.8883 -> 0.8947
Epoch: 5/20
Train loss: 2.5118, val loss: 1.5287
0.8959323591800251
Model improve: 0.8947 -> 0.8959
Epoch: 6/20
Train loss: 2.3709, val loss: 1.4444
0.902035683737937
Model improve: 0.8959 -> 0.9020
Epoch: 7/20
Train loss: 2.1948, val loss: 1.4611
0.9047508358108143
Model improve: 0.9020 -> 0.9048
Epoch: 8/20
Train loss: 2.1754, val loss: 1.4534
0.9064967156572452
Model improve: 0.9048 -> 0.9065
Epoch: 9/20
Train loss: 2.0447, val loss: 1.3995
0.9053321663135363
Epoch: 10/20
Train loss: 2.0918, val loss: 1.3320
0.9113387044753284
Model improve: 0.9065 -> 0.9113
Epoch: 11/20
Train loss: 1.9499, val loss: 1.3367
0.9110659044642183
Epoch: 12/20
Train loss: 1.8729, val loss: 1.3297
0.914335992482065
Model improve: 0.9113 -> 0.9143
Epoch: 13/20
Train loss: 1.8152, val loss: 1.2669
0.9144597888491245
Model improve: 0.9143 -> 0.9145
Epoch: 14/20
Train loss: 1.7864, val loss: 1.2603
0.9157027104682686
Model improve: 0.9145 -> 0.9157
Epoch: 15/20
Train loss: 1.7107, val loss: 1.2485
0.9169887308153312
Model improve: 0.9157 -> 0.9170
Epoch: 16/20
Train loss: 1.7250, val loss: 1.2419
0.917885530516863
Model improve: 0.9170 -> 0.9179
Epoch: 17/20
Train loss: 1.6079, val loss: 1.2485
0.917558765534875
Epoch: 18/20
Train loss: 1.6476, val loss: 1.2498
0.9187787901093867
Model improve: 0.9179 -> 0.9188
Epoch: 19/20
Train loss: 1.6650, val loss: 1.2257
0.918866180455844
Model improve: 0.9188 -> 0.9189
Epoch: 20/20
Train loss: 1.6487, val loss: 1.2414
0.9179754662405045
Date :04/15/2023, 11:55:41
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/20
Train loss: 7.2507, val loss: 2.3068
0.833538682757976
Date :04/15/2023, 12:20:25
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/20
Train loss: 7.2748, val loss: 2.3071
0.8376405117792882
Date :04/15/2023, 13:28:32
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/20
Date :04/15/2023, 13:28:55
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
50
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/20
Train loss: 183.5206, val loss: 173.8952
0.9988523865654547
Date :04/15/2023, 13:29:48
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
50
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/20
Train loss: 183.5206, val loss: 173.8952
0.9988523865654547
Model improve: 0.0000 -> 0.9989
Epoch: 2/20
Date :04/15/2023, 13:30:52
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/20
Train loss: 7.2547, val loss: 2.4021
0.8291519733031036
Model improve: 0.0000 -> 0.8292
Epoch: 2/20
Train loss: 3.3929, val loss: 2.0111
0.86059032124823
Model improve: 0.8292 -> 0.8606
Epoch: 3/20
Train loss: 3.0317, val loss: 1.7794
0.8785924255625523
Model improve: 0.8606 -> 0.8786
Epoch: 4/20
Train loss: 2.7150, val loss: 1.6996
0.8830284796078064
Model improve: 0.8786 -> 0.8830
Epoch: 5/20
Train loss: 2.6291, val loss: 1.6183
0.8870784342390003
Model improve: 0.8830 -> 0.8871
Epoch: 6/20
Train loss: 2.4749, val loss: 1.5110
0.8935914015954718
Model improve: 0.8871 -> 0.8936
Epoch: 7/20
Date :04/15/2023, 14:07:41
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 100
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
30381
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/20
Train loss: 5.9463, val loss: 2.0432
0.8629238831485215
Model improve: 0.0000 -> 0.8629
Epoch: 2/20
Train loss: 2.9519, val loss: 1.8160
0.8854531749039334
Model improve: 0.8629 -> 0.8855
Epoch: 3/20
Train loss: 2.5355, val loss: 1.6581
0.8921701325055712
Model improve: 0.8855 -> 0.8922
Epoch: 4/20
Train loss: 2.3927, val loss: 1.6284
0.8936338340416241
Model improve: 0.8922 -> 0.8936
Epoch: 5/20
Train loss: 2.2382, val loss: 1.5397
0.8991644088500299
Model improve: 0.8936 -> 0.8992
Epoch: 6/20
Train loss: 2.1021, val loss: 1.4860
0.9036217784857324
Model improve: 0.8992 -> 0.9036
Epoch: 7/20
Train loss: 2.1019, val loss: 1.4488
0.9042324060576353
Model improve: 0.9036 -> 0.9042
Epoch: 8/20
Train loss: 1.9547, val loss: 1.4262
0.9060983662742813
Model improve: 0.9042 -> 0.9061
Epoch: 9/20
Train loss: 1.8989, val loss: 1.3777
0.9068097956147123
Model improve: 0.9061 -> 0.9068
Epoch: 10/20
Date :04/15/2023, 15:22:33
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 20
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/20
Train loss: 10.2541, val loss: 3.0849
0.7889115378405359
Model improve: 0.0000 -> 0.7889
Epoch: 2/20
Train loss: 3.8444, val loss: 2.1275
0.8485294247872237
Model improve: 0.7889 -> 0.8485
Epoch: 3/20
Train loss: 3.2448, val loss: 1.8055
0.8756453527807726
Model improve: 0.8485 -> 0.8756
Epoch: 4/20
Train loss: 2.8091, val loss: 1.6528
0.8876287505683697
Model improve: 0.8756 -> 0.8876
Epoch: 5/20
Train loss: 2.7029, val loss: 1.5402
0.8941212158673406
Model improve: 0.8876 -> 0.8941
Epoch: 6/20
Train loss: 2.5116, val loss: 1.4662
0.8985973991344706
Model improve: 0.8941 -> 0.8986
Epoch: 7/20
Train loss: 2.3229, val loss: 1.4785
0.900775565160257
Model improve: 0.8986 -> 0.9008
Epoch: 8/20
Train loss: 2.2906, val loss: 1.4290
0.902665768557358
Model improve: 0.9008 -> 0.9027
Epoch: 9/20
Train loss: 2.1558, val loss: 1.3673
0.9031179313228668
Model improve: 0.9027 -> 0.9031
Epoch: 10/20
Train loss: 2.2048, val loss: 1.3407
0.9072030040836556
Model improve: 0.9031 -> 0.9072
Epoch: 11/20
Train loss: 2.0788, val loss: 1.3561
0.9074512048276442
Model improve: 0.9072 -> 0.9075
Epoch: 12/20
Date :04/15/2023, 16:24:35
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 20
learningrate: 0.002
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.002
    lr: 0.002
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/20
Train loss: 6.2814, val loss: 2.3394
0.8306285460149365
Model improve: 0.0000 -> 0.8306
Epoch: 2/20
Train loss: 3.3146, val loss: 2.1603
0.8582730212967977
Model improve: 0.8306 -> 0.8583
Epoch: 3/20
Train loss: 3.0287, val loss: 1.9403
0.8731490575497658
Model improve: 0.8583 -> 0.8731
Epoch: 4/20
Date :04/15/2023, 16:42:52
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/20
Train loss: 7.0893, val loss: 2.2703
0.8402642258324222
Model improve: 0.0000 -> 0.8403
Epoch: 2/20
Train loss: 3.2296, val loss: 1.9296
0.8689380031103625
Model improve: 0.8403 -> 0.8689
Epoch: 3/20
Train loss: 2.8812, val loss: 1.6738
0.8885469338797765
Model improve: 0.8689 -> 0.8885
Epoch: 4/20
Train loss: 2.5596, val loss: 1.6315
0.8942458611722198
Model improve: 0.8885 -> 0.8942
Epoch: 5/20
Train loss: 2.5021, val loss: 1.5281
0.8965334169136299
Model improve: 0.8942 -> 0.8965
Epoch: 6/20
Train loss: 2.3554, val loss: 1.4466
0.9033078942762217
Model improve: 0.8965 -> 0.9033
Epoch: 7/20
Train loss: 2.1797, val loss: 1.4955
0.9053860688798597
Model improve: 0.9033 -> 0.9054
Epoch: 8/20
Train loss: 2.1654, val loss: 1.4716
0.9069641767716676
Model improve: 0.9054 -> 0.9070
Epoch: 9/20
Train loss: 2.0304, val loss: 1.3776
0.9069351561803648
Epoch: 10/20
Train loss: 2.0817, val loss: 1.3386
0.9114692731933451
Model improve: 0.9070 -> 0.9115
Epoch: 11/20
Train loss: 1.9411, val loss: 1.3429
0.9105387005387248
Epoch: 12/20
Train loss: 1.8633, val loss: 1.3623
0.9118535477245626
Model improve: 0.9115 -> 0.9119
Epoch: 13/20
Train loss: 1.8109, val loss: 1.2844
0.9141831750834575
Model improve: 0.9119 -> 0.9142
Epoch: 14/20
Train loss: 1.7828, val loss: 1.2664
0.9156925840545604
Model improve: 0.9142 -> 0.9157
Epoch: 15/20
Train loss: 1.7041, val loss: 1.2711
0.9158084595202184
Model improve: 0.9157 -> 0.9158
Epoch: 16/20
Train loss: 1.7253, val loss: 1.2494
0.9176703788547472
Model improve: 0.9158 -> 0.9177
Epoch: 17/20
Train loss: 1.6047, val loss: 1.2696
0.9158931896244461
Epoch: 18/20
Train loss: 1.6487, val loss: 1.2687
0.9172292516012276
Epoch: 19/20
Train loss: 1.6670, val loss: 1.2512
0.9181476264219358
Model improve: 0.9177 -> 0.9181
Epoch: 20/20
Train loss: 1.6483, val loss: 1.2632
0.9172778297082171
Date :04/15/2023, 18:34:44
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/20
Train loss: 7.3689, val loss: 2.3517
0.8350709480653544
Model improve: 0.0000 -> 0.8351
Epoch: 2/20
Train loss: 3.2594, val loss: 1.8831
0.8768881069905216
Model improve: 0.8351 -> 0.8769
Epoch: 3/20
Train loss: 2.8932, val loss: 1.7444
0.8851056389307338
Model improve: 0.8769 -> 0.8851
Epoch: 4/20
Train loss: 2.5701, val loss: 1.5701
0.8965217503645793
Model improve: 0.8851 -> 0.8965
Epoch: 5/20
Train loss: 2.5286, val loss: 1.4564
0.9049381559821793
Model improve: 0.8965 -> 0.9049
Epoch: 6/20
Train loss: 2.3787, val loss: 1.4667
0.9053009427980181
Model improve: 0.9049 -> 0.9053
Epoch: 7/20
Train loss: 2.1843, val loss: 1.5125
0.9093880096910693
Model improve: 0.9053 -> 0.9094
Epoch: 8/20
Train loss: 2.1678, val loss: 1.3735
0.910681755484031
Model improve: 0.9094 -> 0.9107
Epoch: 9/20
Train loss: 2.0294, val loss: 1.3534
0.9141933393651592
Model improve: 0.9107 -> 0.9142
Epoch: 10/20
Train loss: 2.0632, val loss: 1.3332
0.919120883494588
Model improve: 0.9142 -> 0.9191
Epoch: 11/20
Train loss: 1.9433, val loss: 1.2863
0.9182989055611682
Epoch: 12/20
Train loss: 1.8661, val loss: 1.2980
0.9199579287875665
Model improve: 0.9191 -> 0.9200
Epoch: 13/20
Train loss: 1.8172, val loss: 1.2732
0.922637568598768
Model improve: 0.9200 -> 0.9226
Epoch: 14/20
Train loss: 1.7812, val loss: 1.2367
0.9228163074248108
Model improve: 0.9226 -> 0.9228
Epoch: 15/20
Train loss: 1.7091, val loss: 1.2204
0.9238779102417688
Model improve: 0.9228 -> 0.9239
Epoch: 16/20
Train loss: 1.7218, val loss: 1.2253
0.9243705153217229
Model improve: 0.9239 -> 0.9244
Epoch: 17/20
Train loss: 1.5806, val loss: 1.2069
0.9258285851102177
Model improve: 0.9244 -> 0.9258
Epoch: 18/20
Train loss: 1.6639, val loss: 1.2098
0.9261988144016332
Model improve: 0.9258 -> 0.9262
Epoch: 19/20
Train loss: 1.6580, val loss: 1.2100
0.9259407351723938
Epoch: 20/20
Train loss: 1.6704, val loss: 1.1945
0.9261362853301076
Date :04/15/2023, 20:38:14
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/20
Date :04/15/2023, 20:39:01
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
Date :04/15/2023, 20:39:52
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
Date :04/15/2023, 20:40:20
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
Date :04/15/2023, 20:40:40
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
Date :04/15/2023, 20:40:51
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/20
Date :04/15/2023, 20:41:43
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/20
Date :04/15/2023, 20:42:25
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/20
Train loss: 9.1677, val loss: 4.7938
0.7323452278044376
Model improve: 0.0000 -> 0.7323
Epoch: 2/20
Train loss: 6.2756, val loss: 3.3046
0.8258050306719948
Model improve: 0.7323 -> 0.8258
Epoch: 3/20
Train loss: 5.4924, val loss: 3.0981
0.8511753269467228
Model improve: 0.8258 -> 0.8512
Epoch: 4/20
Train loss: 4.8635, val loss: 2.6513
0.86551484748457
Model improve: 0.8512 -> 0.8655
Epoch: 5/20
Date :04/15/2023, 21:05:13
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/60
Train loss: 7.0960, val loss: 2.2719
0.8397917139862153
Model improve: 0.0000 -> 0.8398
Epoch: 2/60
Date :04/15/2023, 21:11:26
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/60
Train loss: 7.0908, val loss: 2.2696
0.8402795701617085
Model improve: 0.0000 -> 0.8403
Epoch: 2/60
Train loss: 3.2293, val loss: 1.9040
0.8698538349624276
Model improve: 0.8403 -> 0.8699
Epoch: 3/60
Train loss: 2.8833, val loss: 1.6696
0.8890608825763037
Model improve: 0.8699 -> 0.8891
Epoch: 4/60
Train loss: 2.5604, val loss: 1.6379
0.8938104847934648
Model improve: 0.8891 -> 0.8938
Epoch: 5/60
Train loss: 2.5036, val loss: 1.5194
0.8965564735656741
Model improve: 0.8938 -> 0.8966
Epoch: 6/60
Train loss: 2.3587, val loss: 1.4337
0.9033415323279035
Model improve: 0.8966 -> 0.9033
Epoch: 7/60
Date :04/15/2023, 21:45:11
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/60
Train loss: 7.0867, val loss: 2.2653
0.840851215088221
Model improve: 0.0000 -> 0.8409
Epoch: 2/60
Train loss: 3.2296, val loss: 1.9159
0.8698440346117597
Model improve: 0.8409 -> 0.8698
Epoch: 3/60
Train loss: 2.8896, val loss: 1.6796
0.8890234687830406
Model improve: 0.8698 -> 0.8890
Epoch: 4/60
Train loss: 2.5766, val loss: 1.6557
0.8941382121719715
Model improve: 0.8890 -> 0.8941
Epoch: 5/60
Train loss: 2.5365, val loss: 1.5719
0.8947394433732757
Model improve: 0.8941 -> 0.8947
Epoch: 6/60
Train loss: 2.4075, val loss: 1.5211
0.8979764165090529
Model improve: 0.8947 -> 0.8980
Epoch: 7/60
Train loss: 2.2498, val loss: 1.5323
0.901897261722692
Model improve: 0.8980 -> 0.9019
Epoch: 8/60
Train loss: 2.2511, val loss: 1.5503
0.8991502526668448
Epoch: 9/60
Train loss: 2.1449, val loss: 1.4986
0.8983507865621746
Epoch: 10/60
Train loss: 2.2070, val loss: 1.4543
0.9052155999247429
Model improve: 0.9019 -> 0.9052
Epoch: 11/60
Train loss: 2.0920, val loss: 1.4581
0.902920374542095
Epoch: 12/60
Train loss: 2.0334, val loss: 1.5013
0.9026448349435349
Epoch: 13/60
Train loss: 1.9859, val loss: 1.3978
0.9034666557838092
Epoch: 14/60
Train loss: 1.9707, val loss: 1.4718
0.903422279504814
Epoch: 15/60
Train loss: 1.9061, val loss: 1.4253
0.9044139996393972
Epoch: 16/60
Train loss: 1.9253, val loss: 1.3697
0.9082583939449305
Model improve: 0.9052 -> 0.9083
Epoch: 17/60
Train loss: 1.7924, val loss: 1.4816
0.9010137356877841
Epoch: 18/60
Train loss: 1.8406, val loss: 1.4400
0.9047681918837235
Epoch: 19/60
Train loss: 1.8354, val loss: 1.3255
0.9117744075014058
Model improve: 0.9083 -> 0.9118
Epoch: 20/60
Train loss: 1.8053, val loss: 1.3790
0.9077391264808345
Epoch: 21/60
Train loss: 1.8314, val loss: 1.3875
0.9112163256408102
Epoch: 22/60
Train loss: 1.8029, val loss: 1.3848
0.9102786711888965
Epoch: 23/60
Train loss: 1.7516, val loss: 1.4022
0.9102860140996972
Epoch: 24/60
Train loss: 1.7736, val loss: 1.3605
0.9101941002275963
Epoch: 25/60
Train loss: 1.7169, val loss: 1.3653
0.9117206013313448
Epoch: 26/60
Train loss: 1.7069, val loss: 1.3134
0.9120560337613468
Model improve: 0.9118 -> 0.9121
Epoch: 27/60
Train loss: 1.5960, val loss: 1.3333
0.9105279996400512
Epoch: 28/60
Train loss: 1.6583, val loss: 1.3307
0.9122616112212556
Model improve: 0.9121 -> 0.9123
Epoch: 29/60
Train loss: 1.6559, val loss: 1.3418
0.9089988519611528
Epoch: 30/60
Train loss: 1.6560, val loss: 1.3410
0.9110946653799754
Epoch: 31/60
Train loss: 1.6080, val loss: 1.3260
0.9120606589039326
Epoch: 32/60
Train loss: 1.6050, val loss: 1.3539
0.9108013095804417
Epoch: 33/60
Train loss: 1.6846, val loss: 1.3339
0.9130103202718174
Model improve: 0.9123 -> 0.9130
Epoch: 34/60
Train loss: 1.5512, val loss: 1.3128
0.9137198936200865
Model improve: 0.9130 -> 0.9137
Epoch: 35/60
Train loss: 1.5547, val loss: 1.3344
0.9152990945959734
Model improve: 0.9137 -> 0.9153
Epoch: 36/60
Train loss: 1.4997, val loss: 1.2875
0.9157661155060008
Model improve: 0.9153 -> 0.9158
Epoch: 37/60
Train loss: 1.5101, val loss: 1.3295
0.9158026099000702
Model improve: 0.9158 -> 0.9158
Epoch: 38/60
Train loss: 1.4913, val loss: 1.2831
0.9164323987375379
Model improve: 0.9158 -> 0.9164
Epoch: 39/60
Train loss: 1.4755, val loss: 1.2774
0.9186995768144481
Model improve: 0.9164 -> 0.9187
Epoch: 40/60
Train loss: 1.4882, val loss: 1.2666
0.9150706429472303
Epoch: 41/60
Train loss: 1.5623, val loss: 1.2944
0.9161504503536254
Epoch: 42/60
Train loss: 1.4709, val loss: 1.3035
0.9161746490718208
Epoch: 43/60
Train loss: 1.4338, val loss: 1.2964
0.9172257985098469
Epoch: 44/60
Train loss: 1.4716, val loss: 1.3238
0.9162690980070624
Epoch: 45/60
Train loss: 1.4208, val loss: 1.3053
0.9169960223148881
Epoch: 46/60
Train loss: 1.4193, val loss: 1.2848
0.9175670640043484
Epoch: 47/60
Train loss: 1.4255, val loss: 1.2645
0.9178894709257003
Epoch: 48/60
Train loss: 1.4563, val loss: 1.3145
0.916710195318427
Epoch: 49/60
Train loss: 1.4405, val loss: 1.2574
0.9191259165495045
Model improve: 0.9187 -> 0.9191
Epoch: 50/60
Train loss: 1.4627, val loss: 1.2647
0.9197946889794593
Model improve: 0.9191 -> 0.9198
Epoch: 51/60
Train loss: 1.4089, val loss: 1.2731
0.9180839222578386
Epoch: 52/60
Train loss: 1.4565, val loss: 1.3125
0.9179404577625474
Epoch: 53/60
Train loss: 1.4185, val loss: 1.2567
0.9193970931290033
Epoch: 54/60
Train loss: 1.4006, val loss: 1.2733
0.9192799206918212
Epoch: 55/60
Train loss: 1.4036, val loss: 1.2805
0.919287929531246
Epoch: 56/60
Train loss: 1.4483, val loss: 1.2448
0.9196171351903183
Epoch: 57/60
Train loss: 1.3871, val loss: 1.2860
0.9189186381785363
Epoch: 58/60
Train loss: 1.4817, val loss: 1.2740
0.919772692280068
Epoch: 59/60
Train loss: 1.3935, val loss: 1.2706
0.9193229374667308
Epoch: 60/60
Train loss: 1.4069, val loss: 1.2580
0.9193495743668634
Date :04/16/2023, 06:44:10
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/60
Train loss: 7.1805, val loss: 2.2894
0.8408008798626668
Model improve: 0.0000 -> 0.8408
Epoch: 2/60
Train loss: 3.2383, val loss: 1.8637
0.8746104090351087
Model improve: 0.8408 -> 0.8746
Epoch: 3/60
Date :04/16/2023, 06:57:10
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/60
Train loss: 7.1814, val loss: 2.2914
0.8409889113724327
Model improve: 0.0000 -> 0.8410
Epoch: 2/60
Train loss: 3.2382, val loss: 1.8594
0.8751651562138228
Model improve: 0.8410 -> 0.8752
Epoch: 3/60
Train loss: 2.8764, val loss: 1.7049
0.8863471282196166
Model improve: 0.8752 -> 0.8863
Epoch: 4/60
Train loss: 2.5721, val loss: 1.6553
0.8935526073201812
Model improve: 0.8863 -> 0.8936
Epoch: 5/60
Train loss: 2.5240, val loss: 1.5619
0.8947389296126242
Model improve: 0.8936 -> 0.8947
Epoch: 6/60
Train loss: 2.3976, val loss: 1.5243
0.8991789234089925
Model improve: 0.8947 -> 0.8992
Epoch: 7/60
Train loss: 2.2462, val loss: 1.5262
0.904734573044306
Model improve: 0.8992 -> 0.9047
Epoch: 8/60
Train loss: 2.2375, val loss: 1.4668
0.9058922186850661
Model improve: 0.9047 -> 0.9059
Epoch: 9/60
Train loss: 2.1328, val loss: 1.4889
0.9001192593077374
Epoch: 10/60
Train loss: 2.1867, val loss: 1.4779
0.9033374171192087
Epoch: 11/60
Train loss: 2.0928, val loss: 1.4459
0.9044780929425636
Epoch: 12/60
Train loss: 2.0100, val loss: 1.4331
0.907826264951087
Model improve: 0.9059 -> 0.9078
Epoch: 13/60
Train loss: 1.9801, val loss: 1.3985
0.9072145429403075
Epoch: 14/60
Train loss: 1.9636, val loss: 1.4680
0.905054596586502
Epoch: 15/60
Train loss: 1.8910, val loss: 1.4476
0.907430468453933
Epoch: 16/60
Train loss: 1.9102, val loss: 1.4427
0.9035799892021628
Epoch: 17/60
Train loss: 1.7909, val loss: 1.4231
0.9045661502102176
Epoch: 18/60
Train loss: 1.8237, val loss: 1.4769
0.9062162974082469
Epoch: 19/60
Train loss: 1.8151, val loss: 1.3916
0.9100671162847764
Model improve: 0.9078 -> 0.9101
Epoch: 20/60
Train loss: 1.8056, val loss: 1.4462
0.9050700253492243
Epoch: 21/60
Train loss: 1.8131, val loss: 1.3912
0.9066071408987835
Epoch: 22/60
Train loss: 1.7904, val loss: 1.3723
0.9087184718511289
Epoch: 23/60
Train loss: 1.7362, val loss: 1.3728
0.9079309270566064
Epoch: 24/60
Train loss: 1.7584, val loss: 1.3437
0.911286089040913
Model improve: 0.9101 -> 0.9113
Epoch: 25/60
Train loss: 1.6997, val loss: 1.4030
0.9107559730685827
Epoch: 26/60
Train loss: 1.7013, val loss: 1.3370
0.9105255942277685
Epoch: 27/60
Train loss: 1.5810, val loss: 1.4072
0.9099514061307555
Epoch: 28/60
Train loss: 1.6408, val loss: 1.4070
0.910409395325706
Epoch: 29/60
Train loss: 1.6441, val loss: 1.3757
0.9096438476385404
Epoch: 30/60
Train loss: 1.6358, val loss: 1.3489
0.9100284343657954
Epoch: 31/60
Train loss: 1.5993, val loss: 1.3773
0.9137584822769822
Model improve: 0.9113 -> 0.9138
Epoch: 32/60
Train loss: 1.5946, val loss: 1.3455
0.9122232133571616
Epoch: 33/60
Train loss: 1.6644, val loss: 1.3241
0.9122423835460484
Epoch: 34/60
Train loss: 1.5397, val loss: 1.3418
0.9122664503047619
Epoch: 35/60
Train loss: 1.5328, val loss: 1.3979
0.9123525136828905
Epoch: 36/60
Train loss: 1.4872, val loss: 1.3551
0.9129180800413945
Epoch: 37/60
Train loss: 1.4968, val loss: 1.3277
0.9135608843100048
Epoch: 38/60
Train loss: 1.4804, val loss: 1.3346
0.9124320239456749
Epoch: 39/60
Train loss: 1.4624, val loss: 1.3242
0.9142571648642658
Model improve: 0.9138 -> 0.9143
Epoch: 40/60
Train loss: 1.4740, val loss: 1.3128
0.9143845490962478
Model improve: 0.9143 -> 0.9144
Epoch: 41/60
Train loss: 1.5523, val loss: 1.3183
0.9133682290977434
Epoch: 42/60
Train loss: 1.4627, val loss: 1.3416
0.9124433372673801
Epoch: 43/60
Train loss: 1.4180, val loss: 1.3204
0.914286947847858
Epoch: 44/60
Train loss: 1.4541, val loss: 1.3561
0.9145285996262321
Model improve: 0.9144 -> 0.9145
Epoch: 45/60
Train loss: 1.4005, val loss: 1.3312
0.9146365387921397
Model improve: 0.9145 -> 0.9146
Epoch: 46/60
Train loss: 1.4046, val loss: 1.3234
0.915920566304063
Model improve: 0.9146 -> 0.9159
Epoch: 47/60
Train loss: 1.4108, val loss: 1.2877
0.9165489014310146
Model improve: 0.9159 -> 0.9165
Epoch: 48/60
Train loss: 1.4417, val loss: 1.3367
0.9154322588360609
Epoch: 49/60
Train loss: 1.4252, val loss: 1.2979
0.915444434722331
Epoch: 50/60
Train loss: 1.4529, val loss: 1.3282
0.9157836424678218
Epoch: 51/60
Train loss: 1.3942, val loss: 1.3090
0.9158956705856768
Epoch: 52/60
Train loss: 1.4382, val loss: 1.3288
0.9159833578549073
Epoch: 53/60
Train loss: 1.4032, val loss: 1.2943
0.9171118101650519
Model improve: 0.9165 -> 0.9171
Epoch: 54/60
Train loss: 1.3805, val loss: 1.3211
0.9166011212488576
Epoch: 55/60
Train loss: 1.3842, val loss: 1.3222
0.9168810536792185
Epoch: 56/60
Train loss: 1.4352, val loss: 1.2763
0.9174675318098601
Model improve: 0.9171 -> 0.9175
Epoch: 57/60
Train loss: 1.3724, val loss: 1.3147
0.9164461976604188
Epoch: 58/60
Train loss: 1.4673, val loss: 1.3050
0.9165591637529218
Epoch: 59/60
Train loss: 1.3745, val loss: 1.3132
0.9165436915116997
Epoch: 60/60
Train loss: 1.3934, val loss: 1.2922
0.9171730469884507
Date :04/16/2023, 12:49:13
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/60
Train loss: 7.9994, val loss: 3.2003
0.7762596930608201
Model improve: 0.0000 -> 0.7763
Epoch: 2/60
Train loss: 4.0843, val loss: 2.2674
0.8401279594403374
Model improve: 0.7763 -> 0.8401
Epoch: 3/60
Train loss: 3.7790, val loss: 2.0752
0.857194476280075
Model improve: 0.8401 -> 0.8572
Epoch: 4/60
Train loss: 3.5118, val loss: 2.0900
0.8570089372323625
Epoch: 5/60
Train loss: 3.4523, val loss: 1.9614
0.8655110702914894
Model improve: 0.8572 -> 0.8655
Epoch: 6/60
Train loss: 3.3405, val loss: 1.8059
0.8737774871392171
Model improve: 0.8655 -> 0.8738
Epoch: 7/60
Train loss: 3.2017, val loss: 1.7850
0.8750094091506183
Model improve: 0.8738 -> 0.8750
Epoch: 8/60
Train loss: 3.1922, val loss: 1.7974
0.8734570632605814
Epoch: 9/60
Train loss: 3.0815, val loss: 1.8325
0.8763383367737595
Model improve: 0.8750 -> 0.8763
Epoch: 10/60
Train loss: 3.1397, val loss: 1.7239
0.8832816691676473
Model improve: 0.8763 -> 0.8833
Epoch: 11/60
Train loss: 3.0495, val loss: 1.7061
0.8832553259466864
Epoch: 12/60
Train loss: 2.9668, val loss: 1.7154
0.8838470661468703
Model improve: 0.8833 -> 0.8838
Epoch: 13/60
Train loss: 2.9306, val loss: 1.6591
0.8845150646238347
Model improve: 0.8838 -> 0.8845
Epoch: 14/60
Train loss: 2.9047, val loss: 1.7149
0.8798640768113706
Epoch: 15/60
Train loss: 2.8266, val loss: 1.6562
0.8850509919719751
Model improve: 0.8845 -> 0.8851
Epoch: 16/60
Train loss: 2.8382, val loss: 1.6719
0.8860547816909193
Model improve: 0.8851 -> 0.8861
Epoch: 17/60
Date :04/16/2023, 14:37:17
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/60
Train loss: 7.1172, val loss: 2.2892
0.8414248437717852
Model improve: 0.0000 -> 0.8414
Epoch: 2/60
Train loss: 3.1892, val loss: 1.7886
0.8812279955192821
Model improve: 0.8414 -> 0.8812
Epoch: 3/60
Train loss: 2.8302, val loss: 1.6920
0.8852992505651449
Model improve: 0.8812 -> 0.8853
Epoch: 4/60
Train loss: 2.5340, val loss: 1.6742
0.8919844967256472
Model improve: 0.8853 -> 0.8920
Epoch: 5/60
Train loss: 2.4840, val loss: 1.5598
0.8976460419559223
Model improve: 0.8920 -> 0.8976
Epoch: 6/60
Train loss: 2.3537, val loss: 1.5280
0.8988992016927488
Model improve: 0.8976 -> 0.8989
Epoch: 7/60
Train loss: 2.1879, val loss: 1.5330
0.9033399265191403
Model improve: 0.8989 -> 0.9033
Epoch: 8/60
Train loss: 2.2119, val loss: 1.5482
0.9013458277255092
Epoch: 9/60
Train loss: 2.0772, val loss: 1.5210
0.9004935327255016
Epoch: 10/60
Train loss: 2.1728, val loss: 1.4907
0.9047559591102505
Model improve: 0.9033 -> 0.9048
Epoch: 11/60
Train loss: 2.0253, val loss: 1.4685
0.9021691781556118
Epoch: 12/60
Train loss: 1.9656, val loss: 1.5050
0.9018858160374488
Epoch: 13/60
Train loss: 1.9510, val loss: 1.4361
0.904480443832844
Epoch: 14/60
Train loss: 1.9197, val loss: 1.4924
0.9047277930112699
Epoch: 15/60
Train loss: 1.8580, val loss: 1.4278
0.905768597755665
Model improve: 0.9048 -> 0.9058
Epoch: 16/60
Train loss: 1.8553, val loss: 1.4105
0.9067038407088359
Model improve: 0.9058 -> 0.9067
Epoch: 17/60
Train loss: 1.7510, val loss: 1.4073
0.9076373979596691
Model improve: 0.9067 -> 0.9076
Epoch: 18/60
Train loss: 1.7924, val loss: 1.4114
0.9078461397701963
Model improve: 0.9076 -> 0.9078
Epoch: 19/60
Train loss: 1.7680, val loss: 1.3815
0.9086218871504474
Model improve: 0.9078 -> 0.9086
Epoch: 20/60
Train loss: 1.7424, val loss: 1.3795
0.9084355329337398
Epoch: 21/60
Train loss: 1.7600, val loss: 1.4328
0.9034043093511941
Epoch: 22/60
Train loss: 1.7307, val loss: 1.3880
0.9069348653204484
Epoch: 23/60
Train loss: 1.6860, val loss: 1.3627
0.9051210183372984
Epoch: 24/60
Train loss: 1.7161, val loss: 1.3956
0.908977011923083
Model improve: 0.9086 -> 0.9090
Epoch: 25/60
Train loss: 1.6395, val loss: 1.3887
0.9103531044182149
Model improve: 0.9090 -> 0.9104
Epoch: 26/60
Train loss: 1.6520, val loss: 1.3757
0.9084200253358004
Epoch: 27/60
Train loss: 1.5408, val loss: 1.3860
0.9099665134321578
Epoch: 28/60
Train loss: 1.6043, val loss: 1.3310
0.9094947953559951
Epoch: 29/60
Train loss: 1.6133, val loss: 1.3452
0.910531408918365
Model improve: 0.9104 -> 0.9105
Epoch: 30/60
Train loss: 1.5858, val loss: 1.3644
0.9117110214997901
Model improve: 0.9105 -> 0.9117
Epoch: 31/60
Train loss: 1.5505, val loss: 1.3733
0.9111987032371299
Epoch: 32/60
Train loss: 1.5387, val loss: 1.3104
0.9114193450187406
Epoch: 33/60
Train loss: 1.6324, val loss: 1.3415
0.9105786813762048
Epoch: 34/60
Train loss: 1.4926, val loss: 1.3759
0.9112071582578221
Epoch: 35/60
Train loss: 1.4960, val loss: 1.3484
0.914042192839411
Model improve: 0.9117 -> 0.9140
Epoch: 36/60
Train loss: 1.4460, val loss: 1.3598
0.9120211742364275
Epoch: 37/60
Train loss: 1.4565, val loss: 1.3264
0.9138409692949807
Epoch: 38/60
Date :04/16/2023, 18:02:33
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
18770
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/60
Train loss: 7.5401, val loss: 2.4453
0.8243187506766406
Model improve: 0.0000 -> 0.8243
Epoch: 2/60
Train loss: 3.2427, val loss: 1.9865
0.8647029351393664
Model improve: 0.8243 -> 0.8647
Epoch: 3/60
Train loss: 2.8808, val loss: 1.9189
0.8762411244918874
Model improve: 0.8647 -> 0.8762
Epoch: 4/60
Train loss: 2.5859, val loss: 1.8065
0.8793872706967585
Model improve: 0.8762 -> 0.8794
Epoch: 5/60
Train loss: 2.5187, val loss: 1.7769
0.8847832376409835
Model improve: 0.8794 -> 0.8848
Epoch: 6/60
Train loss: 2.3830, val loss: 1.6827
0.888690196481569
Model improve: 0.8848 -> 0.8887
Epoch: 7/60
Train loss: 2.2099, val loss: 1.6436
0.8935804418704756
Model improve: 0.8887 -> 0.8936
Epoch: 8/60
Train loss: 2.2636, val loss: 1.6199
0.8956562931787307
Model improve: 0.8936 -> 0.8957
Epoch: 9/60
Train loss: 2.0500, val loss: 1.5958
0.8950121928839878
Epoch: 10/60
Train loss: 2.1715, val loss: 1.6307
0.8975305115197995
Model improve: 0.8957 -> 0.8975
Epoch: 11/60
Train loss: 2.1374, val loss: 1.6256
0.8992541328910024
Model improve: 0.8975 -> 0.8993
Epoch: 12/60
Train loss: 1.9320, val loss: 1.6375
0.8959776665573232
Epoch: 13/60
Train loss: 1.9790, val loss: 1.5906
0.8970843381747087
Epoch: 14/60
Train loss: 1.9293, val loss: 1.6089
0.896796236639097
Epoch: 15/60
Train loss: 1.9062, val loss: 1.5622
0.898413419457416
Epoch: 16/60
Train loss: 1.8120, val loss: 1.5932
0.8944748161351991
Epoch: 17/60
Train loss: 1.7812, val loss: 1.6086
0.8975150009772248
Epoch: 18/60
Train loss: 1.7887, val loss: 1.5982
0.8977719444212737
Epoch: 19/60
Train loss: 1.7646, val loss: 1.5704
0.8998411571780389
Model improve: 0.8993 -> 0.8998
Epoch: 20/60
Date :04/16/2023, 19:46:51
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
18770
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/60
Train loss: 7.3392, val loss: 2.4630
0.82338969273839
Model improve: 0.0000 -> 0.8234
Epoch: 2/60
Train loss: 3.3497, val loss: 2.0217
0.8623716067869862
Model improve: 0.8234 -> 0.8624
Epoch: 3/60
Train loss: 3.0052, val loss: 1.9309
0.8715379745178072
Model improve: 0.8624 -> 0.8715
Epoch: 4/60
Train loss: 2.6964, val loss: 1.7870
0.8810787472691428
Model improve: 0.8715 -> 0.8811
Epoch: 5/60
Train loss: 2.6406, val loss: 1.7259
0.8878308402739368
Model improve: 0.8811 -> 0.8878
Epoch: 6/60
Train loss: 2.4869, val loss: 1.6392
0.8943783275450197
Model improve: 0.8878 -> 0.8944
Epoch: 7/60
Train loss: 2.3388, val loss: 1.6960
0.8898763157785299
Epoch: 8/60
Train loss: 2.3807, val loss: 1.6290
0.8954544226784803
Model improve: 0.8944 -> 0.8955
Epoch: 9/60
Train loss: 2.1957, val loss: 1.6697
0.8907418295432564
Epoch: 10/60
Train loss: 2.2784, val loss: 1.6050
0.8968638911599915
Model improve: 0.8955 -> 0.8969
Epoch: 11/60
Train loss: 2.2617, val loss: 1.6301
0.8962719695067117
Epoch: 12/60
Train loss: 2.0454, val loss: 1.5583
0.8987637856911235
Model improve: 0.8969 -> 0.8988
Epoch: 13/60
Train loss: 2.1079, val loss: 1.5972
0.895502084348152
Epoch: 14/60
Train loss: 2.0562, val loss: 1.6518
0.8948100289025578
Epoch: 15/60
Train loss: 2.0227, val loss: 1.5817
0.8944223035052755
Epoch: 16/60
Train loss: 1.9557, val loss: 1.5516
0.8962723063315167
Epoch: 17/60
Train loss: 1.9211, val loss: 1.6232
0.8951155219238871
Epoch: 18/60
Train loss: 1.9215, val loss: 1.6065
0.8968892308698462
Epoch: 19/60
Date :04/16/2023, 21:23:24
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
18770
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/60
Train loss: 7.5267, val loss: 2.4063
0.826321316860882
Model improve: 0.0000 -> 0.8263
Epoch: 2/60
Train loss: 3.2730, val loss: 1.9844
0.8629858671598911
Model improve: 0.8263 -> 0.8630
Epoch: 3/60
Train loss: 2.8966, val loss: 1.8474
0.8767690962915212
Model improve: 0.8630 -> 0.8768
Epoch: 4/60
Train loss: 2.5955, val loss: 1.7830
0.8816534586780644
Model improve: 0.8768 -> 0.8817
Epoch: 5/60
Train loss: 2.5369, val loss: 1.7003
0.8894023338117546
Model improve: 0.8817 -> 0.8894
Epoch: 6/60
Train loss: 2.3922, val loss: 1.6541
0.8919313397322024
Model improve: 0.8894 -> 0.8919
Epoch: 7/60
Train loss: 2.2581, val loss: 1.6206
0.8947861813120951
Model improve: 0.8919 -> 0.8948
Epoch: 8/60
Train loss: 2.2635, val loss: 1.6552
0.8932334510613367
Epoch: 9/60
Train loss: 2.0799, val loss: 1.6479
0.8959887603721264
Model improve: 0.8948 -> 0.8960
Epoch: 10/60
Train loss: 2.1917, val loss: 1.5677
0.8971337938112588
Model improve: 0.8960 -> 0.8971
Epoch: 11/60
Train loss: 2.1782, val loss: 1.6196
0.8951668552323685
Epoch: 12/60
Train loss: 1.9528, val loss: 1.5535
0.8964058401486835
Epoch: 13/60
Train loss: 2.0177, val loss: 1.5744
0.8978344260966328
Model improve: 0.8971 -> 0.8978
Epoch: 14/60
Train loss: 1.9460, val loss: 1.5914
0.8973167993022995
Epoch: 15/60
Train loss: 1.9286, val loss: 1.5366
0.8981455037224365
Model improve: 0.8978 -> 0.8981
Epoch: 16/60
Train loss: 1.8654, val loss: 1.5768
0.8974975188620533
Epoch: 17/60
Train loss: 1.8189, val loss: 1.6130
0.8947283789889816
Epoch: 18/60
Train loss: 1.8142, val loss: 1.6286
0.8991277129047247
Model improve: 0.8981 -> 0.8991
Epoch: 19/60
Train loss: 1.7991, val loss: 1.5631
0.8974191400574794
Epoch: 20/60
Train loss: 1.8223, val loss: 1.6263
0.8938012077639415
Epoch: 21/60
Train loss: 1.7064, val loss: 1.5547
0.8997973771233095
Model improve: 0.8991 -> 0.8998
Epoch: 22/60
Train loss: 1.7747, val loss: 1.6400
0.8965079321720947
Epoch: 23/60
Train loss: 1.7726, val loss: 1.5721
0.8981444195425892
Epoch: 24/60
Train loss: 1.6889, val loss: 1.5656
0.899890220284661
Model improve: 0.8998 -> 0.8999
Epoch: 25/60
Date :04/16/2023, 23:31:46
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 2
18770
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/60
Train loss: 7.6265, val loss: 2.3740
0.83007620156775
Model improve: 0.0000 -> 0.8301
Epoch: 2/60
Train loss: 3.2636, val loss: 1.9422
0.8686712870160925
Model improve: 0.8301 -> 0.8687
Epoch: 3/60
Train loss: 2.9017, val loss: 1.8667
0.8785762889890713
Model improve: 0.8687 -> 0.8786
Epoch: 4/60
Train loss: 2.6064, val loss: 1.7572
0.8835841125028152
Model improve: 0.8786 -> 0.8836
Epoch: 5/60
Train loss: 2.5482, val loss: 1.7514
0.8891189405846744
Model improve: 0.8836 -> 0.8891
Epoch: 6/60
Train loss: 2.4170, val loss: 1.7208
0.8882522192501981
Epoch: 7/60
Train loss: 2.2683, val loss: 1.6728
0.8893455220253452
Model improve: 0.8891 -> 0.8893
Epoch: 8/60
Train loss: 2.2891, val loss: 1.6316
0.8905140016622705
Model improve: 0.8893 -> 0.8905
Epoch: 9/60
Train loss: 2.0952, val loss: 1.6167
0.8925119757133707
Model improve: 0.8905 -> 0.8925
Epoch: 10/60
Train loss: 2.2039, val loss: 1.6354
0.8892380520250475
Epoch: 11/60
Train loss: 2.1929, val loss: 1.6095
0.8968779551949806
Model improve: 0.8925 -> 0.8969
Epoch: 12/60
Train loss: 1.9654, val loss: 1.5578
0.8966579972187031
Epoch: 13/60
Train loss: 2.0335, val loss: 1.5599
0.8965803034593894
Epoch: 14/60
Train loss: 1.9771, val loss: 1.5505
0.8975141999685586
Model improve: 0.8969 -> 0.8975
Epoch: 15/60
Train loss: 1.9336, val loss: 1.5682
0.897196909914904
Epoch: 16/60
Train loss: 1.8727, val loss: 1.6351
0.8934605162286711
Epoch: 17/60
Train loss: 1.8322, val loss: 1.5975
0.8954722585663699
Epoch: 18/60
Train loss: 1.8271, val loss: 1.5905
0.8972761869045125
Epoch: 19/60
Train loss: 1.8120, val loss: 1.5997
0.8966655275663115
Epoch: 20/60
Train loss: 1.8374, val loss: 1.6085
0.8965915077754055
Epoch: 21/60
Train loss: 1.7032, val loss: 1.6066
0.8933180047020122
Epoch: 22/60
Train loss: 1.7794, val loss: 1.6233
0.8938401434490846
Epoch: 23/60
Train loss: 1.7932, val loss: 1.5882
0.8956257303749292
Epoch: 24/60
Train loss: 1.6991, val loss: 1.6119
0.8980318624179073
Model improve: 0.8975 -> 0.8980
Epoch: 25/60
Train loss: 1.7799, val loss: 1.6303
0.8962387065945656
Epoch: 26/60
Train loss: 1.6799, val loss: 1.5729
0.8964231816307341
Epoch: 27/60
Train loss: 1.6797, val loss: 1.5989
0.8996554513182802
Model improve: 0.8980 -> 0.8997
Epoch: 28/60
Train loss: 1.6298, val loss: 1.6238
0.8998925574009007
Model improve: 0.8997 -> 0.8999
Epoch: 29/60
Train loss: 1.5887, val loss: 1.5926
0.8987566660467949
Epoch: 30/60
Train loss: 1.6637, val loss: 1.5875
0.8986197053111824
Epoch: 31/60
Train loss: 1.5860, val loss: 1.5934
0.8990687323108748
Epoch: 32/60
Train loss: 1.5718, val loss: 1.6296
0.8982290974272654
Epoch: 33/60
Train loss: 1.6198, val loss: 1.6101
0.8991093191041386
Epoch: 34/60
Train loss: 1.5714, val loss: 1.6410
0.8989577429256754
Epoch: 35/60
Train loss: 1.6322, val loss: 1.6033
0.900877366101549
Model improve: 0.8999 -> 0.9009
Epoch: 36/60
Train loss: 1.5539, val loss: 1.6001
0.9003633159369355
Epoch: 37/60
Train loss: 1.4608, val loss: 1.6134
0.8996082508826678
Epoch: 38/60
Train loss: 1.4692, val loss: 1.5998
0.9000271995866187
Epoch: 39/60
Train loss: 1.5111, val loss: 1.5525
0.9011420050272912
Model improve: 0.9009 -> 0.9011
Epoch: 40/60
Train loss: 1.4576, val loss: 1.5889
0.9021161185437029
Model improve: 0.9011 -> 0.9021
Epoch: 41/60
Train loss: 1.4720, val loss: 1.5995
0.9024939224207768
Model improve: 0.9021 -> 0.9025
Epoch: 42/60
Train loss: 1.4424, val loss: 1.5884
0.9027314563682525
Model improve: 0.9025 -> 0.9027
Epoch: 43/60
Train loss: 1.5491, val loss: 1.5776
0.9033144126955305
Model improve: 0.9027 -> 0.9033
Epoch: 44/60
Train loss: 1.4540, val loss: 1.5721
0.9030333401694112
Epoch: 45/60
Train loss: 1.4216, val loss: 1.6005
0.9025879216919326
Epoch: 46/60
Train loss: 1.4631, val loss: 1.5738
0.9045028856656422
Model improve: 0.9033 -> 0.9045
Epoch: 47/60
Train loss: 1.4075, val loss: 1.5808
0.9038165050826233
Epoch: 48/60
Train loss: 1.3638, val loss: 1.6064
0.9034207455610259
Epoch: 49/60
Train loss: 1.4483, val loss: 1.5593
0.904685820471983
Model improve: 0.9045 -> 0.9047
Epoch: 50/60
Train loss: 1.4324, val loss: 1.5814
0.905128794240784
Model improve: 0.9047 -> 0.9051
Epoch: 51/60
Train loss: 1.4058, val loss: 1.5785
0.9043040287107031
Epoch: 52/60
Train loss: 1.4558, val loss: 1.5950
0.9046336853618293
Epoch: 53/60
Train loss: 1.3520, val loss: 1.5843
0.9048419660354164
Epoch: 54/60
Train loss: 1.4798, val loss: 1.5770
0.9054233078218139
Model improve: 0.9051 -> 0.9054
Epoch: 55/60
Train loss: 1.3603, val loss: 1.5687
0.9058989461344448
Model improve: 0.9054 -> 0.9059
Epoch: 56/60
Train loss: 1.4526, val loss: 1.5650
0.905864815320847
Epoch: 57/60
Train loss: 1.3488, val loss: 1.5870
0.9051266029345629
Epoch: 58/60
Train loss: 1.4531, val loss: 1.5383
0.9064611320022448
Model improve: 0.9059 -> 0.9065
Epoch: 59/60
Train loss: 1.3775, val loss: 1.5429
0.9060729431259805
Epoch: 60/60
Train loss: 1.4255, val loss: 1.5540
0.9058486180601374
Date :04/17/2023, 08:06:55
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
18770
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/20
Train loss: 7.6209, val loss: 2.4267
0.8257975264563002
Model improve: 0.0000 -> 0.8258
Epoch: 2/20
Train loss: 3.3083, val loss: 1.9893
0.8652681994506559
Model improve: 0.8258 -> 0.8653
Epoch: 3/20
Train loss: 2.9179, val loss: 1.8570
0.876267138966348
Model improve: 0.8653 -> 0.8763
Epoch: 4/20
Train loss: 2.6160, val loss: 1.7182
0.8861967023456213
Model improve: 0.8763 -> 0.8862
Epoch: 5/20
Train loss: 2.5345, val loss: 1.6913
0.8916957069563209
Model improve: 0.8862 -> 0.8917
Epoch: 6/20
Train loss: 2.3831, val loss: 1.6195
0.8938676196270829
Model improve: 0.8917 -> 0.8939
Epoch: 7/20
Train loss: 2.2094, val loss: 1.5730
0.8964839775725228
Model improve: 0.8939 -> 0.8965
Epoch: 8/20
Train loss: 2.2177, val loss: 1.5275
0.8989031360330619
Model improve: 0.8965 -> 0.8989
Epoch: 9/20
Train loss: 2.0036, val loss: 1.5325
0.8992643521631748
Model improve: 0.8989 -> 0.8993
Epoch: 10/20
Train loss: 2.0941, val loss: 1.5124
0.9023022058261064
Model improve: 0.8993 -> 0.9023
Epoch: 11/20
Train loss: 2.0766, val loss: 1.5214
0.9013595443584083
Epoch: 12/20
Train loss: 1.8232, val loss: 1.4930
0.9040571445597706
Model improve: 0.9023 -> 0.9041
Epoch: 13/20
Date :04/17/2023, 09:14:14
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/20
Train loss: 7.6075, val loss: 2.3515
0.8385052820855318
Model improve: 0.0000 -> 0.8385
Epoch: 2/20
Train loss: 3.2813, val loss: 1.8129
0.8839830972053722
Model improve: 0.8385 -> 0.8840
Epoch: 3/20
Train loss: 2.9041, val loss: 1.7180
0.8903267274824133
Model improve: 0.8840 -> 0.8903
Epoch: 4/20
Train loss: 2.5478, val loss: 1.5778
0.9010603106824909
Model improve: 0.8903 -> 0.9011
Epoch: 5/20
Train loss: 2.5031, val loss: 1.5260
0.9030307303481747
Model improve: 0.9011 -> 0.9030
Epoch: 6/20
Train loss: 2.3593, val loss: 1.4531
0.9089928096942762
Model improve: 0.9030 -> 0.9090
Epoch: 7/20
Train loss: 2.1805, val loss: 1.4318
0.9139856383679292
Model improve: 0.9090 -> 0.9140
Epoch: 8/20
Train loss: 2.1476, val loss: 1.4038
0.9156174094998248
Model improve: 0.9140 -> 0.9156
Epoch: 9/20
Train loss: 2.0097, val loss: 1.3527
0.9168135069414273
Model improve: 0.9156 -> 0.9168
Epoch: 10/20
Train loss: 2.0770, val loss: 1.3234
0.9188854774075094
Model improve: 0.9168 -> 0.9189
Epoch: 11/20
Train loss: 1.9509, val loss: 1.2836
0.9214020269915723
Model improve: 0.9189 -> 0.9214
Epoch: 12/20
Train loss: 1.8597, val loss: 1.2826
0.9219478821358006
Model improve: 0.9214 -> 0.9219
Epoch: 13/20
Train loss: 1.8200, val loss: 1.2540
0.92327223906647
Model improve: 0.9219 -> 0.9233
Epoch: 14/20
Train loss: 1.7958, val loss: 1.2191
0.924550735386486
Model improve: 0.9233 -> 0.9246
Epoch: 15/20
Train loss: 1.7114, val loss: 1.1976
0.9244609896306599
Epoch: 16/20
Train loss: 1.7316, val loss: 1.2004
0.9260746132272802
Model improve: 0.9246 -> 0.9261
Epoch: 17/20
Train loss: 1.5823, val loss: 1.2065
0.926559625661062
Model improve: 0.9261 -> 0.9266
Epoch: 18/20
Train loss: 1.6607, val loss: 1.2221
0.9269603023214016
Model improve: 0.9266 -> 0.9270
Epoch: 19/20
Train loss: 1.6820, val loss: 1.2103
0.9267099061250321
Epoch: 20/20
Train loss: 1.6824, val loss: 1.2006
0.9271155114561285
Model improve: 0.9270 -> 0.9271
Date :04/17/2023, 11:08:35
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
Date :04/17/2023, 11:24:03
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
Date :04/17/2023, 11:24:29
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/20
Date :04/17/2023, 11:27:07
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/20
Train loss: 7.8367, val loss: 2.3625
0.8371274621978066
Model improve: 0.0000 -> 0.8371
Epoch: 2/20
Train loss: 3.2650, val loss: 1.7967
0.8840690423578115
Model improve: 0.8371 -> 0.8841
Epoch: 3/20
Train loss: 2.9056, val loss: 1.7100
0.8905611767682634
Model improve: 0.8841 -> 0.8906
Epoch: 4/20
Train loss: 2.5474, val loss: 1.5479
0.9023885746873928
Model improve: 0.8906 -> 0.9024
Epoch: 5/20
Train loss: 2.5062, val loss: 1.5062
0.9038903851028804
Model improve: 0.9024 -> 0.9039
Epoch: 6/20
Train loss: 2.3626, val loss: 1.4526
0.90882268326587
Model improve: 0.9039 -> 0.9088
Epoch: 7/20
Train loss: 2.1822, val loss: 1.4612
0.9125671999698163
Model improve: 0.9088 -> 0.9126
Epoch: 8/20
Date :04/17/2023, 12:11:22
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/20
Train loss: 7.6610, val loss: 2.3395
0.8368216809820402
Model improve: 0.0000 -> 0.8368
Epoch: 2/20
Train loss: 3.2537, val loss: 1.8072
0.8832782708827545
Model improve: 0.8368 -> 0.8833
Epoch: 3/20
Train loss: 2.8954, val loss: 1.6841
0.8905985095806449
Model improve: 0.8833 -> 0.8906
Epoch: 4/20
Train loss: 2.5330, val loss: 1.5507
0.9041391020157946
Model improve: 0.8906 -> 0.9041
Epoch: 5/20
Train loss: 2.4941, val loss: 1.5090
0.9039067986246302
Epoch: 6/20
Train loss: 2.3453, val loss: 1.4618
0.9078615140020981
Model improve: 0.9041 -> 0.9079
Epoch: 7/20
Train loss: 2.1650, val loss: 1.4397
0.9138735793060461
Model improve: 0.9079 -> 0.9139
Epoch: 8/20
Train loss: 2.1315, val loss: 1.4445
0.9142297039978196
Model improve: 0.9139 -> 0.9142
Epoch: 9/20
Train loss: 2.0016, val loss: 1.3570
0.9164452100783312
Model improve: 0.9142 -> 0.9164
Epoch: 10/20
Train loss: 2.0646, val loss: 1.3546
0.9182518415335179
Model improve: 0.9164 -> 0.9183
Epoch: 11/20
Train loss: 1.9275, val loss: 1.2718
0.9210597890502272
Model improve: 0.9183 -> 0.9211
Epoch: 12/20
Train loss: 1.8382, val loss: 1.2524
0.9214686052338162
Model improve: 0.9211 -> 0.9215
Epoch: 13/20
Train loss: 1.8026, val loss: 1.2561
0.921357513499703
Epoch: 14/20
Train loss: 1.7765, val loss: 1.2346
0.9204701928052093
Epoch: 15/20
Train loss: 1.6904, val loss: 1.1963
0.9240022086887114
Model improve: 0.9215 -> 0.9240
Epoch: 16/20
Train loss: 1.7079, val loss: 1.2002
0.9244266598331609
Model improve: 0.9240 -> 0.9244
Epoch: 17/20
Train loss: 1.5642, val loss: 1.2165
0.9251764565901308
Model improve: 0.9244 -> 0.9252
Epoch: 18/20
Date :04/17/2023, 13:51:40
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Train loss: 7.6249, val loss: 2.3073
0.8418661731682415
Model improve: 0.0000 -> 0.8419
Epoch: 2/25
Train loss: 3.2314, val loss: 1.8172
0.8855334135092442
Model improve: 0.8419 -> 0.8855
Epoch: 3/25
Train loss: 2.8771, val loss: 1.6630
0.8934742056838186
Model improve: 0.8855 -> 0.8935
Epoch: 4/25
Train loss: 2.5277, val loss: 1.5380
0.9039481043098453
Model improve: 0.8935 -> 0.9039
Epoch: 5/25
Train loss: 2.4909, val loss: 1.5198
0.9045408984759227
Model improve: 0.9039 -> 0.9045
Epoch: 6/25
Train loss: 2.3628, val loss: 1.4531
0.9073844335244257
Model improve: 0.9045 -> 0.9074
Epoch: 7/25
Train loss: 2.1826, val loss: 1.4556
0.9116217213267052
Model improve: 0.9074 -> 0.9116
Epoch: 8/25
Train loss: 2.1570, val loss: 1.4184
0.912700993862909
Model improve: 0.9116 -> 0.9127
Epoch: 9/25
Train loss: 2.0317, val loss: 1.4160
0.9126564374352193
Epoch: 10/25
Train loss: 2.0985, val loss: 1.3812
0.9160803574021419
Model improve: 0.9127 -> 0.9161
Epoch: 11/25
Train loss: 1.9729, val loss: 1.3131
0.9201337572848871
Model improve: 0.9161 -> 0.9201
Epoch: 12/25
Train loss: 1.8840, val loss: 1.2957
0.919676645373416
Epoch: 13/25
Train loss: 1.8470, val loss: 1.2702
0.9214394926642997
Model improve: 0.9201 -> 0.9214
Epoch: 14/25
Train loss: 1.8178, val loss: 1.2738
0.9207818659981731
Epoch: 15/25
Train loss: 1.7216, val loss: 1.2212
0.9237393248687481
Model improve: 0.9214 -> 0.9237
Epoch: 16/25
Train loss: 1.7352, val loss: 1.2209
0.9238846968531109
Model improve: 0.9237 -> 0.9239
Epoch: 17/25
Train loss: 1.5816, val loss: 1.2254
0.9277602852182744
Model improve: 0.9239 -> 0.9278
Epoch: 18/25
Train loss: 1.6384, val loss: 1.2071
0.9271743774179533
Epoch: 19/25
Train loss: 1.6382, val loss: 1.2100
0.9274079100928742
Epoch: 20/25
Train loss: 1.6240, val loss: 1.2101
0.9288090048306608
Model improve: 0.9278 -> 0.9288
Epoch: 21/25
Train loss: 1.5856, val loss: 1.1860
0.9283799827664563
Epoch: 22/25
Train loss: 1.6026, val loss: 1.1890
0.9283915088663535
Epoch: 23/25
Train loss: 1.5732, val loss: 1.1861
0.9285138775121031
Epoch: 24/25
Train loss: 1.6246, val loss: 1.1912
0.9289997419240252
Model improve: 0.9288 -> 0.9290
Epoch: 25/25
Train loss: 1.5702, val loss: 1.1775
0.9292678214153184
Model improve: 0.9290 -> 0.9293
Date :04/17/2023, 16:07:47
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Train loss: 7.6213, val loss: 2.3413
0.8393786237597631
Model improve: 0.0000 -> 0.8394
Epoch: 2/25
Train loss: 3.2410, val loss: 1.8298
0.8835262510084637
Model improve: 0.8394 -> 0.8835
Epoch: 3/25
Train loss: 2.8844, val loss: 1.6729
0.8925868011425093
Model improve: 0.8835 -> 0.8926
Epoch: 4/25
Train loss: 2.5384, val loss: 1.5379
0.9048049244727226
Model improve: 0.8926 -> 0.9048
Epoch: 5/25
Train loss: 2.4939, val loss: 1.5127
0.9034773809259534
Epoch: 6/25
Train loss: 2.3633, val loss: 1.4602
0.9085383958819223
Model improve: 0.9048 -> 0.9085
Epoch: 7/25
Date :04/17/2023, 09:50:04
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Train loss: 7.6188, val loss: 2.3401
0.8393708701700506
Model improve: 0.0000 -> 0.8394
Epoch: 2/25
Train loss: 3.2413, val loss: 1.8483
0.8831180505835022
Model improve: 0.8394 -> 0.8831
Epoch: 3/25
Train loss: 2.8825, val loss: 1.7074
0.8917348732602335
Model improve: 0.8831 -> 0.8917
Epoch: 4/25
Train loss: 2.5403, val loss: 1.5398
0.9035903379963447
Model improve: 0.8917 -> 0.9036
Epoch: 5/25
Train loss: 2.4922, val loss: 1.5666
0.9018998212464105
Epoch: 6/25
Train loss: 2.3610, val loss: 1.4546
0.9085894616736171
Model improve: 0.9036 -> 0.9086
Epoch: 7/25
Train loss: 2.1898, val loss: 1.4634
0.9129813526938045
Model improve: 0.9086 -> 0.9130
Epoch: 8/25
Train loss: 2.1577, val loss: 1.4739
0.9121694220018154
Epoch: 9/25
Train loss: 2.0307, val loss: 1.3963
0.9149839512336391
Model improve: 0.9130 -> 0.9150
Epoch: 10/25
Train loss: 2.0995, val loss: 1.4002
0.9129637517492551
Epoch: 11/25
Train loss: 1.9732, val loss: 1.3532
0.9174880348056671
Model improve: 0.9150 -> 0.9175
Epoch: 12/25
Train loss: 1.8799, val loss: 1.3075
0.9190785163812785
Model improve: 0.9175 -> 0.9191
Epoch: 13/25
Train loss: 1.8417, val loss: 1.2919
0.921544148248704
Model improve: 0.9191 -> 0.9215
Epoch: 14/25
Train loss: 1.8213, val loss: 1.2670
0.921352529327293
Epoch: 15/25
Train loss: 1.7217, val loss: 1.2349
0.9251121907091749
Model improve: 0.9215 -> 0.9251
Epoch: 16/25
Train loss: 1.7337, val loss: 1.2536
0.9232289168344028
Epoch: 17/25
Train loss: 1.5770, val loss: 1.2469
0.9260058962152213
Model improve: 0.9251 -> 0.9260
Epoch: 18/25
Train loss: 1.6395, val loss: 1.2350
0.9225769644253602
Epoch: 19/25
Train loss: 1.6406, val loss: 1.2107
0.9253431923362769
Epoch: 20/25
Train loss: 1.6249, val loss: 1.2293
0.9269105672828255
Model improve: 0.9260 -> 0.9269
Epoch: 21/25
Date :04/17/2023, 11:06:26
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.0
thrupsample: 100
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
30278
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Train loss: 6.3248, val loss: 2.0870
0.8699143090339676
Model improve: 0.0000 -> 0.8699
Epoch: 2/25
Train loss: 2.9027, val loss: 1.8148
0.8889572603484335
Model improve: 0.8699 -> 0.8890
Epoch: 3/25
Train loss: 2.4992, val loss: 1.6278
0.9006356991627051
Model improve: 0.8890 -> 0.9006
Epoch: 4/25
Train loss: 2.3414, val loss: 1.5471
0.906495353675878
Model improve: 0.9006 -> 0.9065
Epoch: 5/25
Train loss: 2.2116, val loss: 1.5512
0.9087134921321851
Model improve: 0.9065 -> 0.9087
Epoch: 6/25
Train loss: 2.0743, val loss: 1.5344
0.911583520901553
Model improve: 0.9087 -> 0.9116
Epoch: 7/25
Train loss: 2.1089, val loss: 1.4649
0.9106719176087058
Epoch: 8/25
Train loss: 1.9404, val loss: 1.4281
0.9124369819353627
Model improve: 0.9116 -> 0.9124
Epoch: 9/25
Train loss: 1.8867, val loss: 1.3828
0.9148955888171205
Model improve: 0.9124 -> 0.9149
Epoch: 10/25
Train loss: 1.8157, val loss: 1.3306
0.9162761464529785
Model improve: 0.9149 -> 0.9163
Epoch: 11/25
Train loss: 1.7257, val loss: 1.4225
0.9136113841818866
Epoch: 12/25
Train loss: 1.7458, val loss: 1.3530
0.9146561957642951
Epoch: 13/25
Date :04/18/2023, 14:10:31
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.0
thrupsample: 100
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
30278
conv_stem: 21.30% sparsity
blocks.0.0.conv: 29.26% sparsity
blocks.0.1.conv: 28.53% sparsity
blocks.1.0.conv_exp: 28.79% sparsity
blocks.1.0.conv_pwl: 24.02% sparsity
blocks.1.1.conv_exp: 27.30% sparsity
blocks.1.1.conv_pwl: 25.31% sparsity
blocks.1.2.conv_exp: 26.93% sparsity
blocks.1.2.conv_pwl: 25.60% sparsity
blocks.1.3.conv_exp: 27.26% sparsity
blocks.1.3.conv_pwl: 25.73% sparsity
blocks.2.0.conv_exp: 29.01% sparsity
blocks.2.0.conv_pwl: 26.61% sparsity
blocks.2.1.conv_exp: 27.19% sparsity
blocks.2.1.conv_pwl: 25.94% sparsity
blocks.2.2.conv_exp: 26.91% sparsity
blocks.2.2.conv_pwl: 25.71% sparsity
blocks.2.3.conv_exp: 27.24% sparsity
blocks.2.3.conv_pwl: 26.19% sparsity
blocks.3.0.conv_pw: 27.01% sparsity
blocks.3.0.conv_dw: 12.63% sparsity
blocks.3.0.se.conv_reduce: 65.77% sparsity
blocks.3.0.se.conv_expand: 92.36% sparsity
blocks.3.0.conv_pwl: 26.53% sparsity
blocks.3.1.conv_pw: 27.46% sparsity
blocks.3.1.conv_dw: 25.56% sparsity
blocks.3.1.se.conv_reduce: 35.46% sparsity
blocks.3.1.se.conv_expand: 28.11% sparsity
blocks.3.1.conv_pwl: 27.53% sparsity
blocks.3.2.conv_pw: 27.29% sparsity
blocks.3.2.conv_dw: 24.35% sparsity
blocks.3.2.se.conv_reduce: 36.16% sparsity
blocks.3.2.se.conv_expand: 26.82% sparsity
blocks.3.2.conv_pwl: 26.89% sparsity
blocks.3.3.conv_pw: 27.17% sparsity
blocks.3.3.conv_dw: 25.52% sparsity
blocks.3.3.se.conv_reduce: 31.38% sparsity
blocks.3.3.se.conv_expand: 25.65% sparsity
blocks.3.3.conv_pwl: 26.81% sparsity
blocks.3.4.conv_pw: 27.82% sparsity
blocks.3.4.conv_dw: 26.11% sparsity
blocks.3.4.se.conv_reduce: 33.86% sparsity
blocks.3.4.se.conv_expand: 31.67% sparsity
blocks.3.4.conv_pwl: 28.56% sparsity
blocks.3.5.conv_pw: 27.83% sparsity
blocks.3.5.conv_dw: 26.11% sparsity
blocks.3.5.se.conv_reduce: 34.84% sparsity
blocks.3.5.se.conv_expand: 27.60% sparsity
blocks.3.5.conv_pwl: 28.24% sparsity
blocks.4.0.conv_pw: 27.43% sparsity
blocks.4.0.conv_dw: 31.76% sparsity
blocks.4.0.se.conv_reduce: 32.19% sparsity
blocks.4.0.se.conv_expand: 26.09% sparsity
blocks.4.0.conv_pwl: 26.65% sparsity
blocks.4.1.conv_pw: 27.20% sparsity
blocks.4.1.conv_dw: 24.61% sparsity
blocks.4.1.se.conv_reduce: 34.21% sparsity
blocks.4.1.se.conv_expand: 27.85% sparsity
blocks.4.1.conv_pwl: 26.74% sparsity
blocks.4.2.conv_pw: 27.22% sparsity
blocks.4.2.conv_dw: 25.23% sparsity
blocks.4.2.se.conv_reduce: 34.20% sparsity
blocks.4.2.se.conv_expand: 27.93% sparsity
blocks.4.2.conv_pwl: 26.75% sparsity
blocks.4.3.conv_pw: 27.21% sparsity
blocks.4.3.conv_dw: 25.30% sparsity
blocks.4.3.se.conv_reduce: 32.51% sparsity
blocks.4.3.se.conv_expand: 27.93% sparsity
blocks.4.3.conv_pwl: 26.98% sparsity
blocks.4.4.conv_pw: 27.16% sparsity
blocks.4.4.conv_dw: 24.79% sparsity
blocks.4.4.se.conv_reduce: 35.25% sparsity
blocks.4.4.se.conv_expand: 29.12% sparsity
blocks.4.4.conv_pwl: 26.83% sparsity
blocks.4.5.conv_pw: 27.54% sparsity
blocks.4.5.conv_dw: 25.08% sparsity
blocks.4.5.se.conv_reduce: 35.28% sparsity
blocks.4.5.se.conv_expand: 29.24% sparsity
blocks.4.5.conv_pwl: 27.28% sparsity
blocks.4.6.conv_pw: 27.46% sparsity
blocks.4.6.conv_dw: 25.53% sparsity
blocks.4.6.se.conv_reduce: 34.55% sparsity
blocks.4.6.se.conv_expand: 27.40% sparsity
blocks.4.6.conv_pwl: 26.82% sparsity
blocks.4.7.conv_pw: 27.54% sparsity
blocks.4.7.conv_dw: 26.22% sparsity
blocks.4.7.se.conv_reduce: 32.32% sparsity
blocks.4.7.se.conv_expand: 27.38% sparsity
blocks.4.7.conv_pwl: 26.84% sparsity
blocks.4.8.conv_pw: 27.48% sparsity
blocks.4.8.conv_dw: 25.37% sparsity
blocks.4.8.se.conv_reduce: 32.85% sparsity
blocks.4.8.se.conv_expand: 26.74% sparsity
blocks.4.8.conv_pwl: 26.98% sparsity
blocks.5.0.conv_pw: 26.90% sparsity
blocks.5.0.conv_dw: 20.09% sparsity
blocks.5.0.se.conv_reduce: 84.99% sparsity
blocks.5.0.se.conv_expand: 87.15% sparsity
blocks.5.0.conv_pwl: 27.50% sparsity
blocks.5.1.conv_pw: 27.09% sparsity
blocks.5.1.conv_dw: 25.73% sparsity
blocks.5.1.se.conv_reduce: 31.42% sparsity
blocks.5.1.se.conv_expand: 26.72% sparsity
blocks.5.1.conv_pwl: 26.45% sparsity
blocks.5.2.conv_pw: 27.03% sparsity
blocks.5.2.conv_dw: 26.16% sparsity
blocks.5.2.se.conv_reduce: 31.06% sparsity
blocks.5.2.se.conv_expand: 26.40% sparsity
blocks.5.2.conv_pwl: 26.38% sparsity
blocks.5.3.conv_pw: 26.99% sparsity
blocks.5.3.conv_dw: 25.74% sparsity
blocks.5.3.se.conv_reduce: 31.08% sparsity
blocks.5.3.se.conv_expand: 26.57% sparsity
blocks.5.3.conv_pwl: 26.41% sparsity
blocks.5.4.conv_pw: 26.87% sparsity
blocks.5.4.conv_dw: 27.00% sparsity
blocks.5.4.se.conv_reduce: 30.71% sparsity
blocks.5.4.se.conv_expand: 26.52% sparsity
blocks.5.4.conv_pwl: 26.46% sparsity
blocks.5.5.conv_pw: 26.98% sparsity
blocks.5.5.conv_dw: 27.18% sparsity
blocks.5.5.se.conv_reduce: 29.90% sparsity
blocks.5.5.se.conv_expand: 26.28% sparsity
blocks.5.5.conv_pwl: 26.50% sparsity
blocks.5.6.conv_pw: 26.96% sparsity
blocks.5.6.conv_dw: 27.03% sparsity
blocks.5.6.se.conv_reduce: 29.88% sparsity
blocks.5.6.se.conv_expand: 26.63% sparsity
blocks.5.6.conv_pwl: 26.40% sparsity
blocks.5.7.conv_pw: 27.03% sparsity
blocks.5.7.conv_dw: 27.02% sparsity
blocks.5.7.se.conv_reduce: 29.07% sparsity
blocks.5.7.se.conv_expand: 26.19% sparsity
blocks.5.7.conv_pwl: 26.33% sparsity
blocks.5.8.conv_pw: 27.02% sparsity
blocks.5.8.conv_dw: 26.46% sparsity
blocks.5.8.se.conv_reduce: 28.51% sparsity
blocks.5.8.se.conv_expand: 26.08% sparsity
blocks.5.8.conv_pwl: 26.35% sparsity
blocks.5.9.conv_pw: 26.98% sparsity
blocks.5.9.conv_dw: 26.97% sparsity
blocks.5.9.se.conv_reduce: 29.09% sparsity
blocks.5.9.se.conv_expand: 25.82% sparsity
blocks.5.9.conv_pwl: 26.49% sparsity
blocks.5.10.conv_pw: 27.06% sparsity
blocks.5.10.conv_dw: 26.97% sparsity
blocks.5.10.se.conv_reduce: 29.19% sparsity
blocks.5.10.se.conv_expand: 26.18% sparsity
blocks.5.10.conv_pwl: 26.44% sparsity
blocks.5.11.conv_pw: 27.36% sparsity
blocks.5.11.conv_dw: 26.77% sparsity
blocks.5.11.se.conv_reduce: 30.08% sparsity
blocks.5.11.se.conv_expand: 26.05% sparsity
blocks.5.11.conv_pwl: 26.63% sparsity
blocks.5.12.conv_pw: 27.56% sparsity
blocks.5.12.conv_dw: 25.74% sparsity
blocks.5.12.se.conv_reduce: 29.94% sparsity
blocks.5.12.se.conv_expand: 25.92% sparsity
blocks.5.12.conv_pwl: 26.64% sparsity
blocks.5.13.conv_pw: 27.86% sparsity
blocks.5.13.conv_dw: 26.06% sparsity
blocks.5.13.se.conv_reduce: 31.74% sparsity
blocks.5.13.se.conv_expand: 26.39% sparsity
blocks.5.13.conv_pwl: 27.14% sparsity
blocks.5.14.conv_pw: 28.07% sparsity
blocks.5.14.conv_dw: 26.04% sparsity
blocks.5.14.se.conv_reduce: 31.71% sparsity
blocks.5.14.se.conv_expand: 24.53% sparsity
blocks.5.14.conv_pwl: 26.79% sparsity
conv_head: 26.01% sparsity
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Date :04/18/2023, 14:11:32
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.0001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
conv_stem: 21.30% sparsity
blocks.0.0.conv: 29.26% sparsity
blocks.0.1.conv: 28.53% sparsity
blocks.1.0.conv_exp: 28.79% sparsity
blocks.1.0.conv_pwl: 24.02% sparsity
blocks.1.1.conv_exp: 27.30% sparsity
blocks.1.1.conv_pwl: 25.31% sparsity
blocks.1.2.conv_exp: 26.93% sparsity
blocks.1.2.conv_pwl: 25.60% sparsity
blocks.1.3.conv_exp: 27.26% sparsity
blocks.1.3.conv_pwl: 25.73% sparsity
blocks.2.0.conv_exp: 29.01% sparsity
blocks.2.0.conv_pwl: 26.61% sparsity
blocks.2.1.conv_exp: 27.19% sparsity
blocks.2.1.conv_pwl: 25.94% sparsity
blocks.2.2.conv_exp: 26.91% sparsity
blocks.2.2.conv_pwl: 25.71% sparsity
blocks.2.3.conv_exp: 27.24% sparsity
blocks.2.3.conv_pwl: 26.19% sparsity
blocks.3.0.conv_pw: 27.01% sparsity
blocks.3.0.conv_dw: 12.63% sparsity
blocks.3.0.se.conv_reduce: 65.77% sparsity
blocks.3.0.se.conv_expand: 92.36% sparsity
blocks.3.0.conv_pwl: 26.53% sparsity
blocks.3.1.conv_pw: 27.46% sparsity
blocks.3.1.conv_dw: 25.56% sparsity
blocks.3.1.se.conv_reduce: 35.46% sparsity
blocks.3.1.se.conv_expand: 28.11% sparsity
blocks.3.1.conv_pwl: 27.53% sparsity
blocks.3.2.conv_pw: 27.29% sparsity
blocks.3.2.conv_dw: 24.35% sparsity
blocks.3.2.se.conv_reduce: 36.16% sparsity
blocks.3.2.se.conv_expand: 26.82% sparsity
blocks.3.2.conv_pwl: 26.89% sparsity
blocks.3.3.conv_pw: 27.17% sparsity
blocks.3.3.conv_dw: 25.52% sparsity
blocks.3.3.se.conv_reduce: 31.38% sparsity
blocks.3.3.se.conv_expand: 25.65% sparsity
blocks.3.3.conv_pwl: 26.81% sparsity
blocks.3.4.conv_pw: 27.82% sparsity
blocks.3.4.conv_dw: 26.11% sparsity
blocks.3.4.se.conv_reduce: 33.86% sparsity
blocks.3.4.se.conv_expand: 31.67% sparsity
blocks.3.4.conv_pwl: 28.56% sparsity
blocks.3.5.conv_pw: 27.83% sparsity
blocks.3.5.conv_dw: 26.11% sparsity
blocks.3.5.se.conv_reduce: 34.84% sparsity
blocks.3.5.se.conv_expand: 27.60% sparsity
blocks.3.5.conv_pwl: 28.24% sparsity
blocks.4.0.conv_pw: 27.43% sparsity
blocks.4.0.conv_dw: 31.76% sparsity
blocks.4.0.se.conv_reduce: 32.19% sparsity
blocks.4.0.se.conv_expand: 26.09% sparsity
blocks.4.0.conv_pwl: 26.65% sparsity
blocks.4.1.conv_pw: 27.20% sparsity
blocks.4.1.conv_dw: 24.61% sparsity
blocks.4.1.se.conv_reduce: 34.21% sparsity
blocks.4.1.se.conv_expand: 27.85% sparsity
blocks.4.1.conv_pwl: 26.74% sparsity
blocks.4.2.conv_pw: 27.22% sparsity
blocks.4.2.conv_dw: 25.23% sparsity
blocks.4.2.se.conv_reduce: 34.20% sparsity
blocks.4.2.se.conv_expand: 27.93% sparsity
blocks.4.2.conv_pwl: 26.75% sparsity
blocks.4.3.conv_pw: 27.21% sparsity
blocks.4.3.conv_dw: 25.30% sparsity
blocks.4.3.se.conv_reduce: 32.51% sparsity
blocks.4.3.se.conv_expand: 27.93% sparsity
blocks.4.3.conv_pwl: 26.98% sparsity
blocks.4.4.conv_pw: 27.16% sparsity
blocks.4.4.conv_dw: 24.79% sparsity
blocks.4.4.se.conv_reduce: 35.25% sparsity
blocks.4.4.se.conv_expand: 29.12% sparsity
blocks.4.4.conv_pwl: 26.83% sparsity
blocks.4.5.conv_pw: 27.54% sparsity
blocks.4.5.conv_dw: 25.08% sparsity
blocks.4.5.se.conv_reduce: 35.28% sparsity
blocks.4.5.se.conv_expand: 29.24% sparsity
blocks.4.5.conv_pwl: 27.28% sparsity
blocks.4.6.conv_pw: 27.46% sparsity
blocks.4.6.conv_dw: 25.53% sparsity
blocks.4.6.se.conv_reduce: 34.55% sparsity
blocks.4.6.se.conv_expand: 27.40% sparsity
blocks.4.6.conv_pwl: 26.82% sparsity
blocks.4.7.conv_pw: 27.54% sparsity
blocks.4.7.conv_dw: 26.22% sparsity
blocks.4.7.se.conv_reduce: 32.32% sparsity
blocks.4.7.se.conv_expand: 27.38% sparsity
blocks.4.7.conv_pwl: 26.84% sparsity
blocks.4.8.conv_pw: 27.48% sparsity
blocks.4.8.conv_dw: 25.37% sparsity
blocks.4.8.se.conv_reduce: 32.85% sparsity
blocks.4.8.se.conv_expand: 26.74% sparsity
blocks.4.8.conv_pwl: 26.98% sparsity
blocks.5.0.conv_pw: 26.90% sparsity
blocks.5.0.conv_dw: 20.09% sparsity
blocks.5.0.se.conv_reduce: 84.99% sparsity
blocks.5.0.se.conv_expand: 87.15% sparsity
blocks.5.0.conv_pwl: 27.50% sparsity
blocks.5.1.conv_pw: 27.09% sparsity
blocks.5.1.conv_dw: 25.73% sparsity
blocks.5.1.se.conv_reduce: 31.42% sparsity
blocks.5.1.se.conv_expand: 26.72% sparsity
blocks.5.1.conv_pwl: 26.45% sparsity
blocks.5.2.conv_pw: 27.03% sparsity
blocks.5.2.conv_dw: 26.16% sparsity
blocks.5.2.se.conv_reduce: 31.06% sparsity
blocks.5.2.se.conv_expand: 26.40% sparsity
blocks.5.2.conv_pwl: 26.38% sparsity
blocks.5.3.conv_pw: 26.99% sparsity
blocks.5.3.conv_dw: 25.74% sparsity
blocks.5.3.se.conv_reduce: 31.08% sparsity
blocks.5.3.se.conv_expand: 26.57% sparsity
blocks.5.3.conv_pwl: 26.41% sparsity
blocks.5.4.conv_pw: 26.87% sparsity
blocks.5.4.conv_dw: 27.00% sparsity
blocks.5.4.se.conv_reduce: 30.71% sparsity
blocks.5.4.se.conv_expand: 26.52% sparsity
blocks.5.4.conv_pwl: 26.46% sparsity
blocks.5.5.conv_pw: 26.98% sparsity
blocks.5.5.conv_dw: 27.18% sparsity
blocks.5.5.se.conv_reduce: 29.90% sparsity
blocks.5.5.se.conv_expand: 26.28% sparsity
blocks.5.5.conv_pwl: 26.50% sparsity
blocks.5.6.conv_pw: 26.96% sparsity
blocks.5.6.conv_dw: 27.03% sparsity
blocks.5.6.se.conv_reduce: 29.88% sparsity
blocks.5.6.se.conv_expand: 26.63% sparsity
blocks.5.6.conv_pwl: 26.40% sparsity
blocks.5.7.conv_pw: 27.03% sparsity
blocks.5.7.conv_dw: 27.02% sparsity
blocks.5.7.se.conv_reduce: 29.07% sparsity
blocks.5.7.se.conv_expand: 26.19% sparsity
blocks.5.7.conv_pwl: 26.33% sparsity
blocks.5.8.conv_pw: 27.02% sparsity
blocks.5.8.conv_dw: 26.46% sparsity
blocks.5.8.se.conv_reduce: 28.51% sparsity
blocks.5.8.se.conv_expand: 26.08% sparsity
blocks.5.8.conv_pwl: 26.35% sparsity
blocks.5.9.conv_pw: 26.98% sparsity
blocks.5.9.conv_dw: 26.97% sparsity
blocks.5.9.se.conv_reduce: 29.09% sparsity
blocks.5.9.se.conv_expand: 25.82% sparsity
blocks.5.9.conv_pwl: 26.49% sparsity
blocks.5.10.conv_pw: 27.06% sparsity
blocks.5.10.conv_dw: 26.97% sparsity
blocks.5.10.se.conv_reduce: 29.19% sparsity
blocks.5.10.se.conv_expand: 26.18% sparsity
blocks.5.10.conv_pwl: 26.44% sparsity
blocks.5.11.conv_pw: 27.36% sparsity
blocks.5.11.conv_dw: 26.77% sparsity
blocks.5.11.se.conv_reduce: 30.08% sparsity
blocks.5.11.se.conv_expand: 26.05% sparsity
blocks.5.11.conv_pwl: 26.63% sparsity
blocks.5.12.conv_pw: 27.56% sparsity
blocks.5.12.conv_dw: 25.74% sparsity
blocks.5.12.se.conv_reduce: 29.94% sparsity
blocks.5.12.se.conv_expand: 25.92% sparsity
blocks.5.12.conv_pwl: 26.64% sparsity
blocks.5.13.conv_pw: 27.86% sparsity
blocks.5.13.conv_dw: 26.06% sparsity
blocks.5.13.se.conv_reduce: 31.74% sparsity
blocks.5.13.se.conv_expand: 26.39% sparsity
blocks.5.13.conv_pwl: 27.14% sparsity
blocks.5.14.conv_pw: 28.07% sparsity
blocks.5.14.conv_dw: 26.04% sparsity
blocks.5.14.se.conv_reduce: 31.71% sparsity
blocks.5.14.se.conv_expand: 24.53% sparsity
blocks.5.14.conv_pwl: 26.79% sparsity
conv_head: 26.01% sparsity
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0001
    lr: 0.0001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Train loss: 1.5813, val loss: 1.2136
0.928196212173646
Model improve: 0.0000 -> 0.9282
Epoch: 2/25
Train loss: 1.5075, val loss: 1.2012
0.9273455630095216
Epoch: 3/25
Train loss: 1.5729, val loss: 1.2011
0.9273598152647321
Epoch: 4/25
Date :04/18/2023, 14:30:13
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.0001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
conv_stem: 21.30% sparsity
blocks.0.0.conv: 29.26% sparsity
blocks.0.1.conv: 28.53% sparsity
blocks.1.0.conv_exp: 28.79% sparsity
blocks.1.0.conv_pwl: 24.02% sparsity
blocks.1.1.conv_exp: 27.30% sparsity
blocks.1.1.conv_pwl: 25.31% sparsity
blocks.1.2.conv_exp: 26.93% sparsity
blocks.1.2.conv_pwl: 25.60% sparsity
blocks.1.3.conv_exp: 27.26% sparsity
blocks.1.3.conv_pwl: 25.73% sparsity
blocks.2.0.conv_exp: 29.01% sparsity
blocks.2.0.conv_pwl: 26.61% sparsity
blocks.2.1.conv_exp: 27.19% sparsity
blocks.2.1.conv_pwl: 25.94% sparsity
blocks.2.2.conv_exp: 26.91% sparsity
blocks.2.2.conv_pwl: 25.71% sparsity
blocks.2.3.conv_exp: 27.24% sparsity
blocks.2.3.conv_pwl: 26.19% sparsity
blocks.3.0.conv_pw: 27.01% sparsity
blocks.3.0.conv_dw: 12.63% sparsity
blocks.3.0.se.conv_reduce: 65.77% sparsity
blocks.3.0.se.conv_expand: 92.36% sparsity
blocks.3.0.conv_pwl: 26.53% sparsity
blocks.3.1.conv_pw: 27.46% sparsity
blocks.3.1.conv_dw: 25.56% sparsity
blocks.3.1.se.conv_reduce: 35.46% sparsity
blocks.3.1.se.conv_expand: 28.11% sparsity
blocks.3.1.conv_pwl: 27.53% sparsity
blocks.3.2.conv_pw: 27.29% sparsity
blocks.3.2.conv_dw: 24.35% sparsity
blocks.3.2.se.conv_reduce: 36.16% sparsity
blocks.3.2.se.conv_expand: 26.82% sparsity
blocks.3.2.conv_pwl: 26.89% sparsity
blocks.3.3.conv_pw: 27.17% sparsity
blocks.3.3.conv_dw: 25.52% sparsity
blocks.3.3.se.conv_reduce: 31.38% sparsity
blocks.3.3.se.conv_expand: 25.65% sparsity
blocks.3.3.conv_pwl: 26.81% sparsity
blocks.3.4.conv_pw: 27.82% sparsity
blocks.3.4.conv_dw: 26.11% sparsity
blocks.3.4.se.conv_reduce: 33.86% sparsity
blocks.3.4.se.conv_expand: 31.67% sparsity
blocks.3.4.conv_pwl: 28.56% sparsity
blocks.3.5.conv_pw: 27.83% sparsity
blocks.3.5.conv_dw: 26.11% sparsity
blocks.3.5.se.conv_reduce: 34.84% sparsity
blocks.3.5.se.conv_expand: 27.60% sparsity
blocks.3.5.conv_pwl: 28.24% sparsity
blocks.4.0.conv_pw: 27.43% sparsity
blocks.4.0.conv_dw: 31.76% sparsity
blocks.4.0.se.conv_reduce: 32.19% sparsity
blocks.4.0.se.conv_expand: 26.09% sparsity
blocks.4.0.conv_pwl: 26.65% sparsity
blocks.4.1.conv_pw: 27.20% sparsity
blocks.4.1.conv_dw: 24.61% sparsity
blocks.4.1.se.conv_reduce: 34.21% sparsity
blocks.4.1.se.conv_expand: 27.85% sparsity
blocks.4.1.conv_pwl: 26.74% sparsity
blocks.4.2.conv_pw: 27.22% sparsity
blocks.4.2.conv_dw: 25.23% sparsity
blocks.4.2.se.conv_reduce: 34.20% sparsity
blocks.4.2.se.conv_expand: 27.93% sparsity
blocks.4.2.conv_pwl: 26.75% sparsity
blocks.4.3.conv_pw: 27.21% sparsity
blocks.4.3.conv_dw: 25.30% sparsity
blocks.4.3.se.conv_reduce: 32.51% sparsity
blocks.4.3.se.conv_expand: 27.93% sparsity
blocks.4.3.conv_pwl: 26.98% sparsity
blocks.4.4.conv_pw: 27.16% sparsity
blocks.4.4.conv_dw: 24.79% sparsity
blocks.4.4.se.conv_reduce: 35.25% sparsity
blocks.4.4.se.conv_expand: 29.12% sparsity
blocks.4.4.conv_pwl: 26.83% sparsity
blocks.4.5.conv_pw: 27.54% sparsity
blocks.4.5.conv_dw: 25.08% sparsity
blocks.4.5.se.conv_reduce: 35.28% sparsity
blocks.4.5.se.conv_expand: 29.24% sparsity
blocks.4.5.conv_pwl: 27.28% sparsity
blocks.4.6.conv_pw: 27.46% sparsity
blocks.4.6.conv_dw: 25.53% sparsity
blocks.4.6.se.conv_reduce: 34.55% sparsity
blocks.4.6.se.conv_expand: 27.40% sparsity
blocks.4.6.conv_pwl: 26.82% sparsity
blocks.4.7.conv_pw: 27.54% sparsity
blocks.4.7.conv_dw: 26.22% sparsity
blocks.4.7.se.conv_reduce: 32.32% sparsity
blocks.4.7.se.conv_expand: 27.38% sparsity
blocks.4.7.conv_pwl: 26.84% sparsity
blocks.4.8.conv_pw: 27.48% sparsity
blocks.4.8.conv_dw: 25.37% sparsity
blocks.4.8.se.conv_reduce: 32.85% sparsity
blocks.4.8.se.conv_expand: 26.74% sparsity
blocks.4.8.conv_pwl: 26.98% sparsity
blocks.5.0.conv_pw: 26.90% sparsity
blocks.5.0.conv_dw: 20.09% sparsity
blocks.5.0.se.conv_reduce: 84.99% sparsity
blocks.5.0.se.conv_expand: 87.15% sparsity
blocks.5.0.conv_pwl: 27.50% sparsity
blocks.5.1.conv_pw: 27.09% sparsity
blocks.5.1.conv_dw: 25.73% sparsity
blocks.5.1.se.conv_reduce: 31.42% sparsity
blocks.5.1.se.conv_expand: 26.72% sparsity
blocks.5.1.conv_pwl: 26.45% sparsity
blocks.5.2.conv_pw: 27.03% sparsity
blocks.5.2.conv_dw: 26.16% sparsity
blocks.5.2.se.conv_reduce: 31.06% sparsity
blocks.5.2.se.conv_expand: 26.40% sparsity
blocks.5.2.conv_pwl: 26.38% sparsity
blocks.5.3.conv_pw: 26.99% sparsity
blocks.5.3.conv_dw: 25.74% sparsity
blocks.5.3.se.conv_reduce: 31.08% sparsity
blocks.5.3.se.conv_expand: 26.57% sparsity
blocks.5.3.conv_pwl: 26.41% sparsity
blocks.5.4.conv_pw: 26.87% sparsity
blocks.5.4.conv_dw: 27.00% sparsity
blocks.5.4.se.conv_reduce: 30.71% sparsity
blocks.5.4.se.conv_expand: 26.52% sparsity
blocks.5.4.conv_pwl: 26.46% sparsity
blocks.5.5.conv_pw: 26.98% sparsity
blocks.5.5.conv_dw: 27.18% sparsity
blocks.5.5.se.conv_reduce: 29.90% sparsity
blocks.5.5.se.conv_expand: 26.28% sparsity
blocks.5.5.conv_pwl: 26.50% sparsity
blocks.5.6.conv_pw: 26.96% sparsity
blocks.5.6.conv_dw: 27.03% sparsity
blocks.5.6.se.conv_reduce: 29.88% sparsity
blocks.5.6.se.conv_expand: 26.63% sparsity
blocks.5.6.conv_pwl: 26.40% sparsity
blocks.5.7.conv_pw: 27.03% sparsity
blocks.5.7.conv_dw: 27.02% sparsity
blocks.5.7.se.conv_reduce: 29.07% sparsity
blocks.5.7.se.conv_expand: 26.19% sparsity
blocks.5.7.conv_pwl: 26.33% sparsity
blocks.5.8.conv_pw: 27.02% sparsity
blocks.5.8.conv_dw: 26.46% sparsity
blocks.5.8.se.conv_reduce: 28.51% sparsity
blocks.5.8.se.conv_expand: 26.08% sparsity
blocks.5.8.conv_pwl: 26.35% sparsity
blocks.5.9.conv_pw: 26.98% sparsity
blocks.5.9.conv_dw: 26.97% sparsity
blocks.5.9.se.conv_reduce: 29.09% sparsity
blocks.5.9.se.conv_expand: 25.82% sparsity
blocks.5.9.conv_pwl: 26.49% sparsity
blocks.5.10.conv_pw: 27.06% sparsity
blocks.5.10.conv_dw: 26.97% sparsity
blocks.5.10.se.conv_reduce: 29.19% sparsity
blocks.5.10.se.conv_expand: 26.18% sparsity
blocks.5.10.conv_pwl: 26.44% sparsity
blocks.5.11.conv_pw: 27.36% sparsity
blocks.5.11.conv_dw: 26.77% sparsity
blocks.5.11.se.conv_reduce: 30.08% sparsity
blocks.5.11.se.conv_expand: 26.05% sparsity
blocks.5.11.conv_pwl: 26.63% sparsity
blocks.5.12.conv_pw: 27.56% sparsity
blocks.5.12.conv_dw: 25.74% sparsity
blocks.5.12.se.conv_reduce: 29.94% sparsity
blocks.5.12.se.conv_expand: 25.92% sparsity
blocks.5.12.conv_pwl: 26.64% sparsity
blocks.5.13.conv_pw: 27.86% sparsity
blocks.5.13.conv_dw: 26.06% sparsity
blocks.5.13.se.conv_reduce: 31.74% sparsity
blocks.5.13.se.conv_expand: 26.39% sparsity
blocks.5.13.conv_pwl: 27.14% sparsity
blocks.5.14.conv_pw: 28.07% sparsity
blocks.5.14.conv_dw: 26.04% sparsity
blocks.5.14.se.conv_reduce: 31.71% sparsity
blocks.5.14.se.conv_expand: 24.53% sparsity
blocks.5.14.conv_pwl: 26.79% sparsity
conv_head: 26.01% sparsity
Date :04/18/2023, 14:33:21
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.0001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
conv_stem: 21.30% sparsity
blocks.0.0.conv: 29.26% sparsity
blocks.0.1.conv: 28.53% sparsity
blocks.1.0.conv_exp: 28.79% sparsity
blocks.1.0.conv_pwl: 24.02% sparsity
blocks.1.1.conv_exp: 27.30% sparsity
blocks.1.1.conv_pwl: 25.31% sparsity
blocks.1.2.conv_exp: 26.93% sparsity
blocks.1.2.conv_pwl: 25.60% sparsity
blocks.1.3.conv_exp: 27.26% sparsity
blocks.1.3.conv_pwl: 25.73% sparsity
blocks.2.0.conv_exp: 29.01% sparsity
blocks.2.0.conv_pwl: 26.61% sparsity
blocks.2.1.conv_exp: 27.19% sparsity
blocks.2.1.conv_pwl: 25.94% sparsity
blocks.2.2.conv_exp: 26.91% sparsity
blocks.2.2.conv_pwl: 25.71% sparsity
blocks.2.3.conv_exp: 27.24% sparsity
blocks.2.3.conv_pwl: 26.19% sparsity
blocks.3.0.conv_pw: 27.01% sparsity
blocks.3.0.conv_dw: 12.63% sparsity
blocks.3.0.se.conv_reduce: 65.77% sparsity
blocks.3.0.se.conv_expand: 92.36% sparsity
blocks.3.0.conv_pwl: 26.53% sparsity
blocks.3.1.conv_pw: 27.46% sparsity
blocks.3.1.conv_dw: 25.56% sparsity
blocks.3.1.se.conv_reduce: 35.46% sparsity
blocks.3.1.se.conv_expand: 28.11% sparsity
blocks.3.1.conv_pwl: 27.53% sparsity
blocks.3.2.conv_pw: 27.29% sparsity
blocks.3.2.conv_dw: 24.35% sparsity
blocks.3.2.se.conv_reduce: 36.16% sparsity
blocks.3.2.se.conv_expand: 26.82% sparsity
blocks.3.2.conv_pwl: 26.89% sparsity
blocks.3.3.conv_pw: 27.17% sparsity
blocks.3.3.conv_dw: 25.52% sparsity
blocks.3.3.se.conv_reduce: 31.38% sparsity
blocks.3.3.se.conv_expand: 25.65% sparsity
blocks.3.3.conv_pwl: 26.81% sparsity
blocks.3.4.conv_pw: 27.82% sparsity
blocks.3.4.conv_dw: 26.11% sparsity
blocks.3.4.se.conv_reduce: 33.86% sparsity
blocks.3.4.se.conv_expand: 31.67% sparsity
blocks.3.4.conv_pwl: 28.56% sparsity
blocks.3.5.conv_pw: 27.83% sparsity
blocks.3.5.conv_dw: 26.11% sparsity
blocks.3.5.se.conv_reduce: 34.84% sparsity
blocks.3.5.se.conv_expand: 27.60% sparsity
blocks.3.5.conv_pwl: 28.24% sparsity
blocks.4.0.conv_pw: 27.43% sparsity
blocks.4.0.conv_dw: 31.76% sparsity
blocks.4.0.se.conv_reduce: 32.19% sparsity
blocks.4.0.se.conv_expand: 26.09% sparsity
blocks.4.0.conv_pwl: 26.65% sparsity
blocks.4.1.conv_pw: 27.20% sparsity
blocks.4.1.conv_dw: 24.61% sparsity
blocks.4.1.se.conv_reduce: 34.21% sparsity
blocks.4.1.se.conv_expand: 27.85% sparsity
blocks.4.1.conv_pwl: 26.74% sparsity
blocks.4.2.conv_pw: 27.22% sparsity
blocks.4.2.conv_dw: 25.23% sparsity
blocks.4.2.se.conv_reduce: 34.20% sparsity
blocks.4.2.se.conv_expand: 27.93% sparsity
blocks.4.2.conv_pwl: 26.75% sparsity
blocks.4.3.conv_pw: 27.21% sparsity
blocks.4.3.conv_dw: 25.30% sparsity
blocks.4.3.se.conv_reduce: 32.51% sparsity
blocks.4.3.se.conv_expand: 27.93% sparsity
blocks.4.3.conv_pwl: 26.98% sparsity
blocks.4.4.conv_pw: 27.16% sparsity
blocks.4.4.conv_dw: 24.79% sparsity
blocks.4.4.se.conv_reduce: 35.25% sparsity
blocks.4.4.se.conv_expand: 29.12% sparsity
blocks.4.4.conv_pwl: 26.83% sparsity
blocks.4.5.conv_pw: 27.54% sparsity
blocks.4.5.conv_dw: 25.08% sparsity
blocks.4.5.se.conv_reduce: 35.28% sparsity
blocks.4.5.se.conv_expand: 29.24% sparsity
blocks.4.5.conv_pwl: 27.28% sparsity
blocks.4.6.conv_pw: 27.46% sparsity
blocks.4.6.conv_dw: 25.53% sparsity
blocks.4.6.se.conv_reduce: 34.55% sparsity
blocks.4.6.se.conv_expand: 27.40% sparsity
blocks.4.6.conv_pwl: 26.82% sparsity
blocks.4.7.conv_pw: 27.54% sparsity
blocks.4.7.conv_dw: 26.22% sparsity
blocks.4.7.se.conv_reduce: 32.32% sparsity
blocks.4.7.se.conv_expand: 27.38% sparsity
blocks.4.7.conv_pwl: 26.84% sparsity
blocks.4.8.conv_pw: 27.48% sparsity
blocks.4.8.conv_dw: 25.37% sparsity
blocks.4.8.se.conv_reduce: 32.85% sparsity
blocks.4.8.se.conv_expand: 26.74% sparsity
blocks.4.8.conv_pwl: 26.98% sparsity
blocks.5.0.conv_pw: 26.90% sparsity
blocks.5.0.conv_dw: 20.09% sparsity
blocks.5.0.se.conv_reduce: 84.99% sparsity
blocks.5.0.se.conv_expand: 87.15% sparsity
blocks.5.0.conv_pwl: 27.50% sparsity
blocks.5.1.conv_pw: 27.09% sparsity
blocks.5.1.conv_dw: 25.73% sparsity
blocks.5.1.se.conv_reduce: 31.42% sparsity
blocks.5.1.se.conv_expand: 26.72% sparsity
blocks.5.1.conv_pwl: 26.45% sparsity
blocks.5.2.conv_pw: 27.03% sparsity
blocks.5.2.conv_dw: 26.16% sparsity
blocks.5.2.se.conv_reduce: 31.06% sparsity
blocks.5.2.se.conv_expand: 26.40% sparsity
blocks.5.2.conv_pwl: 26.38% sparsity
blocks.5.3.conv_pw: 26.99% sparsity
blocks.5.3.conv_dw: 25.74% sparsity
blocks.5.3.se.conv_reduce: 31.08% sparsity
blocks.5.3.se.conv_expand: 26.57% sparsity
blocks.5.3.conv_pwl: 26.41% sparsity
blocks.5.4.conv_pw: 26.87% sparsity
blocks.5.4.conv_dw: 27.00% sparsity
blocks.5.4.se.conv_reduce: 30.71% sparsity
blocks.5.4.se.conv_expand: 26.52% sparsity
blocks.5.4.conv_pwl: 26.46% sparsity
blocks.5.5.conv_pw: 26.98% sparsity
blocks.5.5.conv_dw: 27.18% sparsity
blocks.5.5.se.conv_reduce: 29.90% sparsity
blocks.5.5.se.conv_expand: 26.28% sparsity
blocks.5.5.conv_pwl: 26.50% sparsity
blocks.5.6.conv_pw: 26.96% sparsity
blocks.5.6.conv_dw: 27.03% sparsity
blocks.5.6.se.conv_reduce: 29.88% sparsity
blocks.5.6.se.conv_expand: 26.63% sparsity
blocks.5.6.conv_pwl: 26.40% sparsity
blocks.5.7.conv_pw: 27.03% sparsity
blocks.5.7.conv_dw: 27.02% sparsity
blocks.5.7.se.conv_reduce: 29.07% sparsity
blocks.5.7.se.conv_expand: 26.19% sparsity
blocks.5.7.conv_pwl: 26.33% sparsity
blocks.5.8.conv_pw: 27.02% sparsity
blocks.5.8.conv_dw: 26.46% sparsity
blocks.5.8.se.conv_reduce: 28.51% sparsity
blocks.5.8.se.conv_expand: 26.08% sparsity
blocks.5.8.conv_pwl: 26.35% sparsity
blocks.5.9.conv_pw: 26.98% sparsity
blocks.5.9.conv_dw: 26.97% sparsity
blocks.5.9.se.conv_reduce: 29.09% sparsity
blocks.5.9.se.conv_expand: 25.82% sparsity
blocks.5.9.conv_pwl: 26.49% sparsity
blocks.5.10.conv_pw: 27.06% sparsity
blocks.5.10.conv_dw: 26.97% sparsity
blocks.5.10.se.conv_reduce: 29.19% sparsity
blocks.5.10.se.conv_expand: 26.18% sparsity
blocks.5.10.conv_pwl: 26.44% sparsity
blocks.5.11.conv_pw: 27.36% sparsity
blocks.5.11.conv_dw: 26.77% sparsity
blocks.5.11.se.conv_reduce: 30.08% sparsity
blocks.5.11.se.conv_expand: 26.05% sparsity
blocks.5.11.conv_pwl: 26.63% sparsity
blocks.5.12.conv_pw: 27.56% sparsity
blocks.5.12.conv_dw: 25.74% sparsity
blocks.5.12.se.conv_reduce: 29.94% sparsity
blocks.5.12.se.conv_expand: 25.92% sparsity
blocks.5.12.conv_pwl: 26.64% sparsity
blocks.5.13.conv_pw: 27.86% sparsity
blocks.5.13.conv_dw: 26.06% sparsity
blocks.5.13.se.conv_reduce: 31.74% sparsity
blocks.5.13.se.conv_expand: 26.39% sparsity
blocks.5.13.conv_pwl: 27.14% sparsity
blocks.5.14.conv_pw: 28.07% sparsity
blocks.5.14.conv_dw: 26.04% sparsity
blocks.5.14.se.conv_reduce: 31.71% sparsity
blocks.5.14.se.conv_expand: 24.53% sparsity
blocks.5.14.conv_pwl: 26.79% sparsity
conv_head: 26.01% sparsity
Date :04/18/2023, 14:34:31
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.0001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
conv_stem: 19.91% sparsity
blocks.0.0.conv: 29.22% sparsity
blocks.0.1.conv: 29.75% sparsity
blocks.1.0.conv_exp: 29.28% sparsity
blocks.1.0.conv_pwl: 23.83% sparsity
blocks.1.1.conv_exp: 27.86% sparsity
blocks.1.1.conv_pwl: 25.34% sparsity
blocks.1.2.conv_exp: 27.40% sparsity
blocks.1.2.conv_pwl: 26.15% sparsity
blocks.1.3.conv_exp: 27.49% sparsity
blocks.1.3.conv_pwl: 26.51% sparsity
blocks.2.0.conv_exp: 29.50% sparsity
blocks.2.0.conv_pwl: 26.25% sparsity
blocks.2.1.conv_exp: 27.91% sparsity
blocks.2.1.conv_pwl: 26.03% sparsity
blocks.2.2.conv_exp: 27.45% sparsity
blocks.2.2.conv_pwl: 26.38% sparsity
blocks.2.3.conv_exp: 27.72% sparsity
blocks.2.3.conv_pwl: 26.52% sparsity
blocks.3.0.conv_pw: 27.25% sparsity
blocks.3.0.conv_dw: 11.46% sparsity
blocks.3.0.se.conv_reduce: 69.53% sparsity
blocks.3.0.se.conv_expand: 91.75% sparsity
blocks.3.0.conv_pwl: 26.76% sparsity
blocks.3.1.conv_pw: 28.05% sparsity
blocks.3.1.conv_dw: 24.48% sparsity
blocks.3.1.se.conv_reduce: 36.71% sparsity
blocks.3.1.se.conv_expand: 28.63% sparsity
blocks.3.1.conv_pwl: 27.56% sparsity
blocks.3.2.conv_pw: 27.57% sparsity
blocks.3.2.conv_dw: 24.37% sparsity
blocks.3.2.se.conv_reduce: 36.05% sparsity
blocks.3.2.se.conv_expand: 28.04% sparsity
blocks.3.2.conv_pwl: 27.28% sparsity
blocks.3.3.conv_pw: 27.47% sparsity
blocks.3.3.conv_dw: 25.28% sparsity
blocks.3.3.se.conv_reduce: 31.71% sparsity
blocks.3.3.se.conv_expand: 25.96% sparsity
blocks.3.3.conv_pwl: 27.25% sparsity
blocks.3.4.conv_pw: 28.17% sparsity
blocks.3.4.conv_dw: 27.02% sparsity
blocks.3.4.se.conv_reduce: 34.65% sparsity
blocks.3.4.se.conv_expand: 32.06% sparsity
blocks.3.4.conv_pwl: 28.64% sparsity
blocks.3.5.conv_pw: 28.14% sparsity
blocks.3.5.conv_dw: 25.48% sparsity
blocks.3.5.se.conv_reduce: 34.30% sparsity
blocks.3.5.se.conv_expand: 28.16% sparsity
blocks.3.5.conv_pwl: 28.66% sparsity
blocks.4.0.conv_pw: 27.59% sparsity
blocks.4.0.conv_dw: 32.18% sparsity
blocks.4.0.se.conv_reduce: 31.86% sparsity
blocks.4.0.se.conv_expand: 26.24% sparsity
blocks.4.0.conv_pwl: 27.00% sparsity
blocks.4.1.conv_pw: 27.68% sparsity
blocks.4.1.conv_dw: 24.50% sparsity
blocks.4.1.se.conv_reduce: 34.68% sparsity
blocks.4.1.se.conv_expand: 28.59% sparsity
blocks.4.1.conv_pwl: 26.96% sparsity
blocks.4.2.conv_pw: 27.75% sparsity
blocks.4.2.conv_dw: 25.41% sparsity
blocks.4.2.se.conv_reduce: 34.78% sparsity
blocks.4.2.se.conv_expand: 28.65% sparsity
blocks.4.2.conv_pwl: 27.17% sparsity
blocks.4.3.conv_pw: 27.59% sparsity
blocks.4.3.conv_dw: 24.50% sparsity
blocks.4.3.se.conv_reduce: 33.59% sparsity
blocks.4.3.se.conv_expand: 28.96% sparsity
blocks.4.3.conv_pwl: 27.09% sparsity
blocks.4.4.conv_pw: 27.77% sparsity
blocks.4.4.conv_dw: 25.86% sparsity
blocks.4.4.se.conv_reduce: 36.06% sparsity
blocks.4.4.se.conv_expand: 29.88% sparsity
blocks.4.4.conv_pwl: 27.01% sparsity
blocks.4.5.conv_pw: 27.95% sparsity
blocks.4.5.conv_dw: 25.16% sparsity
blocks.4.5.se.conv_reduce: 36.93% sparsity
blocks.4.5.se.conv_expand: 32.20% sparsity
blocks.4.5.conv_pwl: 27.73% sparsity
blocks.4.6.conv_pw: 27.83% sparsity
blocks.4.6.conv_dw: 25.87% sparsity
blocks.4.6.se.conv_reduce: 35.82% sparsity
blocks.4.6.se.conv_expand: 28.63% sparsity
blocks.4.6.conv_pwl: 27.30% sparsity
blocks.4.7.conv_pw: 27.98% sparsity
blocks.4.7.conv_dw: 26.38% sparsity
blocks.4.7.se.conv_reduce: 33.10% sparsity
blocks.4.7.se.conv_expand: 27.64% sparsity
blocks.4.7.conv_pwl: 27.25% sparsity
blocks.4.8.conv_pw: 28.07% sparsity
blocks.4.8.conv_dw: 25.02% sparsity
blocks.4.8.se.conv_reduce: 33.25% sparsity
blocks.4.8.se.conv_expand: 26.85% sparsity
blocks.4.8.conv_pwl: 27.59% sparsity
blocks.5.0.conv_pw: 27.48% sparsity
blocks.5.0.conv_dw: 19.58% sparsity
blocks.5.0.se.conv_reduce: 85.29% sparsity
blocks.5.0.se.conv_expand: 87.10% sparsity
blocks.5.0.conv_pwl: 27.77% sparsity
blocks.5.1.conv_pw: 27.44% sparsity
blocks.5.1.conv_dw: 25.59% sparsity
blocks.5.1.se.conv_reduce: 31.89% sparsity
blocks.5.1.se.conv_expand: 27.15% sparsity
blocks.5.1.conv_pwl: 26.87% sparsity
blocks.5.2.conv_pw: 27.47% sparsity
blocks.5.2.conv_dw: 26.04% sparsity
blocks.5.2.se.conv_reduce: 31.68% sparsity
blocks.5.2.se.conv_expand: 26.85% sparsity
blocks.5.2.conv_pwl: 26.69% sparsity
blocks.5.3.conv_pw: 27.50% sparsity
blocks.5.3.conv_dw: 26.67% sparsity
blocks.5.3.se.conv_reduce: 31.91% sparsity
blocks.5.3.se.conv_expand: 27.20% sparsity
blocks.5.3.conv_pwl: 26.88% sparsity
blocks.5.4.conv_pw: 27.44% sparsity
blocks.5.4.conv_dw: 26.89% sparsity
blocks.5.4.se.conv_reduce: 31.41% sparsity
blocks.5.4.se.conv_expand: 26.97% sparsity
blocks.5.4.conv_pwl: 26.80% sparsity
blocks.5.5.conv_pw: 27.36% sparsity
blocks.5.5.conv_dw: 27.39% sparsity
blocks.5.5.se.conv_reduce: 30.37% sparsity
blocks.5.5.se.conv_expand: 27.02% sparsity
blocks.5.5.conv_pwl: 26.74% sparsity
blocks.5.6.conv_pw: 27.32% sparsity
blocks.5.6.conv_dw: 27.50% sparsity
blocks.5.6.se.conv_reduce: 30.39% sparsity
blocks.5.6.se.conv_expand: 26.98% sparsity
blocks.5.6.conv_pwl: 26.66% sparsity
blocks.5.7.conv_pw: 27.28% sparsity
blocks.5.7.conv_dw: 26.86% sparsity
blocks.5.7.se.conv_reduce: 29.71% sparsity
blocks.5.7.se.conv_expand: 26.52% sparsity
blocks.5.7.conv_pwl: 26.53% sparsity
blocks.5.8.conv_pw: 27.24% sparsity
blocks.5.8.conv_dw: 26.18% sparsity
blocks.5.8.se.conv_reduce: 28.79% sparsity
blocks.5.8.se.conv_expand: 26.21% sparsity
blocks.5.8.conv_pwl: 26.60% sparsity
blocks.5.9.conv_pw: 27.36% sparsity
blocks.5.9.conv_dw: 26.41% sparsity
blocks.5.9.se.conv_reduce: 29.29% sparsity
blocks.5.9.se.conv_expand: 26.16% sparsity
blocks.5.9.conv_pwl: 26.69% sparsity
blocks.5.10.conv_pw: 27.19% sparsity
blocks.5.10.conv_dw: 26.29% sparsity
blocks.5.10.se.conv_reduce: 29.31% sparsity
blocks.5.10.se.conv_expand: 26.20% sparsity
blocks.5.10.conv_pwl: 26.64% sparsity
blocks.5.11.conv_pw: 27.53% sparsity
blocks.5.11.conv_dw: 25.94% sparsity
blocks.5.11.se.conv_reduce: 30.26% sparsity
blocks.5.11.se.conv_expand: 26.04% sparsity
blocks.5.11.conv_pwl: 26.84% sparsity
blocks.5.12.conv_pw: 27.70% sparsity
blocks.5.12.conv_dw: 24.83% sparsity
blocks.5.12.se.conv_reduce: 30.30% sparsity
blocks.5.12.se.conv_expand: 26.00% sparsity
blocks.5.12.conv_pwl: 26.82% sparsity
blocks.5.13.conv_pw: 27.91% sparsity
blocks.5.13.conv_dw: 24.93% sparsity
blocks.5.13.se.conv_reduce: 31.99% sparsity
blocks.5.13.se.conv_expand: 26.24% sparsity
blocks.5.13.conv_pwl: 27.39% sparsity
blocks.5.14.conv_pw: 27.89% sparsity
blocks.5.14.conv_dw: 25.25% sparsity
blocks.5.14.se.conv_reduce: 31.90% sparsity
blocks.5.14.se.conv_expand: 24.32% sparsity
blocks.5.14.conv_pwl: 27.00% sparsity
conv_head: 26.53% sparsity
Date :04/18/2023, 14:35:40
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.0001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
conv_stem: 19.91% sparsity
blocks.0.0.conv: 29.22% sparsity
blocks.0.1.conv: 29.75% sparsity
blocks.1.0.conv_exp: 29.28% sparsity
blocks.1.0.conv_pwl: 23.83% sparsity
blocks.1.1.conv_exp: 27.86% sparsity
blocks.1.1.conv_pwl: 25.34% sparsity
blocks.1.2.conv_exp: 27.40% sparsity
blocks.1.2.conv_pwl: 26.15% sparsity
blocks.1.3.conv_exp: 27.49% sparsity
blocks.1.3.conv_pwl: 26.51% sparsity
blocks.2.0.conv_exp: 29.50% sparsity
blocks.2.0.conv_pwl: 26.25% sparsity
blocks.2.1.conv_exp: 27.91% sparsity
blocks.2.1.conv_pwl: 26.03% sparsity
blocks.2.2.conv_exp: 27.45% sparsity
blocks.2.2.conv_pwl: 26.38% sparsity
blocks.2.3.conv_exp: 27.72% sparsity
blocks.2.3.conv_pwl: 26.52% sparsity
blocks.3.0.conv_pw: 27.25% sparsity
blocks.3.0.conv_dw: 11.46% sparsity
blocks.3.0.se.conv_reduce: 69.53% sparsity
blocks.3.0.se.conv_expand: 91.75% sparsity
blocks.3.0.conv_pwl: 26.76% sparsity
blocks.3.1.conv_pw: 28.05% sparsity
blocks.3.1.conv_dw: 24.48% sparsity
blocks.3.1.se.conv_reduce: 36.71% sparsity
blocks.3.1.se.conv_expand: 28.63% sparsity
blocks.3.1.conv_pwl: 27.56% sparsity
blocks.3.2.conv_pw: 27.57% sparsity
blocks.3.2.conv_dw: 24.37% sparsity
blocks.3.2.se.conv_reduce: 36.05% sparsity
blocks.3.2.se.conv_expand: 28.04% sparsity
blocks.3.2.conv_pwl: 27.28% sparsity
blocks.3.3.conv_pw: 27.47% sparsity
blocks.3.3.conv_dw: 25.28% sparsity
blocks.3.3.se.conv_reduce: 31.71% sparsity
blocks.3.3.se.conv_expand: 25.96% sparsity
blocks.3.3.conv_pwl: 27.25% sparsity
blocks.3.4.conv_pw: 28.17% sparsity
blocks.3.4.conv_dw: 27.02% sparsity
blocks.3.4.se.conv_reduce: 34.65% sparsity
blocks.3.4.se.conv_expand: 32.06% sparsity
blocks.3.4.conv_pwl: 28.64% sparsity
blocks.3.5.conv_pw: 28.14% sparsity
blocks.3.5.conv_dw: 25.48% sparsity
blocks.3.5.se.conv_reduce: 34.30% sparsity
blocks.3.5.se.conv_expand: 28.16% sparsity
blocks.3.5.conv_pwl: 28.66% sparsity
blocks.4.0.conv_pw: 27.59% sparsity
blocks.4.0.conv_dw: 32.18% sparsity
blocks.4.0.se.conv_reduce: 31.86% sparsity
blocks.4.0.se.conv_expand: 26.24% sparsity
blocks.4.0.conv_pwl: 27.00% sparsity
blocks.4.1.conv_pw: 27.68% sparsity
blocks.4.1.conv_dw: 24.50% sparsity
blocks.4.1.se.conv_reduce: 34.68% sparsity
blocks.4.1.se.conv_expand: 28.59% sparsity
blocks.4.1.conv_pwl: 26.96% sparsity
blocks.4.2.conv_pw: 27.75% sparsity
blocks.4.2.conv_dw: 25.41% sparsity
blocks.4.2.se.conv_reduce: 34.78% sparsity
blocks.4.2.se.conv_expand: 28.65% sparsity
blocks.4.2.conv_pwl: 27.17% sparsity
blocks.4.3.conv_pw: 27.59% sparsity
blocks.4.3.conv_dw: 24.50% sparsity
blocks.4.3.se.conv_reduce: 33.59% sparsity
blocks.4.3.se.conv_expand: 28.96% sparsity
blocks.4.3.conv_pwl: 27.09% sparsity
blocks.4.4.conv_pw: 27.77% sparsity
blocks.4.4.conv_dw: 25.86% sparsity
blocks.4.4.se.conv_reduce: 36.06% sparsity
blocks.4.4.se.conv_expand: 29.88% sparsity
blocks.4.4.conv_pwl: 27.01% sparsity
blocks.4.5.conv_pw: 27.95% sparsity
blocks.4.5.conv_dw: 25.16% sparsity
blocks.4.5.se.conv_reduce: 36.93% sparsity
blocks.4.5.se.conv_expand: 32.20% sparsity
blocks.4.5.conv_pwl: 27.73% sparsity
blocks.4.6.conv_pw: 27.83% sparsity
blocks.4.6.conv_dw: 25.87% sparsity
blocks.4.6.se.conv_reduce: 35.82% sparsity
blocks.4.6.se.conv_expand: 28.63% sparsity
blocks.4.6.conv_pwl: 27.30% sparsity
blocks.4.7.conv_pw: 27.98% sparsity
blocks.4.7.conv_dw: 26.38% sparsity
blocks.4.7.se.conv_reduce: 33.10% sparsity
blocks.4.7.se.conv_expand: 27.64% sparsity
blocks.4.7.conv_pwl: 27.25% sparsity
blocks.4.8.conv_pw: 28.07% sparsity
blocks.4.8.conv_dw: 25.02% sparsity
blocks.4.8.se.conv_reduce: 33.25% sparsity
blocks.4.8.se.conv_expand: 26.85% sparsity
blocks.4.8.conv_pwl: 27.59% sparsity
blocks.5.0.conv_pw: 27.48% sparsity
blocks.5.0.conv_dw: 19.58% sparsity
blocks.5.0.se.conv_reduce: 85.29% sparsity
blocks.5.0.se.conv_expand: 87.10% sparsity
blocks.5.0.conv_pwl: 27.77% sparsity
blocks.5.1.conv_pw: 27.44% sparsity
blocks.5.1.conv_dw: 25.59% sparsity
blocks.5.1.se.conv_reduce: 31.89% sparsity
blocks.5.1.se.conv_expand: 27.15% sparsity
blocks.5.1.conv_pwl: 26.87% sparsity
blocks.5.2.conv_pw: 27.47% sparsity
blocks.5.2.conv_dw: 26.04% sparsity
blocks.5.2.se.conv_reduce: 31.68% sparsity
blocks.5.2.se.conv_expand: 26.85% sparsity
blocks.5.2.conv_pwl: 26.69% sparsity
blocks.5.3.conv_pw: 27.50% sparsity
blocks.5.3.conv_dw: 26.67% sparsity
blocks.5.3.se.conv_reduce: 31.91% sparsity
blocks.5.3.se.conv_expand: 27.20% sparsity
blocks.5.3.conv_pwl: 26.88% sparsity
blocks.5.4.conv_pw: 27.44% sparsity
blocks.5.4.conv_dw: 26.89% sparsity
blocks.5.4.se.conv_reduce: 31.41% sparsity
blocks.5.4.se.conv_expand: 26.97% sparsity
blocks.5.4.conv_pwl: 26.80% sparsity
blocks.5.5.conv_pw: 27.36% sparsity
blocks.5.5.conv_dw: 27.39% sparsity
blocks.5.5.se.conv_reduce: 30.37% sparsity
blocks.5.5.se.conv_expand: 27.02% sparsity
blocks.5.5.conv_pwl: 26.74% sparsity
blocks.5.6.conv_pw: 27.32% sparsity
blocks.5.6.conv_dw: 27.50% sparsity
blocks.5.6.se.conv_reduce: 30.39% sparsity
blocks.5.6.se.conv_expand: 26.98% sparsity
blocks.5.6.conv_pwl: 26.66% sparsity
blocks.5.7.conv_pw: 27.28% sparsity
blocks.5.7.conv_dw: 26.86% sparsity
blocks.5.7.se.conv_reduce: 29.71% sparsity
blocks.5.7.se.conv_expand: 26.52% sparsity
blocks.5.7.conv_pwl: 26.53% sparsity
blocks.5.8.conv_pw: 27.24% sparsity
blocks.5.8.conv_dw: 26.18% sparsity
blocks.5.8.se.conv_reduce: 28.79% sparsity
blocks.5.8.se.conv_expand: 26.21% sparsity
blocks.5.8.conv_pwl: 26.60% sparsity
blocks.5.9.conv_pw: 27.36% sparsity
blocks.5.9.conv_dw: 26.41% sparsity
blocks.5.9.se.conv_reduce: 29.29% sparsity
blocks.5.9.se.conv_expand: 26.16% sparsity
blocks.5.9.conv_pwl: 26.69% sparsity
blocks.5.10.conv_pw: 27.19% sparsity
blocks.5.10.conv_dw: 26.29% sparsity
blocks.5.10.se.conv_reduce: 29.31% sparsity
blocks.5.10.se.conv_expand: 26.20% sparsity
blocks.5.10.conv_pwl: 26.64% sparsity
blocks.5.11.conv_pw: 27.53% sparsity
blocks.5.11.conv_dw: 25.94% sparsity
blocks.5.11.se.conv_reduce: 30.26% sparsity
blocks.5.11.se.conv_expand: 26.04% sparsity
blocks.5.11.conv_pwl: 26.84% sparsity
blocks.5.12.conv_pw: 27.70% sparsity
blocks.5.12.conv_dw: 24.83% sparsity
blocks.5.12.se.conv_reduce: 30.30% sparsity
blocks.5.12.se.conv_expand: 26.00% sparsity
blocks.5.12.conv_pwl: 26.82% sparsity
blocks.5.13.conv_pw: 27.91% sparsity
blocks.5.13.conv_dw: 24.93% sparsity
blocks.5.13.se.conv_reduce: 31.99% sparsity
blocks.5.13.se.conv_expand: 26.24% sparsity
blocks.5.13.conv_pwl: 27.39% sparsity
blocks.5.14.conv_pw: 27.89% sparsity
blocks.5.14.conv_dw: 25.25% sparsity
blocks.5.14.se.conv_reduce: 31.90% sparsity
blocks.5.14.se.conv_expand: 24.32% sparsity
blocks.5.14.conv_pwl: 27.00% sparsity
conv_head: 26.53% sparsity
Date :04/18/2023, 14:39:50
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.0001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
Date :04/18/2023, 14:41:40
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.0001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
Date :04/18/2023, 14:42:52
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.0001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
Date :04/18/2023, 14:48:41
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.0001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
conv_stem: 19.91% sparsity
blocks.0.0.conv: 20.00% sparsity
blocks.0.1.conv: 20.00% sparsity
blocks.1.0.conv_exp: 20.00% sparsity
blocks.1.0.conv_pwl: 20.01% sparsity
blocks.1.1.conv_exp: 20.00% sparsity
blocks.1.1.conv_pwl: 20.00% sparsity
blocks.1.2.conv_exp: 20.00% sparsity
blocks.1.2.conv_pwl: 20.00% sparsity
blocks.1.3.conv_exp: 20.00% sparsity
blocks.1.3.conv_pwl: 20.00% sparsity
blocks.2.0.conv_exp: 20.00% sparsity
blocks.2.0.conv_pwl: 20.00% sparsity
blocks.2.1.conv_exp: 20.00% sparsity
blocks.2.1.conv_pwl: 20.00% sparsity
blocks.2.2.conv_exp: 20.00% sparsity
blocks.2.2.conv_pwl: 20.00% sparsity
blocks.2.3.conv_exp: 20.00% sparsity
blocks.2.3.conv_pwl: 20.00% sparsity
blocks.3.0.conv_pw: 20.00% sparsity
blocks.3.0.conv_dw: 20.01% sparsity
blocks.3.0.se.conv_reduce: 20.00% sparsity
blocks.3.0.se.conv_expand: 20.00% sparsity
blocks.3.0.conv_pwl: 20.00% sparsity
blocks.3.1.conv_pw: 20.00% sparsity
blocks.3.1.conv_dw: 20.01% sparsity
blocks.3.1.se.conv_reduce: 20.00% sparsity
blocks.3.1.se.conv_expand: 20.00% sparsity
blocks.3.1.conv_pwl: 20.00% sparsity
blocks.3.2.conv_pw: 20.00% sparsity
blocks.3.2.conv_dw: 20.01% sparsity
blocks.3.2.se.conv_reduce: 20.00% sparsity
blocks.3.2.se.conv_expand: 20.00% sparsity
blocks.3.2.conv_pwl: 20.00% sparsity
blocks.3.3.conv_pw: 20.00% sparsity
blocks.3.3.conv_dw: 20.01% sparsity
blocks.3.3.se.conv_reduce: 20.00% sparsity
blocks.3.3.se.conv_expand: 20.00% sparsity
blocks.3.3.conv_pwl: 20.00% sparsity
blocks.3.4.conv_pw: 20.00% sparsity
blocks.3.4.conv_dw: 20.01% sparsity
blocks.3.4.se.conv_reduce: 20.00% sparsity
blocks.3.4.se.conv_expand: 20.00% sparsity
blocks.3.4.conv_pwl: 20.00% sparsity
blocks.3.5.conv_pw: 20.00% sparsity
blocks.3.5.conv_dw: 20.01% sparsity
blocks.3.5.se.conv_reduce: 20.00% sparsity
blocks.3.5.se.conv_expand: 20.00% sparsity
blocks.3.5.conv_pwl: 20.00% sparsity
blocks.4.0.conv_pw: 20.00% sparsity
blocks.4.0.conv_dw: 19.99% sparsity
blocks.4.0.se.conv_reduce: 20.00% sparsity
blocks.4.0.se.conv_expand: 20.00% sparsity
blocks.4.0.conv_pwl: 20.00% sparsity
blocks.4.1.conv_pw: 20.00% sparsity
blocks.4.1.conv_dw: 20.00% sparsity
blocks.4.1.se.conv_reduce: 20.00% sparsity
blocks.4.1.se.conv_expand: 20.00% sparsity
blocks.4.1.conv_pwl: 20.00% sparsity
blocks.4.2.conv_pw: 20.00% sparsity
blocks.4.2.conv_dw: 20.00% sparsity
blocks.4.2.se.conv_reduce: 20.00% sparsity
blocks.4.2.se.conv_expand: 20.00% sparsity
blocks.4.2.conv_pwl: 20.00% sparsity
blocks.4.3.conv_pw: 20.00% sparsity
blocks.4.3.conv_dw: 20.00% sparsity
blocks.4.3.se.conv_reduce: 20.00% sparsity
blocks.4.3.se.conv_expand: 20.00% sparsity
blocks.4.3.conv_pwl: 20.00% sparsity
blocks.4.4.conv_pw: 20.00% sparsity
blocks.4.4.conv_dw: 20.00% sparsity
blocks.4.4.se.conv_reduce: 20.00% sparsity
blocks.4.4.se.conv_expand: 20.00% sparsity
blocks.4.4.conv_pwl: 20.00% sparsity
blocks.4.5.conv_pw: 20.00% sparsity
blocks.4.5.conv_dw: 20.00% sparsity
blocks.4.5.se.conv_reduce: 20.00% sparsity
blocks.4.5.se.conv_expand: 20.00% sparsity
blocks.4.5.conv_pwl: 20.00% sparsity
blocks.4.6.conv_pw: 20.00% sparsity
blocks.4.6.conv_dw: 20.00% sparsity
blocks.4.6.se.conv_reduce: 20.00% sparsity
blocks.4.6.se.conv_expand: 20.00% sparsity
blocks.4.6.conv_pwl: 20.00% sparsity
blocks.4.7.conv_pw: 20.00% sparsity
blocks.4.7.conv_dw: 20.00% sparsity
blocks.4.7.se.conv_reduce: 20.00% sparsity
blocks.4.7.se.conv_expand: 20.00% sparsity
blocks.4.7.conv_pwl: 20.00% sparsity
blocks.4.8.conv_pw: 20.00% sparsity
blocks.4.8.conv_dw: 20.00% sparsity
blocks.4.8.se.conv_reduce: 20.00% sparsity
blocks.4.8.se.conv_expand: 20.00% sparsity
blocks.4.8.conv_pwl: 20.00% sparsity
blocks.5.0.conv_pw: 20.00% sparsity
blocks.5.0.conv_dw: 20.00% sparsity
blocks.5.0.se.conv_reduce: 20.00% sparsity
blocks.5.0.se.conv_expand: 20.00% sparsity
blocks.5.0.conv_pwl: 20.00% sparsity
blocks.5.1.conv_pw: 20.00% sparsity
blocks.5.1.conv_dw: 20.00% sparsity
blocks.5.1.se.conv_reduce: 20.00% sparsity
blocks.5.1.se.conv_expand: 20.00% sparsity
blocks.5.1.conv_pwl: 20.00% sparsity
blocks.5.2.conv_pw: 20.00% sparsity
blocks.5.2.conv_dw: 20.00% sparsity
blocks.5.2.se.conv_reduce: 20.00% sparsity
blocks.5.2.se.conv_expand: 20.00% sparsity
blocks.5.2.conv_pwl: 20.00% sparsity
blocks.5.3.conv_pw: 20.00% sparsity
blocks.5.3.conv_dw: 20.00% sparsity
blocks.5.3.se.conv_reduce: 20.00% sparsity
blocks.5.3.se.conv_expand: 20.00% sparsity
blocks.5.3.conv_pwl: 20.00% sparsity
blocks.5.4.conv_pw: 20.00% sparsity
blocks.5.4.conv_dw: 20.00% sparsity
blocks.5.4.se.conv_reduce: 20.00% sparsity
blocks.5.4.se.conv_expand: 20.00% sparsity
blocks.5.4.conv_pwl: 20.00% sparsity
blocks.5.5.conv_pw: 20.00% sparsity
blocks.5.5.conv_dw: 20.00% sparsity
blocks.5.5.se.conv_reduce: 20.00% sparsity
blocks.5.5.se.conv_expand: 20.00% sparsity
blocks.5.5.conv_pwl: 20.00% sparsity
blocks.5.6.conv_pw: 20.00% sparsity
blocks.5.6.conv_dw: 20.00% sparsity
blocks.5.6.se.conv_reduce: 20.00% sparsity
blocks.5.6.se.conv_expand: 20.00% sparsity
blocks.5.6.conv_pwl: 20.00% sparsity
blocks.5.7.conv_pw: 20.00% sparsity
blocks.5.7.conv_dw: 20.00% sparsity
blocks.5.7.se.conv_reduce: 20.00% sparsity
blocks.5.7.se.conv_expand: 20.00% sparsity
blocks.5.7.conv_pwl: 20.00% sparsity
blocks.5.8.conv_pw: 20.00% sparsity
blocks.5.8.conv_dw: 20.00% sparsity
blocks.5.8.se.conv_reduce: 20.00% sparsity
blocks.5.8.se.conv_expand: 20.00% sparsity
blocks.5.8.conv_pwl: 20.00% sparsity
blocks.5.9.conv_pw: 20.00% sparsity
blocks.5.9.conv_dw: 20.00% sparsity
blocks.5.9.se.conv_reduce: 20.00% sparsity
blocks.5.9.se.conv_expand: 20.00% sparsity
blocks.5.9.conv_pwl: 20.00% sparsity
blocks.5.10.conv_pw: 20.00% sparsity
blocks.5.10.conv_dw: 20.00% sparsity
blocks.5.10.se.conv_reduce: 20.00% sparsity
blocks.5.10.se.conv_expand: 20.00% sparsity
blocks.5.10.conv_pwl: 20.00% sparsity
blocks.5.11.conv_pw: 20.00% sparsity
blocks.5.11.conv_dw: 20.00% sparsity
blocks.5.11.se.conv_reduce: 20.00% sparsity
blocks.5.11.se.conv_expand: 20.00% sparsity
blocks.5.11.conv_pwl: 20.00% sparsity
blocks.5.12.conv_pw: 20.00% sparsity
blocks.5.12.conv_dw: 20.00% sparsity
blocks.5.12.se.conv_reduce: 20.00% sparsity
blocks.5.12.se.conv_expand: 20.00% sparsity
blocks.5.12.conv_pwl: 20.00% sparsity
blocks.5.13.conv_pw: 20.00% sparsity
blocks.5.13.conv_dw: 20.00% sparsity
blocks.5.13.se.conv_reduce: 20.00% sparsity
blocks.5.13.se.conv_expand: 20.00% sparsity
blocks.5.13.conv_pwl: 20.00% sparsity
blocks.5.14.conv_pw: 20.00% sparsity
blocks.5.14.conv_dw: 20.00% sparsity
blocks.5.14.se.conv_reduce: 20.00% sparsity
blocks.5.14.se.conv_expand: 20.00% sparsity
blocks.5.14.conv_pwl: 20.00% sparsity
conv_head: 20.00% sparsity
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0001
    lr: 0.0001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Date :04/18/2023, 14:49:58
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.0001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
conv_stem: 19.91% sparsity
blocks.0.0.conv: 20.00% sparsity
blocks.0.1.conv: 20.00% sparsity
blocks.1.0.conv_exp: 20.00% sparsity
blocks.1.0.conv_pwl: 20.01% sparsity
blocks.1.1.conv_exp: 20.00% sparsity
blocks.1.1.conv_pwl: 20.00% sparsity
blocks.1.2.conv_exp: 20.00% sparsity
blocks.1.2.conv_pwl: 20.00% sparsity
blocks.1.3.conv_exp: 20.00% sparsity
blocks.1.3.conv_pwl: 20.00% sparsity
blocks.2.0.conv_exp: 20.00% sparsity
blocks.2.0.conv_pwl: 20.00% sparsity
blocks.2.1.conv_exp: 20.00% sparsity
blocks.2.1.conv_pwl: 20.00% sparsity
blocks.2.2.conv_exp: 20.00% sparsity
blocks.2.2.conv_pwl: 20.00% sparsity
blocks.2.3.conv_exp: 20.00% sparsity
blocks.2.3.conv_pwl: 20.00% sparsity
blocks.3.0.conv_pw: 20.00% sparsity
blocks.3.0.conv_dw: 20.01% sparsity
blocks.3.0.se.conv_reduce: 20.00% sparsity
blocks.3.0.se.conv_expand: 20.00% sparsity
blocks.3.0.conv_pwl: 20.00% sparsity
blocks.3.1.conv_pw: 20.00% sparsity
blocks.3.1.conv_dw: 20.01% sparsity
blocks.3.1.se.conv_reduce: 20.00% sparsity
blocks.3.1.se.conv_expand: 20.00% sparsity
blocks.3.1.conv_pwl: 20.00% sparsity
blocks.3.2.conv_pw: 20.00% sparsity
blocks.3.2.conv_dw: 20.01% sparsity
blocks.3.2.se.conv_reduce: 20.00% sparsity
blocks.3.2.se.conv_expand: 20.00% sparsity
blocks.3.2.conv_pwl: 20.00% sparsity
blocks.3.3.conv_pw: 20.00% sparsity
blocks.3.3.conv_dw: 20.01% sparsity
blocks.3.3.se.conv_reduce: 20.00% sparsity
blocks.3.3.se.conv_expand: 20.00% sparsity
blocks.3.3.conv_pwl: 20.00% sparsity
blocks.3.4.conv_pw: 20.00% sparsity
blocks.3.4.conv_dw: 20.01% sparsity
blocks.3.4.se.conv_reduce: 20.00% sparsity
blocks.3.4.se.conv_expand: 20.00% sparsity
blocks.3.4.conv_pwl: 20.00% sparsity
blocks.3.5.conv_pw: 20.00% sparsity
blocks.3.5.conv_dw: 20.01% sparsity
blocks.3.5.se.conv_reduce: 20.00% sparsity
blocks.3.5.se.conv_expand: 20.00% sparsity
blocks.3.5.conv_pwl: 20.00% sparsity
blocks.4.0.conv_pw: 20.00% sparsity
blocks.4.0.conv_dw: 19.99% sparsity
blocks.4.0.se.conv_reduce: 20.00% sparsity
blocks.4.0.se.conv_expand: 20.00% sparsity
blocks.4.0.conv_pwl: 20.00% sparsity
blocks.4.1.conv_pw: 20.00% sparsity
blocks.4.1.conv_dw: 20.00% sparsity
blocks.4.1.se.conv_reduce: 20.00% sparsity
blocks.4.1.se.conv_expand: 20.00% sparsity
blocks.4.1.conv_pwl: 20.00% sparsity
blocks.4.2.conv_pw: 20.00% sparsity
blocks.4.2.conv_dw: 20.00% sparsity
blocks.4.2.se.conv_reduce: 20.00% sparsity
blocks.4.2.se.conv_expand: 20.00% sparsity
blocks.4.2.conv_pwl: 20.00% sparsity
blocks.4.3.conv_pw: 20.00% sparsity
blocks.4.3.conv_dw: 20.00% sparsity
blocks.4.3.se.conv_reduce: 20.00% sparsity
blocks.4.3.se.conv_expand: 20.00% sparsity
blocks.4.3.conv_pwl: 20.00% sparsity
blocks.4.4.conv_pw: 20.00% sparsity
blocks.4.4.conv_dw: 20.00% sparsity
blocks.4.4.se.conv_reduce: 20.00% sparsity
blocks.4.4.se.conv_expand: 20.00% sparsity
blocks.4.4.conv_pwl: 20.00% sparsity
blocks.4.5.conv_pw: 20.00% sparsity
blocks.4.5.conv_dw: 20.00% sparsity
blocks.4.5.se.conv_reduce: 20.00% sparsity
blocks.4.5.se.conv_expand: 20.00% sparsity
blocks.4.5.conv_pwl: 20.00% sparsity
blocks.4.6.conv_pw: 20.00% sparsity
blocks.4.6.conv_dw: 20.00% sparsity
blocks.4.6.se.conv_reduce: 20.00% sparsity
blocks.4.6.se.conv_expand: 20.00% sparsity
blocks.4.6.conv_pwl: 20.00% sparsity
blocks.4.7.conv_pw: 20.00% sparsity
blocks.4.7.conv_dw: 20.00% sparsity
blocks.4.7.se.conv_reduce: 20.00% sparsity
blocks.4.7.se.conv_expand: 20.00% sparsity
blocks.4.7.conv_pwl: 20.00% sparsity
blocks.4.8.conv_pw: 20.00% sparsity
blocks.4.8.conv_dw: 20.00% sparsity
blocks.4.8.se.conv_reduce: 20.00% sparsity
blocks.4.8.se.conv_expand: 20.00% sparsity
blocks.4.8.conv_pwl: 20.00% sparsity
blocks.5.0.conv_pw: 20.00% sparsity
blocks.5.0.conv_dw: 20.00% sparsity
blocks.5.0.se.conv_reduce: 20.00% sparsity
blocks.5.0.se.conv_expand: 20.00% sparsity
blocks.5.0.conv_pwl: 20.00% sparsity
blocks.5.1.conv_pw: 20.00% sparsity
blocks.5.1.conv_dw: 20.00% sparsity
blocks.5.1.se.conv_reduce: 20.00% sparsity
blocks.5.1.se.conv_expand: 20.00% sparsity
blocks.5.1.conv_pwl: 20.00% sparsity
blocks.5.2.conv_pw: 20.00% sparsity
blocks.5.2.conv_dw: 20.00% sparsity
blocks.5.2.se.conv_reduce: 20.00% sparsity
blocks.5.2.se.conv_expand: 20.00% sparsity
blocks.5.2.conv_pwl: 20.00% sparsity
blocks.5.3.conv_pw: 20.00% sparsity
blocks.5.3.conv_dw: 20.00% sparsity
blocks.5.3.se.conv_reduce: 20.00% sparsity
blocks.5.3.se.conv_expand: 20.00% sparsity
blocks.5.3.conv_pwl: 20.00% sparsity
blocks.5.4.conv_pw: 20.00% sparsity
blocks.5.4.conv_dw: 20.00% sparsity
blocks.5.4.se.conv_reduce: 20.00% sparsity
blocks.5.4.se.conv_expand: 20.00% sparsity
blocks.5.4.conv_pwl: 20.00% sparsity
blocks.5.5.conv_pw: 20.00% sparsity
blocks.5.5.conv_dw: 20.00% sparsity
blocks.5.5.se.conv_reduce: 20.00% sparsity
blocks.5.5.se.conv_expand: 20.00% sparsity
blocks.5.5.conv_pwl: 20.00% sparsity
blocks.5.6.conv_pw: 20.00% sparsity
blocks.5.6.conv_dw: 20.00% sparsity
blocks.5.6.se.conv_reduce: 20.00% sparsity
blocks.5.6.se.conv_expand: 20.00% sparsity
blocks.5.6.conv_pwl: 20.00% sparsity
blocks.5.7.conv_pw: 20.00% sparsity
blocks.5.7.conv_dw: 20.00% sparsity
blocks.5.7.se.conv_reduce: 20.00% sparsity
blocks.5.7.se.conv_expand: 20.00% sparsity
blocks.5.7.conv_pwl: 20.00% sparsity
blocks.5.8.conv_pw: 20.00% sparsity
blocks.5.8.conv_dw: 20.00% sparsity
blocks.5.8.se.conv_reduce: 20.00% sparsity
blocks.5.8.se.conv_expand: 20.00% sparsity
blocks.5.8.conv_pwl: 20.00% sparsity
blocks.5.9.conv_pw: 20.00% sparsity
blocks.5.9.conv_dw: 20.00% sparsity
blocks.5.9.se.conv_reduce: 20.00% sparsity
blocks.5.9.se.conv_expand: 20.00% sparsity
blocks.5.9.conv_pwl: 20.00% sparsity
blocks.5.10.conv_pw: 20.00% sparsity
blocks.5.10.conv_dw: 20.00% sparsity
blocks.5.10.se.conv_reduce: 20.00% sparsity
blocks.5.10.se.conv_expand: 20.00% sparsity
blocks.5.10.conv_pwl: 20.00% sparsity
blocks.5.11.conv_pw: 20.00% sparsity
blocks.5.11.conv_dw: 20.00% sparsity
blocks.5.11.se.conv_reduce: 20.00% sparsity
blocks.5.11.se.conv_expand: 20.00% sparsity
blocks.5.11.conv_pwl: 20.00% sparsity
blocks.5.12.conv_pw: 20.00% sparsity
blocks.5.12.conv_dw: 20.00% sparsity
blocks.5.12.se.conv_reduce: 20.00% sparsity
blocks.5.12.se.conv_expand: 20.00% sparsity
blocks.5.12.conv_pwl: 20.00% sparsity
blocks.5.13.conv_pw: 20.00% sparsity
blocks.5.13.conv_dw: 20.00% sparsity
blocks.5.13.se.conv_reduce: 20.00% sparsity
blocks.5.13.se.conv_expand: 20.00% sparsity
blocks.5.13.conv_pwl: 20.00% sparsity
blocks.5.14.conv_pw: 20.00% sparsity
blocks.5.14.conv_dw: 20.00% sparsity
blocks.5.14.se.conv_reduce: 20.00% sparsity
blocks.5.14.se.conv_expand: 20.00% sparsity
blocks.5.14.conv_pwl: 20.00% sparsity
conv_head: 20.00% sparsity
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0001
    lr: 0.0001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Date :04/18/2023, 14:50:57
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.0001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
conv_stem: 19.91% sparsity
blocks.0.0.conv: 20.00% sparsity
blocks.0.1.conv: 20.00% sparsity
blocks.1.0.conv_exp: 20.00% sparsity
blocks.1.0.conv_pwl: 20.01% sparsity
blocks.1.1.conv_exp: 20.00% sparsity
blocks.1.1.conv_pwl: 20.00% sparsity
blocks.1.2.conv_exp: 20.00% sparsity
blocks.1.2.conv_pwl: 20.00% sparsity
blocks.1.3.conv_exp: 20.00% sparsity
blocks.1.3.conv_pwl: 20.00% sparsity
blocks.2.0.conv_exp: 20.00% sparsity
blocks.2.0.conv_pwl: 20.00% sparsity
blocks.2.1.conv_exp: 20.00% sparsity
blocks.2.1.conv_pwl: 20.00% sparsity
blocks.2.2.conv_exp: 20.00% sparsity
blocks.2.2.conv_pwl: 20.00% sparsity
blocks.2.3.conv_exp: 20.00% sparsity
blocks.2.3.conv_pwl: 20.00% sparsity
blocks.3.0.conv_pw: 20.00% sparsity
blocks.3.0.conv_dw: 20.01% sparsity
blocks.3.0.se.conv_reduce: 20.00% sparsity
blocks.3.0.se.conv_expand: 20.00% sparsity
blocks.3.0.conv_pwl: 20.00% sparsity
blocks.3.1.conv_pw: 20.00% sparsity
blocks.3.1.conv_dw: 20.01% sparsity
blocks.3.1.se.conv_reduce: 20.00% sparsity
blocks.3.1.se.conv_expand: 20.00% sparsity
blocks.3.1.conv_pwl: 20.00% sparsity
blocks.3.2.conv_pw: 20.00% sparsity
blocks.3.2.conv_dw: 20.01% sparsity
blocks.3.2.se.conv_reduce: 20.00% sparsity
blocks.3.2.se.conv_expand: 20.00% sparsity
blocks.3.2.conv_pwl: 20.00% sparsity
blocks.3.3.conv_pw: 20.00% sparsity
blocks.3.3.conv_dw: 20.01% sparsity
blocks.3.3.se.conv_reduce: 20.00% sparsity
blocks.3.3.se.conv_expand: 20.00% sparsity
blocks.3.3.conv_pwl: 20.00% sparsity
blocks.3.4.conv_pw: 20.00% sparsity
blocks.3.4.conv_dw: 20.01% sparsity
blocks.3.4.se.conv_reduce: 20.00% sparsity
blocks.3.4.se.conv_expand: 20.00% sparsity
blocks.3.4.conv_pwl: 20.00% sparsity
blocks.3.5.conv_pw: 20.00% sparsity
blocks.3.5.conv_dw: 20.01% sparsity
blocks.3.5.se.conv_reduce: 20.00% sparsity
blocks.3.5.se.conv_expand: 20.00% sparsity
blocks.3.5.conv_pwl: 20.00% sparsity
blocks.4.0.conv_pw: 20.00% sparsity
blocks.4.0.conv_dw: 19.99% sparsity
blocks.4.0.se.conv_reduce: 20.00% sparsity
blocks.4.0.se.conv_expand: 20.00% sparsity
blocks.4.0.conv_pwl: 20.00% sparsity
blocks.4.1.conv_pw: 20.00% sparsity
blocks.4.1.conv_dw: 20.00% sparsity
blocks.4.1.se.conv_reduce: 20.00% sparsity
blocks.4.1.se.conv_expand: 20.00% sparsity
blocks.4.1.conv_pwl: 20.00% sparsity
blocks.4.2.conv_pw: 20.00% sparsity
blocks.4.2.conv_dw: 20.00% sparsity
blocks.4.2.se.conv_reduce: 20.00% sparsity
blocks.4.2.se.conv_expand: 20.00% sparsity
blocks.4.2.conv_pwl: 20.00% sparsity
blocks.4.3.conv_pw: 20.00% sparsity
blocks.4.3.conv_dw: 20.00% sparsity
blocks.4.3.se.conv_reduce: 20.00% sparsity
blocks.4.3.se.conv_expand: 20.00% sparsity
blocks.4.3.conv_pwl: 20.00% sparsity
blocks.4.4.conv_pw: 20.00% sparsity
blocks.4.4.conv_dw: 20.00% sparsity
blocks.4.4.se.conv_reduce: 20.00% sparsity
blocks.4.4.se.conv_expand: 20.00% sparsity
blocks.4.4.conv_pwl: 20.00% sparsity
blocks.4.5.conv_pw: 20.00% sparsity
blocks.4.5.conv_dw: 20.00% sparsity
blocks.4.5.se.conv_reduce: 20.00% sparsity
blocks.4.5.se.conv_expand: 20.00% sparsity
blocks.4.5.conv_pwl: 20.00% sparsity
blocks.4.6.conv_pw: 20.00% sparsity
blocks.4.6.conv_dw: 20.00% sparsity
blocks.4.6.se.conv_reduce: 20.00% sparsity
blocks.4.6.se.conv_expand: 20.00% sparsity
blocks.4.6.conv_pwl: 20.00% sparsity
blocks.4.7.conv_pw: 20.00% sparsity
blocks.4.7.conv_dw: 20.00% sparsity
blocks.4.7.se.conv_reduce: 20.00% sparsity
blocks.4.7.se.conv_expand: 20.00% sparsity
blocks.4.7.conv_pwl: 20.00% sparsity
blocks.4.8.conv_pw: 20.00% sparsity
blocks.4.8.conv_dw: 20.00% sparsity
blocks.4.8.se.conv_reduce: 20.00% sparsity
blocks.4.8.se.conv_expand: 20.00% sparsity
blocks.4.8.conv_pwl: 20.00% sparsity
blocks.5.0.conv_pw: 20.00% sparsity
blocks.5.0.conv_dw: 20.00% sparsity
blocks.5.0.se.conv_reduce: 20.00% sparsity
blocks.5.0.se.conv_expand: 20.00% sparsity
blocks.5.0.conv_pwl: 20.00% sparsity
blocks.5.1.conv_pw: 20.00% sparsity
blocks.5.1.conv_dw: 20.00% sparsity
blocks.5.1.se.conv_reduce: 20.00% sparsity
blocks.5.1.se.conv_expand: 20.00% sparsity
blocks.5.1.conv_pwl: 20.00% sparsity
blocks.5.2.conv_pw: 20.00% sparsity
blocks.5.2.conv_dw: 20.00% sparsity
blocks.5.2.se.conv_reduce: 20.00% sparsity
blocks.5.2.se.conv_expand: 20.00% sparsity
blocks.5.2.conv_pwl: 20.00% sparsity
blocks.5.3.conv_pw: 20.00% sparsity
blocks.5.3.conv_dw: 20.00% sparsity
blocks.5.3.se.conv_reduce: 20.00% sparsity
blocks.5.3.se.conv_expand: 20.00% sparsity
blocks.5.3.conv_pwl: 20.00% sparsity
blocks.5.4.conv_pw: 20.00% sparsity
blocks.5.4.conv_dw: 20.00% sparsity
blocks.5.4.se.conv_reduce: 20.00% sparsity
blocks.5.4.se.conv_expand: 20.00% sparsity
blocks.5.4.conv_pwl: 20.00% sparsity
blocks.5.5.conv_pw: 20.00% sparsity
blocks.5.5.conv_dw: 20.00% sparsity
blocks.5.5.se.conv_reduce: 20.00% sparsity
blocks.5.5.se.conv_expand: 20.00% sparsity
blocks.5.5.conv_pwl: 20.00% sparsity
blocks.5.6.conv_pw: 20.00% sparsity
blocks.5.6.conv_dw: 20.00% sparsity
blocks.5.6.se.conv_reduce: 20.00% sparsity
blocks.5.6.se.conv_expand: 20.00% sparsity
blocks.5.6.conv_pwl: 20.00% sparsity
blocks.5.7.conv_pw: 20.00% sparsity
blocks.5.7.conv_dw: 20.00% sparsity
blocks.5.7.se.conv_reduce: 20.00% sparsity
blocks.5.7.se.conv_expand: 20.00% sparsity
blocks.5.7.conv_pwl: 20.00% sparsity
blocks.5.8.conv_pw: 20.00% sparsity
blocks.5.8.conv_dw: 20.00% sparsity
blocks.5.8.se.conv_reduce: 20.00% sparsity
blocks.5.8.se.conv_expand: 20.00% sparsity
blocks.5.8.conv_pwl: 20.00% sparsity
blocks.5.9.conv_pw: 20.00% sparsity
blocks.5.9.conv_dw: 20.00% sparsity
blocks.5.9.se.conv_reduce: 20.00% sparsity
blocks.5.9.se.conv_expand: 20.00% sparsity
blocks.5.9.conv_pwl: 20.00% sparsity
blocks.5.10.conv_pw: 20.00% sparsity
blocks.5.10.conv_dw: 20.00% sparsity
blocks.5.10.se.conv_reduce: 20.00% sparsity
blocks.5.10.se.conv_expand: 20.00% sparsity
blocks.5.10.conv_pwl: 20.00% sparsity
blocks.5.11.conv_pw: 20.00% sparsity
blocks.5.11.conv_dw: 20.00% sparsity
blocks.5.11.se.conv_reduce: 20.00% sparsity
blocks.5.11.se.conv_expand: 20.00% sparsity
blocks.5.11.conv_pwl: 20.00% sparsity
blocks.5.12.conv_pw: 20.00% sparsity
blocks.5.12.conv_dw: 20.00% sparsity
blocks.5.12.se.conv_reduce: 20.00% sparsity
blocks.5.12.se.conv_expand: 20.00% sparsity
blocks.5.12.conv_pwl: 20.00% sparsity
blocks.5.13.conv_pw: 20.00% sparsity
blocks.5.13.conv_dw: 20.00% sparsity
blocks.5.13.se.conv_reduce: 20.00% sparsity
blocks.5.13.se.conv_expand: 20.00% sparsity
blocks.5.13.conv_pwl: 20.00% sparsity
blocks.5.14.conv_pw: 20.00% sparsity
blocks.5.14.conv_dw: 20.00% sparsity
blocks.5.14.se.conv_reduce: 20.00% sparsity
blocks.5.14.se.conv_expand: 20.00% sparsity
blocks.5.14.conv_pwl: 20.00% sparsity
conv_head: 20.00% sparsity
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0001
    lr: 0.0001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Date :04/18/2023, 14:51:11
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.0001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0001
    lr: 0.0001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Date :04/18/2023, 14:52:12
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.0001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
conv_stem: 29.17% sparsity
blocks.0.0.conv: 42.92% sparsity
blocks.0.1.conv: 42.55% sparsity
blocks.1.0.conv_exp: 42.45% sparsity
blocks.1.0.conv_pwl: 36.50% sparsity
blocks.1.1.conv_exp: 40.96% sparsity
blocks.1.1.conv_pwl: 36.96% sparsity
blocks.1.2.conv_exp: 40.38% sparsity
blocks.1.2.conv_pwl: 38.47% sparsity
blocks.1.3.conv_exp: 40.29% sparsity
blocks.1.3.conv_pwl: 38.53% sparsity
blocks.2.0.conv_exp: 42.83% sparsity
blocks.2.0.conv_pwl: 39.15% sparsity
blocks.2.1.conv_exp: 40.56% sparsity
blocks.2.1.conv_pwl: 39.17% sparsity
blocks.2.2.conv_exp: 40.28% sparsity
blocks.2.2.conv_pwl: 38.89% sparsity
blocks.2.3.conv_exp: 40.69% sparsity
blocks.2.3.conv_pwl: 38.87% sparsity
blocks.3.0.conv_pw: 39.98% sparsity
blocks.3.0.conv_dw: 21.40% sparsity
blocks.3.0.se.conv_reduce: 91.11% sparsity
blocks.3.0.se.conv_expand: 94.34% sparsity
blocks.3.0.conv_pwl: 39.43% sparsity
blocks.3.1.conv_pw: 41.06% sparsity
blocks.3.1.conv_dw: 36.59% sparsity
blocks.3.1.se.conv_reduce: 50.33% sparsity
blocks.3.1.se.conv_expand: 41.59% sparsity
blocks.3.1.conv_pwl: 40.75% sparsity
blocks.3.2.conv_pw: 40.72% sparsity
blocks.3.2.conv_dw: 37.70% sparsity
blocks.3.2.se.conv_reduce: 50.90% sparsity
blocks.3.2.se.conv_expand: 40.20% sparsity
blocks.3.2.conv_pwl: 40.11% sparsity
blocks.3.3.conv_pw: 40.63% sparsity
blocks.3.3.conv_dw: 37.80% sparsity
blocks.3.3.se.conv_reduce: 45.18% sparsity
blocks.3.3.se.conv_expand: 38.47% sparsity
blocks.3.3.conv_pwl: 40.10% sparsity
blocks.3.4.conv_pw: 41.27% sparsity
blocks.3.4.conv_dw: 39.26% sparsity
blocks.3.4.se.conv_reduce: 48.56% sparsity
blocks.3.4.se.conv_expand: 46.36% sparsity
blocks.3.4.conv_pwl: 42.31% sparsity
blocks.3.5.conv_pw: 41.09% sparsity
blocks.3.5.conv_dw: 38.56% sparsity
blocks.3.5.se.conv_reduce: 48.97% sparsity
blocks.3.5.se.conv_expand: 40.93% sparsity
blocks.3.5.conv_pwl: 41.89% sparsity
blocks.4.0.conv_pw: 40.64% sparsity
blocks.4.0.conv_dw: 45.44% sparsity
blocks.4.0.se.conv_reduce: 46.22% sparsity
blocks.4.0.se.conv_expand: 38.98% sparsity
blocks.4.0.conv_pwl: 39.62% sparsity
blocks.4.1.conv_pw: 40.71% sparsity
blocks.4.1.conv_dw: 37.01% sparsity
blocks.4.1.se.conv_reduce: 49.01% sparsity
blocks.4.1.se.conv_expand: 41.59% sparsity
blocks.4.1.conv_pwl: 39.99% sparsity
blocks.4.2.conv_pw: 40.59% sparsity
blocks.4.2.conv_dw: 37.72% sparsity
blocks.4.2.se.conv_reduce: 49.43% sparsity
blocks.4.2.se.conv_expand: 41.06% sparsity
blocks.4.2.conv_pwl: 40.00% sparsity
blocks.4.3.conv_pw: 40.58% sparsity
blocks.4.3.conv_dw: 37.19% sparsity
blocks.4.3.se.conv_reduce: 47.22% sparsity
blocks.4.3.se.conv_expand: 41.45% sparsity
blocks.4.3.conv_pwl: 40.12% sparsity
blocks.4.4.conv_pw: 40.66% sparsity
blocks.4.4.conv_dw: 38.32% sparsity
blocks.4.4.se.conv_reduce: 50.05% sparsity
blocks.4.4.se.conv_expand: 42.89% sparsity
blocks.4.4.conv_pwl: 40.09% sparsity
blocks.4.5.conv_pw: 41.04% sparsity
blocks.4.5.conv_dw: 38.02% sparsity
blocks.4.5.se.conv_reduce: 50.27% sparsity
blocks.4.5.se.conv_expand: 43.34% sparsity
blocks.4.5.conv_pwl: 40.53% sparsity
blocks.4.6.conv_pw: 40.91% sparsity
blocks.4.6.conv_dw: 38.70% sparsity
blocks.4.6.se.conv_reduce: 49.04% sparsity
blocks.4.6.se.conv_expand: 40.90% sparsity
blocks.4.6.conv_pwl: 40.06% sparsity
blocks.4.7.conv_pw: 41.12% sparsity
blocks.4.7.conv_dw: 39.47% sparsity
blocks.4.7.se.conv_reduce: 46.47% sparsity
blocks.4.7.se.conv_expand: 40.52% sparsity
blocks.4.7.conv_pwl: 40.03% sparsity
blocks.4.8.conv_pw: 40.91% sparsity
blocks.4.8.conv_dw: 37.22% sparsity
blocks.4.8.se.conv_reduce: 47.46% sparsity
blocks.4.8.se.conv_expand: 39.65% sparsity
blocks.4.8.conv_pwl: 40.17% sparsity
blocks.5.0.conv_pw: 40.14% sparsity
blocks.5.0.conv_dw: 31.39% sparsity
blocks.5.0.se.conv_reduce: 94.08% sparsity
blocks.5.0.se.conv_expand: 91.13% sparsity
blocks.5.0.conv_pwl: 40.77% sparsity
blocks.5.1.conv_pw: 40.45% sparsity
blocks.5.1.conv_dw: 38.46% sparsity
blocks.5.1.se.conv_reduce: 45.63% sparsity
blocks.5.1.se.conv_expand: 39.80% sparsity
blocks.5.1.conv_pwl: 39.59% sparsity
blocks.5.2.conv_pw: 40.34% sparsity
blocks.5.2.conv_dw: 39.26% sparsity
blocks.5.2.se.conv_reduce: 45.20% sparsity
blocks.5.2.se.conv_expand: 39.29% sparsity
blocks.5.2.conv_pwl: 39.40% sparsity
blocks.5.3.conv_pw: 40.25% sparsity
blocks.5.3.conv_dw: 38.90% sparsity
blocks.5.3.se.conv_reduce: 45.39% sparsity
blocks.5.3.se.conv_expand: 39.83% sparsity
blocks.5.3.conv_pwl: 39.56% sparsity
blocks.5.4.conv_pw: 40.23% sparsity
blocks.5.4.conv_dw: 40.34% sparsity
blocks.5.4.se.conv_reduce: 44.96% sparsity
blocks.5.4.se.conv_expand: 39.68% sparsity
blocks.5.4.conv_pwl: 39.55% sparsity
blocks.5.5.conv_pw: 40.28% sparsity
blocks.5.5.conv_dw: 40.83% sparsity
blocks.5.5.se.conv_reduce: 44.08% sparsity
blocks.5.5.se.conv_expand: 39.53% sparsity
blocks.5.5.conv_pwl: 39.54% sparsity
blocks.5.6.conv_pw: 40.20% sparsity
blocks.5.6.conv_dw: 40.89% sparsity
blocks.5.6.se.conv_reduce: 43.90% sparsity
blocks.5.6.se.conv_expand: 39.63% sparsity
blocks.5.6.conv_pwl: 39.45% sparsity
blocks.5.7.conv_pw: 40.25% sparsity
blocks.5.7.conv_dw: 40.37% sparsity
blocks.5.7.se.conv_reduce: 42.98% sparsity
blocks.5.7.se.conv_expand: 39.30% sparsity
blocks.5.7.conv_pwl: 39.34% sparsity
blocks.5.8.conv_pw: 40.30% sparsity
blocks.5.8.conv_dw: 39.76% sparsity
blocks.5.8.se.conv_reduce: 42.26% sparsity
blocks.5.8.se.conv_expand: 38.92% sparsity
blocks.5.8.conv_pwl: 39.40% sparsity
blocks.5.9.conv_pw: 40.32% sparsity
blocks.5.9.conv_dw: 40.52% sparsity
blocks.5.9.se.conv_reduce: 43.04% sparsity
blocks.5.9.se.conv_expand: 38.64% sparsity
blocks.5.9.conv_pwl: 39.58% sparsity
blocks.5.10.conv_pw: 40.45% sparsity
blocks.5.10.conv_dw: 40.59% sparsity
blocks.5.10.se.conv_reduce: 43.11% sparsity
blocks.5.10.se.conv_expand: 38.90% sparsity
blocks.5.10.conv_pwl: 39.56% sparsity
blocks.5.11.conv_pw: 40.76% sparsity
blocks.5.11.conv_dw: 40.99% sparsity
blocks.5.11.se.conv_reduce: 44.42% sparsity
blocks.5.11.se.conv_expand: 39.11% sparsity
blocks.5.11.conv_pwl: 39.68% sparsity
blocks.5.12.conv_pw: 41.04% sparsity
blocks.5.12.conv_dw: 39.84% sparsity
blocks.5.12.se.conv_reduce: 44.65% sparsity
blocks.5.12.se.conv_expand: 38.72% sparsity
blocks.5.12.conv_pwl: 39.70% sparsity
blocks.5.13.conv_pw: 41.52% sparsity
blocks.5.13.conv_dw: 39.36% sparsity
blocks.5.13.se.conv_reduce: 46.89% sparsity
blocks.5.13.se.conv_expand: 39.19% sparsity
blocks.5.13.conv_pwl: 40.54% sparsity
blocks.5.14.conv_pw: 41.64% sparsity
blocks.5.14.conv_dw: 39.58% sparsity
blocks.5.14.se.conv_reduce: 46.48% sparsity
blocks.5.14.se.conv_expand: 36.68% sparsity
blocks.5.14.conv_pwl: 40.04% sparsity
conv_head: 38.95% sparsity
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0001
    lr: 0.0001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Date :04/18/2023, 14:52:34
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 1e-05
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
conv_stem: 42.13% sparsity
blocks.0.0.conv: 59.24% sparsity
blocks.0.1.conv: 58.41% sparsity
blocks.1.0.conv_exp: 58.85% sparsity
blocks.1.0.conv_pwl: 52.00% sparsity
blocks.1.1.conv_exp: 57.34% sparsity
blocks.1.1.conv_pwl: 53.27% sparsity
blocks.1.2.conv_exp: 57.07% sparsity
blocks.1.2.conv_pwl: 54.57% sparsity
blocks.1.3.conv_exp: 56.98% sparsity
blocks.1.3.conv_pwl: 54.45% sparsity
blocks.2.0.conv_exp: 59.87% sparsity
blocks.2.0.conv_pwl: 54.98% sparsity
blocks.2.1.conv_exp: 57.33% sparsity
blocks.2.1.conv_pwl: 55.65% sparsity
blocks.2.2.conv_exp: 56.89% sparsity
blocks.2.2.conv_pwl: 54.99% sparsity
blocks.2.3.conv_exp: 57.41% sparsity
blocks.2.3.conv_pwl: 54.73% sparsity
blocks.3.0.conv_pw: 56.34% sparsity
blocks.3.0.conv_dw: 38.19% sparsity
blocks.3.0.se.conv_reduce: 96.70% sparsity
blocks.3.0.se.conv_expand: 96.00% sparsity
blocks.3.0.conv_pwl: 56.17% sparsity
blocks.3.1.conv_pw: 57.62% sparsity
blocks.3.1.conv_dw: 53.17% sparsity
blocks.3.1.se.conv_reduce: 66.46% sparsity
blocks.3.1.se.conv_expand: 58.12% sparsity
blocks.3.1.conv_pwl: 57.26% sparsity
blocks.3.2.conv_pw: 57.17% sparsity
blocks.3.2.conv_dw: 53.28% sparsity
blocks.3.2.se.conv_reduce: 66.56% sparsity
blocks.3.2.se.conv_expand: 56.51% sparsity
blocks.3.2.conv_pwl: 56.63% sparsity
blocks.3.3.conv_pw: 57.39% sparsity
blocks.3.3.conv_dw: 53.28% sparsity
blocks.3.3.se.conv_reduce: 61.91% sparsity
blocks.3.3.se.conv_expand: 54.54% sparsity
blocks.3.3.conv_pwl: 56.78% sparsity
blocks.3.4.conv_pw: 58.16% sparsity
blocks.3.4.conv_dw: 55.32% sparsity
blocks.3.4.se.conv_reduce: 64.89% sparsity
blocks.3.4.se.conv_expand: 62.63% sparsity
blocks.3.4.conv_pwl: 59.19% sparsity
blocks.3.5.conv_pw: 57.97% sparsity
blocks.3.5.conv_dw: 54.47% sparsity
blocks.3.5.se.conv_reduce: 65.64% sparsity
blocks.3.5.se.conv_expand: 56.71% sparsity
blocks.3.5.conv_pwl: 58.80% sparsity
blocks.4.0.conv_pw: 57.47% sparsity
blocks.4.0.conv_dw: 61.17% sparsity
blocks.4.0.se.conv_reduce: 62.83% sparsity
blocks.4.0.se.conv_expand: 54.87% sparsity
blocks.4.0.conv_pwl: 56.11% sparsity
blocks.4.1.conv_pw: 57.35% sparsity
blocks.4.1.conv_dw: 53.44% sparsity
blocks.4.1.se.conv_reduce: 65.24% sparsity
blocks.4.1.se.conv_expand: 57.67% sparsity
blocks.4.1.conv_pwl: 56.57% sparsity
blocks.4.2.conv_pw: 57.36% sparsity
blocks.4.2.conv_dw: 54.64% sparsity
blocks.4.2.se.conv_reduce: 65.98% sparsity
blocks.4.2.se.conv_expand: 57.73% sparsity
blocks.4.2.conv_pwl: 56.74% sparsity
blocks.4.3.conv_pw: 57.23% sparsity
blocks.4.3.conv_dw: 53.96% sparsity
blocks.4.3.se.conv_reduce: 64.08% sparsity
blocks.4.3.se.conv_expand: 58.30% sparsity
blocks.4.3.conv_pwl: 56.72% sparsity
blocks.4.4.conv_pw: 57.53% sparsity
blocks.4.4.conv_dw: 55.79% sparsity
blocks.4.4.se.conv_reduce: 66.20% sparsity
blocks.4.4.se.conv_expand: 59.37% sparsity
blocks.4.4.conv_pwl: 56.66% sparsity
blocks.4.5.conv_pw: 57.74% sparsity
blocks.4.5.conv_dw: 55.06% sparsity
blocks.4.5.se.conv_reduce: 67.35% sparsity
blocks.4.5.se.conv_expand: 60.22% sparsity
blocks.4.5.conv_pwl: 57.49% sparsity
blocks.4.6.conv_pw: 57.88% sparsity
blocks.4.6.conv_dw: 55.71% sparsity
blocks.4.6.se.conv_reduce: 65.39% sparsity
blocks.4.6.se.conv_expand: 57.57% sparsity
blocks.4.6.conv_pwl: 56.76% sparsity
blocks.4.7.conv_pw: 57.82% sparsity
blocks.4.7.conv_dw: 56.32% sparsity
blocks.4.7.se.conv_reduce: 62.96% sparsity
blocks.4.7.se.conv_expand: 56.96% sparsity
blocks.4.7.conv_pwl: 56.83% sparsity
blocks.4.8.conv_pw: 57.88% sparsity
blocks.4.8.conv_dw: 54.27% sparsity
blocks.4.8.se.conv_reduce: 64.39% sparsity
blocks.4.8.se.conv_expand: 55.68% sparsity
blocks.4.8.conv_pwl: 56.86% sparsity
blocks.5.0.conv_pw: 56.68% sparsity
blocks.5.0.conv_dw: 48.11% sparsity
blocks.5.0.se.conv_reduce: 96.75% sparsity
blocks.5.0.se.conv_expand: 93.06% sparsity
blocks.5.0.conv_pwl: 57.29% sparsity
blocks.5.1.conv_pw: 57.18% sparsity
blocks.5.1.conv_dw: 54.85% sparsity
blocks.5.1.se.conv_reduce: 62.10% sparsity
blocks.5.1.se.conv_expand: 56.42% sparsity
blocks.5.1.conv_pwl: 56.16% sparsity
blocks.5.2.conv_pw: 57.11% sparsity
blocks.5.2.conv_dw: 55.45% sparsity
blocks.5.2.se.conv_reduce: 61.60% sparsity
blocks.5.2.se.conv_expand: 56.04% sparsity
blocks.5.2.conv_pwl: 55.92% sparsity
blocks.5.3.conv_pw: 57.09% sparsity
blocks.5.3.conv_dw: 56.00% sparsity
blocks.5.3.se.conv_reduce: 61.99% sparsity
blocks.5.3.se.conv_expand: 56.50% sparsity
blocks.5.3.conv_pwl: 56.14% sparsity
blocks.5.4.conv_pw: 56.99% sparsity
blocks.5.4.conv_dw: 56.32% sparsity
blocks.5.4.se.conv_reduce: 61.46% sparsity
blocks.5.4.se.conv_expand: 56.32% sparsity
blocks.5.4.conv_pwl: 56.01% sparsity
blocks.5.5.conv_pw: 57.00% sparsity
blocks.5.5.conv_dw: 57.54% sparsity
blocks.5.5.se.conv_reduce: 60.81% sparsity
blocks.5.5.se.conv_expand: 56.19% sparsity
blocks.5.5.conv_pwl: 56.08% sparsity
blocks.5.6.conv_pw: 56.87% sparsity
blocks.5.6.conv_dw: 57.52% sparsity
blocks.5.6.se.conv_reduce: 60.73% sparsity
blocks.5.6.se.conv_expand: 56.00% sparsity
blocks.5.6.conv_pwl: 55.96% sparsity
blocks.5.7.conv_pw: 57.03% sparsity
blocks.5.7.conv_dw: 57.79% sparsity
blocks.5.7.se.conv_reduce: 59.77% sparsity
blocks.5.7.se.conv_expand: 55.63% sparsity
blocks.5.7.conv_pwl: 55.97% sparsity
blocks.5.8.conv_pw: 57.05% sparsity
blocks.5.8.conv_dw: 57.88% sparsity
blocks.5.8.se.conv_reduce: 59.30% sparsity
blocks.5.8.se.conv_expand: 55.11% sparsity
blocks.5.8.conv_pwl: 55.81% sparsity
blocks.5.9.conv_pw: 57.18% sparsity
blocks.5.9.conv_dw: 58.21% sparsity
blocks.5.9.se.conv_reduce: 60.02% sparsity
blocks.5.9.se.conv_expand: 55.22% sparsity
blocks.5.9.conv_pwl: 56.11% sparsity
blocks.5.10.conv_pw: 57.33% sparsity
blocks.5.10.conv_dw: 58.63% sparsity
blocks.5.10.se.conv_reduce: 60.08% sparsity
blocks.5.10.se.conv_expand: 55.32% sparsity
blocks.5.10.conv_pwl: 56.02% sparsity
blocks.5.11.conv_pw: 57.57% sparsity
blocks.5.11.conv_dw: 58.64% sparsity
blocks.5.11.se.conv_reduce: 61.18% sparsity
blocks.5.11.se.conv_expand: 55.57% sparsity
blocks.5.11.conv_pwl: 56.27% sparsity
blocks.5.12.conv_pw: 57.91% sparsity
blocks.5.12.conv_dw: 57.95% sparsity
blocks.5.12.se.conv_reduce: 61.70% sparsity
blocks.5.12.se.conv_expand: 54.99% sparsity
blocks.5.12.conv_pwl: 56.29% sparsity
blocks.5.13.conv_pw: 58.59% sparsity
blocks.5.13.conv_dw: 57.63% sparsity
blocks.5.13.se.conv_reduce: 64.07% sparsity
blocks.5.13.se.conv_expand: 55.31% sparsity
blocks.5.13.conv_pwl: 57.30% sparsity
blocks.5.14.conv_pw: 58.62% sparsity
blocks.5.14.conv_dw: 58.39% sparsity
blocks.5.14.se.conv_reduce: 64.24% sparsity
blocks.5.14.se.conv_expand: 52.42% sparsity
blocks.5.14.conv_pwl: 56.61% sparsity
conv_head: 55.39% sparsity
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 1e-05
    lr: 1e-05
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Train loss: 1.8519, val loss: 1.3270
0.9170837264347174
Model improve: 0.0000 -> 0.9171
Epoch: 2/25
Train loss: 1.7279, val loss: 1.2744
0.9180437208627965
Model improve: 0.9171 -> 0.9180
Epoch: 3/25
Train loss: 1.7815, val loss: 1.2724
0.9194488523448654
Model improve: 0.9180 -> 0.9194
Epoch: 4/25
Train loss: 1.6161, val loss: 1.2677
0.919250910965515
Epoch: 5/25
Train loss: 1.7104, val loss: 1.2684
0.9196551136118596
Model improve: 0.9194 -> 0.9197
Epoch: 6/25
Train loss: 1.6655, val loss: 1.2649
0.9204504407898231
Model improve: 0.9197 -> 0.9205
Epoch: 7/25
Train loss: 1.5559, val loss: 1.2827
0.9206752357554806
Model improve: 0.9205 -> 0.9207
Epoch: 8/25
Train loss: 1.5875, val loss: 1.2577
0.9209019179575486
Model improve: 0.9207 -> 0.9209
Epoch: 9/25
Train loss: 1.5200, val loss: 1.2674
0.9213480386878653
Model improve: 0.9209 -> 0.9213
Epoch: 10/25
Date :04/18/2023, 15:44:34
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 1e-05
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 1e-05
    lr: 1e-05
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Date :04/18/2023, 15:45:03
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Train loss: 7.1080, val loss: 2.3943
0.8333188750808624
Model improve: 0.0000 -> 0.8333
Epoch: 2/25
Train loss: 3.2532, val loss: 1.8984
0.8804327356345782
Model improve: 0.8333 -> 0.8804
Epoch: 3/25
Train loss: 2.9016, val loss: 1.7549
0.8880746144987796
Model improve: 0.8804 -> 0.8881
Epoch: 4/25
Date :04/18/2023, 16:06:25
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Train loss: 7.0892, val loss: 2.3884
0.8345330150129163
Model improve: 0.0000 -> 0.8345
Epoch: 2/25
Train loss: 3.2452, val loss: 1.9021
0.8799671153324428
Model improve: 0.8345 -> 0.8800
Epoch: 3/25
Train loss: 2.8983, val loss: 1.7806
0.8869609472630581
Model improve: 0.8800 -> 0.8870
Epoch: 4/25
Train loss: 2.5810, val loss: 1.6544
0.894855695934733
Model improve: 0.8870 -> 0.8949
Epoch: 5/25
Date :04/18/2023, 16:41:29
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Train loss: 7.5839, val loss: 2.6323
0.812362335394137
Model improve: 0.0000 -> 0.8124
Epoch: 2/25
Train loss: 3.7860, val loss: 2.0798
0.8651472359949207
Model improve: 0.8124 -> 0.8651
Epoch: 3/25
Date :04/18/2023, 16:52:30
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Train loss: 7.7610, val loss: 2.6275
0.8206770047220339
Model improve: 0.0000 -> 0.8207
Epoch: 2/25
Date :04/18/2023, 16:59:00
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Train loss: 7.4920, val loss: 2.4515
0.8296895545815889
Model improve: 0.0000 -> 0.8297
Epoch: 2/25
Train loss: 3.3069, val loss: 1.9503
0.8735723168694165
Model improve: 0.8297 -> 0.8736
Epoch: 3/25
Train loss: 2.9498, val loss: 1.8267
0.885442051846221
Model improve: 0.8736 -> 0.8854
Epoch: 4/25
Train loss: 2.6322, val loss: 1.6268
0.8954630507078574
Model improve: 0.8854 -> 0.8955
Epoch: 5/25
Train loss: 2.5869, val loss: 1.5890
0.8969413502123507
Model improve: 0.8955 -> 0.8969
Epoch: 6/25
Date :04/18/2023, 17:28:29
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Train loss: 7.0890, val loss: 2.3954
0.8344052761473114
Model improve: 0.0000 -> 0.8344
Epoch: 2/25
Train loss: 3.2504, val loss: 1.9126
0.8794979433397425
Model improve: 0.8344 -> 0.8795
Epoch: 3/25
Train loss: 2.9009, val loss: 1.7757
0.8876792292546316
Model improve: 0.8795 -> 0.8877
Epoch: 4/25
Train loss: 2.5732, val loss: 1.6603
0.8951347572055754
Model improve: 0.8877 -> 0.8951
Epoch: 5/25
Train loss: 2.5350, val loss: 1.6209
0.8951689018798842
Model improve: 0.8951 -> 0.8952
Epoch: 6/25
Train loss: 2.4118, val loss: 1.5846
0.9039783712354972
Model improve: 0.8952 -> 0.9040
Epoch: 7/25
Train loss: 2.2294, val loss: 1.5334
0.909221966036113
Model improve: 0.9040 -> 0.9092
Epoch: 8/25
Train loss: 2.2000, val loss: 1.5080
0.9100467685523395
Model improve: 0.9092 -> 0.9100
Epoch: 9/25
Train loss: 2.1088, val loss: 1.4716
0.9084161708789025
Epoch: 10/25
Train loss: 2.1312, val loss: 1.3813
0.9135857779145551
Model improve: 0.9100 -> 0.9136
Epoch: 11/25
Train loss: 2.0213, val loss: 1.4121
0.912997549444274
Epoch: 12/25
Train loss: 1.9355, val loss: 1.3558
0.9123733983701301
Epoch: 13/25
Train loss: 1.8760, val loss: 1.3906
0.9156387213755542
Model improve: 0.9136 -> 0.9156
Epoch: 14/25
Train loss: 1.8604, val loss: 1.3396
0.9177368831306829
Model improve: 0.9156 -> 0.9177
Epoch: 15/25
Train loss: 1.7708, val loss: 1.3188
0.9183778058639954
Model improve: 0.9177 -> 0.9184
Epoch: 16/25
Train loss: 1.7764, val loss: 1.3473
0.9167936408497066
Epoch: 17/25
Train loss: 1.6208, val loss: 1.2893
0.9223934780330084
Model improve: 0.9184 -> 0.9224
Epoch: 18/25
Train loss: 1.6866, val loss: 1.2848
0.9202797647982276
Epoch: 19/25
Train loss: 1.6672, val loss: 1.2746
0.9202464112776509
Epoch: 20/25
Date :04/18/2023, 19:17:16
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Train loss: 7.0946, val loss: 2.3894
0.8341034822878839
Model improve: 0.0000 -> 0.8341
Epoch: 2/25
Train loss: 3.2511, val loss: 1.9203
0.8794230266535291
Model improve: 0.8341 -> 0.8794
Epoch: 3/25
Train loss: 2.9024, val loss: 1.7686
0.8878216088097588
Model improve: 0.8794 -> 0.8878
Epoch: 4/25
Train loss: 2.5780, val loss: 1.6523
0.8935190960440275
Model improve: 0.8878 -> 0.8935
Epoch: 5/25
Train loss: 2.5385, val loss: 1.5987
0.8967554475077538
Model improve: 0.8935 -> 0.8968
Epoch: 6/25
Train loss: 2.4134, val loss: 1.6538
0.9033080372726098
Model improve: 0.8968 -> 0.9033
Epoch: 7/25
Train loss: 2.2326, val loss: 1.5553
0.9070909747667101
Model improve: 0.9033 -> 0.9071
Epoch: 8/25
Train loss: 2.1992, val loss: 1.4831
0.909748664978136
Model improve: 0.9071 -> 0.9097
Epoch: 9/25
Train loss: 2.1091, val loss: 1.4803
0.9109132374746826
Model improve: 0.9097 -> 0.9109
Epoch: 10/25
Train loss: 2.1289, val loss: 1.4113
0.910389840902019
Epoch: 11/25
Train loss: 2.0169, val loss: 1.4110
0.915276313716635
Model improve: 0.9109 -> 0.9153
Epoch: 12/25
Date :04/18/2023, 20:17:53
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
Date :04/18/2023, 20:19:13
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Train loss: 7.4696, val loss: 2.3681
0.8358383388563076
Model improve: 0.0000 -> 0.8358
Epoch: 2/25
Date :04/18/2023, 20:26:47
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 256
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
Date :04/18/2023, 20:28:36
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 36
validbs: 144
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/60
Date :04/18/2023, 20:28:47
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 36
validbs: 144
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/60
Train loss: 8.0143, val loss: 2.3695
0.8359118028578606
Model improve: 0.0000 -> 0.8359
Epoch: 2/60
Train loss: 3.2991, val loss: 1.8501
0.8769935655494416
Model improve: 0.8359 -> 0.8770
Epoch: 3/60
Date :04/18/2023, 20:39:49
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/60
Train loss: 7.5395, val loss: 2.3075
0.8412796400148328
Model improve: 0.0000 -> 0.8413
Epoch: 2/60
Train loss: 3.2533, val loss: 1.8137
0.8828937663943324
Model improve: 0.8413 -> 0.8829
Epoch: 3/60
Train loss: 2.7383, val loss: 1.7514
0.8940782266101824
Model improve: 0.8829 -> 0.8941
Epoch: 4/60
Train loss: 2.5627, val loss: 1.5698
0.9007285109785766
Model improve: 0.8941 -> 0.9007
Epoch: 5/60
Train loss: 2.4009, val loss: 1.5600
0.9041153069049811
Model improve: 0.9007 -> 0.9041
Epoch: 6/60
Train loss: 2.3754, val loss: 1.5388
0.9037269331846627
Epoch: 7/60
Train loss: 2.1947, val loss: 1.4782
0.9073477588161072
Model improve: 0.9041 -> 0.9073
Epoch: 8/60
Train loss: 2.1667, val loss: 1.5029
0.9121913898398775
Model improve: 0.9073 -> 0.9122
Epoch: 9/60
Train loss: 2.1919, val loss: 1.4294
0.9128118595103281
Model improve: 0.9122 -> 0.9128
Epoch: 10/60
Train loss: 2.1365, val loss: 1.4393
0.9115713819724979
Epoch: 11/60
Train loss: 2.0531, val loss: 1.4223
0.9144845811493585
Model improve: 0.9128 -> 0.9145
Epoch: 12/60
Train loss: 2.0304, val loss: 1.4137
0.9134010389100444
Epoch: 13/60
Train loss: 1.9652, val loss: 1.3885
0.9119612950208786
Epoch: 14/60
Train loss: 1.9107, val loss: 1.4026
0.9148114925718223
Model improve: 0.9145 -> 0.9148
Epoch: 15/60
Train loss: 1.8959, val loss: 1.3844
0.915044177638502
Model improve: 0.9148 -> 0.9150
Epoch: 16/60
Train loss: 1.8812, val loss: 1.4166
0.9163385282133966
Model improve: 0.9150 -> 0.9163
Epoch: 17/60
Train loss: 1.9135, val loss: 1.4232
0.9138679189834615
Epoch: 18/60
Train loss: 1.8441, val loss: 1.3808
0.9153788276668349
Epoch: 19/60
Train loss: 1.8822, val loss: 1.3476
0.9162332694136851
Epoch: 20/60
Train loss: 1.7905, val loss: 1.3997
0.9149396210127937
Epoch: 21/60
Date :04/19/2023, 07:00:31
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/60
Train loss: 7.5200, val loss: 2.3256
0.8407302974017202
Model improve: 0.0000 -> 0.8407
Epoch: 2/60
Train loss: 3.2591, val loss: 1.8464
0.8811855185276447
Model improve: 0.8407 -> 0.8812
Epoch: 3/60
Train loss: 2.7379, val loss: 1.7535
0.8921395344463704
Model improve: 0.8812 -> 0.8921
Epoch: 4/60
Train loss: 2.5622, val loss: 1.5872
0.8996834368124109
Model improve: 0.8921 -> 0.8997
Epoch: 5/60
Train loss: 2.4014, val loss: 1.5490
0.9029968853851621
Model improve: 0.8997 -> 0.9030
Epoch: 6/60
Train loss: 2.3822, val loss: 1.4979
0.9039528727747996
Model improve: 0.9030 -> 0.9040
Epoch: 7/60
Train loss: 2.1977, val loss: 1.5020
0.9047686152213674
Model improve: 0.9040 -> 0.9048
Epoch: 8/60
Train loss: 2.1660, val loss: 1.5533
0.9066509968493334
Model improve: 0.9048 -> 0.9067
Epoch: 9/60
Train loss: 2.1885, val loss: 1.3897
0.9102743768065188
Model improve: 0.9067 -> 0.9103
Epoch: 10/60
Train loss: 2.1288, val loss: 1.4913
0.9060791733511577
Epoch: 11/60
Train loss: 2.0554, val loss: 1.4419
0.9125745932104566
Model improve: 0.9103 -> 0.9126
Epoch: 12/60
Train loss: 2.0250, val loss: 1.3997
0.9154130504213881
Model improve: 0.9126 -> 0.9154
Epoch: 13/60
Train loss: 1.9632, val loss: 1.4156
0.9103984404843852
Epoch: 14/60
Train loss: 1.9138, val loss: 1.4055
0.9136975921673623
Epoch: 15/60
Train loss: 1.8934, val loss: 1.4651
0.9086660117938999
Epoch: 16/60
Train loss: 1.8821, val loss: 1.4808
0.9104322649131166
Epoch: 17/60
Train loss: 1.9083, val loss: 1.4002
0.9136027490866601
Epoch: 18/60
Train loss: 1.8372, val loss: 1.4472
0.9092691107286717
Epoch: 19/60
Train loss: 1.8763, val loss: 1.4353
0.9105644199554839
Epoch: 20/60
Train loss: 1.7846, val loss: 1.4030
0.91308743256155
Epoch: 21/60
Train loss: 1.8505, val loss: 1.4113
0.9154224736775607
Model improve: 0.9154 -> 0.9154
Epoch: 22/60
Train loss: 1.8141, val loss: 1.3557
0.9160754879548813
Model improve: 0.9154 -> 0.9161
Epoch: 23/60
Train loss: 1.7463, val loss: 1.3166
0.915744360812116
Epoch: 24/60
Train loss: 1.7029, val loss: 1.3303
0.915692991905405
Epoch: 25/60
Train loss: 1.6702, val loss: 1.3510
0.914296898891661
Epoch: 26/60
Train loss: 1.7856, val loss: 1.2372
0.9215760686139509
Model improve: 0.9161 -> 0.9216
Epoch: 27/60
Train loss: 1.7011, val loss: 1.3001
0.9193637513989946
Epoch: 28/60
Train loss: 1.6209, val loss: 1.2975
0.9198225398147668
Epoch: 29/60
Train loss: 1.5886, val loss: 1.2993
0.9183479331054755
Epoch: 30/60
Train loss: 1.5961, val loss: 1.2659
0.9214016014428508
Epoch: 31/60
Train loss: 1.6263, val loss: 1.2970
0.9198478432101809
Epoch: 32/60
Train loss: 1.4897, val loss: 1.2615
0.920334783365612
Epoch: 33/60
Train loss: 1.5866, val loss: 1.2410
0.9200873972207837
Epoch: 34/60
Train loss: 1.4973, val loss: 1.3273
0.9194313965173909
Epoch: 35/60
Train loss: 1.5206, val loss: 1.2854
0.919039826242458
Epoch: 36/60
Train loss: 1.3707, val loss: 1.3120
0.9197124397984876
Epoch: 37/60
Train loss: 1.5050, val loss: 1.2620
0.9228822408710518
Model improve: 0.9216 -> 0.9229
Epoch: 38/60
Train loss: 1.4509, val loss: 1.2482
0.9211382750841293
Epoch: 39/60
Train loss: 1.4667, val loss: 1.2609
0.9211304903222323
Epoch: 40/60
Date :04/19/2023, 10:34:34
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/60
Train loss: 7.5525, val loss: 2.2644
0.846543117028054
Model improve: 0.0000 -> 0.8465
Epoch: 2/60
Train loss: 3.2624, val loss: 1.8146
0.8822520546874002
Model improve: 0.8465 -> 0.8823
Epoch: 3/60
Train loss: 2.7574, val loss: 1.6996
0.8939744845539783
Model improve: 0.8823 -> 0.8940
Epoch: 4/60
Train loss: 2.5663, val loss: 1.5800
0.8978466807219132
Model improve: 0.8940 -> 0.8978
Epoch: 5/60
Train loss: 2.3851, val loss: 1.5318
0.9027757065279574
Model improve: 0.8978 -> 0.9028
Epoch: 6/60
Train loss: 2.3665, val loss: 1.4977
0.9049679716370955
Model improve: 0.9028 -> 0.9050
Epoch: 7/60
Train loss: 2.1831, val loss: 1.5013
0.9039618797170721
Epoch: 8/60
Train loss: 2.1771, val loss: 1.5008
0.9064381436180895
Model improve: 0.9050 -> 0.9064
Epoch: 9/60
Train loss: 2.1951, val loss: 1.4828
0.9107926258042852
Model improve: 0.9064 -> 0.9108
Epoch: 10/60
Train loss: 2.1257, val loss: 1.4382
0.9110977675298074
Model improve: 0.9108 -> 0.9111
Epoch: 11/60
Train loss: 2.0714, val loss: 1.4493
0.9109097139693083
Epoch: 12/60
Train loss: 2.0196, val loss: 1.4084
0.9127694851951218
Model improve: 0.9111 -> 0.9128
Epoch: 13/60
Train loss: 1.9816, val loss: 1.4357
0.9121354863523039
Epoch: 14/60
Train loss: 1.9076, val loss: 1.4227
0.9133484318359694
Model improve: 0.9128 -> 0.9133
Epoch: 15/60
Train loss: 1.8850, val loss: 1.3821
0.9129591944788534
Epoch: 16/60
Train loss: 1.8846, val loss: 1.4651
0.9148399438217623
Model improve: 0.9133 -> 0.9148
Epoch: 17/60
Train loss: 1.8956, val loss: 1.4330
0.9125724794479577
Epoch: 18/60
Train loss: 1.8455, val loss: 1.4566
0.9105379269143045
Epoch: 19/60
Train loss: 1.8558, val loss: 1.4549
0.9157237685386688
Model improve: 0.9148 -> 0.9157
Epoch: 20/60
Train loss: 1.7983, val loss: 1.4015
0.9101411470086962
Epoch: 21/60
Train loss: 1.8441, val loss: 1.3506
0.915608630844866
Epoch: 22/60
Train loss: 1.8166, val loss: 1.3475
0.9157571012445348
Model improve: 0.9157 -> 0.9158
Epoch: 23/60
Train loss: 1.7460, val loss: 1.3893
0.9114638151640799
Epoch: 24/60
Train loss: 1.6907, val loss: 1.3447
0.9139911597339848
Epoch: 25/60
Train loss: 1.6686, val loss: 1.3077
0.9153972357955933
Epoch: 26/60
Train loss: 1.7818, val loss: 1.3018
0.9183873461157263
Model improve: 0.9158 -> 0.9184
Epoch: 27/60
Train loss: 1.7119, val loss: 1.2992
0.9223007017327298
Model improve: 0.9184 -> 0.9223
Epoch: 28/60
Train loss: 1.6181, val loss: 1.3522
0.918338416624382
Epoch: 29/60
Train loss: 1.5950, val loss: 1.3138
0.9182673628509934
Epoch: 30/60
Train loss: 1.5930, val loss: 1.2963
0.9204673091376117
Epoch: 31/60
Train loss: 1.6278, val loss: 1.2865
0.9181104543591867
Epoch: 32/60
Train loss: 1.5030, val loss: 1.2879
0.9166106169340756
Epoch: 33/60
Train loss: 1.5919, val loss: 1.2927
0.9180372107195471
Epoch: 34/60
Train loss: 1.4866, val loss: 1.2938
0.918960758434897
Epoch: 35/60
Train loss: 1.5280, val loss: 1.2931
0.9170374077007725
Epoch: 36/60
Train loss: 1.3731, val loss: 1.2930
0.9204599359579986
Epoch: 37/60
Train loss: 1.5043, val loss: 1.2897
0.9188840200466029
Epoch: 38/60
Train loss: 1.4460, val loss: 1.2818
0.9191148663881682
Epoch: 39/60
Train loss: 1.4709, val loss: 1.2421
0.9227128868158134
Model improve: 0.9223 -> 0.9227
Epoch: 40/60
Train loss: 1.4361, val loss: 1.2881
0.9201018293751965
Epoch: 41/60
Train loss: 1.4117, val loss: 1.2630
0.9238463682702714
Model improve: 0.9227 -> 0.9238
Epoch: 42/60
Train loss: 1.5310, val loss: 1.2685
0.9228406569261478
Epoch: 43/60
Train loss: 1.5116, val loss: 1.2292
0.9257206134855562
Model improve: 0.9238 -> 0.9257
Epoch: 44/60
Train loss: 1.4281, val loss: 1.2456
0.9251060104609173
Epoch: 45/60
Train loss: 1.4764, val loss: 1.2493
0.9224412447486169
Epoch: 46/60
Train loss: 1.4391, val loss: 1.2522
0.9223790927282992
Epoch: 47/60
Train loss: 1.4005, val loss: 1.2281
0.9250171611277377
Epoch: 48/60
Train loss: 1.4300, val loss: 1.2498
0.9241959289882162
Epoch: 49/60
Train loss: 1.3427, val loss: 1.2430
0.9238381651207584
Epoch: 50/60
Train loss: 1.4174, val loss: 1.2444
0.9243540501414236
Epoch: 51/60
Train loss: 1.3858, val loss: 1.2538
0.9234573143850799
Epoch: 52/60
Train loss: 1.3060, val loss: 1.2462
0.9248975850921236
Epoch: 53/60
Train loss: 1.3682, val loss: 1.2370
0.9246686164549112
Epoch: 54/60
Date :04/20/2023, 14:35:14
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 224
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnet_b0_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Date :04/20/2023, 14:35:48
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 224
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnet_b0_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Date :04/20/2023, 14:37:23
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 224
fmax: 16000
trainbs: 48
validbs: 192
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnet_b0_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Date :04/20/2023, 14:42:01
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 224
fmax: 16000
trainbs: 48
validbs: 96
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnet_b0_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Train loss: 9.1886, val loss: 4.1837
0.6435294237368586
Model improve: 0.0000 -> 0.6435
Epoch: 2/25
Train loss: 4.6551, val loss: 3.1496
0.7606343755731907
Model improve: 0.6435 -> 0.7606
Epoch: 3/25
Train loss: 3.9885, val loss: 2.6463
0.803125324532599
Model improve: 0.7606 -> 0.8031
Epoch: 4/25
Date :04/25/2023, 04:03:30
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/60
Date :04/25/2023, 04:04:10
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/60
Train loss: 7.5561, val loss: 2.2662
0.8464468090542572
Model improve: 0.0000 -> 0.8464
Epoch: 2/60
Train loss: 3.2622, val loss: 1.8059
0.882091607093339
Model improve: 0.8464 -> 0.8821
Epoch: 3/60
Date :05/09/2023, 18:05:41
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
60304
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/20
Date :05/09/2023, 18:06:33
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
60304
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/20
Date :05/09/2023, 18:06:59
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
60304
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/20
Train loss: 7.2171, val loss: 4.1095
0.5732220588189245
Model improve: 0.0000 -> 0.5732
Epoch: 2/20
Train loss: 5.0393, val loss: 3.1686
0.6927572497420372
Model improve: 0.5732 -> 0.6928
Epoch: 3/20
Date :05/09/2023, 18:44:14
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
60304
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/20
Train loss: 7.4803, val loss: 4.0094
0.6004096194984652
Model improve: 0.0000 -> 0.6004
Epoch: 2/20
Train loss: 4.9091, val loss: 3.0567
0.7180513074313172
Model improve: 0.6004 -> 0.7181
Epoch: 3/20
Train loss: 4.3797, val loss: 2.6372
0.7688032482842634
Model improve: 0.7181 -> 0.7688
Epoch: 4/20
Train loss: 4.1113, val loss: 2.4051
0.8003368016947567
Model improve: 0.7688 -> 0.8003
Epoch: 5/20
Train loss: 3.8658, val loss: 2.2103
0.8186300253916011
Model improve: 0.8003 -> 0.8186
Epoch: 6/20
Train loss: 3.6616, val loss: 2.0696
0.8339163607006059
Model improve: 0.8186 -> 0.8339
Epoch: 7/20
Date :05/09/2023, 20:05:17
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Date :05/09/2023, 20:05:54
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Date :05/09/2023, 20:06:39
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Date :05/09/2023, 20:07:04
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
60304
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/20
Date :05/09/2023, 20:07:51
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
60304
Date :05/09/2023, 20:08:01
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
65256
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/20
Date :05/09/2023, 20:08:45
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
65256
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/20
Date :05/09/2023, 20:18:21
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
65256
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/20
Train loss: 7.5040, val loss: 4.2681
0.5839897161839421
f1: 0.3078046642971907
Model improve: 0.0000 -> 0.5840
Epoch: 2/20
Train loss: 4.9923, val loss: 3.1275
0.7171363696521715
f1: 0.5437970991005592
Model improve: 0.5840 -> 0.7171
Epoch: 3/20
Train loss: 4.4700, val loss: 2.7320
0.7683750054901277
f1: 0.6208123791102514
Model improve: 0.7171 -> 0.7684
Epoch: 4/20
Train loss: 4.1340, val loss: 2.3794
0.8006787502105528
f1: 0.6762995207959908
Model improve: 0.7684 -> 0.8007
Epoch: 5/20
Train loss: 3.8855, val loss: 2.2787
0.8211180904551401
f1: 0.7042150095461889
Model improve: 0.8007 -> 0.8211
Epoch: 6/20
Train loss: 3.7120, val loss: 2.0699
0.8339546431885132
f1: 0.7244787482974191
Model improve: 0.8211 -> 0.8340
Epoch: 7/20
Train loss: 3.5696, val loss: 2.0246
0.8457377856283227
f1: 0.7433849226160758
Model improve: 0.8340 -> 0.8457
Epoch: 8/20
Train loss: 3.4592, val loss: 2.0426
0.8481474741192685
f1: 0.7498659421585099
Model improve: 0.8457 -> 0.8481
Epoch: 9/20
Train loss: 3.3461, val loss: 1.8297
0.8617194216720052
f1: 0.7711579380877206
Model improve: 0.8481 -> 0.8617
Epoch: 10/20
Date :05/09/2023, 22:17:27
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 1
65256
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/20
Date :05/09/2023, 22:18:45
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 1
65256
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/20
Date :05/09/2023, 22:21:18
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 1
65256
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/20
Train loss: 7.5279, val loss: 4.4226
0.5745457033033083
f1: 0.292921155976236
Model improve: 0.0000 -> 0.5745
Epoch: 2/20
Date :05/09/2023, 22:34:53
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 1
65256
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/20
Date :05/09/2023, 23:24:57
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 40
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 1
65256
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/40
Date :05/09/2023, 23:27:52
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 40
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 1
65256
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/40
Train loss: 7.4763, val loss: 4.4099
0.5766852685738544
f1: 0.2868901826942381
Model improve: 0.0000 -> 0.2869
Epoch: 2/40
Train loss: 4.9882, val loss: 3.2184
0.7137707072670246
f1: 0.5269441624365483
Model improve: 0.2869 -> 0.5269
Epoch: 3/40
Train loss: 4.4662, val loss: 2.7930
0.7632756319812289
f1: 0.6117165676515718
Model improve: 0.5269 -> 0.6117
Epoch: 4/40
Train loss: 4.1522, val loss: 2.4061
0.7985771780933102
f1: 0.671546808901968
Model improve: 0.6117 -> 0.6715
Epoch: 5/40
Train loss: 3.9053, val loss: 2.3838
0.8152790991948764
f1: 0.68929774552993
Model improve: 0.6715 -> 0.6893
Epoch: 6/40
Train loss: 3.7635, val loss: 2.1131
0.8288731002377031
f1: 0.7160632294696995
Model improve: 0.6893 -> 0.7161
Epoch: 7/40
Train loss: 3.6455, val loss: 2.0539
0.8402292760840367
f1: 0.7389002611703254
Model improve: 0.7161 -> 0.7389
Epoch: 8/40
Train loss: 3.5584, val loss: 2.0276
0.8463244314805993
f1: 0.7425009738994934
Model improve: 0.7389 -> 0.7425
Epoch: 9/40
Train loss: 3.4751, val loss: 1.9257
0.8545559093566973
f1: 0.7568515984138682
Model improve: 0.7425 -> 0.7569
Epoch: 10/40
Train loss: 3.4048, val loss: 2.0791
0.8511276853527285
f1: 0.7466287630293752
Epoch: 11/40
Train loss: 3.2955, val loss: 1.8884
0.8604488873204118
f1: 0.768831715096919
Model improve: 0.7569 -> 0.7688
Epoch: 12/40
Train loss: 3.2323, val loss: 1.8292
0.8647121800143986
f1: 0.7761162960136001
Model improve: 0.7688 -> 0.7761
Epoch: 13/40
Train loss: 3.2040, val loss: 1.7469
0.8717957801795632
f1: 0.7857635535935992
Model improve: 0.7761 -> 0.7858
Epoch: 14/40
Train loss: 3.1387, val loss: 1.8579
0.8700289080873106
f1: 0.7807254249167689
Epoch: 15/40
Train loss: 3.1290, val loss: 1.7570
0.8757139254834256
f1: 0.7896433399188937
Model improve: 0.7858 -> 0.7896
Epoch: 16/40
Train loss: 3.0549, val loss: 1.7177
0.8757829578667192
f1: 0.790885551527706
Model improve: 0.7896 -> 0.7909
Epoch: 17/40
Train loss: 3.0142, val loss: 1.7247
0.8774593373889338
f1: 0.7971586971586971
Model improve: 0.7909 -> 0.7972
Epoch: 18/40
Train loss: 2.9858, val loss: 1.7201
0.8807257892623918
f1: 0.8000696136442743
Model improve: 0.7972 -> 0.8001
Epoch: 19/40
Train loss: 2.9570, val loss: 1.7241
0.880774783109712
f1: 0.8002489540472322
Model improve: 0.8001 -> 0.8002
Epoch: 20/40
Train loss: 2.8970, val loss: 1.6276
0.885240995545664
f1: 0.8055715984372346
Model improve: 0.8002 -> 0.8056
Epoch: 21/40
Train loss: 2.8336, val loss: 1.5927
0.8877747672977129
f1: 0.8114103520792348
Model improve: 0.8056 -> 0.8114
Epoch: 22/40
Train loss: 2.7700, val loss: 1.5865
0.887810051342297
f1: 0.8110390487771924
Epoch: 23/40
Train loss: 2.7705, val loss: 1.5335
0.890456006113868
f1: 0.8168281730087449
Model improve: 0.8114 -> 0.8168
Epoch: 24/40
Train loss: 2.7412, val loss: 1.6002
0.8892974999479136
f1: 0.818705825842311
Model improve: 0.8168 -> 0.8187
Epoch: 25/40
Train loss: 2.7250, val loss: 1.5245
0.8917029134699032
f1: 0.8168266971060228
Epoch: 26/40
Train loss: 2.6573, val loss: 1.5108
0.8938133675332622
f1: 0.8212661298473771
Model improve: 0.8187 -> 0.8213
Epoch: 27/40
Train loss: 2.6726, val loss: 1.5895
0.8931525326384177
f1: 0.8213665611275662
Epoch: 28/40
Train loss: 2.6416, val loss: 1.5226
0.8960825418457631
f1: 0.8237587818077918
Epoch: 29/40
Train loss: 2.5593, val loss: 1.5041
0.8976834747274843
f1: 0.8269003876622283
Epoch: 30/40
Train loss: 2.5717, val loss: 1.4582
0.8993863040490913
f1: 0.8263870276065397
Epoch: 31/40
Train loss: 2.5357, val loss: 1.4692
0.898770495859783
f1: 0.8307630279205725
Epoch: 32/40
Train loss: 2.5345, val loss: 1.4828
0.8993356909915255
f1: 0.8304231821405654
Epoch: 33/40
Train loss: 2.4934, val loss: 1.4827
0.8994538481486049
f1: 0.8309921131909419
Epoch: 34/40
Train loss: 2.5010, val loss: 1.4227
0.9011250391481747
f1: 0.831470450438391
Epoch: 35/40
Train loss: 2.5068, val loss: 1.4787
0.9003951394374069
f1: 0.8311206954646541
Epoch: 36/40
Train loss: 2.5112, val loss: 1.4536
0.9013086437788155
f1: 0.8313867633239359
Epoch: 37/40
Train loss: 2.4309, val loss: 1.4948
0.9006616173665194
f1: 0.8321562531679789
Epoch: 38/40
Train loss: 2.4279, val loss: 1.4435
0.9017504551897708
f1: 0.8323970037453184
Epoch: 39/40
Train loss: 2.4511, val loss: 1.4441
0.9018489218777919
f1: 0.8333221633938743
Epoch: 40/40
Train loss: 2.4758, val loss: 1.4821
0.9004122274196562
f1: 0.8333954023766802
Date :05/10/2023, 08:38:48
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 40
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b2
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 1
65256
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/40
Date :05/10/2023, 08:39:07
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 40
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b2
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 1
65256
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/40
Train loss: 7.6138, val loss: 4.0599
0.5997409739951141
f1: 0.3553193422367584
Model improve: 0.0000 -> 0.3553
Epoch: 2/40
Train loss: 4.9702, val loss: 3.0339
0.7205451166153545
f1: 0.5505897203266144
Model improve: 0.3553 -> 0.5506
Epoch: 3/40
Train loss: 4.4640, val loss: 2.7955
0.7662004871805816
f1: 0.6075294117647059
Model improve: 0.5506 -> 0.6075
Epoch: 4/40
Train loss: 4.1360, val loss: 2.5194
0.7939602920387707
f1: 0.6586717267552182
Model improve: 0.6075 -> 0.6587
Epoch: 5/40
Train loss: 3.9060, val loss: 2.3113
0.8177300644040889
f1: 0.7006552308032099
Model improve: 0.6587 -> 0.7007
Epoch: 6/40
Train loss: 3.7554, val loss: 2.1036
0.8290643272458124
f1: 0.718793060806275
Model improve: 0.7007 -> 0.7188
Epoch: 7/40
Train loss: 3.6415, val loss: 2.0909
0.8381982915159374
f1: 0.7353416394384403
Model improve: 0.7188 -> 0.7353
Epoch: 8/40
Train loss: 3.5336, val loss: 2.0175
0.844937700486868
f1: 0.7416761525327261
Model improve: 0.7353 -> 0.7417
Epoch: 9/40
Train loss: 3.4541, val loss: 1.9475
0.8515992517842329
f1: 0.7545729061816667
Model improve: 0.7417 -> 0.7546
Epoch: 10/40
Train loss: 3.3985, val loss: 1.9874
0.8601755606839455
f1: 0.7616011592102879
Model improve: 0.7546 -> 0.7616
Epoch: 11/40
Train loss: 3.2670, val loss: 1.8794
0.8631587823204989
f1: 0.7694187467271774
Model improve: 0.7616 -> 0.7694
Epoch: 12/40
Train loss: 3.2095, val loss: 1.8370
0.8655561670295318
f1: 0.7777239374221239
Model improve: 0.7694 -> 0.7777
Epoch: 13/40
Train loss: 3.1918, val loss: 1.8019
0.8702144003615654
f1: 0.7847508945774841
Model improve: 0.7777 -> 0.7848
Epoch: 14/40
Train loss: 3.1216, val loss: 1.8078
0.8704600749746002
f1: 0.7804625116664938
Epoch: 15/40
Train loss: 3.1156, val loss: 1.7810
0.8725581684943265
f1: 0.7867674988777238
Model improve: 0.7848 -> 0.7868
Epoch: 16/40
Train loss: 3.0405, val loss: 1.7893
0.8769611474389478
f1: 0.7956719897018404
Model improve: 0.7868 -> 0.7957
Epoch: 17/40
Train loss: 2.9899, val loss: 1.6821
0.8808700193560645
f1: 0.8027810284298204
Model improve: 0.7957 -> 0.8028
Epoch: 18/40
Train loss: 2.9774, val loss: 1.7663
0.8808442134057186
f1: 0.799916617447799
Epoch: 19/40
Train loss: 2.9487, val loss: 1.6691
0.8825531604851086
f1: 0.8042222527850365
Model improve: 0.8028 -> 0.8042
Epoch: 20/40
Train loss: 2.8622, val loss: 1.6360
0.8862098693985536
f1: 0.8072734674491977
Model improve: 0.8042 -> 0.8073
Epoch: 21/40
Train loss: 2.8168, val loss: 1.5701
0.8890360026750134
f1: 0.8127792067144984
Model improve: 0.8073 -> 0.8128
Epoch: 22/40
Train loss: 2.7407, val loss: 1.5534
0.8890421213141226
f1: 0.8099318534511841
Epoch: 23/40
Train loss: 2.7400, val loss: 1.5539
0.8925934135154113
f1: 0.8179875877505343
Model improve: 0.8128 -> 0.8180
Epoch: 24/40
Train loss: 2.7013, val loss: 1.5833
0.8913429227370834
f1: 0.814890780432776
Epoch: 25/40
Train loss: 2.7082, val loss: 1.4994
0.8961458816073816
f1: 0.8206505320398777
Model improve: 0.8180 -> 0.8207
Epoch: 26/40
Train loss: 2.6334, val loss: 1.5107
0.8975137739862264
f1: 0.8257135127823617
Model improve: 0.8207 -> 0.8257
Epoch: 27/40
Train loss: 2.6302, val loss: 1.5293
0.8970436167457236
f1: 0.8231740428998099
Epoch: 28/40
Train loss: 2.6096, val loss: 1.4997
0.8990641717573636
f1: 0.8264579414135985
Epoch: 29/40
Train loss: 2.5347, val loss: 1.4811
0.899815722609779
f1: 0.8299915754001684
Epoch: 30/40
Train loss: 2.5347, val loss: 1.4375
0.9013379349607915
f1: 0.8295035841093321
Epoch: 31/40
Train loss: 2.5198, val loss: 1.4517
0.9023743177859977
f1: 0.8307464395138211
Epoch: 32/40
Train loss: 2.4938, val loss: 1.4530
0.9024371853226214
f1: 0.8328493280205308
Epoch: 33/40
Train loss: 2.4673, val loss: 1.4662
0.9020423399579457
f1: 0.8328156650911547
Epoch: 34/40
Train loss: 2.4561, val loss: 1.3848
0.9039463146444533
f1: 0.8336944564818827
Epoch: 35/40
Train loss: 2.4766, val loss: 1.4581
0.9027682537883558
f1: 0.8343111470766742
Epoch: 36/40
Train loss: 2.4762, val loss: 1.4241
0.9035314265424248
f1: 0.8351906552094521
Epoch: 37/40
Train loss: 2.3898, val loss: 1.4725
0.9034199962747156
f1: 0.8348957982059756
Epoch: 38/40
Train loss: 2.3874, val loss: 1.4120
0.9046228879632998
f1: 0.8369787305308993
Epoch: 39/40
Train loss: 2.4229, val loss: 1.4048
0.9043492657024964
f1: 0.8356297562774481
Epoch: 40/40
Train loss: 2.4340, val loss: 1.4395
0.9034568495179917
f1: 0.8351618674678459
Date :05/10/2023, 20:02:21
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 40
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 1
65256
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/40
Date :05/10/2023, 20:03:51
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 40
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 1
65256
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/40
0.8298377036027235
f1: 0.7552764695621839
Model improve: 0.0000 -> 0.7553
Epoch: 2/40
Date :05/10/2023, 20:07:55
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 40
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b2
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 1
65256
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/40
Val cmap: 0.8367
Model improve: 0.0000 -> 0.8367
Epoch: 2/40
Date :05/10/2023, 20:10:42
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 40
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 1
65256
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/40
Date :05/10/2023, 20:10:55
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 40
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 1
65256
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/40
Date :05/10/2023, 20:12:58
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :05/10/2023, 20:13:42
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :05/10/2023, 20:14:18
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Val cmap: 0.5961
Model improve: 0.0000 -> 0.5961
Epoch: 2/60
Date :05/10/2023, 20:26:54
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Val cmap: 0.5480
Model improve: 0.0000 -> 0.5480
Epoch: 2/60
Val cmap: 0.6625
Model improve: 0.5480 -> 0.6625
Epoch: 3/60
Val cmap: 0.7110
Model improve: 0.6625 -> 0.7110
Epoch: 4/60
Val cmap: 0.7323
Model improve: 0.7110 -> 0.7323
Epoch: 5/60
Val cmap: 0.7468
Model improve: 0.7323 -> 0.7468
Epoch: 6/60
Val cmap: 0.7604
Model improve: 0.7468 -> 0.7604
Epoch: 7/60
Val cmap: 0.7685
Model improve: 0.7604 -> 0.7685
Epoch: 8/60
Val cmap: 0.7733
Model improve: 0.7685 -> 0.7733
Epoch: 9/60
Val cmap: 0.7821
Model improve: 0.7733 -> 0.7821
Epoch: 10/60
Val cmap: 0.7817
Epoch: 11/60
Val cmap: 0.7924
Model improve: 0.7821 -> 0.7924
Epoch: 12/60
Val cmap: 0.7886
Epoch: 13/60
Val cmap: 0.7954
Model improve: 0.7924 -> 0.7954
Epoch: 14/60
Val cmap: 0.7995
Model improve: 0.7954 -> 0.7995
Epoch: 15/60
Val cmap: 0.8028
Model improve: 0.7995 -> 0.8028
Epoch: 16/60
Val cmap: 0.8042
Model improve: 0.8028 -> 0.8042
Epoch: 17/60
Val cmap: 0.8071
Model improve: 0.8042 -> 0.8071
Epoch: 18/60
Val cmap: 0.8049
Epoch: 19/60
Val cmap: 0.8096
Model improve: 0.8071 -> 0.8096
Epoch: 20/60
Val cmap: 0.8110
Model improve: 0.8096 -> 0.8110
Epoch: 21/60
Val cmap: 0.8154
Model improve: 0.8110 -> 0.8154
Epoch: 22/60
Val cmap: 0.8163
Model improve: 0.8154 -> 0.8163
Epoch: 23/60
Val cmap: 0.8169
Model improve: 0.8163 -> 0.8169
Epoch: 24/60
Val cmap: 0.8191
Model improve: 0.8169 -> 0.8191
Epoch: 25/60
Val cmap: 0.8214
Model improve: 0.8191 -> 0.8214
Epoch: 26/60
Val cmap: 0.8132
Epoch: 27/60
Val cmap: 0.8226
Model improve: 0.8214 -> 0.8226
Epoch: 28/60
Val cmap: 0.8170
Epoch: 29/60
Val cmap: 0.8242
Model improve: 0.8226 -> 0.8242
Epoch: 30/60
Val cmap: 0.8221
Epoch: 31/60
Val cmap: 0.8232
Epoch: 32/60
Val cmap: 0.8276
Model improve: 0.8242 -> 0.8276
Epoch: 33/60
Val cmap: 0.8253
Epoch: 34/60
Val cmap: 0.8280
Model improve: 0.8276 -> 0.8280
Epoch: 35/60
Val cmap: 0.8271
Epoch: 36/60
Val cmap: 0.8275
Epoch: 37/60
Val cmap: 0.8332
Model improve: 0.8280 -> 0.8332
Epoch: 38/60
Val cmap: 0.8298
Epoch: 39/60
Val cmap: 0.8296
Epoch: 40/60
Val cmap: 0.8364
Model improve: 0.8332 -> 0.8364
Epoch: 41/60
Val cmap: 0.8327
Epoch: 42/60
Val cmap: 0.8352
Epoch: 43/60
Val cmap: 0.8341
Epoch: 44/60
Val cmap: 0.8361
Epoch: 45/60
Val cmap: 0.8371
Model improve: 0.8364 -> 0.8371
Epoch: 46/60
Val cmap: 0.8354
Epoch: 47/60
Val cmap: 0.8399
Model improve: 0.8371 -> 0.8399
Epoch: 48/60
Val cmap: 0.8392
Epoch: 49/60
Val cmap: 0.8390
Epoch: 50/60
Val cmap: 0.8391
Epoch: 51/60
Val cmap: 0.8408
Model improve: 0.8399 -> 0.8408
Epoch: 52/60
Val cmap: 0.8438
Model improve: 0.8408 -> 0.8438
Epoch: 53/60
Val cmap: 0.8362
Epoch: 54/60
Val cmap: 0.8404
Epoch: 55/60
Val cmap: 0.8360
Epoch: 56/60
Val cmap: 0.8383
Epoch: 57/60
Val cmap: 0.8444
Model improve: 0.8438 -> 0.8444
Epoch: 58/60
Val cmap: 0.8410
Epoch: 59/60
Val cmap: 0.8412
Epoch: 60/60
Val cmap: 0.8411
Date :05/11/2023, 08:00:33
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :05/11/2023, 08:01:22
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :05/11/2023, 08:02:56
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Val cmap: 0.8416
Model improve: 0.0000 -> 0.8416
Epoch: 2/60
Val cmap: 0.8368
Epoch: 3/60
Date :05/11/2023, 08:06:53
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b2
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :05/11/2023, 08:10:56
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b2
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :05/11/2023, 08:18:09
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b2
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :05/11/2023, 08:18:29
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b2
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Val cmap: 0.5415
Model improve: 0.0000 -> 0.5415
Epoch: 2/60
Date :05/11/2023, 08:31:52
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b2
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :05/11/2023, 08:33:07
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b2
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :05/11/2023, 08:35:57
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Val cmap: 0.3978
Model improve: 0.0000 -> 0.3978
Epoch: 2/60
Date :05/11/2023, 08:43:54
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Val cmap: 0.3990
Model improve: 0.0000 -> 0.3990
Epoch: 2/60
Val cmap: 0.5753
Model improve: 0.3990 -> 0.5753
Epoch: 3/60
Val cmap: 0.6278
Model improve: 0.5753 -> 0.6278
Epoch: 4/60
Date :05/11/2023, 09:07:54
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :05/11/2023, 09:10:43
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :05/11/2023, 09:11:39
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :05/11/2023, 09:14:03
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :05/11/2023, 09:14:46
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :05/11/2023, 09:19:19
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :05/11/2023, 09:21:04
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 11.7557
Epoch: 2/60
Date :05/11/2023, 09:28:11
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :05/11/2023, 09:28:41
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :05/11/2023, 09:30:20
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 11.7557
Train loss: 11.7557, val loss: 6.5879
Val cmap: 0.2983
Model improve: 0.0000 -> 0.2983
Epoch: 2/60
Date :05/11/2023, 09:39:16
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 11.7557
Train loss: 11.7557, val loss: 6.5879
Val cmap: 0.2983
Model improve: 0.0000 -> 0.2983
Epoch: 2/60
Date :05/11/2023, 09:46:58
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 11.7557
Epoch: 2/60
Train loss: 5.9817
Epoch: 3/60
Train loss: 5.1102
Epoch: 4/60
Train loss: 4.4518
Epoch: 5/60
Train loss: 3.9098
Epoch: 6/60
Train loss: 3.5379
Epoch: 7/60
Train loss: 3.2582
Epoch: 8/60
Train loss: 2.9534
Epoch: 9/60
Train loss: 2.6503
Epoch: 10/60
Train loss: 2.3651
Epoch: 11/60
Train loss: 2.1431
Epoch: 12/60
Train loss: 1.9740
Epoch: 13/60
Train loss: 1.8092
Epoch: 14/60
Train loss: 1.6810
Epoch: 15/60
Train loss: 1.5643
Epoch: 16/60
Train loss: 1.4747
Epoch: 17/60
Train loss: 1.4087
Epoch: 18/60
Train loss: 1.3559
Epoch: 19/60
Train loss: 1.2937
Epoch: 20/60
Train loss: 1.2528
Epoch: 21/60
Train loss: 1.2083
Epoch: 22/60
Train loss: 1.1786
Epoch: 23/60
Train loss: 1.1464
Epoch: 24/60
Train loss: 1.1190
Epoch: 25/60
Train loss: 1.0905
Epoch: 26/60
Train loss: 1.0695
Epoch: 27/60
Train loss: 1.0496
Epoch: 28/60
Train loss: 1.0318
Epoch: 29/60
Train loss: 1.0065
Epoch: 30/60
Train loss: 0.9808
Epoch: 31/60
Train loss: 0.9607
Epoch: 32/60
Train loss: 0.9387
Train loss: 0.9387, val loss: 5.5758
Val cmap: 0.5422
Model improve: 0.0000 -> 0.5422
Epoch: 33/60
Train loss: 0.9130
Train loss: 0.9130, val loss: 5.5770
Val cmap: 0.5426
Model improve: 0.5422 -> 0.5426
Epoch: 34/60
Train loss: 0.8966
Train loss: 0.8966, val loss: 5.4823
Val cmap: 0.5468
Model improve: 0.5426 -> 0.5468
Epoch: 35/60
Train loss: 0.8779
Train loss: 0.8779, val loss: 5.6439
Val cmap: 0.5475
Model improve: 0.5468 -> 0.5475
Epoch: 36/60
Train loss: 0.8596
Train loss: 0.8596, val loss: 5.6100
Val cmap: 0.5447
Epoch: 37/60
Train loss: 0.8455
Train loss: 0.8455, val loss: 5.5916
Val cmap: 0.5436
Epoch: 38/60
Train loss: 0.8299
Train loss: 0.8299, val loss: 5.6188
Val cmap: 0.5501
Model improve: 0.5475 -> 0.5501
Epoch: 39/60
Train loss: 0.8121
Train loss: 0.8121, val loss: 5.7673
Val cmap: 0.5507
Model improve: 0.5501 -> 0.5507
Epoch: 40/60
Train loss: 0.7951
Train loss: 0.7951, val loss: 5.7172
Val cmap: 0.5505
Epoch: 41/60
Train loss: 0.7791
Train loss: 0.7791, val loss: 5.7583
Val cmap: 0.5494
Epoch: 42/60
Train loss: 0.7615
Train loss: 0.7615, val loss: 5.7594
Val cmap: 0.5554
Model improve: 0.5507 -> 0.5554
Epoch: 43/60
Train loss: 0.7499
Train loss: 0.7499, val loss: 5.7897
Val cmap: 0.5535
Epoch: 44/60
Train loss: 0.7367
Train loss: 0.7367, val loss: 5.8098
Val cmap: 0.5512
Epoch: 45/60
Train loss: 0.7232
Train loss: 0.7232, val loss: 5.8003
Val cmap: 0.5527
Epoch: 46/60
Train loss: 0.7136
Train loss: 0.7136, val loss: 5.8667
Val cmap: 0.5529
Epoch: 47/60
Train loss: 0.7015
Train loss: 0.7015, val loss: 5.9523
Val cmap: 0.5523
Epoch: 48/60
Train loss: 0.6891
Train loss: 0.6891, val loss: 5.9954
Val cmap: 0.5507
Epoch: 49/60
Train loss: 0.6799
Train loss: 0.6799, val loss: 6.1201
Val cmap: 0.5528
Epoch: 50/60
Date :05/11/2023, 14:37:38
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 11.7056
Epoch: 2/60
Train loss: 5.8381
Epoch: 3/60
Train loss: 4.9372
Epoch: 4/60
Train loss: 4.2666
Epoch: 5/60
Train loss: 3.7173
Epoch: 6/60
Train loss: 3.3071
Epoch: 7/60
Train loss: 3.0168
Train loss: 3.0168, val loss: 5.5468
Val cmap: 0.4661
Model improve: 0.0000 -> 0.4661
Epoch: 8/60
Date :05/11/2023, 15:23:12
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 7.7192
Epoch: 2/60
Train loss: 5.1713
Epoch: 3/60
Train loss: 4.4390
Epoch: 4/60
Train loss: 3.8652
Epoch: 5/60
Train loss: 3.3702
Epoch: 6/60
Train loss: 2.9547
Train loss: 2.9547, val loss: 4.8355
Val cmap: 0.5169
Model improve: 0.0000 -> 0.5169
Epoch: 7/60
Date :05/11/2023, 16:23:02
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 7.6201
Train loss: 7.6201, val loss: 5.5248
Val cmap: 0.4317
Model improve: 0.0000 -> 0.4317
Epoch: 2/60
Date :05/11/2023, 16:34:26
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 7.5768
Train loss: 7.5768, val loss: 5.3436
Val cmap: 0.4633
Model improve: 0.0000 -> 0.4633
Epoch: 2/60
Date :05/11/2023, 16:47:22
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :05/11/2023, 16:47:54
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :05/11/2023, 16:49:09
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 7.4059
Train loss: 7.4059, val loss: 4.9282
Val cmap: 0.5063
Model improve: 0.0000 -> 0.5063
Epoch: 2/60
Train loss: 4.8388
Train loss: 4.8388, val loss: 4.2425
Val cmap: 0.5903
Model improve: 0.5063 -> 0.5903
Epoch: 3/60
Date :05/11/2023, 17:17:39
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 7.4755
Train loss: 7.4755, val loss: 4.8463
Val cmap: 0.5091
Model improve: 0.0000 -> 0.5091
Epoch: 2/60
Date :05/11/2023, 17:38:05
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Date :05/11/2023, 17:38:52
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 1
Date :05/11/2023, 17:39:28
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :05/11/2023, 17:39:56
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :05/11/2023, 17:40:26
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 7.6709, val loss: 4.5086
0.5486091323545154
Model improve: 0.0000 -> 0.5486
Epoch: 2/60
Date :05/11/2023, 17:53:57
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 7.7517, val loss: 4.6106
0.5429374525820718
Model improve: 0.0000 -> 0.5429
Epoch: 2/60
Train loss: 5.1148, val loss: 3.6831
0.6507828367832436
Model improve: 0.5429 -> 0.6508
Epoch: 3/60
Train loss: 4.6181, val loss: 3.2462
0.7010602218728768
Model improve: 0.6508 -> 0.7011
Epoch: 4/60
Train loss: 4.3025, val loss: 3.0306
0.7268565671870434
Model improve: 0.7011 -> 0.7269
Epoch: 5/60
Train loss: 4.0632, val loss: 2.9373
0.7340388560722844
Model improve: 0.7269 -> 0.7340
Epoch: 6/60
Date :05/11/2023, 18:50:28
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 7.7630, val loss: 4.5948
0.5421492866807034
Model improve: 0.0000 -> 0.5421
Epoch: 2/60
Date :05/11/2023, 19:05:54
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 7.6997, val loss: 4.4718
0.5530834087997902
Model improve: 0.0000 -> 0.5531
Epoch: 2/60
Train loss: 4.9875, val loss: 3.6890
0.655711049980501
Model improve: 0.5531 -> 0.6557
Epoch: 3/60
Train loss: 4.4840, val loss: 3.2016
0.7126950563859019
Model improve: 0.6557 -> 0.7127
Epoch: 4/60
Train loss: 4.1668, val loss: 2.9899
0.7332404038566169
Model improve: 0.7127 -> 0.7332
Epoch: 5/60
Date :05/11/2023, 19:51:34
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b2
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 7.8108, val loss: 4.3345
0.5663414137419633
Model improve: 0.0000 -> 0.5663
Epoch: 2/60
Train loss: 4.9651, val loss: 3.5051
0.6671513470046387
Model improve: 0.5663 -> 0.6672
Epoch: 3/60
Train loss: 4.4566, val loss: 3.1961
0.7117084606371558
Model improve: 0.6672 -> 0.7117
Epoch: 4/60
Train loss: 4.1533, val loss: 2.9858
0.731617800625503
Model improve: 0.7117 -> 0.7316
Epoch: 5/60
Train loss: 3.9131, val loss: 2.8390
0.7503968977164599
Model improve: 0.7316 -> 0.7504
Epoch: 6/60
Date :05/11/2023, 22:43:22
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b2
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Epoch: 2/60
Epoch: 3/60
Epoch: 4/60
Epoch: 5/60
Epoch: 6/60
Epoch: 7/60
Epoch: 8/60
Epoch: 9/60
Epoch: 10/60
Epoch: 11/60
Epoch: 12/60
Epoch: 13/60
Epoch: 14/60
Epoch: 15/60
Epoch: 16/60
Epoch: 17/60
Epoch: 18/60
Epoch: 19/60
Epoch: 20/60
Epoch: 21/60
Epoch: 22/60
Epoch: 23/60
Epoch: 24/60
Epoch: 25/60
Epoch: 26/60
Epoch: 27/60
Epoch: 28/60
Epoch: 29/60
Epoch: 30/60
Epoch: 31/60
Epoch: 32/60
Epoch: 33/60
Epoch: 34/60
Epoch: 35/60
Epoch: 36/60
Epoch: 37/60
Epoch: 38/60
Epoch: 39/60
Epoch: 40/60
Epoch: 41/60
Epoch: 42/60
Train loss: 2.4454, val loss: 1.9336
0.8406078482177471
Model improve: 0.0000 -> 0.8406
Epoch: 43/60
Train loss: 2.4197, val loss: 1.9238
0.8410348284575269
Model improve: 0.8406 -> 0.8410
Epoch: 44/60
Train loss: 2.4257, val loss: 1.9812
0.8368317264913396
Epoch: 45/60
Train loss: 2.4450, val loss: 1.9427
0.8400180669135305
Epoch: 46/60
Train loss: 2.3381, val loss: 1.9612
0.8409932436110863
Epoch: 47/60
Train loss: 2.3664, val loss: 1.9382
0.8464552352746584
Model improve: 0.8410 -> 0.8465
Epoch: 48/60
Train loss: 2.3620, val loss: 1.9307
0.8412630169044527
Epoch: 49/60
Train loss: 2.3624, val loss: 1.9070
0.8439953355140257
Epoch: 50/60
Train loss: 2.3579, val loss: 1.9310
0.8445610871622071
Epoch: 51/60
Train loss: 2.3152, val loss: 1.9272
0.8442623361727317
Epoch: 52/60
Train loss: 2.3202, val loss: 1.9125
0.844799088234267
Epoch: 53/60
Train loss: 2.2909, val loss: 1.9073
0.8434186988898024
Epoch: 54/60
Train loss: 2.2406, val loss: 1.9710
0.8430906042974259
Epoch: 55/60
Train loss: 2.2941, val loss: 1.9297
0.8459628075351672
Epoch: 56/60
Train loss: 2.2702, val loss: 1.9196
0.845843455230502
Epoch: 57/60
Train loss: 2.2797, val loss: 1.9415
0.8454252902437535
Epoch: 58/60
Train loss: 2.3145, val loss: 1.9217
0.8453462851704867
Epoch: 59/60
Date :05/12/2023, 10:32:00
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b2
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
65200
Date :05/12/2023, 10:32:09
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b2
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
65200
Date :05/12/2023, 10:32:40
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b2
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
65200
Date :05/12/2023, 10:32:47
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b2
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
65200
Date :05/12/2023, 10:33:43
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b2
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :05/12/2023, 10:34:30
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b2
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :05/12/2023, 11:05:37
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b2
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :05/12/2023, 11:06:06
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b2
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :05/12/2023, 11:10:59
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b2
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :05/12/2023, 11:11:09
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b2
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
0.8430775185428709
Epoch: 2/60
0.841575778642651
Epoch: 3/60
Date :05/12/2023, 11:14:40
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b2
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
0.8430775185428709
Epoch: 2/60
Date :05/12/2023, 11:18:13
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b2
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
0.8436766380762992
Epoch: 2/60
0.8436766380762992
Epoch: 3/60
Date :05/12/2023, 11:22:23
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b2
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
0.25784670552451316
Epoch: 2/60
Date :05/12/2023, 11:24:16
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b2
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
0.25784670552451316
Epoch: 2/60
Date :05/12/2023, 14:27:13
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :05/12/2023, 14:35:08
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Date :05/12/2023, 15:10:50
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :05/12/2023, 15:12:23
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 7.7649
Train loss: 7.7649, val loss: 4.4858
Val cmap: 0.5604
Model improve: 0.0000 -> 0.5604
Epoch: 2/100
Train loss: 4.8794
Train loss: 4.8794, val loss: 3.7534
Val cmap: 0.6440
Model improve: 0.5604 -> 0.6440
Epoch: 3/100
Train loss: 4.1192
Train loss: 4.1192, val loss: 3.6061
Val cmap: 0.6609
Model improve: 0.6440 -> 0.6609
Epoch: 4/100
Train loss: 3.5425
Train loss: 3.5425, val loss: 3.6509
Val cmap: 0.6577
Epoch: 5/100
Date :05/12/2023, 16:00:49
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 50
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/50
Date :05/12/2023, 16:05:17
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 50
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/50
Train loss: 7.5090
Train loss: 7.5090, val loss: 4.5514
Val cmap: 0.5524
Model improve: 0.0000 -> 0.5524
Epoch: 2/50
Train loss: 4.8968
Train loss: 4.8968, val loss: 3.8126
Val cmap: 0.6390
Model improve: 0.5524 -> 0.6390
Epoch: 3/50
Train loss: 4.1411
Date :05/12/2023, 16:41:13
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 7.5068, val loss: 4.4589
0.5564109594437531
Model improve: 0.0000 -> 0.5564
Epoch: 2/60
Train loss: 4.9499, val loss: 3.5604
0.6679973692693818
Model improve: 0.5564 -> 0.6680
Epoch: 3/60
Train loss: 4.1308, val loss: 3.4084
0.681799082510598
Model improve: 0.6680 -> 0.6818
Epoch: 4/60
Train loss: 3.5387, val loss: 3.3590
0.6888089513424218
Model improve: 0.6818 -> 0.6888
Epoch: 5/60
Date :05/12/2023, 19:21:25
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :05/12/2023, 19:21:48
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 7.8412, val loss: 4.4093
0.5593936732271703
Model improve: 0.0000 -> 0.5594
Epoch: 2/60
Train loss: 4.9618, val loss: 3.6598
0.6561010706160145
Model improve: 0.5594 -> 0.6561
Epoch: 3/60
Train loss: 4.4659, val loss: 3.2247
0.706994711183567
Model improve: 0.6561 -> 0.7070
Epoch: 4/60
Date :05/12/2023, 19:56:42
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :05/12/2023, 19:57:52
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :05/12/2023, 19:59:31
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :05/12/2023, 19:59:58
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :05/12/2023, 20:01:47
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :05/12/2023, 20:02:09
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 7.6868
Epoch: 2/60
Train loss: 4.9743
Epoch: 3/60
Train loss: 4.4679
Epoch: 4/60
Train loss: 4.1595
Epoch: 5/60
Train loss: 3.9160
Epoch: 6/60
Train loss: 3.7840
Epoch: 7/60
Train loss: 3.6804
Epoch: 8/60
Train loss: 3.5873
Epoch: 9/60
Train loss: 3.4883
Epoch: 10/60
Train loss: 3.4452
Epoch: 11/60
Train loss: 3.3595
Epoch: 12/60
Train loss: 3.2813
Epoch: 13/60
Train loss: 3.2602
Epoch: 14/60
Train loss: 3.2167
Epoch: 15/60
Train loss: 3.1965
Epoch: 16/60
Train loss: 3.1368
Epoch: 17/60
Train loss: 3.1196
Epoch: 18/60
Train loss: 3.0944
Epoch: 19/60
Train loss: 3.0882
Epoch: 20/60
Train loss: 3.0098
Epoch: 21/60
Train loss: 2.9712
Epoch: 22/60
Train loss: 2.9039
Epoch: 23/60
Train loss: 2.9082
Epoch: 24/60
Train loss: 2.8770
Epoch: 25/60
Train loss: 2.8865
Epoch: 26/60
Train loss: 2.8306
Epoch: 27/60
Train loss: 2.8380
Epoch: 28/60
Train loss: 2.8069
Epoch: 29/60
Train loss: 2.7501
Epoch: 30/60
Train loss: 2.7451
Epoch: 31/60
Train loss: 2.7286
Epoch: 32/60
Train loss: 2.6970
Epoch: 33/60
Train loss: 2.6503
Epoch: 34/60
Train loss: 2.6538
Epoch: 35/60
Train loss: 2.6701
Epoch: 36/60
Train loss: 2.6513
Epoch: 37/60
Train loss: 2.5524
Epoch: 38/60
Train loss: 2.5373
Epoch: 39/60
Train loss: 2.5699
Epoch: 40/60
Train loss: 2.5355
Epoch: 41/60
Train loss: 2.5158
Epoch: 42/60
Train loss: 2.5133
Epoch: 43/60
Train loss: 2.4749
Epoch: 44/60
Train loss: 2.4813
Epoch: 45/60
Train loss: 2.5046
Epoch: 46/60
Train loss: 2.3961
Epoch: 47/60
Train loss: 2.4294
Epoch: 48/60
Train loss: 2.4244
Epoch: 49/60
Train loss: 2.4219
Epoch: 50/60
Train loss: 2.4139
Epoch: 51/60
Train loss: 2.3658
Train loss: 2.3658, val loss: 1.4129
Val cmap: 0.90596
Saved models
Model improve: 0.0000 -> 0.9060
Epoch: 52/60
Train loss: 2.3856
Train loss: 2.3856, val loss: 1.3829
Val cmap: 0.90679
Saved models
Model improve: 0.9060 -> 0.9068
Epoch: 53/60
Train loss: 2.3714
Train loss: 2.3714, val loss: 1.3821
Val cmap: 0.90659
Saved models
Epoch: 54/60
Train loss: 2.3205
Train loss: 2.3205, val loss: 1.4546
Val cmap: 0.90579
Saved models
Epoch: 55/60
Train loss: 2.3634
Train loss: 2.3634, val loss: 1.4234
Val cmap: 0.90681
Saved models
Model improve: 0.9068 -> 0.9068
Epoch: 56/60
Train loss: 2.3424
Train loss: 2.3424, val loss: 1.4047
Val cmap: 0.90678
Saved models
Epoch: 57/60
Train loss: 2.3455
Train loss: 2.3455, val loss: 1.4221
Val cmap: 0.90692
Saved models
Model improve: 0.9068 -> 0.9069
Epoch: 58/60
Train loss: 2.3870
Train loss: 2.3870, val loss: 1.3821
Val cmap: 0.90738
Saved models
Model improve: 0.9069 -> 0.9074
Epoch: 59/60
Train loss: 2.3515
Train loss: 2.3515, val loss: 1.3643
Val cmap: 0.90759
Saved models
Model improve: 0.9074 -> 0.9076
Epoch: 60/60
Train loss: 2.3664
Train loss: 2.3664, val loss: 1.4379
Val cmap: 0.90640
Saved models
Date :05/13/2023, 06:52:07
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b2
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 7.9504
Epoch: 2/100
Train loss: 4.9933
Epoch: 3/100
Train loss: 4.4850
Epoch: 4/100
Train loss: 4.1524
Epoch: 5/100
Train loss: 3.9285
Epoch: 6/100
Train loss: 3.7780
Epoch: 7/100
Train loss: 3.6748
Epoch: 8/100
Train loss: 3.5670
Epoch: 9/100
Train loss: 3.4934
Epoch: 10/100
Train loss: 3.4285
Epoch: 11/100
Train loss: 3.3296
Epoch: 12/100
Train loss: 3.2906
Epoch: 13/100
Train loss: 3.2838
Epoch: 14/100
Train loss: 3.2067
Epoch: 15/100
Train loss: 3.2104
Epoch: 16/100
Train loss: 3.1320
Epoch: 17/100
Train loss: 3.1215
Epoch: 18/100
Train loss: 3.0927
Epoch: 19/100
Train loss: 3.0823
Epoch: 20/100
Train loss: 3.0288
Epoch: 21/100
Train loss: 2.9826
Epoch: 22/100
Train loss: 2.9395
Epoch: 23/100
Train loss: 2.9545
Epoch: 24/100
Train loss: 2.9192
Epoch: 25/100
Train loss: 2.9284
Epoch: 26/100
Train loss: 2.8721
Epoch: 27/100
Train loss: 2.8784
Epoch: 28/100
Train loss: 2.8734
Epoch: 29/100
Train loss: 2.8081
Epoch: 30/100
Train loss: 2.8063
Epoch: 31/100
Train loss: 2.7906
Epoch: 32/100
Train loss: 2.7742
Epoch: 33/100
Train loss: 2.7413
Epoch: 34/100
Train loss: 2.7506
Epoch: 35/100
Train loss: 2.7431
Epoch: 36/100
Train loss: 2.7378
Epoch: 37/100
Train loss: 2.6559
Epoch: 38/100
Train loss: 2.6505
Epoch: 39/100
Train loss: 2.6722
Epoch: 40/100
Train loss: 2.6559
Epoch: 41/100
Train loss: 2.6445
Epoch: 42/100
Train loss: 2.6391
Epoch: 43/100
Train loss: 2.5944
Epoch: 44/100
Train loss: 2.6179
Epoch: 45/100
Train loss: 2.6136
Epoch: 46/100
Train loss: 2.5296
Epoch: 47/100
Train loss: 2.5591
Epoch: 48/100
Train loss: 2.5470
Epoch: 49/100
Train loss: 2.5430
Epoch: 50/100
Train loss: 2.5224
Epoch: 51/100
Train loss: 2.4976
Train loss: 2.4976, val loss: 1.5082
Val cmap: 0.90158
Saved models
Model improve: 0.0000 -> 0.9016
Epoch: 52/100
Train loss: 2.5061
Train loss: 2.5061, val loss: 1.4242
Val cmap: 0.90096
Saved models
Epoch: 53/100
Train loss: 2.4747
Train loss: 2.4747, val loss: 1.4230
Val cmap: 0.90230
Saved models
Model improve: 0.9016 -> 0.9023
Epoch: 54/100
Train loss: 2.4153
Train loss: 2.4153, val loss: 1.5352
Val cmap: 0.90093
Saved models
Epoch: 55/100
Train loss: 2.4603
Train loss: 2.4603, val loss: 1.4711
Val cmap: 0.90252
Saved models
Model improve: 0.9023 -> 0.9025
Epoch: 56/100
Train loss: 2.4288
Train loss: 2.4288, val loss: 1.4264
Val cmap: 0.90435
Saved models
Model improve: 0.9025 -> 0.9043
Epoch: 57/100
Train loss: 2.4222
Date :05/13/2023, 12:24:53
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b2
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 7.8584
Epoch: 2/100
Train loss: 4.9968
Epoch: 3/100
Train loss: 4.4920
Epoch: 4/100
Train loss: 4.1630
Epoch: 5/100
Train loss: 3.9313
Epoch: 6/100
Train loss: 3.7826
Epoch: 7/100
Train loss: 3.6768
Epoch: 8/100
Train loss: 3.5673
Epoch: 9/100
Train loss: 3.5006
Epoch: 10/100
Train loss: 3.4306
Epoch: 11/100
Train loss: 3.3357
Epoch: 12/100
Train loss: 3.2867
Epoch: 13/100
Train loss: 3.2834
Epoch: 14/100
Train loss: 3.2108
Epoch: 15/100
Train loss: 3.2045
Epoch: 16/100
Train loss: 3.1314
Epoch: 17/100
Train loss: 3.1261
Epoch: 18/100
Train loss: 3.0951
Epoch: 19/100
Train loss: 3.0864
Epoch: 20/100
Train loss: 3.0240
Epoch: 21/100
Train loss: 2.9854
Epoch: 22/100
Train loss: 2.9356
Epoch: 23/100
Train loss: 2.9500
Epoch: 24/100
Train loss: 2.9204
Epoch: 25/100
Train loss: 2.9298
Epoch: 26/100
Train loss: 2.8749
Epoch: 27/100
Train loss: 2.8797
Epoch: 28/100
Train loss: 2.8762
Epoch: 29/100
Train loss: 2.8090
Epoch: 30/100
Train loss: 2.8106
Epoch: 31/100
Train loss: 2.7934
Epoch: 32/100
Train loss: 2.7772
Epoch: 33/100
Train loss: 2.7361
Epoch: 34/100
Train loss: 2.7469
Epoch: 35/100
Train loss: 2.7465
Epoch: 36/100
Train loss: 2.7357
Epoch: 37/100
Train loss: 2.6548
Epoch: 38/100
Train loss: 2.6478
Epoch: 39/100
Train loss: 2.6751
Epoch: 40/100
Train loss: 2.6567
Epoch: 41/100
Train loss: 2.6463
Epoch: 42/100
Train loss: 2.6418
Epoch: 43/100
Train loss: 2.5997
Epoch: 44/100
Train loss: 2.6195
Epoch: 45/100
Train loss: 2.6178
Epoch: 46/100
Train loss: 2.5345
Epoch: 47/100
Train loss: 2.5567
Epoch: 48/100
Train loss: 2.5502
Epoch: 49/100
Train loss: 2.5448
Epoch: 50/100
Train loss: 2.5286
Epoch: 51/100
Train loss: 2.5013
Epoch: 52/100
Train loss: 2.5167
Epoch: 53/100
Train loss: 2.4799
Epoch: 54/100
Train loss: 2.4313
Epoch: 55/100
Train loss: 2.4583
Epoch: 56/100
Train loss: 2.4334
Epoch: 57/100
Train loss: 2.4226
Epoch: 58/100
Train loss: 2.4496
Epoch: 59/100
Train loss: 2.4000
Epoch: 60/100
Train loss: 2.4069
Epoch: 61/100
Train loss: 2.3645
Epoch: 62/100
Train loss: 2.4208
Epoch: 63/100
Train loss: 2.3808
Epoch: 64/100
Train loss: 2.3772
Epoch: 65/100
Train loss: 2.3459
Epoch: 66/100
Train loss: 2.3183
Epoch: 67/100
Train loss: 2.3208
Epoch: 68/100
Train loss: 2.3043
Epoch: 69/100
Train loss: 2.2682
Epoch: 70/100
Train loss: 2.2482
Epoch: 71/100
Train loss: 2.2657
Train loss: 2.2657, val loss: 1.3618
Val cmap: 0.91022
Saved models
Model improve: 0.00000 -> 0.91022
Epoch: 72/100
Train loss: 2.2506
Train loss: 2.2506, val loss: 1.4112
Val cmap: 0.90834
Saved models
Epoch: 73/100
Train loss: 2.3152
Train loss: 2.3152, val loss: 1.4092
Val cmap: 0.90900
Saved models
Epoch: 74/100
Train loss: 2.2341
Train loss: 2.2341, val loss: 1.4250
Val cmap: 0.91033
Saved models
Model improve: 0.91022 -> 0.91033
Epoch: 75/100
Train loss: 2.2804
Train loss: 2.2804, val loss: 1.3944
Val cmap: 0.91013
Saved models
Epoch: 76/100
Train loss: 2.2629
Train loss: 2.2629, val loss: 1.3481
Val cmap: 0.91082
Saved models
Model improve: 0.91033 -> 0.91082
Epoch: 77/100
Train loss: 2.2028
Train loss: 2.2028, val loss: 1.4186
Val cmap: 0.90982
Saved models
Epoch: 78/100
Train loss: 2.2071
Train loss: 2.2071, val loss: 1.3827
Val cmap: 0.91099
Saved models
Model improve: 0.91082 -> 0.91099
Epoch: 79/100
Train loss: 2.1544
Train loss: 2.1544, val loss: 1.3952
Val cmap: 0.91099
Saved models
Epoch: 80/100
Train loss: 2.1856
Train loss: 2.1856, val loss: 1.3809
Val cmap: 0.91110
Saved models
Model improve: 0.91099 -> 0.91110
Epoch: 81/100
Train loss: 2.1698
Train loss: 2.1698, val loss: 1.4403
Val cmap: 0.91097
Saved models
Epoch: 82/100
Train loss: 2.1124
Train loss: 2.1124, val loss: 1.3767
Val cmap: 0.91135
Saved models
Model improve: 0.91110 -> 0.91135
Epoch: 83/100
Train loss: 2.1343
Train loss: 2.1343, val loss: 1.3850
Val cmap: 0.91149
Saved models
Model improve: 0.91135 -> 0.91149
Epoch: 84/100
Train loss: 2.1372
Train loss: 2.1372, val loss: 1.3547
Val cmap: 0.91138
Saved models
Epoch: 85/100
Train loss: 2.1624
Train loss: 2.1624, val loss: 1.4264
Val cmap: 0.91140
Saved models
Epoch: 86/100
Train loss: 2.1364
Train loss: 2.1364, val loss: 1.3602
Val cmap: 0.91226
Saved models
Model improve: 0.91149 -> 0.91226
Epoch: 87/100
Train loss: 2.2350
Train loss: 2.2350, val loss: 1.3783
Val cmap: 0.91184
Saved models
Epoch: 88/100
Train loss: 2.1426
Train loss: 2.1426, val loss: 1.3470
Val cmap: 0.91261
Saved models
Model improve: 0.91226 -> 0.91261
Epoch: 89/100
Train loss: 2.1009
Train loss: 2.1009, val loss: 1.3544
Val cmap: 0.91299
Saved models
Model improve: 0.91261 -> 0.91299
Epoch: 90/100
Train loss: 2.1088
Train loss: 2.1088, val loss: 1.3357
Val cmap: 0.91294
Saved models
Epoch: 91/100
Train loss: 2.1597
Train loss: 2.1597, val loss: 1.3474
Val cmap: 0.91275
Saved models
Epoch: 92/100
Train loss: 2.1062
Train loss: 2.1062, val loss: 1.3694
Val cmap: 0.91287
Saved models
Epoch: 93/100
Train loss: 2.0964
Train loss: 2.0964, val loss: 1.3337
Val cmap: 0.91336
Saved models
Model improve: 0.91299 -> 0.91336
Epoch: 94/100
Train loss: 2.1214
Train loss: 2.1214, val loss: 1.3371
Val cmap: 0.91360
Saved models
Model improve: 0.91336 -> 0.91360
Epoch: 95/100
Train loss: 2.0954
Train loss: 2.0954, val loss: 1.3489
Val cmap: 0.91294
Saved models
Epoch: 96/100
Train loss: 2.1063
Train loss: 2.1063, val loss: 1.3532
Val cmap: 0.91294
Saved models
Epoch: 97/100
Train loss: 2.1010
Train loss: 2.1010, val loss: 1.3999
Val cmap: 0.91257
Saved models
Epoch: 98/100
Train loss: 2.0847
Train loss: 2.0847, val loss: 1.3612
Val cmap: 0.91311
Saved models
Epoch: 99/100
Train loss: 2.1189
Train loss: 2.1189, val loss: 1.3599
Val cmap: 0.91302
Saved models
Epoch: 100/100
Train loss: 2.0883
Train loss: 2.0883, val loss: 1.3783
Val cmap: 0.91286
Saved models
Date :05/17/2023, 18:03:44
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/25
Date :05/17/2023, 18:04:15
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/25
Date :05/17/2023, 19:20:52
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/25
Date :05/18/2023, 06:01:28
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/25
Date :05/18/2023, 06:02:02
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/25
Train loss: 7.2248
Epoch: 2/25
Train loss: 5.0518
Epoch: 3/25
Train loss: 4.5293
Epoch: 4/25
Train loss: 4.1902
Epoch: 5/25
Train loss: 3.9270
Epoch: 6/25
Train loss: 3.7496
Epoch: 7/25
Train loss: 3.6102
Epoch: 8/25
Train loss: 3.4993
Epoch: 9/25
Train loss: 3.3767
Epoch: 10/25
Train loss: 3.2921
Epoch: 11/25
Train loss: 3.1608
Epoch: 12/25
Train loss: 3.0864
Epoch: 13/25
Train loss: 3.0279
Epoch: 14/25
Train loss: 2.9394
Epoch: 15/25
Train loss: 2.9069
Epoch: 16/25
Train loss: 2.8230
Epoch: 17/25
Train loss: 2.7600
Train loss: 2.7600, val loss: 1.5307
Val cmap: 0.89530
Model improve: 0.00000 -> 0.89530
Epoch: 18/25
Date :05/18/2023, 10:33:29
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/25
Train loss: 7.4393
Epoch: 2/25
Train loss: 5.0001
Epoch: 3/25
Train loss: 4.4654
Epoch: 4/25
Train loss: 4.1119
Epoch: 5/25
Train loss: 3.8500
Epoch: 6/25
Train loss: 3.6777
Epoch: 7/25
Train loss: 3.5199
Epoch: 8/25
Train loss: 3.4211
Epoch: 9/25
Train loss: 3.2954
Epoch: 10/25
Train loss: 3.2155
Epoch: 11/25
Train loss: 3.0833
Epoch: 12/25
Train loss: 2.9938
Epoch: 13/25
Train loss: 2.9460
Epoch: 14/25
Train loss: 2.8525
Epoch: 15/25
Train loss: 2.8079
Epoch: 16/25
Train loss: 2.7260
Epoch: 17/25
Train loss: 2.6640
Train loss: 2.6640, val loss: 1.5339
Val cmap: 0.89608
Saved models
Model improve: 0.00000 -> 0.89608
Epoch: 18/25
Train loss: 2.6271
Train loss: 2.6271, val loss: 1.5927
Val cmap: 0.89713
Saved models
Model improve: 0.89608 -> 0.89713
Epoch: 19/25
Train loss: 2.5994
Train loss: 2.5994, val loss: 1.4615
Val cmap: 0.90025
Saved models
Model improve: 0.89713 -> 0.90025
Epoch: 20/25
Train loss: 2.5207
Train loss: 2.5207, val loss: 1.4346
Val cmap: 0.90173
Saved models
Model improve: 0.90025 -> 0.90173
Epoch: 21/25
Train loss: 2.4619
Train loss: 2.4619, val loss: 1.4161
Val cmap: 0.90330
Saved models
Model improve: 0.90173 -> 0.90330
Epoch: 22/25
Train loss: 2.4072
Train loss: 2.4072, val loss: 1.3687
Val cmap: 0.90374
Saved models
Model improve: 0.90330 -> 0.90374
Epoch: 23/25
Train loss: 2.4405
Train loss: 2.4405, val loss: 1.3750
Val cmap: 0.90456
Saved models
Model improve: 0.90374 -> 0.90456
Epoch: 24/25
Train loss: 2.3976
Train loss: 2.3976, val loss: 1.4005
Val cmap: 0.90448
Saved models
Epoch: 25/25
Train loss: 2.4500
Train loss: 2.4500, val loss: 1.3617
Val cmap: 0.90487
Saved models
Model improve: 0.90456 -> 0.90487
Date :05/18/2023, 17:44:30
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 16
validbs: 192
epochwarmup: 0
totalepoch: 50
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/50
Train loss: 6.8465
Epoch: 2/50
Train loss: 5.0303
Epoch: 3/50
Train loss: 4.4684
Epoch: 4/50
Date :05/20/2023, 19:24:13
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :05/20/2023, 19:24:59
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 48
validbs: 192
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :05/20/2023, 19:26:11
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 48
validbs: 192
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 7.8904
Epoch: 2/100
Train loss: 5.1343
Epoch: 3/100
Train loss: 4.5602
Epoch: 4/100
Train loss: 4.2311
Epoch: 5/100
Train loss: 4.0270
Epoch: 6/100
Train loss: 3.8222
Epoch: 7/100
Train loss: 3.6627
Epoch: 8/100
Train loss: 3.5103
Epoch: 9/100
Train loss: 3.4520
Epoch: 10/100
Train loss: 3.3688
Epoch: 11/100
Train loss: 3.3081
Epoch: 12/100
Train loss: 3.2251
Epoch: 13/100
Train loss: 3.1712
Epoch: 14/100
Train loss: 3.0973
Epoch: 15/100
Train loss: 3.1095
Epoch: 16/100
Train loss: 3.0199
Epoch: 17/100
Train loss: 2.9847
Epoch: 18/100
Train loss: 2.9180
Epoch: 19/100
Train loss: 2.9226
Epoch: 20/100
Train loss: 2.8639
Epoch: 21/100
Train loss: 2.8305
Epoch: 22/100
Train loss: 2.8377
Epoch: 23/100
Train loss: 2.7924
Epoch: 24/100
Train loss: 2.7992
Epoch: 25/100
Train loss: 2.7461
Epoch: 26/100
Train loss: 2.7456
Epoch: 27/100
Train loss: 2.7016
Epoch: 28/100
Train loss: 2.7444
Epoch: 29/100
Train loss: 2.6597
Epoch: 30/100
Train loss: 2.6448
Epoch: 31/100
Train loss: 2.6054
Epoch: 32/100
Train loss: 2.6182
Epoch: 33/100
Train loss: 2.5026
Epoch: 34/100
Train loss: 2.5722
Epoch: 35/100
Train loss: 2.4988
Epoch: 36/100
Train loss: 2.4964
Epoch: 37/100
Train loss: 2.5358
Epoch: 38/100
Train loss: 2.4820
Epoch: 39/100
Train loss: 2.4535
Epoch: 40/100
Train loss: 2.4967
Epoch: 41/100
Train loss: 2.4466
Epoch: 42/100
Train loss: 2.3959
Epoch: 43/100
Train loss: 2.3571
Epoch: 44/100
Train loss: 2.3661
Epoch: 45/100
Train loss: 2.3831
Epoch: 46/100
Train loss: 2.3219
Epoch: 47/100
Train loss: 2.3291
Epoch: 48/100
Train loss: 2.3275
Epoch: 49/100
Train loss: 2.2765
Epoch: 50/100
Train loss: 2.2797
Epoch: 51/100
Train loss: 2.2857
Epoch: 52/100
Train loss: 2.2996
Epoch: 53/100
Train loss: 2.2598
Epoch: 54/100
Train loss: 2.2375
Epoch: 55/100
Train loss: 2.1691
Epoch: 56/100
Train loss: 2.1356
Epoch: 57/100
Train loss: 2.1746
Epoch: 58/100
Train loss: 2.1886
Epoch: 59/100
Train loss: 2.1417
Epoch: 60/100
Train loss: 2.1523
Epoch: 61/100
Train loss: 2.1073
Epoch: 62/100
Train loss: 2.1240
Epoch: 63/100
Train loss: 2.1228
Epoch: 64/100
Train loss: 2.0869
Epoch: 65/100
Train loss: 2.0757
Epoch: 66/100
Train loss: 2.0940
Epoch: 67/100
Train loss: 2.1139
Epoch: 68/100
Train loss: 2.0386
Epoch: 69/100
Train loss: 1.9764
Epoch: 70/100
Train loss: 2.0029
Epoch: 71/100
Train loss: 2.0471
Epoch: 72/100
Train loss: 1.9896
Train loss: 1.9896, val loss: 1.2502
Val cmap: 0.91280
Saved models
Model improve: 0.00000 -> 0.91280
Epoch: 73/100
Train loss: 2.0299
Train loss: 2.0299, val loss: 1.2873
Val cmap: 0.91374
Saved models
Model improve: 0.91280 -> 0.91374
Epoch: 74/100
Train loss: 1.9737
Train loss: 1.9737, val loss: 1.3017
Val cmap: 0.91348
Saved models
Epoch: 75/100
Train loss: 2.0045
Train loss: 2.0045, val loss: 1.3079
Val cmap: 0.91341
Saved models
Epoch: 76/100
Train loss: 1.9446
Train loss: 1.9446, val loss: 1.3171
Val cmap: 0.91312
Saved models
Epoch: 77/100
Train loss: 1.9617
Train loss: 1.9617, val loss: 1.2779
Val cmap: 0.91380
Saved models
Model improve: 0.91374 -> 0.91380
Epoch: 78/100
Train loss: 1.9552
Train loss: 1.9552, val loss: 1.2791
Val cmap: 0.91404
Saved models
Model improve: 0.91380 -> 0.91404
Epoch: 79/100
Train loss: 1.8843
Train loss: 1.8843, val loss: 1.3419
Val cmap: 0.91407
Saved models
Model improve: 0.91404 -> 0.91407
Epoch: 80/100
Train loss: 1.9638
Train loss: 1.9638, val loss: 1.2624
Val cmap: 0.91494
Saved models
Model improve: 0.91407 -> 0.91494
Epoch: 81/100
Train loss: 1.8537
Train loss: 1.8537, val loss: 1.3603
Val cmap: 0.91421
Saved models
Epoch: 82/100
Train loss: 1.9257
Train loss: 1.9257, val loss: 1.3512
Val cmap: 0.91426
Saved models
Epoch: 83/100
Train loss: 1.8888
Train loss: 1.8888, val loss: 1.2557
Val cmap: 0.91532
Saved models
Model improve: 0.91494 -> 0.91532
Epoch: 84/100
Train loss: 1.9022
Train loss: 1.9022, val loss: 1.2857
Val cmap: 0.91444
Saved models
Epoch: 85/100
Train loss: 1.8906
Train loss: 1.8906, val loss: 1.2817
Val cmap: 0.91445
Saved models
Epoch: 86/100
Train loss: 1.9089
Train loss: 1.9089, val loss: 1.2518
Val cmap: 0.91507
Saved models
Epoch: 87/100
Train loss: 1.9152
Train loss: 1.9152, val loss: 1.2649
Val cmap: 0.91527
Saved models
Epoch: 88/100
Train loss: 1.8759
Train loss: 1.8759, val loss: 1.2476
Val cmap: 0.91545
Saved models
Model improve: 0.91532 -> 0.91545
Epoch: 89/100
Train loss: 1.8878
Train loss: 1.8878, val loss: 1.2588
Val cmap: 0.91507
Saved models
Epoch: 90/100
Train loss: 1.8827
Train loss: 1.8827, val loss: 1.3016
Val cmap: 0.91554
Saved models
Model improve: 0.91545 -> 0.91554
Epoch: 91/100
Train loss: 1.8832
Train loss: 1.8832, val loss: 1.2506
Val cmap: 0.91538
Saved models
Epoch: 92/100
Train loss: 1.8872
Train loss: 1.8872, val loss: 1.3000
Val cmap: 0.91520
Saved models
Epoch: 93/100
Train loss: 1.9012
Train loss: 1.9012, val loss: 1.2993
Val cmap: 0.91556
Saved models
Model improve: 0.91554 -> 0.91556
Epoch: 94/100
Train loss: 1.8818
Train loss: 1.8818, val loss: 1.2963
Val cmap: 0.91487
Saved models
Epoch: 95/100
Train loss: 1.9351
Train loss: 1.9351, val loss: 1.3066
Val cmap: 0.91579
Saved models
Model improve: 0.91556 -> 0.91579
Epoch: 96/100
Train loss: 1.8580
Train loss: 1.8580, val loss: 1.2455
Val cmap: 0.91527
Saved models
Epoch: 97/100
Train loss: 1.8913
Train loss: 1.8913, val loss: 1.2499
Val cmap: 0.91526
Saved models
Epoch: 98/100
Train loss: 1.8487
Train loss: 1.8487, val loss: 1.2859
Val cmap: 0.91594
Saved models
Model improve: 0.91579 -> 0.91594
Epoch: 99/100
Train loss: 1.8808
Train loss: 1.8808, val loss: 1.3082
Val cmap: 0.91559
Saved models
Epoch: 100/100
Train loss: 1.8555
Train loss: 1.8555, val loss: 1.3190
Val cmap: 0.91562
Saved models
