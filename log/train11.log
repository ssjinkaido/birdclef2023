{'train': <audiomentations.core.composition.Compose object at 0x7ff71d9793c0>, 'valid': <audiomentations.core.composition.Compose object at 0x7ff71d979600>}
{'train': <audiomentations.core.composition.SpecCompose object at 0x7ff71d733880>, 'valid': <audiomentations.core.composition.SpecCompose object at 0x7ff71d979690>}
Fold: 0
Epoch: 1/40
<audiomentations.core.composition.Compose object at 0x7f0a7923fb80>
<audiomentations.core.composition.SpecCompose object at 0x7f0a78ff3880>
<audiomentations.core.composition.Compose object at 0x7f0a7853fa60>
<audiomentations.core.composition.SpecCompose object at 0x7f0a7853fb80>
<audiomentations.core.composition.Compose object at 0x7f0a784cdc90>
<audiomentations.core.composition.SpecCompose object at 0x7f0a7853f9d0>
<audiomentations.core.composition.Compose object at 0x7f0a784dc700>
<audiomentations.core.composition.SpecCompose object at 0x7f0a784dc7c0>
Fold: 0
Epoch: 1/40
<audiomentations.core.composition.Compose object at 0x7f0a42aefe80>
<audiomentations.core.composition.SpecCompose object at 0x7f0a42aeff40>
Fold: 0
Epoch: 1/40
Model improve: 0.0000 -> 0.3932
Epoch: 2/40
Model improve: 0.3932 -> 0.3938
Epoch: 3/40
Model improve: 0.3938 -> 0.3944
Epoch: 4/40
Model improve: 0.3944 -> 0.3947
Epoch: 5/40
Model improve: 0.3947 -> 0.3953
Epoch: 6/40
Model improve: 0.3953 -> 0.3959
Epoch: 7/40
Model improve: 0.3959 -> 0.3966
Epoch: 8/40
Model improve: 0.3966 -> 0.3978
Epoch: 9/40
Model improve: 0.3978 -> 0.3991
Epoch: 10/40
Model improve: 0.3991 -> 0.4004
Epoch: 11/40
Model improve: 0.4004 -> 0.4033
Epoch: 12/40
Model improve: 0.4033 -> 0.4069
Epoch: 13/40
Model improve: 0.4069 -> 0.4116
Epoch: 14/40
Model improve: 0.4116 -> 0.4179
Epoch: 15/40
Model improve: 0.4179 -> 0.4243
Epoch: 16/40
Model improve: 0.4243 -> 0.4365
Epoch: 17/40
Model improve: 0.4365 -> 0.4461
Epoch: 18/40
Model improve: 0.4461 -> 0.4601
Epoch: 19/40
Model improve: 0.4601 -> 0.4769
Epoch: 20/40
Model improve: 0.4769 -> 0.4908
Epoch: 21/40
Model improve: 0.4908 -> 0.5086
Epoch: 22/40
Model improve: 0.5086 -> 0.5274
Epoch: 23/40
Model improve: 0.5274 -> 0.5470
Epoch: 24/40
Model improve: 0.5470 -> 0.5638
Epoch: 25/40
Model improve: 0.5638 -> 0.5829
Epoch: 26/40
Model improve: 0.5829 -> 0.5962
Epoch: 27/40
Model improve: 0.5962 -> 0.6089
Epoch: 28/40
Model improve: 0.6089 -> 0.6176
Epoch: 29/40
Model improve: 0.6176 -> 0.6266
<audiomentations.core.composition.Compose object at 0x7fe153b667a0>
<audiomentations.core.composition.SpecCompose object at 0x7fe153b7ca30>
Fold: 0
Epoch: 1/40
Model improve: 0.0000 -> 0.3932
Epoch: 2/40
Model improve: 0.3932 -> 0.3932
Epoch: 3/40
<audiomentations.core.composition.Compose object at 0x7f03f6b8ee30>
<audiomentations.core.composition.SpecCompose object at 0x7f03f6bfac80>
Fold: 0
Epoch: 1/40
Model improve: 0.0000 -> 0.3932
Epoch: 2/40
Epoch: 3/40
<audiomentations.core.composition.Compose object at 0x7f09a9088e20>
<audiomentations.core.composition.SpecCompose object at 0x7f09aa717dc0>
Fold: 0
Epoch: 1/40
<audiomentations.core.composition.Compose object at 0x7f096c349120>
<audiomentations.core.composition.SpecCompose object at 0x7f096c6fda20>
<audiomentations.core.composition.Compose object at 0x7f0961888460>
<audiomentations.core.composition.SpecCompose object at 0x7f0961888100>
Fold: 0
Fold: 0
Fold: 0
Epoch: 1/40
<audiomentations.core.composition.Compose object at 0x7f096196afb0>
<audiomentations.core.composition.SpecCompose object at 0x7f0961969d50>
Fold: 0
Epoch: 1/40
Model improve: 0.0000 -> 0.3944
Epoch: 2/40
Epoch: 3/40
<audiomentations.core.composition.Compose object at 0x7f09312aafe0>
<audiomentations.core.composition.SpecCompose object at 0x7f0961719ae0>
<audiomentations.core.composition.Compose object at 0x7f0930f0c130>
<audiomentations.core.composition.SpecCompose object at 0x7f09312ab640>
<audiomentations.core.composition.Compose object at 0x7f0930ec5570>
<audiomentations.core.composition.SpecCompose object at 0x7f0930ec50f0>
<audiomentations.core.composition.Compose object at 0x7f0930ef73a0>
<audiomentations.core.composition.SpecCompose object at 0x7f0930ef7400>
<audiomentations.core.composition.Compose object at 0x7f092a1241f0>
<audiomentations.core.composition.SpecCompose object at 0x7f092a862d40>
<audiomentations.core.composition.Compose object at 0x7f092a10d900>
<audiomentations.core.composition.SpecCompose object at 0x7f092a10ec80>
<audiomentations.core.composition.Compose object at 0x7f092a072c20>
<audiomentations.core.composition.SpecCompose object at 0x7f092a072d40>
<audiomentations.core.composition.Compose object at 0x7f092a127220>
<audiomentations.core.composition.SpecCompose object at 0x7f092a126380>
<audiomentations.core.composition.Compose object at 0x7f091df3a620>
<audiomentations.core.composition.SpecCompose object at 0x7f091df3a800>
<audiomentations.core.composition.Compose object at 0x7f092a10e4a0>
<audiomentations.core.composition.SpecCompose object at 0x7f092a10fd30>
<audiomentations.core.composition.Compose object at 0x7f092a10d9c0>
<audiomentations.core.composition.SpecCompose object at 0x7f092a10f6a0>
<audiomentations.core.composition.Compose object at 0x7f092a10e6b0>
<audiomentations.core.composition.SpecCompose object at 0x7f092a10fbb0>
<audiomentations.core.composition.Compose object at 0x7f092a10f1f0>
<audiomentations.core.composition.SpecCompose object at 0x7f092a10e440>
<audiomentations.core.composition.Compose object at 0x7f092a10d810>
<audiomentations.core.composition.SpecCompose object at 0x7f092a10f580>
<audiomentations.core.composition.Compose object at 0x7f092a10e020>
<audiomentations.core.composition.SpecCompose object at 0x7f092a10e140>
<audiomentations.core.composition.Compose object at 0x7f092a070a30>
<audiomentations.core.composition.SpecCompose object at 0x7f092a10ee00>
<audiomentations.core.composition.Compose object at 0x7f092a072170>
<audiomentations.core.composition.SpecCompose object at 0x7f092a071720>
<audiomentations.core.composition.Compose object at 0x7f092a0709d0>
<audiomentations.core.composition.SpecCompose object at 0x7f092a071c60>
<audiomentations.core.composition.Compose object at 0x7f092a0723b0>
<audiomentations.core.composition.SpecCompose object at 0x7f092a070c40>
<audiomentations.core.composition.Compose object at 0x7f092a073a00>
<audiomentations.core.composition.SpecCompose object at 0x7f092a073c10>
<audiomentations.core.composition.Compose object at 0x7f092a073ac0>
<audiomentations.core.composition.SpecCompose object at 0x7f092a073ee0>
<audiomentations.core.composition.Compose object at 0x7f092a073dc0>
<audiomentations.core.composition.SpecCompose object at 0x7f092a072380>
<audiomentations.core.composition.Compose object at 0x7f092a070bb0>
<audiomentations.core.composition.SpecCompose object at 0x7f092a070be0>
<audiomentations.core.composition.Compose object at 0x7f092a0707f0>
<audiomentations.core.composition.SpecCompose object at 0x7f092a072440>
<audiomentations.core.composition.Compose object at 0x7f092a072320>
<audiomentations.core.composition.SpecCompose object at 0x7f092a073880>
<audiomentations.core.composition.Compose object at 0x7f092a070850>
<audiomentations.core.composition.SpecCompose object at 0x7f092a072170>
<audiomentations.core.composition.Compose object at 0x7f092a0704f0>
<audiomentations.core.composition.SpecCompose object at 0x7f092a070a30>
<audiomentations.core.composition.Compose object at 0x7f092a0733d0>
<audiomentations.core.composition.SpecCompose object at 0x7f092a0739a0>
<audiomentations.core.composition.Compose object at 0x7f092a071840>
<audiomentations.core.composition.SpecCompose object at 0x7f092a073640>
<audiomentations.core.composition.Compose object at 0x7f092a071090>
<audiomentations.core.composition.SpecCompose object at 0x7f092a070b50>
<audiomentations.core.composition.Compose object at 0x7f092a072470>
<audiomentations.core.composition.SpecCompose object at 0x7f092a0729b0>
<audiomentations.core.composition.Compose object at 0x7f092a072b60>
<audiomentations.core.composition.SpecCompose object at 0x7f092a071030>
<audiomentations.core.composition.Compose object at 0x7f092a0717b0>
<audiomentations.core.composition.SpecCompose object at 0x7f092a073370>
<audiomentations.core.composition.Compose object at 0x7f092a071f30>
<audiomentations.core.composition.SpecCompose object at 0x7f092a071960>
<audiomentations.core.composition.Compose object at 0x7f092a070430>
<audiomentations.core.composition.SpecCompose object at 0x7f092a071810>
<audiomentations.core.composition.Compose object at 0x7f092a073760>
<audiomentations.core.composition.SpecCompose object at 0x7f092a070c10>
<audiomentations.core.composition.Compose object at 0x7f092a073250>
<audiomentations.core.composition.SpecCompose object at 0x7f092a073310>
<audiomentations.core.composition.Compose object at 0x7f092a070280>
<audiomentations.core.composition.SpecCompose object at 0x7f092a0711b0>
<audiomentations.core.composition.Compose object at 0x7f092a070340>
<audiomentations.core.composition.SpecCompose object at 0x7f092a070ee0>
<audiomentations.core.composition.Compose object at 0x7f092a0706a0>
<audiomentations.core.composition.SpecCompose object at 0x7f092a071ae0>
<audiomentations.core.composition.Compose object at 0x7f092a10c880>
<audiomentations.core.composition.SpecCompose object at 0x7f092a10c4f0>
<audiomentations.core.composition.Compose object at 0x7f092a073070>
<audiomentations.core.composition.SpecCompose object at 0x7f092a10fcd0>
<audiomentations.core.composition.Compose object at 0x7f092a10fd00>
<audiomentations.core.composition.SpecCompose object at 0x7f092a071d50>
<audiomentations.core.composition.Compose object at 0x7f092a070e20>
<audiomentations.core.composition.SpecCompose object at 0x7f092a10f340>
<audiomentations.core.composition.Compose object at 0x7f092a070940>
<audiomentations.core.composition.SpecCompose object at 0x7f092a070ac0>
<audiomentations.core.composition.Compose object at 0x7f092a073340>
<audiomentations.core.composition.SpecCompose object at 0x7f092a073130>
<audiomentations.core.composition.Compose object at 0x7f092a072b90>
<audiomentations.core.composition.SpecCompose object at 0x7f092a0722c0>
<audiomentations.core.composition.Compose object at 0x7f092a071720>
<audiomentations.core.composition.SpecCompose object at 0x7f092a071360>
<audiomentations.core.composition.Compose object at 0x7f092a071c60>
<audiomentations.core.composition.SpecCompose object at 0x7f092a071ff0>
<audiomentations.core.composition.Compose object at 0x7f092a0706a0>
<audiomentations.core.composition.SpecCompose object at 0x7f092a072020>
<audiomentations.core.composition.Compose object at 0x7f092a073910>
<audiomentations.core.composition.SpecCompose object at 0x7f092a071fc0>
<audiomentations.core.composition.Compose object at 0x7f092a0711e0>
<audiomentations.core.composition.SpecCompose object at 0x7f092a071360>
<audiomentations.core.composition.Compose object at 0x7f092a073f70>
<audiomentations.core.composition.SpecCompose object at 0x7f092a0718a0>
<audiomentations.core.composition.Compose object at 0x7f092a0700a0>
<audiomentations.core.composition.SpecCompose object at 0x7f092a070940>
<audiomentations.core.composition.Compose object at 0x7f092a071ae0>
<audiomentations.core.composition.SpecCompose object at 0x7f092a072410>
<audiomentations.core.composition.Compose object at 0x7f092a0732b0>
<audiomentations.core.composition.SpecCompose object at 0x7f092a070f40>
<audiomentations.core.composition.Compose object at 0x7f092a071570>
<audiomentations.core.composition.SpecCompose object at 0x7f092a071450>
<audiomentations.core.composition.Compose object at 0x7f092a070190>
<audiomentations.core.composition.SpecCompose object at 0x7f092a071660>
<audiomentations.core.composition.Compose object at 0x7f091df39060>
<audiomentations.core.composition.SpecCompose object at 0x7f092a0720e0>
<audiomentations.core.composition.Compose object at 0x7f091df391e0>
<audiomentations.core.composition.SpecCompose object at 0x7f091df3bf70>
<audiomentations.core.composition.Compose object at 0x7f092a072020>
<audiomentations.core.composition.SpecCompose object at 0x7f092a0733d0>
<audiomentations.core.composition.Compose object at 0x7f091df3b5b0>
<audiomentations.core.composition.SpecCompose object at 0x7f092a073010>
<audiomentations.core.composition.Compose object at 0x7f091df39210>
<audiomentations.core.composition.SpecCompose object at 0x7f091df3bee0>
<audiomentations.core.composition.Compose object at 0x7f091df3a290>
<audiomentations.core.composition.SpecCompose object at 0x7f091df380d0>
<audiomentations.core.composition.Compose object at 0x7f091df39ff0>
<audiomentations.core.composition.SpecCompose object at 0x7f091df390f0>
<audiomentations.core.composition.Compose object at 0x7f091df3a080>
<audiomentations.core.composition.SpecCompose object at 0x7f091df38130>
<audiomentations.core.composition.Compose object at 0x7f091df39e70>
<audiomentations.core.composition.SpecCompose object at 0x7f091df392d0>
<audiomentations.core.composition.Compose object at 0x7f091df39c00>
<audiomentations.core.composition.SpecCompose object at 0x7f091df39030>
<audiomentations.core.composition.Compose object at 0x7f091df39ab0>
<audiomentations.core.composition.SpecCompose object at 0x7f091df3b610>
<audiomentations.core.composition.Compose object at 0x7f091df39960>
<audiomentations.core.composition.SpecCompose object at 0x7f091df3ace0>
<audiomentations.core.composition.Compose object at 0x7f091df39060>
<audiomentations.core.composition.SpecCompose object at 0x7f091df39630>
<audiomentations.core.composition.Compose object at 0x7f091df3a110>
<audiomentations.core.composition.SpecCompose object at 0x7f091df39600>
<audiomentations.core.composition.Compose object at 0x7f091df39e10>
<audiomentations.core.composition.SpecCompose object at 0x7f091df3a0b0>
<audiomentations.core.composition.Compose object at 0x7f091df38f40>
<audiomentations.core.composition.SpecCompose object at 0x7f091df398d0>
<audiomentations.core.composition.Compose object at 0x7f091df39a50>
<audiomentations.core.composition.SpecCompose object at 0x7f091df38fd0>
<audiomentations.core.composition.Compose object at 0x7f091df38be0>
<audiomentations.core.composition.SpecCompose object at 0x7f091df392a0>
<audiomentations.core.composition.Compose object at 0x7f091df38a30>
<audiomentations.core.composition.SpecCompose object at 0x7f091df39d80>
<audiomentations.core.composition.Compose object at 0x7f091df383d0>
<audiomentations.core.composition.SpecCompose object at 0x7f091df3a170>
<audiomentations.core.composition.Compose object at 0x7f091df386d0>
<audiomentations.core.composition.SpecCompose object at 0x7f091df393c0>
<audiomentations.core.composition.Compose object at 0x7f091df385e0>
<audiomentations.core.composition.SpecCompose object at 0x7f091df39750>
<audiomentations.core.composition.Compose object at 0x7f092a0723e0>
<audiomentations.core.composition.SpecCompose object at 0x7f092a071a80>
<audiomentations.core.composition.Compose object at 0x7f092a071150>
<audiomentations.core.composition.SpecCompose object at 0x7f092a070670>
<audiomentations.core.composition.Compose object at 0x7f092a072410>
<audiomentations.core.composition.SpecCompose object at 0x7f092a073070>
<audiomentations.core.composition.Compose object at 0x7f092a070d60>
<audiomentations.core.composition.SpecCompose object at 0x7f092a071c30>
<audiomentations.core.composition.Compose object at 0x7f092a0714e0>
<audiomentations.core.composition.SpecCompose object at 0x7f092a0707c0>
<audiomentations.core.composition.Compose object at 0x7f092a0723e0>
<audiomentations.core.composition.SpecCompose object at 0x7f092a073d00>
<audiomentations.core.composition.Compose object at 0x7f091df395d0>
<audiomentations.core.composition.SpecCompose object at 0x7f092a072260>
<audiomentations.core.composition.Compose object at 0x7f091df3aef0>
<audiomentations.core.composition.SpecCompose object at 0x7f091df38a30>
<audiomentations.core.composition.Compose object at 0x7f091df395d0>
<audiomentations.core.composition.SpecCompose object at 0x7f091df38d60>
<audiomentations.core.composition.Compose object at 0x7f091df399f0>
<audiomentations.core.composition.SpecCompose object at 0x7f091df38a90>
Fold: 0
Epoch: 1/40
Model improve: 0.0000 -> 0.3931
Epoch: 2/40
Model improve: 0.3931 -> 0.3937
Epoch: 3/40
Model improve: 0.3937 -> 0.3938
Epoch: 4/40
Fold: 0
Epoch: 1/40
Fold: 0
Epoch: 1/40
Model improve: 0.0000 -> 0.3936
Epoch: 2/40
Fold: 0
Epoch: 1/40
Model improve: 0.0000 -> 0.3937
Epoch: 2/40
Epoch: 3/40
Epoch: 4/40
Epoch: 5/40
Epoch: 6/40
Epoch: 7/40
Model improve: 0.3937 -> 0.3946
Epoch: 8/40
Model improve: 0.3946 -> 0.3959
Fold: 0
Epoch: 1/100
Fold: 0
Fold: 0
Epoch: 1/100
Epoch: 1/100
Fold: 0
Epoch: 1/100
Model improve: 0.0000 -> 0.9994
Epoch: 2/100
Epoch: 3/100
Epoch: 4/100
Epoch: 5/100
Epoch: 6/100
Epoch: 7/100
Epoch: 8/100
Epoch: 9/100
Epoch: 10/100
Epoch: 11/100
Epoch: 12/100
Epoch: 13/100
Epoch: 14/100
Epoch: 15/100
Epoch: 16/100
Epoch: 17/100
Epoch: 18/100
Epoch: 19/100
Epoch: 20/100
Epoch: 21/100
Epoch: 22/100
Fold: 0
Epoch: 1/100
Epoch: 2/100
Epoch: 3/100
Epoch: 4/100
Epoch: 5/100
Model improve: 0.0000 -> 0.7442
Epoch: 6/100
Model improve: 0.7442 -> 0.7976
Epoch: 7/100
Model improve: 0.7976 -> 0.8392
Epoch: 8/100
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0
    maximize: False
    weight_decay: 0.01
)
<torch.optim.lr_scheduler.LambdaLR object at 0x7fa217143700>
Epoch: 1/100
<class '__main__.CFG'>
Fold: 0
16941
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0
    maximize: False
    weight_decay: 0.01
)
<torch.optim.lr_scheduler.LambdaLR object at 0x7f9580942740>
Epoch: 1/100
<class '__main__.CFG'>
<class '__main__.CFG'>
<class '__main__.CFG'>
Fold: 0
16941
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0
    maximize: False
    weight_decay: 0.01
)
<torch.optim.lr_scheduler.LambdaLR object at 0x7fb94eb9ed40>
Epoch: 1/100
Epoch: 2/100
Epoch: 3/100
Epoch: 4/100
Model improve: 0.0000 -> 0.7318
Epoch: 5/100
Model improve: 0.7318 -> 0.7670
Epoch: 6/100
Model improve: 0.7670 -> 0.7964
Epoch: 7/100
Model improve: 0.7964 -> 0.8084
Epoch: 8/100
Model improve: 0.8084 -> 0.8226
Epoch: 9/100
Epoch: 10/100
Model improve: 0.8226 -> 0.8292
Epoch: 11/100
Epoch: 12/100
Model improve: 0.8292 -> 0.8326
Epoch: 13/100
Model improve: 0.8326 -> 0.8369
Epoch: 14/100
Epoch: 15/100
Epoch: 16/100
Epoch: 17/100
Epoch: 18/100
Model improve: 0.8369 -> 0.8403
Epoch: 19/100
Epoch: 20/100
<class '__main__.CFG'>
Fold: 0
16941
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0
    maximize: False
    weight_decay: 0.01
)
<torch.optim.lr_scheduler.LambdaLR object at 0x7ff3696274f0>
Epoch: 1/100
<class '__main__.CFG'>
Fold: 0
16941
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0
    maximize: False
    weight_decay: 0.01
)
<torch.optim.lr_scheduler.LambdaLR object at 0x7f22202876d0>
Epoch: 1/100
Epoch: 2/100
Epoch: 3/100
Epoch: 4/100
Epoch: 5/100
Model improve: 0.0000 -> 0.7091
Epoch: 6/100
Model improve: 0.7091 -> 0.7490
Epoch: 7/100
Model improve: 0.7490 -> 0.7777
Epoch: 8/100
Model improve: 0.7777 -> 0.7937
Epoch: 9/100
Model improve: 0.7937 -> 0.8010
Epoch: 10/100
Model improve: 0.8010 -> 0.8118
Epoch: 11/100
Date :03/21/2023, 08:45:41
5
32000
2048
20
224
20
16000
128
256
10
100
0.0004
0.01
50
Fold: 0
16941
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :03/21/2023, 08:49:37
Date :03/21/2023, 08:49:37
Duration: 5
Duration: 5
Sample rate: 32000
Sample rate: 32000
nfft: 2048
nfft: 2048
fmin: 20
fmin: 20
nmels: 224
nmels: 224
fmax: 16000
fmax: 16000
trainbs: 128
trainbs: 128
validbs: 256
validbs: 256
epochwarmup: 10
epochwarmup: 10
totalepoch: 100
totalepoch: 100
learningrate: 0.0004
learningrate: 0.0004
weightdecay: 0.01
weightdecay: 0.01
thrupsample: 50
thrupsample: 50
Fold: 0
Fold: 0
16941
16941
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0
    maximize: False
    weight_decay: 0.01
)
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Epoch: 1/100
Date :03/21/2023, 08:50:08
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 224
fmax: 16000
trainbs: 128
validbs: 256
epochwarmup: 10
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
Fold: 0
16941
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :03/21/2023, 08:51:49
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 224
fmax: 16000
trainbs: 128
validbs: 256
epochwarmup: 10
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
Fold: 0
16941
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Epoch: 2/100
Epoch: 3/100
Epoch: 4/100
Epoch: 5/100
Model improve: 0.0000 -> 0.7026
Epoch: 6/100
Model improve: 0.7026 -> 0.7361
Epoch: 7/100
Model improve: 0.7361 -> 0.7674
Epoch: 8/100
Model improve: 0.7674 -> 0.7862
Epoch: 9/100
Date :03/21/2023, 10:39:26
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 224
fmax: 16000
trainbs: 128
validbs: 256
epochwarmup: 10
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
Fold: 0
16941
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Model improve: 0.7862 -> 0.7937
Epoch: 10/100
Epoch: 2/100
Epoch: 3/100
Model improve: 0.7937 -> 0.8114
Epoch: 11/100
Epoch: 4/100
Model improve: 0.8114 -> 0.8190
Epoch: 12/100
Epoch: 5/100
Model improve: 0.0000 -> 0.7232
Epoch: 6/100
Model improve: 0.8190 -> 0.8218
Epoch: 13/100
Model improve: 0.7232 -> 0.7545
Epoch: 7/100
Model improve: 0.8218 -> 0.8287
Epoch: 14/100
Model improve: 0.7545 -> 0.7786
Epoch: 8/100
Model improve: 0.7786 -> 0.7979
Epoch: 9/100
Model improve: 0.8287 -> 0.8343
Epoch: 15/100
Model improve: 0.7979 -> 0.8041
Epoch: 10/100
Model improve: 0.8343 -> 0.8402
Epoch: 16/100
Model improve: 0.8041 -> 0.8170
Epoch: 11/100
Model improve: 0.8170 -> 0.8194
Epoch: 12/100
Epoch: 17/100
Model improve: 0.8194 -> 0.8225
Epoch: 13/100
Model improve: 0.8225 -> 0.8288
Epoch: 18/100
Epoch: 14/100
Date :03/21/2023, 12:40:29
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 224
fmax: 16000
trainbs: 128
validbs: 256
epochwarmup: 10
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
Fold: 0
16941
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Epoch: 2/100
Epoch: 3/100
Epoch: 4/100
Epoch: 5/100
Model improve: 0.0000 -> 0.7211
Epoch: 6/100
Model improve: 0.7211 -> 0.7539
Epoch: 7/100
Model improve: 0.7539 -> 0.7778
Epoch: 8/100
Model improve: 0.7778 -> 0.7936
Epoch: 9/100
Model improve: 0.7936 -> 0.8045
Epoch: 10/100
Model improve: 0.8045 -> 0.8130
Epoch: 11/100
Model improve: 0.8130 -> 0.8167
Epoch: 12/100
Model improve: 0.8167 -> 0.8259
Epoch: 13/100
Model improve: 0.8259 -> 0.8322
Epoch: 14/100
Epoch: 15/100
Model improve: 0.8322 -> 0.8330
Epoch: 16/100
Model improve: 0.8330 -> 0.8372
Epoch: 17/100
Epoch: 18/100
Model improve: 0.8372 -> 0.8390
Epoch: 19/100
Epoch: 20/100
Epoch: 21/100
Model improve: 0.8390 -> 0.8411
Epoch: 22/100
Epoch: 23/100
Epoch: 24/100
Epoch: 25/100
Model improve: 0.8411 -> 0.8433
Epoch: 26/100
Epoch: 27/100
Epoch: 28/100
Epoch: 29/100
Model improve: 0.8433 -> 0.8443
Epoch: 30/100
Epoch: 31/100
Epoch: 32/100
Epoch: 33/100
Epoch: 34/100
Model improve: 0.8443 -> 0.8471
Epoch: 35/100
Epoch: 36/100
Epoch: 37/100
Epoch: 38/100
Epoch: 39/100
Epoch: 40/100
Epoch: 41/100
Epoch: 42/100
Epoch: 43/100
Epoch: 44/100
Epoch: 45/100
Epoch: 46/100
Epoch: 47/100
Epoch: 48/100
Epoch: 49/100
Epoch: 50/100
Epoch: 51/100
Epoch: 52/100
Epoch: 53/100
Epoch: 54/100
Model improve: 0.8471 -> 0.8476
Epoch: 55/100
Epoch: 56/100
Epoch: 57/100
Epoch: 58/100
Model improve: 0.8476 -> 0.8477
Epoch: 59/100
Model improve: 0.8477 -> 0.8491
Epoch: 60/100
Model improve: 0.8491 -> 0.8505
Epoch: 61/100
Epoch: 62/100
Epoch: 63/100
Epoch: 64/100
Epoch: 65/100
Epoch: 66/100
Epoch: 67/100
Epoch: 68/100
Epoch: 69/100
Epoch: 70/100
Epoch: 71/100
Epoch: 72/100
Epoch: 73/100
Epoch: 74/100
Epoch: 75/100
Epoch: 76/100
Epoch: 77/100
Epoch: 78/100
Epoch: 79/100
Model improve: 0.8505 -> 0.8514
Epoch: 80/100
Model improve: 0.8514 -> 0.8515
Epoch: 81/100
Epoch: 82/100
Date :03/22/2023, 03:36:09
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 224
fmax: 16000
trainbs: 128
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnet_b0_ns
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Epoch: 2/100
Date :03/22/2023, 03:55:16
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 224
fmax: 16000
trainbs: 128
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnet_b0_ns
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Date :03/22/2023, 03:56:20
Duration: 5
Sample rate: 32000
nfft: 512
fmin: 20
nmels: 512
fmax: 16000
trainbs: 128
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_b2
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Model improve: 0.0000 -> 0.6223
Epoch: 2/100
Model improve: 0.6223 -> 0.6947
Epoch: 3/100
Date :03/22/2023, 04:13:24
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 224
fmax: 16000
trainbs: 256
validbs: 512
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_b2
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Date :03/22/2023, 04:14:26
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 224
fmax: 16000
trainbs: 64
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_b1
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Model improve: 0.0000 -> 0.6804
Epoch: 2/100
Model improve: 0.6804 -> 0.7503
Epoch: 3/100
Model improve: 0.7503 -> 0.7824
Epoch: 4/100
Model improve: 0.7824 -> 0.8037
Epoch: 5/100
Model improve: 0.8037 -> 0.8222
Epoch: 6/100
Model improve: 0.8222 -> 0.8260
Epoch: 7/100
Model improve: 0.8260 -> 0.8348
Epoch: 8/100
Model improve: 0.8348 -> 0.8396
Epoch: 9/100
Model improve: 0.8396 -> 0.8498
Epoch: 10/100
Epoch: 11/100
Model improve: 0.8498 -> 0.8516
Epoch: 12/100
Model improve: 0.8516 -> 0.8543
Epoch: 13/100
Epoch: 14/100
Epoch: 15/100
Epoch: 16/100
Model improve: 0.8543 -> 0.8582
Epoch: 17/100
Model improve: 0.8582 -> 0.8591
Epoch: 18/100
Epoch: 19/100
Model improve: 0.8591 -> 0.8604
Epoch: 20/100
Model improve: 0.8604 -> 0.8621
Epoch: 21/100
Epoch: 22/100
Epoch: 23/100
Date :03/22/2023, 07:18:23
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 224
fmax: 16000
trainbs: 64
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_b3
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Model improve: 0.0000 -> 0.6957
Epoch: 2/100
Model improve: 0.6957 -> 0.7627
Epoch: 3/100
Model improve: 0.7627 -> 0.7867
Epoch: 4/100
Model improve: 0.7867 -> 0.8165
Epoch: 5/100
Model improve: 0.8165 -> 0.8253
Epoch: 6/100
Model improve: 0.8253 -> 0.8348
Epoch: 7/100
Epoch: 8/100
Model improve: 0.8348 -> 0.8437
Epoch: 9/100
Model improve: 0.8437 -> 0.8451
Epoch: 10/100
Model improve: 0.8451 -> 0.8464
Epoch: 11/100
Epoch: 12/100
Model improve: 0.8464 -> 0.8474
Epoch: 13/100
Model improve: 0.8474 -> 0.8487
Epoch: 14/100
Epoch: 15/100
Model improve: 0.8487 -> 0.8533
Epoch: 16/100
Epoch: 17/100
Model improve: 0.8533 -> 0.8564
Epoch: 18/100
Date :03/22/2023, 09:58:21
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 224
fmax: 16000
trainbs: 128
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 100
model_name: tf_efficientnetv2_b1
Fold: 0
30278
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Model improve: 0.0000 -> 0.7093
Epoch: 2/100
Model improve: 0.7093 -> 0.7813
Epoch: 3/100
Model improve: 0.7813 -> 0.8069
Epoch: 4/100
Model improve: 0.8069 -> 0.8217
Epoch: 5/100
Model improve: 0.8217 -> 0.8277
Epoch: 6/100
Model improve: 0.8277 -> 0.8389
Epoch: 7/100
Model improve: 0.8389 -> 0.8431
Epoch: 8/100
Model improve: 0.8431 -> 0.8442
Epoch: 9/100
Model improve: 0.8442 -> 0.8466
Epoch: 10/100
Model improve: 0.8466 -> 0.8468
Epoch: 11/100
Model improve: 0.8468 -> 0.8488
Epoch: 12/100
Model improve: 0.8488 -> 0.8511
Epoch: 13/100
Model improve: 0.8511 -> 0.8519
Epoch: 14/100
Epoch: 15/100
Model improve: 0.8519 -> 0.8524
Epoch: 16/100
Model improve: 0.8524 -> 0.8554
Epoch: 17/100
Model improve: 0.8554 -> 0.8558
Epoch: 18/100
Epoch: 19/100
Model improve: 0.8558 -> 0.8575
Epoch: 20/100
Epoch: 21/100
Model improve: 0.8575 -> 0.8598
Epoch: 22/100
Model improve: 0.8598 -> 0.8607
Epoch: 23/100
Model improve: 0.8607 -> 0.8625
Epoch: 24/100
Epoch: 25/100
Epoch: 26/100
Epoch: 27/100
Date :03/22/2023, 15:12:17
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 256
fmax: 16000
trainbs: 64
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 100
model_name: tf_efficientnetv2_b1
Fold: 0
30278
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :03/22/2023, 15:13:51
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 256
fmax: 16000
trainbs: 64
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 25
model_name: tf_efficientnetv2_b1
Fold: 0
15314
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Model improve: 0.0000 -> 0.6305
Epoch: 2/100
Model improve: 0.6305 -> 0.7220
Epoch: 3/100
Model improve: 0.7220 -> 0.7578
Epoch: 4/100
Model improve: 0.7578 -> 0.7835
Epoch: 5/100
Model improve: 0.7835 -> 0.8009
Epoch: 6/100
Model improve: 0.8009 -> 0.8121
Epoch: 7/100
Model improve: 0.8121 -> 0.8192
Epoch: 8/100
Model improve: 0.8192 -> 0.8317
Epoch: 9/100
Model improve: 0.8317 -> 0.8323
Epoch: 10/100
Date :03/22/2023, 16:22:07
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Model improve: 0.0000 -> 0.6833
Epoch: 2/100
Model improve: 0.6833 -> 0.7580
Epoch: 3/100
Model improve: 0.7580 -> 0.7889
Epoch: 4/100
Model improve: 0.7889 -> 0.8109
Epoch: 5/100
Model improve: 0.8109 -> 0.8169
Epoch: 6/100
Model improve: 0.8169 -> 0.8263
Epoch: 7/100
Model improve: 0.8263 -> 0.8372
Epoch: 8/100
Model improve: 0.8372 -> 0.8390
Epoch: 9/100
Model improve: 0.8390 -> 0.8457
Epoch: 10/100
Model improve: 0.8457 -> 0.8473
Epoch: 11/100
Epoch: 12/100
Model improve: 0.8473 -> 0.8515
Epoch: 13/100
Model improve: 0.8515 -> 0.8517
Epoch: 14/100
Epoch: 15/100
Model improve: 0.8517 -> 0.8526
Epoch: 16/100
Model improve: 0.8526 -> 0.8537
Epoch: 17/100
Epoch: 18/100
Model improve: 0.8537 -> 0.8554
Epoch: 19/100
Model improve: 0.8554 -> 0.8606
Epoch: 20/100
Epoch: 21/100
Epoch: 22/100
Epoch: 23/100
Model improve: 0.8606 -> 0.8612
Epoch: 24/100
Model improve: 0.8612 -> 0.8612
Epoch: 25/100
Epoch: 26/100
Epoch: 27/100
Model improve: 0.8612 -> 0.8621
Epoch: 28/100
Epoch: 29/100
Epoch: 30/100
Epoch: 31/100
Epoch: 32/100
Epoch: 33/100
Epoch: 34/100
Epoch: 35/100
Epoch: 36/100
Epoch: 37/100
Model improve: 0.8621 -> 0.8635
Epoch: 38/100
Model improve: 0.8635 -> 0.8688
Epoch: 39/100
Epoch: 40/100
Epoch: 41/100
Epoch: 42/100
Epoch: 43/100
Epoch: 44/100
Epoch: 45/100
Epoch: 46/100
Epoch: 47/100
Epoch: 48/100
Epoch: 49/100
Epoch: 50/100
Epoch: 51/100
Epoch: 52/100
Epoch: 53/100
Epoch: 54/100
Epoch: 55/100
Model improve: 0.8688 -> 0.8696
Epoch: 56/100
Epoch: 57/100
Epoch: 58/100
Date :03/22/2023, 23:28:06
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 224
fmax: 16000
trainbs: 64
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 60
model_name: tf_efficientnetv2_b1
Fold: 0
21533
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Model improve: 0.0000 -> 0.6945
Epoch: 2/100
Model improve: 0.6945 -> 0.7660
Epoch: 3/100
Model improve: 0.7660 -> 0.7983
Epoch: 4/100
Model improve: 0.7983 -> 0.8172
Epoch: 5/100
Model improve: 0.8172 -> 0.8214
Epoch: 6/100
Model improve: 0.8214 -> 0.8324
Epoch: 7/100
Model improve: 0.8324 -> 0.8412
Epoch: 8/100
Model improve: 0.8412 -> 0.8418
Epoch: 9/100
Model improve: 0.8418 -> 0.8435
Epoch: 10/100
Model improve: 0.8435 -> 0.8438
Epoch: 11/100
Model improve: 0.8438 -> 0.8469
Epoch: 12/100
Date :03/23/2023, 01:15:31
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 224
fmax: 16000
trainbs: 64
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.4
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Model improve: 0.0000 -> 0.6819
Epoch: 2/100
Model improve: 0.6819 -> 0.7526
Epoch: 3/100
Model improve: 0.7526 -> 0.7902
Epoch: 4/100
Model improve: 0.7902 -> 0.8046
Epoch: 5/100
Model improve: 0.8046 -> 0.8189
Epoch: 6/100
Model improve: 0.8189 -> 0.8270
Epoch: 7/100
Model improve: 0.8270 -> 0.8293
Epoch: 8/100
Model improve: 0.8293 -> 0.8376
Epoch: 9/100
Model improve: 0.8376 -> 0.8473
Epoch: 10/100
Epoch: 11/100
Model improve: 0.8473 -> 0.8475
Epoch: 12/100
Model improve: 0.8475 -> 0.8491
Epoch: 13/100
Model improve: 0.8491 -> 0.8582
Epoch: 14/100
Epoch: 15/100
Epoch: 16/100
Epoch: 17/100
Model improve: 0.8582 -> 0.8601
Epoch: 18/100
Epoch: 19/100
Model improve: 0.8601 -> 0.8618
Epoch: 20/100
Epoch: 21/100
Epoch: 22/100
Epoch: 23/100
Epoch: 24/100
Model improve: 0.8618 -> 0.8653
Epoch: 25/100
Model improve: 0.8653 -> 0.8655
Epoch: 26/100
Epoch: 27/100
Epoch: 28/100
Epoch: 29/100
Epoch: 30/100
Epoch: 31/100
Epoch: 32/100
Epoch: 33/100
Model improve: 0.8655 -> 0.8655
Epoch: 34/100
Epoch: 35/100
Model improve: 0.8655 -> 0.8689
Epoch: 36/100
Epoch: 37/100
Epoch: 38/100
Epoch: 39/100
Epoch: 40/100
Model improve: 0.8689 -> 0.8690
Epoch: 41/100
Model improve: 0.8690 -> 0.8700
Epoch: 42/100
Model improve: 0.8700 -> 0.8705
Epoch: 43/100
Epoch: 44/100
Model improve: 0.8705 -> 0.8710
Epoch: 45/100
Date :03/23/2023, 07:24:07
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 224
fmax: 16000
trainbs: 128
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_b2
mix_up: 0.4
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Model improve: 0.0000 -> 0.6348
Epoch: 2/100
Model improve: 0.6348 -> 0.7237
Epoch: 3/100
Model improve: 0.7237 -> 0.7717
Epoch: 4/100
Date :03/23/2023, 07:57:35
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 224
fmax: 16000
trainbs: 128
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.6
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Model improve: 0.0000 -> 0.7006
Epoch: 2/100
Model improve: 0.7006 -> 0.7513
Epoch: 3/100
Model improve: 0.7513 -> 0.7853
Epoch: 4/100
Model improve: 0.7853 -> 0.8119
Epoch: 5/100
Model improve: 0.8119 -> 0.8158
Epoch: 6/100
Model improve: 0.8158 -> 0.8229
Epoch: 7/100
Epoch: 8/100
Model improve: 0.8229 -> 0.8318
Epoch: 9/100
Epoch: 10/100
Epoch: 11/100
Model improve: 0.8318 -> 0.8346
Epoch: 12/100
Epoch: 13/100
Epoch: 14/100
Epoch: 15/100
Epoch: 16/100
Epoch: 17/100
Model improve: 0.8346 -> 0.8358
Epoch: 18/100
Epoch: 19/100
Epoch: 20/100
Epoch: 21/100
Model improve: 0.8358 -> 0.8370
Epoch: 22/100
Epoch: 23/100
Epoch: 24/100
Epoch: 25/100
Epoch: 26/100
Epoch: 27/100
Epoch: 28/100
Epoch: 29/100
Epoch: 30/100
Epoch: 31/100
Epoch: 32/100
Epoch: 33/100
Epoch: 34/100
Epoch: 35/100
Model improve: 0.8370 -> 0.8376
Epoch: 36/100
Model improve: 0.8376 -> 0.8383
Epoch: 37/100
Epoch: 38/100
Model improve: 0.8383 -> 0.8403
Epoch: 39/100
Epoch: 40/100
Epoch: 41/100
Epoch: 42/100
Epoch: 43/100
Model improve: 0.8403 -> 0.8403
Epoch: 44/100
Epoch: 45/100
Model improve: 0.8403 -> 0.8434
Epoch: 46/100
Epoch: 47/100
Epoch: 48/100
Epoch: 49/100
Model improve: 0.8434 -> 0.8448
Epoch: 50/100
Epoch: 51/100
Epoch: 52/100
Model improve: 0.8448 -> 0.8450
Epoch: 53/100
Epoch: 54/100
Model improve: 0.8450 -> 0.8450
Epoch: 55/100
Date :03/23/2023, 15:18:22
Duration: 5
Sample rate: 32000
nfft: 1024
fmin: 20
nmels: 224
fmax: 16000
trainbs: 128
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0001
weightdecay: 0.0
thrupsample: 40
model_name: tf_efficientnetv2_b1
mix_up: 0.4
Fold: 0
17727
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0001
    lr: 0.0001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Model improve: 0.0000 -> 0.4943
Epoch: 2/100
Model improve: 0.4943 -> 0.5680
Epoch: 3/100
Model improve: 0.5680 -> 0.6338
Epoch: 4/100
Model improve: 0.6338 -> 0.6725
Epoch: 5/100
Model improve: 0.6725 -> 0.7019
Epoch: 6/100
Model improve: 0.7019 -> 0.7242
Epoch: 7/100
Model improve: 0.7242 -> 0.7443
Epoch: 8/100
Model improve: 0.7443 -> 0.7582
Epoch: 9/100
Model improve: 0.7582 -> 0.7708
Epoch: 10/100
Model improve: 0.7708 -> 0.7802
Epoch: 11/100
Model improve: 0.7802 -> 0.7902
Epoch: 12/100
Model improve: 0.7902 -> 0.7952
Epoch: 13/100
Model improve: 0.7952 -> 0.8022
Epoch: 14/100
Model improve: 0.8022 -> 0.8064
Epoch: 15/100
Model improve: 0.8064 -> 0.8079
Epoch: 16/100
Model improve: 0.8079 -> 0.8138
Epoch: 17/100
Model improve: 0.8138 -> 0.8175
Epoch: 18/100
Model improve: 0.8175 -> 0.8191
Epoch: 19/100
Model improve: 0.8191 -> 0.8207
Epoch: 20/100
Model improve: 0.8207 -> 0.8209
Epoch: 21/100
Model improve: 0.8209 -> 0.8239
Epoch: 22/100
Model improve: 0.8239 -> 0.8247
Epoch: 23/100
Model improve: 0.8247 -> 0.8250
Epoch: 24/100
Model improve: 0.8250 -> 0.8280
Epoch: 25/100
Model improve: 0.8280 -> 0.8299
Epoch: 26/100
Epoch: 27/100
Model improve: 0.8299 -> 0.8307
Epoch: 28/100
Model improve: 0.8307 -> 0.8314
Epoch: 29/100
Model improve: 0.8314 -> 0.8331
Epoch: 30/100
Epoch: 31/100
Epoch: 32/100
Epoch: 33/100
Epoch: 34/100
Model improve: 0.8331 -> 0.8332
Epoch: 35/100
Model improve: 0.8332 -> 0.8352
Epoch: 36/100
Epoch: 37/100
Model improve: 0.8352 -> 0.8360
Epoch: 38/100
Epoch: 39/100
Epoch: 40/100
Epoch: 41/100
Epoch: 42/100
Epoch: 43/100
Epoch: 44/100
Epoch: 45/100
Epoch: 46/100
Model improve: 0.8360 -> 0.8361
Epoch: 47/100
Epoch: 48/100
Epoch: 49/100
Epoch: 50/100
Model improve: 0.8361 -> 0.8375
Epoch: 51/100
Epoch: 52/100
Epoch: 53/100
Epoch: 54/100
Epoch: 55/100
Epoch: 56/100
Epoch: 57/100
Epoch: 58/100
Epoch: 59/100
Model improve: 0.8375 -> 0.8376
Epoch: 60/100
Epoch: 61/100
Epoch: 62/100
Epoch: 63/100
Epoch: 64/100
Epoch: 65/100
Epoch: 66/100
Model improve: 0.8376 -> 0.8385
Epoch: 67/100
Epoch: 68/100
Epoch: 69/100
Epoch: 70/100
Epoch: 71/100
Epoch: 72/100
Epoch: 73/100
Epoch: 74/100
Epoch: 75/100
Epoch: 76/100
Epoch: 77/100
Model improve: 0.8385 -> 0.8385
Epoch: 78/100
Epoch: 79/100
Epoch: 80/100
Epoch: 81/100
Epoch: 82/100
Epoch: 83/100
Model improve: 0.8385 -> 0.8388
Epoch: 84/100
Epoch: 85/100
Model improve: 0.8388 -> 0.8390
Epoch: 86/100
Epoch: 87/100
Epoch: 88/100
Epoch: 89/100
Epoch: 90/100
Epoch: 91/100
Epoch: 92/100
Epoch: 93/100
Epoch: 94/100
Epoch: 95/100
Model improve: 0.8390 -> 0.8392
Epoch: 96/100
Model improve: 0.8392 -> 0.8392
Epoch: 97/100
Epoch: 98/100
Epoch: 99/100
Epoch: 100/100
Date :03/24/2023, 02:06:34
Duration: 5
Sample rate: 32000
nfft: 1024
fmin: 20
nmels: 224
fmax: 16000
trainbs: 128
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 60
model_name: tf_efficientnetv2_b1
mix_up: 0.4
Fold: 0
21533
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Model improve: 0.0000 -> 0.6789
Epoch: 2/100
Model improve: 0.6789 -> 0.7605
Epoch: 3/100
Model improve: 0.7605 -> 0.7994
Epoch: 4/100
Model improve: 0.7994 -> 0.8154
Epoch: 5/100
Model improve: 0.8154 -> 0.8210
Epoch: 6/100
Model improve: 0.8210 -> 0.8311
Epoch: 7/100
Model improve: 0.8311 -> 0.8326
Epoch: 8/100
Model improve: 0.8326 -> 0.8349
Epoch: 9/100
Model improve: 0.8349 -> 0.8370
Epoch: 10/100
Model improve: 0.8370 -> 0.8378
Epoch: 11/100
Model improve: 0.8378 -> 0.8420
Epoch: 12/100
Epoch: 13/100
Epoch: 14/100
Epoch: 15/100
Epoch: 16/100
Epoch: 17/100
Model improve: 0.8420 -> 0.8441
Epoch: 18/100
Epoch: 19/100
Epoch: 20/100
Epoch: 21/100
Epoch: 22/100
Epoch: 23/100
Epoch: 24/100
Epoch: 25/100
Epoch: 26/100
Epoch: 27/100
Epoch: 28/100
Epoch: 29/100
Epoch: 30/100
Epoch: 31/100
Epoch: 32/100
Epoch: 33/100
Epoch: 34/100
Epoch: 35/100
Epoch: 36/100
Epoch: 37/100
Epoch: 38/100
Epoch: 39/100
Epoch: 40/100
Epoch: 41/100
Epoch: 42/100
Epoch: 43/100
Model improve: 0.8441 -> 0.8456
Epoch: 44/100
Epoch: 45/100
Epoch: 46/100
Epoch: 47/100
Model improve: 0.8456 -> 0.8459
Epoch: 48/100
Epoch: 49/100
Epoch: 50/100
Epoch: 51/100
Epoch: 52/100
Epoch: 53/100
Epoch: 54/100
Epoch: 55/100
Epoch: 56/100
Epoch: 57/100
Epoch: 58/100
Model improve: 0.8459 -> 0.8475
Epoch: 59/100
Epoch: 60/100
Epoch: 61/100
Epoch: 62/100
Epoch: 63/100
Epoch: 64/100
Epoch: 65/100
Epoch: 66/100
Epoch: 67/100
Model improve: 0.8475 -> 0.8498
Epoch: 68/100
Model improve: 0.8498 -> 0.8501
Epoch: 69/100
Epoch: 70/100
Epoch: 71/100
Epoch: 72/100
Model improve: 0.8501 -> 0.8503
Epoch: 73/100
Epoch: 74/100
Model improve: 0.8503 -> 0.8518
Epoch: 75/100
Epoch: 76/100
Model improve: 0.8518 -> 0.8528
Epoch: 77/100
Epoch: 78/100
Epoch: 79/100
Epoch: 80/100
Epoch: 81/100
Epoch: 82/100
Epoch: 83/100
Epoch: 84/100
Epoch: 85/100
Epoch: 86/100
Epoch: 87/100
Epoch: 88/100
Epoch: 89/100
Epoch: 90/100
Epoch: 91/100
Epoch: 92/100
Epoch: 93/100
Epoch: 94/100
Epoch: 95/100
Epoch: 96/100
Epoch: 97/100
Model improve: 0.8528 -> 0.8529
Epoch: 98/100
Epoch: 99/100
Epoch: 100/100
Date :03/27/2023, 04:27:40
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 224
fmax: 16000
trainbs: 128
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 60
model_name: tf_efficientnet_b1_ns
mix_up: 0.4
Fold: 1
21594
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Date :03/27/2023, 08:29:02
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 224
fmax: 16000
trainbs: 128
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 60
model_name: tf_efficientnet_b1_ns
mix_up: 0.4
Fold: 1
21594
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Date :03/27/2023, 08:35:05
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 224
fmax: 16000
trainbs: 128
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnet_v2_b1
mix_up: 0.4
Fold: 1
Date :03/27/2023, 08:36:17
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 224
fmax: 16000
trainbs: 128
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.4
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Date :03/27/2023, 08:42:40
Date :03/27/2023, 08:42:40
Duration: 5
Duration: 5
Sample rate: 32000
Sample rate: 32000
nfft: 2048
nfft: 2048
fmin: 20
fmin: 20
nmels: 224
nmels: 224
fmax: 16000
fmax: 16000
trainbs: 128
trainbs: 128
validbs: 256
validbs: 256
epochwarmup: 0
epochwarmup: 0
totalepoch: 100
totalepoch: 100
learningrate: 0.0004
learningrate: 0.0004
weightdecay: 0.0
weightdecay: 0.0
thrupsample: 50
thrupsample: 50
model_name: tf_efficientnetv2_b1
model_name: tf_efficientnetv2_b1
mix_up: 0.4
mix_up: 0.4
Fold: 1
Fold: 1
19629
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Epoch: 1/100
Model improve: 0.0000 -> 0.6450
Model improve: 0.0000 -> 0.6450
Epoch: 2/100
Epoch: 2/100
Model improve: 0.6450 -> 0.7302
Model improve: 0.6450 -> 0.7302
Epoch: 3/100
Epoch: 3/100
Model improve: 0.7302 -> 0.7713
Model improve: 0.7302 -> 0.7713
Epoch: 4/100
Epoch: 4/100
Model improve: 0.7713 -> 0.7951
Model improve: 0.7713 -> 0.7951
Epoch: 5/100
Epoch: 5/100
Model improve: 0.7951 -> 0.7970
Model improve: 0.7951 -> 0.7970
Epoch: 6/100
Epoch: 6/100
Model improve: 0.7970 -> 0.8061
Model improve: 0.7970 -> 0.8061
Epoch: 7/100
Epoch: 7/100
Fold: 1
Fold: 1
50
50
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Epoch: 1/100
Fold: 1
Fold: 1
50
50
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Epoch: 1/100
Fold: 1
Fold: 1
50
50
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Epoch: 1/100
Fold: 1
Fold: 1
50
50
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Epoch: 1/100
Model improve: 0.0000 -> 0.9994
Model improve: 0.0000 -> 0.9994
Epoch: 2/100
Epoch: 2/100
Fold: 1
Fold: 1
19629
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Epoch: 1/100
Date :03/27/2023, 10:49:57
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 224
fmax: 16000
trainbs: 128
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.4
Fold: 1
19629
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Date :03/27/2023, 10:54:32
Date :03/27/2023, 10:54:32
Duration: 5
Duration: 5
Sample rate: 32000
Sample rate: 32000
nfft: 2048
nfft: 2048
fmin: 20
fmin: 20
nmels: 224
nmels: 224
fmax: 16000
fmax: 16000
trainbs: 128
trainbs: 128
validbs: 256
validbs: 256
epochwarmup: 0
epochwarmup: 0
totalepoch: 100
totalepoch: 100
learningrate: 0.0004
learningrate: 0.0004
weightdecay: 0.0
weightdecay: 0.0
thrupsample: 50
thrupsample: 50
model_name: tf_efficientnetv2_b1
model_name: tf_efficientnetv2_b1
mix_up: 0.4
mix_up: 0.4
Fold: 1
Fold: 1
19629
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Epoch: 1/100
Date :03/27/2023, 10:56:06
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 224
fmax: 16000
trainbs: 128
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.4
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Model improve: 0.0000 -> 0.4761
Epoch: 2/100
Model improve: 0.4761 -> 0.4773
Epoch: 3/100
Model improve: 0.4773 -> 0.4789
Epoch: 4/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Model improve: 0.0000 -> 0.5244
Epoch: 2/100
Model improve: 0.5244 -> 0.6160
Epoch: 3/100
Model improve: 0.6160 -> 0.6563
Epoch: 4/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.004
    lr: 0.004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.004
    lr: 0.004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.004
    lr: 0.004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Model improve: 0.0000 -> 0.4708
Epoch: 2/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.004
    lr: 0.004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Date :03/27/2023, 13:34:10
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.4
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Model improve: 0.0000 -> 0.4729
Epoch: 2/100
Epoch: 3/100
Fold: 1
13828
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Fold: 1
13828
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Model improve: 0.0000 -> 0.9479
Epoch: 2/100
Fold: 1
13828
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Model improve: 0.0000 -> 0.7103
Epoch: 2/100
Epoch: 3/100
Epoch: 4/100
Epoch: 5/100
Fold: 1
7550
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Fold: 1
7550
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Model improve: 0.0000 -> 0.9479
Fold: 1
7550
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Model improve: 0.0000 -> 0.8098
Epoch: 2/100
Fold: 1
7550
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Model improve: 0.0000 -> 0.8183
Epoch: 2/100
Epoch: 3/100
Date :03/27/2023, 14:16:08
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.4
Fold: 1
7550
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Model improve: 0.0000 -> 0.8183
Epoch: 2/100
Epoch: 3/100
Fold: 1
7550
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Model improve: 0.0000 -> 0.8098
Epoch: 2/100
Epoch: 3/100
Fold: 1
7550
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Model improve: 0.0000 -> 0.8098
Epoch: 2/100
Epoch: 3/100
Epoch: 4/100
Fold: 1
7550
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Model improve: 0.0000 -> 0.7137
Epoch: 2/100
Epoch: 3/100
Epoch: 4/100
Date :03/27/2023, 14:32:46
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.4
Fold: 1
7550
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Model improve: 0.0000 -> 0.7137
Epoch: 2/100
Fold: 1
7550
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Fold: 1
7550
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Model improve: 0.0000 -> 0.7194
Epoch: 2/100
Epoch: 3/100
Fold: 1
7550
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Model improve: 0.0000 -> 0.7557
Epoch: 2/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.004
    lr: 0.004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Model improve: 0.0000 -> 0.8224
Epoch: 2/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.004
    lr: 0.004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Date :03/27/2023, 15:12:28
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.004
weightdecay: 0.0
thrupsample: 50
model_name: eca_nfnet_l0
mix_up: 0.4
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.004
    lr: 0.004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Model improve: 0.0000 -> 0.5529
Epoch: 2/100
Model improve: 0.5529 -> 0.6499
Epoch: 3/100
Model improve: 0.6499 -> 0.6933
Epoch: 4/100
Model improve: 0.6933 -> 0.7214
Epoch: 5/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Date :03/27/2023, 16:00:01
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 128
epochwarmup: 5
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 50
model_name: eca_nfnet_l0
mix_up: 0.4
Date :03/27/2023, 16:00:16
Date :03/27/2023, 16:00:16
Duration: 5
Duration: 5
Sample rate: 32000
Sample rate: 32000
nfft: 2048
nfft: 2048
fmin: 20
fmin: 20
nmels: 128
nmels: 128
fmax: 16000
fmax: 16000
trainbs: 64
trainbs: 64
validbs: 128
validbs: 128
epochwarmup: 5
epochwarmup: 5
totalepoch: 100
totalepoch: 100
learningrate: 0.0004
learningrate: 0.0004
weightdecay: 0.0
weightdecay: 0.0
thrupsample: 50
thrupsample: 50
model_name: eca_nfnet_l0
model_name: eca_nfnet_l0
mix_up: 0.4
mix_up: 0.4
Fold: 1
Fold: 1
19629
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0
    maximize: False
    weight_decay: 0.0
)
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Epoch: 1/100
Fold: 1
Fold: 1
19629
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0
    maximize: False
    weight_decay: 0.0
)
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Epoch: 1/100
Date :03/27/2023, 16:01:16
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 128
epochwarmup: 5
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 50
model_name: eca_nfnet_l0
mix_up: 0.4
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Model improve: 0.0000 -> 0.4767
Epoch: 2/100
Epoch: 3/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Model improve: 0.0000 -> 0.4756
Epoch: 2/100
Epoch: 3/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Model improve: 0.0000 -> 0.4771
Date :03/27/2023, 16:32:27
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 50
model_name: eca_nfnet_l0
mix_up: 0.4
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Date :03/27/2023, 16:33:28
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 50
model_name: eca_nfnet_l0
mix_up: 0.4
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Model improve: 0.0000 -> 0.5578
Epoch: 2/100
Model improve: 0.5578 -> 0.6443
Epoch: 3/100
Model improve: 0.6443 -> 0.6857
Epoch: 4/100
Model improve: 0.6857 -> 0.7119
Epoch: 5/100
Model improve: 0.7119 -> 0.7253
Epoch: 6/100
Model improve: 0.7253 -> 0.7487
Epoch: 7/100
Model improve: 0.7487 -> 0.7494
Epoch: 8/100
Model improve: 0.7494 -> 0.7585
Epoch: 9/100
Model improve: 0.7585 -> 0.7670
Epoch: 10/100
Model improve: 0.7670 -> 0.7779
Epoch: 11/100
Epoch: 12/100
Model improve: 0.7779 -> 0.7807
Epoch: 13/100
Epoch: 14/100
Model improve: 0.7807 -> 0.7879
Epoch: 15/100
Epoch: 16/100
Model improve: 0.7879 -> 0.7904
Epoch: 17/100
Model improve: 0.7904 -> 0.7940
Epoch: 18/100
Model improve: 0.7940 -> 0.7961
Epoch: 19/100
Model improve: 0.7961 -> 0.7970
Epoch: 20/100
Epoch: 21/100
Model improve: 0.7970 -> 0.8006
Epoch: 22/100
Epoch: 23/100
Model improve: 0.8006 -> 0.8012
Epoch: 24/100
Model improve: 0.8012 -> 0.8014
Epoch: 25/100
Model improve: 0.8014 -> 0.8050
Epoch: 26/100
Model improve: 0.8050 -> 0.8068
Epoch: 27/100
Model improve: 0.8068 -> 0.8090
Epoch: 28/100
Epoch: 29/100
Epoch: 30/100
Epoch: 31/100
Epoch: 32/100
Model improve: 0.8090 -> 0.8150
Epoch: 33/100
Epoch: 34/100
Epoch: 35/100
Epoch: 36/100
Model improve: 0.8150 -> 0.8156
Epoch: 37/100
Model improve: 0.8156 -> 0.8169
Epoch: 38/100
Epoch: 39/100
Epoch: 40/100
Epoch: 41/100
Model improve: 0.8169 -> 0.8188
Epoch: 42/100
Epoch: 43/100
Epoch: 44/100
Epoch: 45/100
Epoch: 46/100
Epoch: 47/100
Epoch: 48/100
Model improve: 0.8188 -> 0.8215
Epoch: 49/100
Epoch: 50/100
Model improve: 0.8215 -> 0.8223
Epoch: 51/100
Epoch: 52/100
Epoch: 53/100
Epoch: 54/100
Epoch: 55/100
Model improve: 0.8223 -> 0.8232
Epoch: 56/100
Epoch: 57/100
Model improve: 0.8232 -> 0.8279
Epoch: 58/100
Epoch: 59/100
Epoch: 60/100
Epoch: 61/100
Epoch: 62/100
Epoch: 63/100
Epoch: 64/100
Epoch: 65/100
Epoch: 66/100
Date :03/28/2023, 01:26:29
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 50
model_name: eca_nfnet_l0
mix_up: 0.4
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Model improve: 0.0000 -> 0.4758
Epoch: 2/100
Date :03/28/2023, 02:10:11
Date :03/28/2023, 02:10:11
Duration: 5
Duration: 5
Sample rate: 32000
Sample rate: 32000
nfft: 2048
nfft: 2048
fmin: 20
fmin: 20
nmels: 128
nmels: 128
fmax: 16000
fmax: 16000
trainbs: 64
trainbs: 64
validbs: 128
validbs: 128
epochwarmup: 0
epochwarmup: 0
totalepoch: 100
totalepoch: 100
learningrate: 0.0004
learningrate: 0.0004
weightdecay: 0.0
weightdecay: 0.0
thrupsample: 50
thrupsample: 50
model_name: eca_nfnet_l0
model_name: eca_nfnet_l0
mix_up: 0.4
mix_up: 0.4
Fold: 1
Fold: 1
19629
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Epoch: 1/100
Fold: 1
Fold: 1
19629
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Epoch: 1/100
Fold: 1
Fold: 1
19629
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Epoch: 1/100
Model improve: 0.0000 -> 0.4761
Model improve: 0.0000 -> 0.4761
Epoch: 2/100
Epoch: 2/100
Epoch: 3/100
Epoch: 3/100
Fold: 1
Fold: 1
19629
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Epoch: 1/100
Fold: 1
Fold: 1
19629
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Epoch: 1/100
Model improve: 0.0000 -> 0.4763
Model improve: 0.0000 -> 0.4763
Epoch: 2/100
Epoch: 2/100
Fold: 1
Fold: 1
19629
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Epoch: 1/100
Model improve: 0.0000 -> 0.5238
Model improve: 0.0000 -> 0.5238
Epoch: 2/100
Epoch: 2/100
Date :03/28/2023, 03:22:41
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 50
model_name: eca_nfnet_l0
mix_up: 0.4
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Model improve: 0.0000 -> 0.4763
Epoch: 2/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Model improve: 0.0000 -> 0.4757
Epoch: 2/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Model improve: 0.0000 -> 0.5579
Epoch: 2/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Date :03/28/2023, 04:23:09
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 50
model_name: eca_nfnet_l0
mix_up: 0.4
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Model improve: 0.0000 -> 0.5688
Epoch: 2/100
Model improve: 0.5688 -> 0.6619
Epoch: 3/100
Model improve: 0.6619 -> 0.7027
Epoch: 4/100
Model improve: 0.7027 -> 0.7201
Epoch: 5/100
Model improve: 0.7201 -> 0.7419
Epoch: 6/100
Model improve: 0.7419 -> 0.7484
Epoch: 7/100
Date :03/28/2023, 05:22:35
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 50
model_name: eca_nfnet_l1
mix_up: 0.4
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Model improve: 0.0000 -> 0.5142
Epoch: 2/100
Model improve: 0.5142 -> 0.5987
Epoch: 3/100
Model improve: 0.5987 -> 0.6422
Epoch: 4/100
Model improve: 0.6422 -> 0.6691
Epoch: 5/100
Model improve: 0.6691 -> 0.6893
Epoch: 6/100
Model improve: 0.6893 -> 0.7028
Epoch: 7/100
Model improve: 0.7028 -> 0.7228
Epoch: 8/100
Model improve: 0.7228 -> 0.7338
Epoch: 9/100
Epoch: 10/100
Model improve: 0.7338 -> 0.7365
Epoch: 11/100
Model improve: 0.7365 -> 0.7403
Epoch: 12/100
Model improve: 0.7403 -> 0.7530
Epoch: 13/100
Date :03/28/2023, 06:38:26
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 224
fmax: 16000
trainbs: 128
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 50
model_name: eca_nfnet_l0
mix_up: 0.4
Date :03/28/2023, 07:12:30
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 224
fmax: 16000
trainbs: 128
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 50
model_name: eca_nfnet_l0
mix_up: 0.4
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Model improve: 0.0000 -> 0.5070
Epoch: 2/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Model improve: 0.0000 -> 0.5023
Date :03/28/2023, 07:26:59
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 224
fmax: 16000
trainbs: 128
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 50
model_name: eca_nfnet_l0
mix_up: 0.4
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Model improve: 0.0000 -> 0.4810
Date :03/28/2023, 07:33:40
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 64
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 50
model_name: eca_nfnet_l0
mix_up: 0.4
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Model improve: 0.0000 -> 0.4869
Epoch: 2/100
Model improve: 0.4869 -> 0.5453
Epoch: 3/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Model improve: 0.0000 -> 0.4829
Epoch: 2/100
Model improve: 0.4829 -> 0.5214
Epoch: 3/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Model improve: 0.0000 -> 0.5702
Epoch: 2/100
Model improve: 0.5702 -> 0.6564
Epoch: 3/100
Model improve: 0.6564 -> 0.7007
Epoch: 4/100
Model improve: 0.7007 -> 0.7216
Epoch: 5/100
Model improve: 0.7216 -> 0.7386
Epoch: 6/100
Model improve: 0.7386 -> 0.7516
Epoch: 7/100
Model improve: 0.7516 -> 0.7535
Epoch: 8/100
Model improve: 0.7535 -> 0.7645
Epoch: 9/100
Model improve: 0.7645 -> 0.7657
Epoch: 10/100
Model improve: 0.7657 -> 0.7699
Epoch: 11/100
Model improve: 0.7699 -> 0.7708
Epoch: 12/100
Model improve: 0.7708 -> 0.7822
Epoch: 13/100
Epoch: 14/100
Epoch: 15/100
Date :03/28/2023, 10:36:14
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 64
fmax: 16000
trainbs: 128
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 50
model_name: eca_nfnet_l0
mix_up: 0.4
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Model improve: 0.0000 -> 0.5429
Epoch: 2/100
Model improve: 0.5429 -> 0.6152
Epoch: 3/100
Date :03/28/2023, 12:10:55
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 50
model_name: eca_nfnet_l0
mix_up: 0.4
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Date :03/28/2023, 08:40:41
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 50
model_name: eca_nfnet_l0
mix_up: 0.4
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Date :03/28/2023, 12:45:12
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 50
model_name: eca_nfnet_l0
mix_up: 0.4
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Date :03/28/2023, 13:07:30
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 50
model_name: eca_nfnet_l0
mix_up: 0.2
hop_length: 512
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Model improve: 0.0000 -> 0.5634
Epoch: 2/100
Model improve: 0.5634 -> 0.6441
Epoch: 3/100
Model improve: 0.6441 -> 0.6916
Epoch: 4/100
Model improve: 0.6916 -> 0.7209
Epoch: 5/100
Model improve: 0.7209 -> 0.7318
Epoch: 6/100
Model improve: 0.7318 -> 0.7396
Epoch: 7/100
Model improve: 0.7396 -> 0.7445
Epoch: 8/100
Model improve: 0.7445 -> 0.7495
Epoch: 9/100
Model improve: 0.7495 -> 0.7653
Epoch: 10/100
Epoch: 11/100
Model improve: 0.7653 -> 0.7735
Epoch: 12/100
Model improve: 0.7735 -> 0.7801
Epoch: 13/100
Epoch: 14/100
Epoch: 15/100
Epoch: 16/100
Epoch: 17/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Date :03/28/2023, 11:39:34
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 50
model_name: eca_nfnet_l0
mix_up: 0.6
hop_length: 512
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Model improve: 0.0000 -> 0.4785
Epoch: 2/100
Model improve: 0.4785 -> 0.4825
Epoch: 3/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Model improve: 0.0000 -> 0.5360
Epoch: 2/100
Model improve: 0.5360 -> 0.6397
Epoch: 3/100
Model improve: 0.6397 -> 0.6913
Epoch: 4/100
Model improve: 0.6913 -> 0.7143
Epoch: 5/100
Model improve: 0.7143 -> 0.7423
Epoch: 6/100
Model improve: 0.7423 -> 0.7491
Epoch: 7/100
Model improve: 0.7491 -> 0.7583
Epoch: 8/100
Model improve: 0.7583 -> 0.7643
Epoch: 9/100
Model improve: 0.7643 -> 0.7733
Epoch: 10/100
Epoch: 11/100
Model improve: 0.7733 -> 0.7797
Epoch: 12/100
Model improve: 0.7797 -> 0.7840
Epoch: 13/100
Epoch: 14/100
Epoch: 15/100
Model improve: 0.7840 -> 0.7841
Epoch: 16/100
Model improve: 0.7841 -> 0.7841
Epoch: 17/100
Epoch: 18/100
Model improve: 0.7841 -> 0.7914
Epoch: 19/100
Epoch: 20/100
Epoch: 21/100
Epoch: 22/100
Model improve: 0.7914 -> 0.7978
Epoch: 23/100
Epoch: 24/100
Epoch: 25/100
Epoch: 26/100
Epoch: 27/100
Epoch: 28/100
Epoch: 29/100
Epoch: 30/100
Epoch: 31/100
Epoch: 32/100
Model improve: 0.7978 -> 0.7981
Epoch: 33/100
Epoch: 34/100
Epoch: 35/100
Date :03/28/2023, 19:33:02
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 50
model_name: eca_nfnet_l0
mix_up: 0.6
hop_length: 320
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Model improve: 0.0000 -> 0.5812
Epoch: 2/100
Model improve: 0.5812 -> 0.6460
Epoch: 3/100
Model improve: 0.6460 -> 0.6828
Epoch: 4/100
Model improve: 0.6828 -> 0.7027
Epoch: 5/100
Model improve: 0.7027 -> 0.7215
Epoch: 6/100
Epoch: 7/100
Date :03/28/2023, 20:54:07
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 60
model_name: eca_nfnet_l0
mix_up: 0.6
hop_length: 512
Fold: 1
21594
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Model improve: 0.0000 -> 0.6014
Epoch: 2/100
Model improve: 0.6014 -> 0.6622
Epoch: 3/100
Model improve: 0.6622 -> 0.6750
Epoch: 4/100
Model improve: 0.6750 -> 0.6807
Date :03/28/2023, 21:48:47
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 50
model_name: eca_nfnet_l1
mix_up: 0.6
hop_length: 512
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Model improve: 0.0000 -> 0.6016
Epoch: 2/100
Model improve: 0.6016 -> 0.6732
Epoch: 3/100
Model improve: 0.6732 -> 0.7042
Epoch: 4/100
Model improve: 0.7042 -> 0.7152
Epoch: 5/100
Model improve: 0.7152 -> 0.7196
Epoch: 6/100
Model improve: 0.7196 -> 0.7314
Epoch: 7/100
Model improve: 0.7314 -> 0.7337
Epoch: 8/100
Date :03/28/2023, 23:28:55
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 50
model_name: eca_nfnet_l0
mix_up: 0.6
hop_length: 320
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Model improve: 0.0000 -> 0.5751
Epoch: 2/100
Model improve: 0.5751 -> 0.6360
Epoch: 3/100
Model improve: 0.6360 -> 0.6766
Epoch: 4/100
Model improve: 0.6766 -> 0.6998
Epoch: 5/100
Date :03/29/2023, 00:23:23
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 50
model_name: eca_nfnet_l0
mix_up: 0.6
hop_length: 768
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Model improve: 0.0000 -> 0.5972
Epoch: 2/100
Model improve: 0.5972 -> 0.6498
Epoch: 3/100
Model improve: 0.6498 -> 0.6758
Epoch: 4/100
Model improve: 0.6758 -> 0.6983
Epoch: 5/100
Model improve: 0.6983 -> 0.7034
Epoch: 6/100
Model improve: 0.7034 -> 0.7105
Epoch: 7/100
Date :03/29/2023, 01:36:40
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.00035
weightdecay: 0.0
thrupsample: 50
model_name: eca_nfnet_l0
mix_up: 0.6
hop_length: 512
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.00035
    lr: 0.00035
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Model improve: 0.0000 -> 0.5930
Epoch: 2/100
Model improve: 0.5930 -> 0.6492
Epoch: 3/100
Model improve: 0.6492 -> 0.6835
Epoch: 4/100
Model improve: 0.6835 -> 0.6995
Epoch: 5/100
Model improve: 0.6995 -> 0.7101
Epoch: 6/100
Model improve: 0.7101 -> 0.7145
Epoch: 7/100
Model improve: 0.7145 -> 0.7181
Epoch: 8/100

                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Model improve: 0.8190 -> 0.8199
Epoch: 40/100
                                                                                          Model improve: 0.8199 -> 0.8214
Epoch: 41/100
                                             Model improve: 0.Model improve: 0.7032 -> 0.7261
Epoch: 5/100
Model improve: 0.7261 -> 0.7378
Epoch: 6/100
Date :03/30/2023, 00:39:23
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.6
hop_length: 256
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Model improve: 0.0000 -> 0.5648
Epoch: 2/100
Model improve: 0.5648 -> 0.6626
Epoch: 3/100
Model improve: 0.6626 -> 0.7101
Epoch: 4/100
Model improve: 0.7101 -> 0.7328
Epoch: 5/100
Model improve: 0.7328 -> 0.7435
Epoch: 6/100
Model improve: 0.7435 -> 0.7558
Epoch: 7/100
Model improve: 0.7558 -> 0.7663
Epoch: 8/100
Model improve: 0.7663 -> 0.7715
Epoch: 9/100
Model improve: 0.7715 -> 0.7751
Epoch: 10/100
Model improve: 0.7751 -> 0.7783
Epoch: 11/100
Date :03/31/2023, 01:34:50
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 50
model_name: eca_nfnet_l0
mix_up: 0.6
hop_length: 256
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Date :04/02/2023, 10:29:38
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.1
hop_length: 256
train_with_mixup: True
num_channels: 3
use_spec_augmenter: True
Date :04/02/2023, 10:30:00
Date :04/02/2023, 10:30:00
Duration: 5
Duration: 5
Sample rate: 32000
Sample rate: 32000
nfft: 2048
nfft: 2048
fmin: 20
fmin: 20
nmels: 128
nmels: 128
fmax: 16000
fmax: 16000
trainbs: 64
trainbs: 64
validbs: 256
validbs: 256
epochwarmup: 0
epochwarmup: 0
totalepoch: 100
totalepoch: 100
learningrate: 0.001
learningrate: 0.001
weightdecay: 0.01
weightdecay: 0.01
thrupsample: 50
thrupsample: 50
model_name: tf_efficientnetv2_b1
model_name: tf_efficientnetv2_b1
mix_up: 0.1
mix_up: 0.1
hop_length: 256
hop_length: 256
train_with_mixup: True
train_with_mixup: True
num_channels: 3
num_channels: 3
use_spec_augmenter: True
use_spec_augmenter: True
Date :04/02/2023, 10:30:25
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.1
hop_length: 256
train_with_mixup: True
num_channels: 3
use_spec_augmenter: True
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :04/02/2023, 11:02:02
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.1
hop_length: 256
train_with_mixup: True
num_channels: 3
use_spec_augmenter: True
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :04/02/2023, 11:04:21
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.1
hop_length: 256
train_with_mixup: True
num_channels: 3
use_spec_augmenter: True
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :04/02/2023, 11:45:04
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.1
hop_length: 256
train_with_mixup: True
num_channels: 3
use_spec_augmenter: True
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Model improve: 0.0000 -> 0.5581
Epoch: 2/100
Model improve: 0.5581 -> 0.6531
Epoch: 3/100
Model improve: 0.6531 -> 0.6968
Epoch: 4/100
Model improve: 0.6968 -> 0.7170
Epoch: 5/100
Model improve: 0.7170 -> 0.7441
Epoch: 6/100
Model improve: 0.7441 -> 0.7621
Epoch: 7/100
Model improve: 0.7621 -> 0.7698
Epoch: 8/100
Model improve: 0.7698 -> 0.7759
Epoch: 9/100
Model improve: 0.7759 -> 0.7838
Epoch: 10/100
Model improve: 0.7838 -> 0.7861
Epoch: 11/100
Model improve: 0.7861 -> 0.7927
Epoch: 12/100
Model improve: 0.7927 -> 0.7964
Epoch: 13/100
Model improve: 0.7964 -> 0.8072
Epoch: 14/100
Epoch: 15/100
Model improve: 0.8072 -> 0.8094
Epoch: 16/100
Model improve: 0.8094 -> 0.8149
Epoch: 17/100
Epoch: 18/100
Model improve: 0.8149 -> 0.8184
Epoch: 19/100
Model improve: 0.8184 -> 0.8195
Epoch: 20/100
Model improve: 0.8195 -> 0.8233
Epoch: 21/100
Epoch: 22/100
Epoch: 23/100
Model improve: 0.8233 -> 0.8247
Epoch: 24/100
Date :04/02/2023, 19:21:55
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Model improve: 0.0000 -> 0.5183
Epoch: 2/100
Model improve: 0.5183 -> 0.6148
Epoch: 3/100
Model improve: 0.6148 -> 0.6707
Epoch: 4/100
Model improve: 0.6707 -> 0.7117
Epoch: 5/100
Model improve: 0.7117 -> 0.7354
Epoch: 6/100
Model improve: 0.7354 -> 0.7540
Epoch: 7/100
Model improve: 0.7540 -> 0.7695
Epoch: 8/100
Model improve: 0.7695 -> 0.7803
Epoch: 9/100
Model improve: 0.7803 -> 0.7915
Epoch: 10/100
Model improve: 0.7915 -> 0.7958
Epoch: 11/100
Model improve: 0.7958 -> 0.8078
Epoch: 12/100
Model improve: 0.8078 -> 0.8129
Epoch: 13/100
Model improve: 0.8129 -> 0.8142
Epoch: 14/100
Model improve: 0.8142 -> 0.8195
Epoch: 15/100
Model improve: 0.8195 -> 0.8226
Epoch: 16/100
Model improve: 0.8226 -> 0.8241
Epoch: 17/100
Model improve: 0.8241 -> 0.8298
Epoch: 18/100
Model improve: 0.8298 -> 0.8320
Epoch: 19/100
Model improve: 0.8320 -> 0.8372
Epoch: 20/100
Model improve: 0.8372 -> 0.8373
Epoch: 21/100
Model improve: 0.8373 -> 0.8396
Epoch: 22/100
Epoch: 23/100
Model improve: 0.8396 -> 0.8402
Epoch: 24/100
Model improve: 0.8402 -> 0.8423
Epoch: 25/100
Model improve: 0.8423 -> 0.8443
Epoch: 26/100
Model improve: 0.8443 -> 0.8480
Epoch: 27/100
Epoch: 28/100
Model improve: 0.8480 -> 0.8501
Epoch: 29/100
Epoch: 30/100
Epoch: 31/100
Model improve: 0.8501 -> 0.8529
Epoch: 32/100
Model improve: 0.8529 -> 0.8533
Epoch: 33/100
Model improve: 0.8533 -> 0.8537
Epoch: 34/100
Model improve: 0.8537 -> 0.8562
Epoch: 35/100
Epoch: 36/100
Epoch: 37/100
Model improve: 0.8562 -> 0.8590
Epoch: 38/100
Epoch: 39/100
Epoch: 40/100
Epoch: 41/100
Model improve: 0.8590 -> 0.8622
Epoch: 42/100
Model improve: 0.8622 -> 0.8628
Epoch: 43/100
Epoch: 44/100
Epoch: 45/100
Model improve: 0.8628 -> 0.8632
Epoch: 46/100
Epoch: 47/100
Model improve: 0.8632 -> 0.8676
Epoch: 48/100
Epoch: 49/100
Epoch: 50/100
Epoch: 51/100
Model improve: 0.8676 -> 0.8682
Epoch: 52/100
Epoch: 53/100
Epoch: 54/100
Model improve: 0.8682 -> 0.8686
Epoch: 55/100
Model improve: 0.8686 -> 0.8699
Epoch: 56/100
Epoch: 57/100
Model improve: 0.8699 -> 0.8716
Epoch: 58/100
Epoch: 59/100
Epoch: 60/100
Epoch: 61/100
Epoch: 62/100
Epoch: 63/100
Epoch: 64/100
Epoch: 65/100
Epoch: 66/100
Model improve: 0.8716 -> 0.8718
Epoch: 67/100
Model improve: 0.8718 -> 0.8727
Epoch: 68/100
Model improve: 0.8727 -> 0.8750
Epoch: 69/100
Epoch: 70/100
Epoch: 71/100
Epoch: 72/100
Epoch: 73/100
Epoch: 74/100
Epoch: 75/100
Epoch: 76/100
Epoch: 77/100
Epoch: 78/100
Epoch: 79/100
Model improve: 0.8750 -> 0.8753
Epoch: 80/100
Epoch: 81/100
Epoch: 82/100
Epoch: 83/100
Epoch: 84/100
Epoch: 85/100
Epoch: 86/100
Model improve: 0.8753 -> 0.8755
Epoch: 87/100
Epoch: 88/100
Epoch: 89/100
Epoch: 90/100
Date :04/04/2023, 01:11:30
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :04/04/2023, 01:24:52
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.3
hop_length: 256
train_with_mixup: True
num_channels: 3
use_spec_augmenter: False
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Model improve: 0.0000 -> 0.5789
Epoch: 2/100
Date :04/04/2023, 03:11:46
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.3
hop_length: 256
train_with_mixup: True
num_channels: 3
use_spec_augmenter: False
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :04/04/2023, 04:10:38
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.3
hop_length: 256
train_with_mixup: True
num_channels: 3
use_spec_augmenter: False
Date :04/04/2023, 04:10:46
Date :04/04/2023, 04:10:46
Duration: 5
Duration: 5
Sample rate: 32000
Sample rate: 32000
nfft: 2048
nfft: 2048
fmin: 20
fmin: 20
nmels: 128
nmels: 128
fmax: 16000
fmax: 16000
trainbs: 64
trainbs: 64
validbs: 256
validbs: 256
epochwarmup: 0
epochwarmup: 0
totalepoch: 100
totalepoch: 100
learningrate: 0.001
learningrate: 0.001
weightdecay: 0.01
weightdecay: 0.01
thrupsample: 50
thrupsample: 50
model_name: tf_efficientnetv2_b1
model_name: tf_efficientnetv2_b1
mix_up: 0.3
mix_up: 0.3
hop_length: 256
hop_length: 256
train_with_mixup: True
train_with_mixup: True
num_channels: 3
num_channels: 3
use_spec_augmenter: False
use_spec_augmenter: False
Fold: 1
Fold: 1
19629
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Epoch: 1/100
Model improve: 0.0000 -> 0.4757
Model improve: 0.0000 -> 0.4757
Epoch: 2/100
Epoch: 2/100
Fold: 1
Fold: 1
19629
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Epoch: 1/100
Fold: 1
Fold: 1
19629
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Epoch: 1/100
Date :04/04/2023, 04:13:27
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.3
hop_length: 256
train_with_mixup: True
num_channels: 3
use_spec_augmenter: False
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Model improve: 0.0000 -> 0.5590
Epoch: 2/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Fold: 1
19629
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Model improve: 0.0000 -> 0.5678
Epoch: 2/100
Model improve: 0.5678 -> 0.6406
Epoch: 3/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :04/04/2023, 06:54:39
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.3
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :04/04/2023, 07:02:17
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 0
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.3
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :04/04/2023, 07:08:48
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 0
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.3
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Model improve: 0.0000 -> 0.5599
Epoch: 2/100
Date :04/04/2023, 10:18:12
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 0
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.3
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Fold: 1
138
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Model improve: 0.0000 -> 0.9832
Epoch: 2/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Model improve: 0.0000 -> 0.5683
Epoch: 2/100
Model improve: 0.5683 -> 0.6516
Epoch: 3/100
Model improve: 0.6516 -> 0.7045
Epoch: 4/100
Date :04/04/2023, 11:10:47
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 0
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.3
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Model improve: 0.0000 -> 0.5606
Epoch: 2/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Model improve: 0.0000 -> 0.5481
Epoch: 2/100
Model improve: 0.5481 -> 0.6256
Epoch: 3/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Model improve: 0.0000 -> 0.5537
Epoch: 2/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Model improve: 0.0000 -> 0.5671
Epoch: 2/100
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 10.4940, val loss: 7.8359
Model improve: 0.0000 -> 0.5590
Epoch: 2/100
Train loss: 9.1707, val loss: 5.8774
Model improve: 0.5590 -> 0.6509
Epoch: 3/100
Train loss: 8.4673, val loss: 4.9840
Model improve: 0.6509 -> 0.6968
Epoch: 4/100
Train loss: 8.1385, val loss: 4.3677
Model improve: 0.6968 -> 0.7376
Epoch: 5/100
Train loss: 7.8106, val loss: 4.1010
Model improve: 0.7376 -> 0.7511
Epoch: 6/100
Train loss: 7.6269, val loss: 4.0758
Model improve: 0.7511 -> 0.7574
Epoch: 7/100
Train loss: 7.3224, val loss: 3.8592
Model improve: 0.7574 -> 0.7747
Epoch: 8/100
Train loss: 7.1884, val loss: 3.8279
Model improve: 0.7747 -> 0.7823
Epoch: 9/100
Train loss: 7.1617, val loss: 3.5643
Model improve: 0.7823 -> 0.7890
Epoch: 10/100
Date :04/04/2023, 12:30:57
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 0
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :04/04/2023, 12:40:02
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 0
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 10.7076, val loss: 8.8070
Model improve: 0.0000 -> 0.5208
Epoch: 2/100
Train loss: 9.5190, val loss: 6.2793
Model improve: 0.5208 -> 0.6054
Epoch: 3/100
Train loss: 8.4741, val loss: 5.1403
Model improve: 0.6054 -> 0.6735
Epoch: 4/100
Train loss: 7.9805, val loss: 4.3445
Model improve: 0.6735 -> 0.7194
Epoch: 5/100
Train loss: 7.6460, val loss: 4.0016
Model improve: 0.7194 -> 0.7428
Epoch: 6/100
Train loss: 7.4084, val loss: 3.8996
Model improve: 0.7428 -> 0.7547
Epoch: 7/100
Train loss: 7.0526, val loss: 3.6643
Model improve: 0.7547 -> 0.7716
Epoch: 8/100
Train loss: 6.9276, val loss: 3.3893
Model improve: 0.7716 -> 0.7839
Epoch: 9/100
Train loss: 7.0006, val loss: 3.4290
Model improve: 0.7839 -> 0.7965
Epoch: 10/100
Train loss: 6.7335, val loss: 3.1379
Model improve: 0.7965 -> 0.8022
Epoch: 11/100
Train loss: 6.6000, val loss: 3.1488
Model improve: 0.8022 -> 0.8102
Epoch: 12/100
Train loss: 6.5441, val loss: 2.9603
Model improve: 0.8102 -> 0.8156
Epoch: 13/100
Train loss: 6.3689, val loss: 2.9319
Model improve: 0.8156 -> 0.8223
Epoch: 14/100
Train loss: 6.3125, val loss: 2.8999
Model improve: 0.8223 -> 0.8284
Epoch: 15/100
Train loss: 6.3594, val loss: 2.7898
Model improve: 0.8284 -> 0.8287
Epoch: 16/100
Train loss: 6.2088, val loss: 2.8073
Model improve: 0.8287 -> 0.8308
Epoch: 17/100
Train loss: 6.0588, val loss: 2.6363
Model improve: 0.8308 -> 0.8367
Epoch: 18/100
Train loss: 6.1064, val loss: 2.7364
Model improve: 0.8367 -> 0.8411
Epoch: 19/100
Train loss: 6.1522, val loss: 2.7562
Model improve: 0.8411 -> 0.8476
Epoch: 20/100
Train loss: 6.1375, val loss: 2.6156
Epoch: 21/100
Train loss: 6.0759, val loss: 2.6707
Model improve: 0.8476 -> 0.8486
Epoch: 22/100
Train loss: 5.8887, val loss: 2.6629
Epoch: 23/100
Train loss: 5.7854, val loss: 2.6257
Model improve: 0.8486 -> 0.8553
Epoch: 24/100
Train loss: 5.9288, val loss: 2.5972
Epoch: 25/100
Train loss: 5.8366, val loss: 2.4641
Model improve: 0.8553 -> 0.8572
Epoch: 26/100
Train loss: 5.7542, val loss: 2.4549
Epoch: 27/100
Train loss: 5.7902, val loss: 2.6525
Model improve: 0.8572 -> 0.8578
Epoch: 28/100
Train loss: 5.7498, val loss: 2.4605
Model improve: 0.8578 -> 0.8590
Epoch: 29/100
Train loss: 5.6634, val loss: 2.5630
Model improve: 0.8590 -> 0.8594
Epoch: 30/100
Train loss: 5.6397, val loss: 2.4280
Model improve: 0.8594 -> 0.8640
Epoch: 31/100
Train loss: 5.6183, val loss: 2.3952
Epoch: 32/100
Train loss: 5.6295, val loss: 2.3434
Model improve: 0.8640 -> 0.8662
Epoch: 33/100
Train loss: 5.4276, val loss: 2.4633
Epoch: 34/100
Train loss: 5.4570, val loss: 2.3492
Model improve: 0.8662 -> 0.8681
Epoch: 35/100
Train loss: 5.6325, val loss: 2.4500
Model improve: 0.8681 -> 0.8714
Epoch: 36/100
Train loss: 5.4041, val loss: 2.3535
Epoch: 37/100
Train loss: 5.5507, val loss: 2.3310
Epoch: 38/100
Train loss: 5.4484, val loss: 2.2205
Epoch: 39/100
Train loss: 5.4214, val loss: 2.3051
Epoch: 40/100
Train loss: 5.3952, val loss: 2.3376
Epoch: 41/100
Train loss: 5.1718, val loss: 2.3511
Epoch: 42/100
Train loss: 5.4517, val loss: 2.2331
Epoch: 43/100
Train loss: 5.3696, val loss: 2.3384
Epoch: 44/100
Train loss: 5.3872, val loss: 2.1986
Epoch: 45/100
Train loss: 5.4114, val loss: 2.1808
Epoch: 46/100
Train loss: 5.1989, val loss: 2.2553
Model improve: 0.8714 -> 0.8731
Epoch: 47/100
Train loss: 5.2322, val loss: 2.3078
Model improve: 0.8731 -> 0.8764
Epoch: 48/100
Train loss: 5.4429, val loss: 2.3456
Model improve: 0.8764 -> 0.8766
Epoch: 49/100
Train loss: 5.2125, val loss: 2.1601
Model improve: 0.8766 -> 0.8771
Epoch: 50/100
Train loss: 5.2335, val loss: 2.1624
Epoch: 51/100
Train loss: 5.2255, val loss: 2.2150
Epoch: 52/100
Train loss: 5.3145, val loss: 2.2898
Epoch: 53/100
Train loss: 5.1406, val loss: 2.1643
Model improve: 0.8771 -> 0.8797
Epoch: 54/100
Train loss: 5.0375, val loss: 2.0984
Epoch: 55/100
Train loss: 5.0641, val loss: 2.0816
Epoch: 56/100
Train loss: 5.0993, val loss: 2.2224
Epoch: 57/100
Train loss: 5.1682, val loss: 2.1942
Model improve: 0.8797 -> 0.8812
Epoch: 58/100
Train loss: 5.2197, val loss: 2.0779
Epoch: 59/100
Train loss: 5.1112, val loss: 2.0959
Epoch: 60/100
Train loss: 5.1726, val loss: 2.0576
Model improve: 0.8812 -> 0.8820
Epoch: 61/100
Train loss: 4.9707, val loss: 2.0807
Model improve: 0.8820 -> 0.8830
Epoch: 62/100
Train loss: 5.1300, val loss: 2.0809
Epoch: 63/100
Train loss: 5.0020, val loss: 2.0491
Epoch: 64/100
Train loss: 5.0129, val loss: 2.0113
Epoch: 65/100
Train loss: 5.1310, val loss: 2.0505
Epoch: 66/100
Train loss: 5.0473, val loss: 2.0460
Model improve: 0.8830 -> 0.8836
Epoch: 67/100
Train loss: 5.0844, val loss: 2.0101
Epoch: 68/100
Train loss: 4.8715, val loss: 2.0344
Model improve: 0.8836 -> 0.8840
Epoch: 69/100
Train loss: 5.1263, val loss: 2.0009
Model improve: 0.8840 -> 0.8851
Epoch: 70/100
Train loss: 4.8920, val loss: 2.0049
Epoch: 71/100
Train loss: 4.8929, val loss: 2.1387
Model improve: 0.8851 -> 0.8877
Epoch: 72/100
Train loss: 4.9340, val loss: 2.0318
Epoch: 73/100
Train loss: 4.7740, val loss: 1.9640
Epoch: 74/100
Train loss: 4.8853, val loss: 2.0055
Epoch: 75/100
Train loss: 4.9682, val loss: 2.0680
Epoch: 76/100
Train loss: 4.8149, val loss: 1.9737
Epoch: 77/100
Train loss: 4.9390, val loss: 1.9961
Epoch: 78/100
Train loss: 4.7883, val loss: 2.1537
Epoch: 79/100
Train loss: 4.9447, val loss: 2.0094
Epoch: 80/100
Train loss: 4.7059, val loss: 1.9514
Epoch: 81/100
Train loss: 4.8976, val loss: 2.0384
Epoch: 82/100
Train loss: 5.0537, val loss: 1.9681
Epoch: 83/100
Train loss: 4.8162, val loss: 2.0726
Epoch: 84/100
Train loss: 4.8794, val loss: 1.9933
Epoch: 85/100
Train loss: 4.7906, val loss: 2.0581
Epoch: 86/100
Train loss: 4.7890, val loss: 2.0626
Model improve: 0.8877 -> 0.8878
Epoch: 87/100
Train loss: 4.7419, val loss: 1.9377
Epoch: 88/100
Train loss: 4.9743, val loss: 2.0016
Epoch: 89/100
Train loss: 4.8200, val loss: 1.9683
Model improve: 0.8878 -> 0.8880
Epoch: 90/100
Train loss: 4.9051, val loss: 2.0800
Model improve: 0.8880 -> 0.8891
Epoch: 91/100
Train loss: 4.7154, val loss: 1.9588
Epoch: 92/100
Train loss: 4.7329, val loss: 1.9791
Epoch: 93/100
Train loss: 4.9140, val loss: 1.9845
Epoch: 94/100
Train loss: 4.7831, val loss: 2.0154
Epoch: 95/100
Train loss: 4.9368, val loss: 2.0085
Epoch: 96/100
Train loss: 4.8300, val loss: 2.0209
Epoch: 97/100
Train loss: 4.7828, val loss: 2.0465
Epoch: 98/100
Train loss: 4.7812, val loss: 1.9528
Epoch: 99/100
Train loss: 5.0280, val loss: 1.9581
Epoch: 100/100
Train loss: 4.8071, val loss: 1.9480
Date :04/04/2023, 21:30:01
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 0
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnet_b1_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 10.7918, val loss: 9.2637
Model improve: 0.0000 -> 0.5095
Epoch: 2/100
Train loss: 9.7497, val loss: 7.0338
Model improve: 0.5095 -> 0.5903
Date :04/04/2023, 21:40:54
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 0
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 0.001
Date :04/04/2023, 21:41:11
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 0
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnet_b1_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 10.5877, val loss: 7.7794
Model improve: 0.0000 -> 0.5510
Epoch: 2/100
Train loss: 9.2841, val loss: 5.9736
Model improve: 0.5510 -> 0.6404
Epoch: 3/100
Train loss: 8.4707, val loss: 5.5400
Model improve: 0.6404 -> 0.6840
Epoch: 4/100
Train loss: 8.0439, val loss: 4.4477
Model improve: 0.6840 -> 0.7252
Epoch: 5/100
Train loss: 7.7476, val loss: 4.3860
Model improve: 0.7252 -> 0.7374
Epoch: 6/100
Train loss: 7.5581, val loss: 4.1143
Model improve: 0.7374 -> 0.7591
Epoch: 7/100
Train loss: 7.2483, val loss: 4.0359
Model improve: 0.7591 -> 0.7695
Epoch: 8/100
Train loss: 7.1563, val loss: 3.4991
Model improve: 0.7695 -> 0.7808
Epoch: 9/100
Train loss: 7.1848, val loss: 3.6582
Model improve: 0.7808 -> 0.7827
Epoch: 10/100
Train loss: 6.9276, val loss: 3.4875
Model improve: 0.7827 -> 0.7906
Epoch: 11/100
Train loss: 6.8401, val loss: 3.4133
Model improve: 0.7906 -> 0.8004
Epoch: 12/100
Train loss: 6.7934, val loss: 3.4870
Epoch: 13/100
Train loss: 6.6543, val loss: 3.2760
Model improve: 0.8004 -> 0.8024
Epoch: 14/100
Date :04/04/2023, 22:51:25
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 0
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b3
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
Date :04/04/2023, 22:51:38
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 0
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b3
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 10.5304, val loss: 7.6848
Model improve: 0.0000 -> 0.5529
Epoch: 2/100
Train loss: 9.1054, val loss: 5.4465
Model improve: 0.5529 -> 0.6480
Epoch: 3/100
Train loss: 8.2073, val loss: 4.5927
Model improve: 0.6480 -> 0.7012
Epoch: 4/100
Train loss: 7.7433, val loss: 4.1188
Model improve: 0.7012 -> 0.7381
Epoch: 5/100
Train loss: 7.4378, val loss: 3.8034
Model improve: 0.7381 -> 0.7577
Epoch: 6/100
Train loss: 7.1925, val loss: 3.6396
Model improve: 0.7577 -> 0.7769
Epoch: 7/100
Train loss: 6.8974, val loss: 3.3968
Model improve: 0.7769 -> 0.7902
Epoch: 8/100
Train loss: 6.7677, val loss: 3.2545
Model improve: 0.7902 -> 0.7968
Epoch: 9/100
Train loss: 6.7746, val loss: 3.1764
Model improve: 0.7968 -> 0.8100
Epoch: 10/100
Train loss: 6.5545, val loss: 3.0352
Model improve: 0.8100 -> 0.8167
Epoch: 11/100
Train loss: 6.4095, val loss: 3.1178
Model improve: 0.8167 -> 0.8206
Epoch: 12/100
Train loss: 6.3761, val loss: 2.9277
Model improve: 0.8206 -> 0.8287
Epoch: 13/100
Train loss: 6.2871, val loss: 2.8346
Model improve: 0.8287 -> 0.8312
Epoch: 14/100
Train loss: 6.1386, val loss: 2.8224
Model improve: 0.8312 -> 0.8344
Epoch: 15/100
Train loss: 6.1400, val loss: 2.7356
Model improve: 0.8344 -> 0.8373
Epoch: 16/100
Train loss: 6.0987, val loss: 2.6668
Model improve: 0.8373 -> 0.8407
Epoch: 17/100
Train loss: 5.9276, val loss: 2.6718
Epoch: 18/100
Train loss: 5.9760, val loss: 2.7006
Model improve: 0.8407 -> 0.8429
Epoch: 19/100
Train loss: 5.9274, val loss: 2.8191
Model improve: 0.8429 -> 0.8435
Epoch: 20/100
Train loss: 6.0092, val loss: 2.7090
Model improve: 0.8435 -> 0.8458
Epoch: 21/100
Train loss: 5.9279, val loss: 2.7679
Model improve: 0.8458 -> 0.8511
Epoch: 22/100
Train loss: 5.7517, val loss: 2.5576
Model improve: 0.8511 -> 0.8548
Epoch: 23/100
Train loss: 5.6592, val loss: 2.5769
Epoch: 24/100
Train loss: 5.7440, val loss: 2.5659
Model improve: 0.8548 -> 0.8552
Epoch: 25/100
Train loss: 5.6504, val loss: 2.4363
Model improve: 0.8552 -> 0.8559
Epoch: 26/100
Train loss: 5.6491, val loss: 2.4433
Model improve: 0.8559 -> 0.8577
Epoch: 27/100
Train loss: 5.6145, val loss: 2.6120
Model improve: 0.8577 -> 0.8641
Epoch: 28/100
Train loss: 5.5868, val loss: 2.4160
Epoch: 29/100
Train loss: 5.4583, val loss: 2.4510
Epoch: 30/100
Train loss: 5.4195, val loss: 2.4485
Epoch: 31/100
Train loss: 5.4661, val loss: 2.3475
Model improve: 0.8641 -> 0.8652
Epoch: 32/100
Train loss: 5.5221, val loss: 2.3549
Model improve: 0.8652 -> 0.8661
Epoch: 33/100
Train loss: 5.3033, val loss: 2.3651
Model improve: 0.8661 -> 0.8723
Epoch: 34/100
Train loss: 5.2545, val loss: 2.4105
Epoch: 35/100
Train loss: 5.4440, val loss: 2.5675
Epoch: 36/100
Train loss: 5.2327, val loss: 2.2153
Model improve: 0.8723 -> 0.8723
Epoch: 37/100
Train loss: 5.3652, val loss: 2.2359
Epoch: 38/100
Train loss: 5.2759, val loss: 2.2940
Epoch: 39/100
Train loss: 5.2686, val loss: 2.2447
Epoch: 40/100
Train loss: 5.2588, val loss: 2.2994
Model improve: 0.8723 -> 0.8741
Epoch: 41/100
Train loss: 5.0629, val loss: 2.3134
Model improve: 0.8741 -> 0.8748
Epoch: 42/100
Train loss: 5.3489, val loss: 2.2347
Model improve: 0.8748 -> 0.8761
Epoch: 43/100
Train loss: 5.2265, val loss: 2.3160
Epoch: 44/100
Train loss: 5.2301, val loss: 2.2212
Epoch: 45/100
Train loss: 5.2569, val loss: 2.2327
Epoch: 46/100
Train loss: 5.0100, val loss: 2.2821
Epoch: 47/100
Train loss: 5.1157, val loss: 2.2738
Epoch: 48/100
Train loss: 5.2484, val loss: 2.2997
Model improve: 0.8761 -> 0.8766
Epoch: 49/100
Train loss: 5.0924, val loss: 2.1650
Epoch: 50/100
Train loss: 5.0442, val loss: 2.1676
Epoch: 51/100
Train loss: 4.9992, val loss: 2.1801
Model improve: 0.8766 -> 0.8792
Epoch: 52/100
Train loss: 5.1830, val loss: 2.2374
Epoch: 53/100
Train loss: 4.9794, val loss: 2.2120
Model improve: 0.8792 -> 0.8798
Epoch: 54/100
Train loss: 4.8800, val loss: 2.1515
Epoch: 55/100
Train loss: 4.8932, val loss: 2.0833
Model improve: 0.8798 -> 0.8803
Epoch: 56/100
Train loss: 4.8942, val loss: 2.1022
Model improve: 0.8803 -> 0.8833
Epoch: 57/100
Train loss: 4.9667, val loss: 2.2308
Epoch: 58/100
Train loss: 5.0481, val loss: 2.0770
Epoch: 59/100
Train loss: 4.8707, val loss: 2.1327
Epoch: 60/100
Train loss: 4.9887, val loss: 2.0989
Epoch: 61/100
Train loss: 4.8243, val loss: 2.1002
Epoch: 62/100
Train loss: 4.8997, val loss: 2.0998
Epoch: 63/100
Train loss: 4.8370, val loss: 2.0072
Model improve: 0.8833 -> 0.8837
Epoch: 64/100
Train loss: 4.8574, val loss: 2.0457
Epoch: 65/100
Train loss: 4.9212, val loss: 2.0788
Model improve: 0.8837 -> 0.8841
Epoch: 66/100
Train loss: 4.8667, val loss: 2.0031
Model improve: 0.8841 -> 0.8867
Epoch: 67/100
Train loss: 4.8723, val loss: 1.9870
Epoch: 68/100
Train loss: 4.6734, val loss: 2.0313
Epoch: 69/100
Train loss: 4.9258, val loss: 2.0292
Epoch: 70/100
Train loss: 4.7366, val loss: 2.0091
Epoch: 71/100
Train loss: 4.6827, val loss: 2.1431
Epoch: 72/100
Train loss: 4.7891, val loss: 2.0161
Epoch: 73/100
Train loss: 4.6058, val loss: 2.0042
Epoch: 74/100
Train loss: 4.7192, val loss: 2.0422
Epoch: 75/100
Train loss: 4.8440, val loss: 2.0751
Epoch: 76/100
Train loss: 4.6518, val loss: 1.9750
Epoch: 77/100
Train loss: 4.7104, val loss: 2.0299
Epoch: 78/100
Train loss: 4.6320, val loss: 2.1030
Model improve: 0.8867 -> 0.8872
Epoch: 79/100
Train loss: 4.7716, val loss: 1.9643
Epoch: 80/100
Train loss: 4.5789, val loss: 1.9532
Epoch: 81/100
Train loss: 4.6996, val loss: 2.0205
Epoch: 82/100
Train loss: 4.8360, val loss: 1.9713
Epoch: 83/100
Train loss: 4.6524, val loss: 2.0277
Epoch: 84/100
Train loss: 4.6990, val loss: 1.9994
Epoch: 85/100
Train loss: 4.5957, val loss: 1.9846
Model improve: 0.8872 -> 0.8877
Epoch: 86/100
Train loss: 4.5781, val loss: 2.0077
Epoch: 87/100
Train loss: 4.5570, val loss: 1.9625
Epoch: 88/100
Train loss: 4.7627, val loss: 1.9835
Epoch: 89/100
Train loss: 4.6333, val loss: 1.9640
Epoch: 90/100
Train loss: 4.7189, val loss: 2.0815
Epoch: 91/100
Train loss: 4.5956, val loss: 1.9621
Epoch: 92/100
Train loss: 4.5497, val loss: 1.9856
Epoch: 93/100
Train loss: 4.7407, val loss: 1.9756
Epoch: 94/100
Date :04/05/2023, 07:11:09
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 0
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b3
mix_up: 0.5
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: True
use_drop_path: False
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 10.5718, val loss: 7.9985
Model improve: 0.0000 -> 0.5431
Epoch: 2/100
Train loss: 9.3032, val loss: 5.7418
Model improve: 0.5431 -> 0.6463
Epoch: 3/100
Train loss: 8.4573, val loss: 4.8296
Model improve: 0.6463 -> 0.7014
Epoch: 4/100
Train loss: 7.9605, val loss: 4.2179
Model improve: 0.7014 -> 0.7384
Epoch: 5/100
Train loss: 7.7047, val loss: 3.9708
Model improve: 0.7384 -> 0.7636
Epoch: 6/100
Train loss: 7.4601, val loss: 3.6288
Model improve: 0.7636 -> 0.7759
Epoch: 7/100
Train loss: 7.1959, val loss: 3.7074
Model improve: 0.7759 -> 0.7932
Epoch: 8/100
Train loss: 7.1323, val loss: 3.6112
Model improve: 0.7932 -> 0.8029
Epoch: 9/100
Train loss: 7.0123, val loss: 3.3722
Model improve: 0.8029 -> 0.8093
Epoch: 10/100
Train loss: 6.8794, val loss: 3.3298
Model improve: 0.8093 -> 0.8187
Epoch: 11/100
Train loss: 6.8690, val loss: 3.3145
Model improve: 0.8187 -> 0.8229
Epoch: 12/100
Train loss: 6.7556, val loss: 3.0860
Model improve: 0.8229 -> 0.8318
Epoch: 13/100
Train loss: 6.6144, val loss: 2.8464
Epoch: 14/100
Train loss: 6.5705, val loss: 3.0571
Model improve: 0.8318 -> 0.8366
Epoch: 15/100
Train loss: 6.5830, val loss: 3.1601
Model improve: 0.8366 -> 0.8385
Epoch: 16/100
Train loss: 6.4276, val loss: 2.9935
Model improve: 0.8385 -> 0.8456
Epoch: 17/100
Train loss: 6.4232, val loss: 2.9470
Epoch: 18/100
Train loss: 6.4400, val loss: 2.6425
Model improve: 0.8456 -> 0.8520
Epoch: 19/100
Train loss: 6.4759, val loss: 2.8663
Epoch: 20/100
Train loss: 6.2673, val loss: 2.7423
Epoch: 21/100
Train loss: 6.1707, val loss: 2.8363
Model improve: 0.8520 -> 0.8550
Epoch: 22/100
Train loss: 6.1983, val loss: 2.7932
Model improve: 0.8550 -> 0.8562
Epoch: 23/100
Train loss: 6.1769, val loss: 2.7731
Model improve: 0.8562 -> 0.8571
Epoch: 24/100
Train loss: 6.1365, val loss: 2.6816
Model improve: 0.8571 -> 0.8576
Epoch: 25/100
Train loss: 6.2447, val loss: 2.7121
Model improve: 0.8576 -> 0.8595
Epoch: 26/100
Train loss: 6.0183, val loss: 2.6759
Model improve: 0.8595 -> 0.8599
Epoch: 27/100
Train loss: 5.9135, val loss: 2.5845
Model improve: 0.8599 -> 0.8611
Epoch: 28/100
Train loss: 5.9834, val loss: 2.6377
Model improve: 0.8611 -> 0.8647
Epoch: 29/100
Train loss: 5.9559, val loss: 2.8023
Epoch: 30/100
Train loss: 5.7872, val loss: 2.6585
Epoch: 31/100
Train loss: 6.0649, val loss: 2.5679
Epoch: 32/100
Date :04/05/2023, 09:59:01
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 0
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b3
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: True
use_drop_path: False
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 10.3965, val loss: 7.7821
Model improve: 0.0000 -> 0.5579
Epoch: 2/100
Train loss: 8.9986, val loss: 5.8995
Model improve: 0.5579 -> 0.6431
Epoch: 3/100
Train loss: 8.2745, val loss: 5.2880
Model improve: 0.6431 -> 0.6865
Epoch: 4/100
Train loss: 7.8019, val loss: 5.0010
Model improve: 0.6865 -> 0.7124
Epoch: 5/100
Train loss: 7.5921, val loss: 4.5591
Model improve: 0.7124 -> 0.7320
Epoch: 6/100
Train loss: 7.3423, val loss: 4.3576
Model improve: 0.7320 -> 0.7417
Epoch: 7/100
Date :04/05/2023, 10:42:35
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b3
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: True
use_drop_path: False
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 10.4645, val loss: 7.7252
Model improve: 0.0000 -> 0.5494
Epoch: 2/100
Train loss: 8.9747, val loss: 5.8101
Model improve: 0.5494 -> 0.6343
Epoch: 3/100
Train loss: 8.1218, val loss: 5.0357
Model improve: 0.6343 -> 0.6887
Epoch: 4/100
Train loss: 7.6236, val loss: 4.4587
Model improve: 0.6887 -> 0.7263
Epoch: 5/100
Train loss: 7.3768, val loss: 4.2866
Model improve: 0.7263 -> 0.7379
Epoch: 6/100
Train loss: 7.1303, val loss: 4.0245
Model improve: 0.7379 -> 0.7515
Epoch: 7/100
Train loss: 6.7747, val loss: 3.9152
Model improve: 0.7515 -> 0.7649
Epoch: 8/100
Train loss: 6.6936, val loss: 3.6413
Model improve: 0.7649 -> 0.7774
Epoch: 9/100
Train loss: 6.7733, val loss: 3.8291
Model improve: 0.7774 -> 0.7863
Epoch: 10/100
Train loss: 6.5184, val loss: 3.6350
Epoch: 11/100
Train loss: 6.3681, val loss: 3.7034
Model improve: 0.7863 -> 0.7893
Epoch: 12/100
Train loss: 6.3164, val loss: 3.5173
Model improve: 0.7893 -> 0.7944
Epoch: 13/100
Train loss: 6.2204, val loss: 3.2956
Model improve: 0.7944 -> 0.8064
Epoch: 14/100
Train loss: 6.0684, val loss: 3.3713
Epoch: 15/100
Train loss: 6.1783, val loss: 3.3152
Epoch: 16/100
Train loss: 6.0077, val loss: 3.2179
Model improve: 0.8064 -> 0.8112
Epoch: 17/100
Train loss: 5.8557, val loss: 3.1177
Model improve: 0.8112 -> 0.8126
Epoch: 18/100
Train loss: 5.9051, val loss: 3.0676
Model improve: 0.8126 -> 0.8191
Epoch: 19/100
Train loss: 5.9265, val loss: 3.2287
Model improve: 0.8191 -> 0.8216
Epoch: 20/100
Train loss: 5.9624, val loss: 3.0707
Model improve: 0.8216 -> 0.8280
Epoch: 21/100
Train loss: 5.8818, val loss: 3.0804
Epoch: 22/100
Train loss: 5.7327, val loss: 3.0567
Epoch: 23/100
Train loss: 5.5620, val loss: 3.1671
Epoch: 24/100
Train loss: 5.7190, val loss: 3.0635
Model improve: 0.8280 -> 0.8289
Epoch: 25/100
Train loss: 5.6199, val loss: 2.9909
Model improve: 0.8289 -> 0.8323
Epoch: 26/100
Train loss: 5.5987, val loss: 2.9958
Epoch: 27/100
Train loss: 5.6221, val loss: 3.0662
Epoch: 28/100
Train loss: 5.5265, val loss: 2.9818
Epoch: 29/100
Train loss: 5.4758, val loss: 2.9167
Model improve: 0.8323 -> 0.8335
Epoch: 30/100
Train loss: 5.3748, val loss: 2.8706
Model improve: 0.8335 -> 0.8379
Epoch: 31/100
Train loss: 5.4327, val loss: 2.9452
Epoch: 32/100
Train loss: 5.3769, val loss: 3.0810
Epoch: 33/100
Train loss: 5.2331, val loss: 2.9654
Epoch: 34/100
Train loss: 5.1816, val loss: 3.0100
Epoch: 35/100
Train loss: 5.3677, val loss: 2.9653
Epoch: 36/100
Train loss: 5.1625, val loss: 2.9376
Epoch: 37/100
Train loss: 5.3346, val loss: 2.7302
Model improve: 0.8379 -> 0.8447
Epoch: 38/100
Train loss: 5.3230, val loss: 2.7946
Epoch: 39/100
Train loss: 5.2149, val loss: 2.8518
Epoch: 40/100
Train loss: 5.2447, val loss: 2.9151
Epoch: 41/100
Train loss: 4.9853, val loss: 2.8088
Epoch: 42/100
Train loss: 5.2826, val loss: 2.8323
Epoch: 43/100
Train loss: 5.1429, val loss: 2.8435
Epoch: 44/100
Train loss: 5.1949, val loss: 2.7902
Epoch: 45/100
Train loss: 5.2317, val loss: 2.9157
Epoch: 46/100
Train loss: 4.9378, val loss: 2.7917
Epoch: 47/100
Train loss: 5.0532, val loss: 2.8819
Epoch: 48/100
Train loss: 5.2845, val loss: 2.9508
Epoch: 49/100
Train loss: 5.0025, val loss: 2.7158
Model improve: 0.8447 -> 0.8480
Epoch: 50/100
Train loss: 5.0168, val loss: 2.7864
Epoch: 51/100
Train loss: 4.9678, val loss: 2.7615
Epoch: 52/100
Train loss: 5.0453, val loss: 2.7832
Epoch: 53/100
Train loss: 4.9055, val loss: 2.7567
Epoch: 54/100
Train loss: 4.7844, val loss: 2.7219
Epoch: 55/100
Train loss: 4.8866, val loss: 2.6581
Epoch: 56/100
Train loss: 4.8583, val loss: 2.7423
Epoch: 57/100
Train loss: 4.9558, val loss: 2.7507
Epoch: 58/100
Train loss: 4.9567, val loss: 2.7573
Epoch: 59/100
Train loss: 4.8240, val loss: 2.6347
Model improve: 0.8480 -> 0.8490
Epoch: 60/100
Train loss: 4.9447, val loss: 2.6540
Epoch: 61/100
Train loss: 4.7328, val loss: 2.7141
Epoch: 62/100
Train loss: 4.8208, val loss: 2.6156
Model improve: 0.8490 -> 0.8495
Epoch: 63/100
Train loss: 4.7405, val loss: 2.7629
Epoch: 64/100
Train loss: 4.8079, val loss: 2.6502
Epoch: 65/100
Train loss: 4.8762, val loss: 2.5904
Model improve: 0.8495 -> 0.8536
Epoch: 66/100
Train loss: 4.8545, val loss: 2.5515
Model improve: 0.8536 -> 0.8566
Epoch: 67/100
Train loss: 4.8173, val loss: 2.6271
Epoch: 68/100
Train loss: 4.6389, val loss: 2.5787
Epoch: 69/100
Train loss: 4.8705, val loss: 2.5560
Epoch: 70/100
Train loss: 4.6448, val loss: 2.5924
Epoch: 71/100
Train loss: 4.6229, val loss: 2.7402
Epoch: 72/100
Train loss: 4.6781, val loss: 2.6148
Epoch: 73/100
Train loss: 4.5059, val loss: 2.5980
Epoch: 74/100
Train loss: 4.6250, val loss: 2.6217
Epoch: 75/100
Train loss: 4.7185, val loss: 2.6491
Epoch: 76/100
Train loss: 4.5746, val loss: 2.5004
Epoch: 77/100
Train loss: 4.6343, val loss: 2.6363
Epoch: 78/100
Date :04/05/2023, 19:44:04
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Train loss: 10.7218, val loss: 8.7946
Model improve: 0.0000 -> 0.5200
Epoch: 2/100
Train loss: 9.5352, val loss: 6.4097
Model improve: 0.5200 -> 0.6005
Epoch: 3/100
Train loss: 8.5061, val loss: 5.4289
Model improve: 0.6005 -> 0.6599
Epoch: 4/100
Train loss: 8.0107, val loss: 4.7273
Model improve: 0.6599 -> 0.7025
Epoch: 5/100
Date :04/05/2023, 20:14:35
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Train loss: 10.6668, val loss: 8.5905
Model improve: 0.0000 -> 0.5235
Epoch: 2/100
Train loss: 9.4593, val loss: 6.4906
Model improve: 0.5235 -> 0.6057
Epoch: 3/100
Train loss: 8.4798, val loss: 5.5102
Model improve: 0.6057 -> 0.6544
Epoch: 4/100
Train loss: 8.0044, val loss: 4.7718
Model improve: 0.6544 -> 0.7003
Epoch: 5/100
Train loss: 7.6579, val loss: 4.3877
Model improve: 0.7003 -> 0.7211
Epoch: 6/100
Train loss: 7.4525, val loss: 4.2482
Model improve: 0.7211 -> 0.7401
Epoch: 7/100
Train loss: 7.0768, val loss: 4.1436
Model improve: 0.7401 -> 0.7465
Epoch: 8/100
Train loss: 6.9333, val loss: 3.7941
Model improve: 0.7465 -> 0.7559
Epoch: 9/100
Train loss: 7.0354, val loss: 4.0001
Model improve: 0.7559 -> 0.7639
Epoch: 10/100
Train loss: 6.7601, val loss: 3.6110
Model improve: 0.7639 -> 0.7782
Epoch: 11/100
Train loss: 6.6166, val loss: 3.7478
Model improve: 0.7782 -> 0.7806
Epoch: 12/100
Train loss: 6.5834, val loss: 3.6197
Model improve: 0.7806 -> 0.7880
Epoch: 13/100
Train loss: 6.4009, val loss: 3.5160
Model improve: 0.7880 -> 0.7896
Epoch: 14/100
Train loss: 6.3120, val loss: 3.4223
Model improve: 0.7896 -> 0.7967
Epoch: 15/100
Date :04/05/2023, 21:54:14
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnet_b1_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 3
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Train loss: 10.5588, val loss: 8.3058
Model improve: 0.0000 -> 0.5410
Epoch: 2/100
Train loss: 9.2958, val loss: 6.2974
Model improve: 0.5410 -> 0.6166
Epoch: 3/100
Train loss: 8.4979, val loss: 5.7695
Model improve: 0.6166 -> 0.6626
Epoch: 4/100
Train loss: 8.1080, val loss: 5.0216
Model improve: 0.6626 -> 0.6966
Epoch: 5/100
Train loss: 7.7987, val loss: 4.7004
Model improve: 0.6966 -> 0.7262
Epoch: 6/100
Train loss: 7.5987, val loss: 4.6319
Model improve: 0.7262 -> 0.7348
Epoch: 7/100
Date :04/05/2023, 22:39:12
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: eca_nfnet_l0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 3
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Date :04/05/2023, 22:44:21
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 3
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Train loss: 9.7728, val loss: 6.6129
Model improve: 0.0000 -> 0.6169
Epoch: 2/100
Train loss: 7.5640, val loss: 5.3695
Model improve: 0.6169 -> 0.6852
Epoch: 3/100
Train loss: 6.4575, val loss: 4.6572
Model improve: 0.6852 -> 0.7290
Epoch: 4/100
Train loss: 5.9809, val loss: 4.1442
Model improve: 0.7290 -> 0.7556
Epoch: 5/100
Train loss: 5.7263, val loss: 3.9507
Model improve: 0.7556 -> 0.7753
Epoch: 6/100
Train loss: 5.3808, val loss: 3.5615
Model improve: 0.7753 -> 0.7956
Epoch: 7/100
Train loss: 5.0220, val loss: 3.8867
Epoch: 8/100
Train loss: 4.9366, val loss: 3.4058
Model improve: 0.7956 -> 0.7970
Epoch: 9/100
Train loss: 5.0379, val loss: 3.7452
Epoch: 10/100
Train loss: 4.7091, val loss: 3.5215
Model improve: 0.7970 -> 0.7990
Epoch: 11/100
Train loss: 4.5837, val loss: 3.5553
Model improve: 0.7990 -> 0.8047
Epoch: 12/100
Train loss: 4.5243, val loss: 3.3458
Model improve: 0.8047 -> 0.8123
Epoch: 13/100
Train loss: 4.3975, val loss: 3.5528
Epoch: 14/100
Train loss: 4.2238, val loss: 3.3193
Model improve: 0.8123 -> 0.8172
Epoch: 15/100
Train loss: 4.3611, val loss: 3.0793
Model improve: 0.8172 -> 0.8282
Epoch: 16/100
Train loss: 4.1630, val loss: 3.1740
Epoch: 17/100
Train loss: 4.0083, val loss: 3.1459
Epoch: 18/100
Train loss: 4.1049, val loss: 3.1796
Epoch: 19/100
Train loss: 4.0830, val loss: 3.2708
Epoch: 20/100
Train loss: 4.1231, val loss: 3.2068
Epoch: 21/100
Train loss: 4.0523, val loss: 3.2057
Model improve: 0.8282 -> 0.8288
Epoch: 22/100
Train loss: 3.8590, val loss: 3.1853
Epoch: 23/100
Train loss: 3.6757, val loss: 3.1210
Epoch: 24/100
Train loss: 3.8866, val loss: 3.1061
Model improve: 0.8288 -> 0.8340
Epoch: 25/100
Train loss: 3.7831, val loss: 2.9481
Model improve: 0.8340 -> 0.8363
Epoch: 26/100
Train loss: 3.6953, val loss: 2.9609
Epoch: 27/100
Train loss: 3.7436, val loss: 3.2314
Epoch: 28/100
Train loss: 3.6665, val loss: 3.0353
Epoch: 29/100
Train loss: 3.5643, val loss: 2.9558
Epoch: 30/100
Train loss: 3.4690, val loss: 2.9179
Epoch: 31/100
Train loss: 3.4991, val loss: 2.8556
Model improve: 0.8363 -> 0.8393
Epoch: 32/100
Train loss: 3.4987, val loss: 2.9734
Epoch: 33/100
Train loss: 3.3053, val loss: 2.9219
Epoch: 34/100
Train loss: 3.3287, val loss: 2.9045
Epoch: 35/100
Train loss: 3.5078, val loss: 3.0361
Epoch: 36/100
Train loss: 3.2754, val loss: 2.8704
Epoch: 37/100
Train loss: 3.4523, val loss: 2.8980
Epoch: 38/100
Date :04/06/2023, 03:14:45
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.3
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Train loss: 10.3100, val loss: 7.6859
Model improve: 0.0000 -> 0.5567
Epoch: 2/100
Train loss: 8.1700, val loss: 5.4933
Model improve: 0.5567 -> 0.6535
Epoch: 3/100
Train loss: 6.9527, val loss: 4.5777
Model improve: 0.6535 -> 0.7057
Epoch: 4/100
Train loss: 6.4557, val loss: 4.1593
Model improve: 0.7057 -> 0.7426
Epoch: 5/100
Train loss: 5.9694, val loss: 3.9637
Model improve: 0.7426 -> 0.7551
Epoch: 6/100
Train loss: 5.7671, val loss: 3.7498
Model improve: 0.7551 -> 0.7784
Epoch: 7/100
Train loss: 5.2626, val loss: 3.8372
Model improve: 0.7784 -> 0.7871
Epoch: 8/100
Train loss: 5.0420, val loss: 3.4359
Model improve: 0.7871 -> 0.7938
Epoch: 9/100
Train loss: 4.9814, val loss: 3.4026
Model improve: 0.7938 -> 0.7954
Epoch: 10/100
Train loss: 4.9362, val loss: 3.3184
Model improve: 0.7954 -> 0.8106
Epoch: 11/100
Train loss: 4.6845, val loss: 3.4532
Epoch: 12/100
Train loss: 4.7360, val loss: 3.2948
Model improve: 0.8106 -> 0.8140
Epoch: 13/100
Train loss: 4.5447, val loss: 3.2421
Model improve: 0.8140 -> 0.8179
Epoch: 14/100
Train loss: 4.4150, val loss: 3.2357
Model improve: 0.8179 -> 0.8222
Epoch: 15/100
Train loss: 4.3394, val loss: 3.1461
Model improve: 0.8222 -> 0.8247
Epoch: 16/100
Train loss: 4.1599, val loss: 3.2260
Epoch: 17/100
Train loss: 4.0813, val loss: 3.1113
Model improve: 0.8247 -> 0.8248
Epoch: 18/100
Date :04/06/2023, 11:43:06
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.3
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: True
use_drop_path: False
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :04/06/2023, 11:44:56
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: True
use_drop_path: False
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Train loss: 10.1752, val loss: 7.2149
Model improve: 0.0000 -> 0.5710
Epoch: 2/100
Train loss: 7.8216, val loss: 5.2088
Model improve: 0.5710 -> 0.6661
Epoch: 3/100
Train loss: 6.5922, val loss: 4.4927
Model improve: 0.6661 -> 0.7094
Epoch: 4/100
Train loss: 5.9927, val loss: 3.8409
Model improve: 0.7094 -> 0.7468
Epoch: 5/100
Train loss: 5.6659, val loss: 3.8648
Model improve: 0.7468 -> 0.7609
Epoch: 6/100
Train loss: 5.1585, val loss: 3.6713
Model improve: 0.7609 -> 0.7803
Epoch: 7/100
Train loss: 4.8938, val loss: 3.4364
Model improve: 0.7803 -> 0.7897
Epoch: 8/100
Train loss: 4.8823, val loss: 3.4208
Model improve: 0.7897 -> 0.7942
Epoch: 9/100
Train loss: 4.5731, val loss: 3.3935
Model improve: 0.7942 -> 0.8016
Epoch: 10/100
Train loss: 4.4275, val loss: 3.2121
Model improve: 0.8016 -> 0.8094
Epoch: 11/100
Train loss: 4.3607, val loss: 3.3635
Epoch: 12/100
Train loss: 4.4103, val loss: 3.2661
Model improve: 0.8094 -> 0.8104
Epoch: 13/100
Train loss: 4.1470, val loss: 3.1877
Model improve: 0.8104 -> 0.8113
Epoch: 14/100
Train loss: 3.9407, val loss: 3.3288
Epoch: 15/100
Train loss: 4.0279, val loss: 3.0965
Model improve: 0.8113 -> 0.8218
Epoch: 16/100
Train loss: 3.9329, val loss: 3.1136
Model improve: 0.8218 -> 0.8225
Epoch: 17/100
Train loss: 3.7712, val loss: 3.1455
Model improve: 0.8225 -> 0.8254
Epoch: 18/100
Train loss: 3.7097, val loss: 2.9984
Model improve: 0.8254 -> 0.8312
Epoch: 19/100
Train loss: 3.6083, val loss: 3.1357
Epoch: 20/100
Train loss: 3.4497, val loss: 3.0098
Epoch: 21/100
Train loss: 3.5017, val loss: 2.9322
Model improve: 0.8312 -> 0.8326
Epoch: 22/100
Train loss: 3.5455, val loss: 2.8692
Model improve: 0.8326 -> 0.8357
Epoch: 23/100
Train loss: 3.6181, val loss: 3.0888
Epoch: 24/100
Train loss: 3.6250, val loss: 2.9737
Epoch: 25/100
Train loss: 3.5353, val loss: 2.8931
Epoch: 26/100
Train loss: 3.2391, val loss: 2.8043
Model improve: 0.8357 -> 0.8438
Epoch: 27/100
Train loss: 2.9862, val loss: 2.8534
Epoch: 28/100
Train loss: 3.3116, val loss: 2.9182
Epoch: 29/100
Train loss: 3.3980, val loss: 2.8403
Epoch: 30/100
Train loss: 3.2916, val loss: 2.8457
Model improve: 0.8438 -> 0.8466
Epoch: 31/100
Train loss: 3.1984, val loss: 2.9068
Epoch: 32/100
Train loss: 3.1277, val loss: 2.9035
Epoch: 33/100
Train loss: 3.0591, val loss: 2.8881
Epoch: 34/100
Train loss: 3.0987, val loss: 2.8560
Epoch: 35/100
Train loss: 2.9899, val loss: 2.9100
Epoch: 36/100
Train loss: 3.1099, val loss: 2.8871
Epoch: 37/100
Train loss: 3.1242, val loss: 2.7950
Epoch: 38/100
Train loss: 3.0803, val loss: 2.8207
Epoch: 39/100
Train loss: 2.8529, val loss: 2.8259
Epoch: 40/100
Train loss: 2.8246, val loss: 2.7186
Model improve: 0.8466 -> 0.8487
Epoch: 41/100
Train loss: 3.0324, val loss: 2.9596
Epoch: 42/100
Train loss: 2.8023, val loss: 2.7074
Model improve: 0.8487 -> 0.8513
Epoch: 43/100
Train loss: 2.8693, val loss: 2.8428
Epoch: 44/100
Train loss: 2.9823, val loss: 2.7486
Epoch: 45/100
Train loss: 2.9870, val loss: 2.8187
Epoch: 46/100
Train loss: 2.8889, val loss: 2.8884
Epoch: 47/100
Train loss: 2.8361, val loss: 2.7642
Epoch: 48/100
Train loss: 2.9017, val loss: 2.7908
Epoch: 49/100
Train loss: 2.7853, val loss: 2.7930
Epoch: 50/100
Train loss: 2.8331, val loss: 2.6262
Model improve: 0.8513 -> 0.8518
Epoch: 51/100
Train loss: 2.6788, val loss: 2.7916
Model improve: 0.8518 -> 0.8519
Epoch: 52/100
Train loss: 2.8954, val loss: 2.7452
Epoch: 53/100
Train loss: 2.7998, val loss: 2.6802
Epoch: 54/100
Train loss: 2.7606, val loss: 2.7354
Epoch: 55/100
Train loss: 2.6931, val loss: 2.6587
Model improve: 0.8519 -> 0.8530
Epoch: 56/100
Train loss: 2.5874, val loss: 2.7762
Model improve: 0.8530 -> 0.8558
Epoch: 57/100
Train loss: 2.7243, val loss: 2.6491
Epoch: 58/100
Train loss: 2.4891, val loss: 2.5947
Epoch: 59/100
Train loss: 2.5679, val loss: 2.5916
Epoch: 60/100
Train loss: 2.4869, val loss: 2.7048
Epoch: 61/100
Train loss: 2.6524, val loss: 2.6602
Epoch: 62/100
Train loss: 2.5010, val loss: 2.6342
Epoch: 63/100
Train loss: 2.6982, val loss: 2.6057
Model improve: 0.8558 -> 0.8572
Epoch: 64/100
Train loss: 2.4872, val loss: 2.6468
Epoch: 65/100
Train loss: 2.4873, val loss: 2.5318
Epoch: 66/100
Train loss: 2.5662, val loss: 2.6265
Model improve: 0.8572 -> 0.8607
Epoch: 67/100
Train loss: 2.6610, val loss: 2.6355
Epoch: 68/100
Train loss: 2.5389, val loss: 2.7256
Epoch: 69/100
Train loss: 2.4867, val loss: 2.6347
Epoch: 70/100
Train loss: 2.6187, val loss: 2.6702
Epoch: 71/100
Train loss: 2.3163, val loss: 2.5655
Epoch: 72/100
Date :04/06/2023, 19:59:26
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 224
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.1
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: False
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :04/06/2023, 20:00:38
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 224
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.1
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: False
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 10.5013, val loss: 8.0097
Model improve: 0.0000 -> 0.5372
Epoch: 2/100
Train loss: 8.8956, val loss: 5.9333
Model improve: 0.5372 -> 0.6277
Epoch: 3/100
Train loss: 8.0213, val loss: 5.0257
Model improve: 0.6277 -> 0.6800
Epoch: 4/100
Train loss: 7.4131, val loss: 4.5358
Model improve: 0.6800 -> 0.7067
Epoch: 5/100
Train loss: 7.1114, val loss: 4.2742
Model improve: 0.7067 -> 0.7332
Epoch: 6/100
Train loss: 6.7899, val loss: 3.9963
Model improve: 0.7332 -> 0.7467
Epoch: 7/100
Train loss: 6.6214, val loss: 3.8239
Model improve: 0.7467 -> 0.7638
Epoch: 8/100
Train loss: 6.4675, val loss: 3.7897
Model improve: 0.7638 -> 0.7671
Epoch: 9/100
Date :04/06/2023, 20:59:48
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.3
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: False
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 10.5686, val loss: 8.1471
Model improve: 0.0000 -> 0.5363
Epoch: 2/100
Date :04/07/2023, 03:34:15
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.3
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: False
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :04/07/2023, 03:38:28
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 9.3255, val loss: 5.3261
Model improve: 0.0000 -> 0.6781
Epoch: 2/100
Train loss: 7.8170, val loss: 4.4518
Model improve: 0.6781 -> 0.7347
Epoch: 3/100
Train loss: 7.2246, val loss: 4.3005
Model improve: 0.7347 -> 0.7560
Epoch: 4/100
Train loss: 6.9606, val loss: 3.9242
Model improve: 0.7560 -> 0.7702
Epoch: 5/100
Train loss: 6.8665, val loss: 3.8216
Model improve: 0.7702 -> 0.7817
Epoch: 6/100
Train loss: 6.7056, val loss: 3.6552
Model improve: 0.7817 -> 0.7852
Epoch: 7/100
Train loss: 6.3786, val loss: 3.5843
Model improve: 0.7852 -> 0.7896
Epoch: 8/100
Train loss: 6.3317, val loss: 3.3819
Model improve: 0.7896 -> 0.8029
Epoch: 9/100
Train loss: 6.4554, val loss: 3.4630
Model improve: 0.8029 -> 0.8078
Epoch: 10/100
Train loss: 6.2825, val loss: 3.2771
Epoch: 11/100
Train loss: 6.1823, val loss: 3.2558
Model improve: 0.8078 -> 0.8096
Epoch: 12/100
Train loss: 6.1265, val loss: 3.3268
Epoch: 13/100
Train loss: 6.0115, val loss: 3.2839
Model improve: 0.8096 -> 0.8112
Epoch: 14/100
Train loss: 5.9100, val loss: 3.1117
Model improve: 0.8112 -> 0.8215
Epoch: 15/100
Date :04/07/2023, 05:17:13
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 9.3489, val loss: 4.9726
Model improve: 0.0000 -> 0.6940
Epoch: 2/100
Train loss: 7.8077, val loss: 4.0581
Model improve: 0.6940 -> 0.7561
Epoch: 3/100
Train loss: 7.2408, val loss: 4.0364
Model improve: 0.7561 -> 0.7716
Epoch: 4/100
Train loss: 6.9612, val loss: 3.4055
Model improve: 0.7716 -> 0.7933
Epoch: 5/100
Train loss: 6.8545, val loss: 3.2772
Model improve: 0.7933 -> 0.8056
Epoch: 6/100
Train loss: 6.6997, val loss: 3.1185
Model improve: 0.8056 -> 0.8120
Epoch: 7/100
Train loss: 6.3882, val loss: 2.9979
Model improve: 0.8120 -> 0.8222
Epoch: 8/100
Fold: 0
19577
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 9.0541, val loss: 4.3579
Model improve: 0.0000 -> 0.7443
Epoch: 2/100
Train loss: 7.4491, val loss: 3.6613
Model improve: 0.7443 -> 0.7932
Epoch: 3/100
Train loss: 6.9236, val loss: 3.4579
Model improve: 0.7932 -> 0.8136
Epoch: 4/100
Train loss: 6.7461, val loss: 2.9790
Model improve: 0.8136 -> 0.8280
Epoch: 5/100
Train loss: 6.6185, val loss: 3.0809
Model improve: 0.8280 -> 0.8343
Epoch: 6/100
Train loss: 6.5262, val loss: 2.8056
Model improve: 0.8343 -> 0.8421
Epoch: 7/100
Train loss: 6.2269, val loss: 2.7769
Model improve: 0.8421 -> 0.8444
Epoch: 8/100
Train loss: 6.2386, val loss: 2.6581
Epoch: 9/100
Train loss: 6.2947, val loss: 2.7523
Model improve: 0.8444 -> 0.8528
Epoch: 10/100
Train loss: 6.0445, val loss: 2.6155
Epoch: 11/100
Train loss: 6.0053, val loss: 2.6680
Model improve: 0.8528 -> 0.8553
Epoch: 12/100
Train loss: 6.0142, val loss: 2.4976
Model improve: 0.8553 -> 0.8574
Epoch: 13/100
Train loss: 5.9183, val loss: 2.5262
Model improve: 0.8574 -> 0.8605
Epoch: 14/100
Train loss: 5.8192, val loss: 2.4961
Epoch: 15/100
Train loss: 5.9205, val loss: 2.4934
Epoch: 16/100
Train loss: 5.8060, val loss: 2.4339
Model improve: 0.8605 -> 0.8609
Epoch: 17/100
Train loss: 5.6917, val loss: 2.4193
Model improve: 0.8609 -> 0.8614
Epoch: 18/100
Train loss: 5.7294, val loss: 2.3209
Model improve: 0.8614 -> 0.8650
Epoch: 19/100
Train loss: 5.6883, val loss: 2.4527
Model improve: 0.8650 -> 0.8697
Epoch: 20/100
Train loss: 5.7572, val loss: 2.2629
Epoch: 21/100
Train loss: 5.7567, val loss: 2.4543
Epoch: 22/100
Train loss: 5.5467, val loss: 2.3162
Model improve: 0.8697 -> 0.8698
Epoch: 23/100
Train loss: 5.4520, val loss: 2.3541
Model improve: 0.8698 -> 0.8714
Epoch: 24/100
Train loss: 5.5882, val loss: 2.3197
Model improve: 0.8714 -> 0.8734
Epoch: 25/100
Train loss: 5.5939, val loss: 2.2219
Epoch: 26/100
Train loss: 5.4650, val loss: 2.1997
Epoch: 27/100
Train loss: 5.5644, val loss: 2.2969
Model improve: 0.8734 -> 0.8738
Epoch: 28/100
Train loss: 5.4620, val loss: 2.2664
Model improve: 0.8738 -> 0.8757
Epoch: 29/100
Train loss: 5.3909, val loss: 2.2373
Epoch: 30/100
Train loss: 5.3551, val loss: 2.2268
Epoch: 31/100
Train loss: 5.3562, val loss: 2.2225
Epoch: 32/100
Train loss: 5.3585, val loss: 2.2218
Epoch: 33/100
Train loss: 5.2080, val loss: 2.2619
Epoch: 34/100
Train loss: 5.2346, val loss: 2.2879
Epoch: 35/100
Train loss: 5.3822, val loss: 2.3397
Model improve: 0.8757 -> 0.8779
Epoch: 36/100
Train loss: 5.1489, val loss: 2.1132
Model improve: 0.8779 -> 0.8783
Epoch: 37/100
Train loss: 5.3491, val loss: 2.1289
Model improve: 0.8783 -> 0.8795
Epoch: 38/100
Train loss: 5.2333, val loss: 2.0420
Model improve: 0.8795 -> 0.8796
Epoch: 39/100
Train loss: 5.2026, val loss: 2.1523
Epoch: 40/100
Train loss: 5.2337, val loss: 2.2549
Epoch: 41/100
Train loss: 5.0613, val loss: 2.1786
Epoch: 42/100
Train loss: 5.3003, val loss: 2.1727
Model improve: 0.8796 -> 0.8802
Epoch: 43/100
Train loss: 5.2051, val loss: 2.2365
Model improve: 0.8802 -> 0.8804
Epoch: 44/100
Train loss: 5.2628, val loss: 2.1046
Epoch: 45/100
Train loss: 5.2335, val loss: 2.1620
Model improve: 0.8804 -> 0.8808
Epoch: 46/100
Train loss: 4.9722, val loss: 2.1111
Model improve: 0.8808 -> 0.8811
Epoch: 47/100
Date :04/07/2023, 10:23:23
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 9.0599, val loss: 4.5291
Model improve: 0.0000 -> 0.7377
Epoch: 2/100
Train loss: 7.4178, val loss: 3.5981
Model improve: 0.7377 -> 0.7890
Epoch: 3/100
Train loss: 6.8637, val loss: 3.3100
Model improve: 0.7890 -> 0.8165
Epoch: 4/100
Train loss: 6.6970, val loss: 3.0101
Model improve: 0.8165 -> 0.8249
Epoch: 5/100
Train loss: 6.5670, val loss: 2.9557
Model improve: 0.8249 -> 0.8350
Epoch: 6/100
Train loss: 6.4666, val loss: 2.7413
Model improve: 0.8350 -> 0.8412
Epoch: 7/100
Train loss: 6.1616, val loss: 2.7613
Model improve: 0.8412 -> 0.8428
Epoch: 8/100
Train loss: 6.1743, val loss: 2.5426
Model improve: 0.8428 -> 0.8489
Epoch: 9/100
Train loss: 6.2352, val loss: 2.7153
Model improve: 0.8489 -> 0.8555
Epoch: 10/100
Train loss: 5.9849, val loss: 2.6113
Epoch: 11/100
Train loss: 5.9507, val loss: 2.5776
Epoch: 12/100
Train loss: 5.9573, val loss: 2.5276
Epoch: 13/100
Train loss: 5.8623, val loss: 2.3905
Model improve: 0.8555 -> 0.8611
Epoch: 14/100
Train loss: 5.7391, val loss: 2.4330
Model improve: 0.8611 -> 0.8621
Epoch: 15/100
Train loss: 5.8663, val loss: 2.4080
Epoch: 16/100
Train loss: 5.7481, val loss: 2.4309
Epoch: 17/100
Train loss: 5.6343, val loss: 2.3017
Epoch: 18/100
Train loss: 5.6833, val loss: 2.2800
Model improve: 0.8621 -> 0.8655
Epoch: 19/100
Train loss: 5.6443, val loss: 2.4410
Model improve: 0.8655 -> 0.8688
Epoch: 20/100
Train loss: 5.7085, val loss: 2.3116
Model improve: 0.8688 -> 0.8700
Epoch: 21/100
Train loss: 5.7078, val loss: 2.4823
Epoch: 22/100
Train loss: 5.5130, val loss: 2.2805
Epoch: 23/100
Train loss: 5.4118, val loss: 2.3420
Model improve: 0.8700 -> 0.8713
Epoch: 24/100
Train loss: 5.5241, val loss: 2.2550
Epoch: 25/100
Train loss: 5.5615, val loss: 2.1755
Model improve: 0.8713 -> 0.8719
Epoch: 26/100
Train loss: 5.4187, val loss: 2.1620
Epoch: 27/100
Train loss: 5.5222, val loss: 2.3642
Epoch: 28/100
Train loss: 5.4110, val loss: 2.2108
Model improve: 0.8719 -> 0.8725
Epoch: 29/100
Train loss: 5.3617, val loss: 2.1852
Model improve: 0.8725 -> 0.8746
Epoch: 30/100
Train loss: 5.3028, val loss: 2.1998
Epoch: 31/100
Train loss: 5.3219, val loss: 2.2042
Epoch: 32/100
Train loss: 5.3330, val loss: 2.2131
Model improve: 0.8746 -> 0.8747
Epoch: 33/100
Train loss: 5.1666, val loss: 2.1897
Model improve: 0.8747 -> 0.8751
Epoch: 34/100
Train loss: 5.1854, val loss: 2.2648
Epoch: 35/100
Train loss: 5.3293, val loss: 2.3289
Model improve: 0.8751 -> 0.8760
Epoch: 36/100
Train loss: 5.1054, val loss: 2.0808
Model improve: 0.8760 -> 0.8769
Epoch: 37/100
Train loss: 5.3236, val loss: 2.1152
Model improve: 0.8769 -> 0.8808
Epoch: 38/100
Train loss: 5.2029, val loss: 2.0448
Epoch: 39/100
Train loss: 5.1699, val loss: 2.0699
Epoch: 40/100
Train loss: 5.1904, val loss: 2.1846
Epoch: 41/100
Train loss: 5.0273, val loss: 2.1294
Epoch: 42/100
Train loss: 5.2547, val loss: 2.1505
Epoch: 43/100
Train loss: 5.1723, val loss: 2.1595
Epoch: 44/100
Train loss: 5.2158, val loss: 2.0393
Epoch: 45/100
Train loss: 5.2093, val loss: 2.1378
Epoch: 46/100
Train loss: 4.9349, val loss: 2.1210
Epoch: 47/100
Train loss: 5.0953, val loss: 2.1524
Model improve: 0.8808 -> 0.8826
Epoch: 48/100
Train loss: 5.2461, val loss: 2.2524
Epoch: 49/100
Train loss: 5.0391, val loss: 2.0573
Epoch: 50/100
Train loss: 5.0462, val loss: 2.0481
Epoch: 51/100
Train loss: 4.9973, val loss: 2.0435
Model improve: 0.8826 -> 0.8836
Epoch: 52/100
Train loss: 5.0953, val loss: 2.1332
Epoch: 53/100
Train loss: 4.9679, val loss: 2.1819
Model improve: 0.8836 -> 0.8838
Epoch: 54/100
Train loss: 4.9052, val loss: 2.0505
Epoch: 55/100
Train loss: 4.8934, val loss: 2.0308
Epoch: 56/100
Train loss: 4.8851, val loss: 2.1141
Epoch: 57/100
Train loss: 5.0617, val loss: 2.0690
Model improve: 0.8838 -> 0.8848
Epoch: 58/100
Train loss: 5.0375, val loss: 1.9636
Epoch: 59/100
Train loss: 4.9054, val loss: 2.0158
Model improve: 0.8848 -> 0.8854
Epoch: 60/100
Train loss: 5.0282, val loss: 1.9863
Model improve: 0.8854 -> 0.8859
Epoch: 61/100
Train loss: 4.8084, val loss: 1.9828
Epoch: 62/100
Train loss: 4.9455, val loss: 1.9513
Epoch: 63/100
Train loss: 4.9006, val loss: 1.9502
Epoch: 64/100
Train loss: 4.9013, val loss: 1.9328
Epoch: 65/100
Train loss: 4.9675, val loss: 1.9540
Model improve: 0.8859 -> 0.8890
Epoch: 66/100
Train loss: 4.9311, val loss: 2.0428
Epoch: 67/100
Train loss: 4.8745, val loss: 1.9448
Epoch: 68/100
Train loss: 4.7288, val loss: 1.9351
Epoch: 69/100
Train loss: 4.9902, val loss: 1.9504
Epoch: 70/100
Train loss: 4.7822, val loss: 1.9187
Epoch: 71/100
Train loss: 4.7497, val loss: 2.1074
Epoch: 72/100
Train loss: 4.8547, val loss: 1.9380
Epoch: 73/100
Train loss: 4.6399, val loss: 1.8780
Model improve: 0.8890 -> 0.8897
Epoch: 74/100
Train loss: 4.7514, val loss: 1.9416
Epoch: 75/100
Train loss: 4.8477, val loss: 2.0009
Model improve: 0.8897 -> 0.8898
Epoch: 76/100
Train loss: 4.7525, val loss: 1.8849
Epoch: 77/100
Train loss: 4.7707, val loss: 1.9643
Model improve: 0.8898 -> 0.8900
Epoch: 78/100
Train loss: 4.6779, val loss: 2.1002
Epoch: 79/100
Train loss: 4.8242, val loss: 1.9338
Epoch: 80/100
Train loss: 4.6478, val loss: 1.8817
Epoch: 81/100
Train loss: 4.7657, val loss: 1.9863
Epoch: 82/100
Train loss: 4.9190, val loss: 1.8930
Epoch: 83/100
Train loss: 4.7271, val loss: 1.9826
Model improve: 0.8900 -> 0.8907
Epoch: 84/100
Train loss: 4.7970, val loss: 1.9415
Epoch: 85/100
Train loss: 4.7062, val loss: 1.9310
Epoch: 86/100
Train loss: 4.6904, val loss: 1.9567
Model improve: 0.8907 -> 0.8908
Epoch: 87/100
Train loss: 4.6944, val loss: 1.8556
Epoch: 88/100
Train loss: 4.7949, val loss: 1.9241
Epoch: 89/100
Train loss: 4.6989, val loss: 1.8946
Epoch: 90/100
Train loss: 4.7895, val loss: 2.0114
Epoch: 91/100
Train loss: 4.6209, val loss: 1.9108
Epoch: 92/100
Train loss: 4.6974, val loss: 1.9172
Epoch: 93/100
Train loss: 4.8408, val loss: 1.8921
Epoch: 94/100
Train loss: 4.6794, val loss: 1.9439
Epoch: 95/100
Train loss: 4.8385, val loss: 1.9480
Epoch: 96/100
Train loss: 4.7841, val loss: 1.9532
Model improve: 0.8908 -> 0.8909
Epoch: 97/100
Train loss: 4.6648, val loss: 2.0065
Epoch: 98/100
Train loss: 4.6593, val loss: 1.8967
Epoch: 99/100
Train loss: 4.9285, val loss: 1.9176
Epoch: 100/100
Train loss: 4.6873, val loss: 1.8863
Date :04/09/2023, 23:09:36
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 16
nmels: 256
fmax: 16386
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 0
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13552
Fold: 0
13552
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 9.5729, val loss: 9.8669
Model improve: 0.0000 -> 0.4903
Epoch: 2/100
Train loss: 8.0744, val loss: 6.7740
Model improve: 0.4903 -> 0.5773
Epoch: 3/100
Train loss: 7.4008, val loss: 6.3501
Model improve: 0.5773 -> 0.6098
Epoch: 4/100
Train loss: 6.8167, val loss: 5.9808
Model improve: 0.6098 -> 0.6416
Epoch: 5/100
Train loss: 6.5859, val loss: 5.4196
Model improve: 0.6416 -> 0.6701
Epoch: 6/100
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :04/13/2023, 03:39:10
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 16
nmels: 256
fmax: 16386
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
Date :04/14/2023, 00:22:45
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/20
Date :04/14/2023, 00:23:55
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/20
Train loss: 8.5284, val loss: 3.9183
0.6683997583200791
Model improve: 0.0000 -> 0.6684
Epoch: 2/20
Train loss: 4.5217, val loss: 2.8892
0.771711596236326
Model improve: 0.6684 -> 0.7717
Epoch: 3/20
Train loss: 3.8101, val loss: 2.6341
0.8116210799582988
Model improve: 0.7717 -> 0.8116
Epoch: 4/20
Train loss: 3.4545, val loss: 2.1950
0.846326567731316
Model improve: 0.8116 -> 0.8463
Epoch: 5/20
Train loss: 3.2422, val loss: 2.0404
0.864809482822967
Model improve: 0.8463 -> 0.8648
Epoch: 6/20
Train loss: 3.0072, val loss: 1.9588
0.8646094757296994
Epoch: 7/20
Train loss: 2.7151, val loss: 1.8567
0.8751129278754176
Model improve: 0.8648 -> 0.8751
Epoch: 8/20
Train loss: 2.6292, val loss: 1.6871
0.8839195051004587
Model improve: 0.8751 -> 0.8839
Epoch: 9/20
Train loss: 2.6539, val loss: 1.6800
0.8916585344561689
Model improve: 0.8839 -> 0.8917
Epoch: 10/20
Train loss: 2.3914, val loss: 1.5497
0.8927829334388633
Model improve: 0.8917 -> 0.8928
Epoch: 11/20
Train loss: 2.2818, val loss: 1.5591
0.8955342604201113
Model improve: 0.8928 -> 0.8955
Epoch: 12/20
Train loss: 2.2037, val loss: 1.4287
0.8997571343263989
Model improve: 0.8955 -> 0.8998
Epoch: 13/20
Train loss: 2.0804, val loss: 1.4087
0.9025469756933584
Model improve: 0.8998 -> 0.9025
Epoch: 14/20
Train loss: 1.9452, val loss: 1.3630
0.9035342426282488
Model improve: 0.9025 -> 0.9035
Epoch: 15/20
Train loss: 1.9849, val loss: 1.3273
0.9058739377022331
Model improve: 0.9035 -> 0.9059
Epoch: 16/20
Train loss: 1.9147, val loss: 1.2941
0.9082332127127423
Model improve: 0.9059 -> 0.9082
Epoch: 17/20
Train loss: 1.7682, val loss: 1.2530
0.9092859512038818
Model improve: 0.9082 -> 0.9093
Epoch: 18/20
Train loss: 1.8138, val loss: 1.2676
0.9099842021604854
Model improve: 0.9093 -> 0.9100
Epoch: 19/20
Train loss: 1.8330, val loss: 1.3083
0.9115186089999976
Model improve: 0.9100 -> 0.9115
Epoch: 20/20
Train loss: 1.9066, val loss: 1.2684
0.9100604006554204
Date :04/14/2023, 02:33:08
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 8.4847, val loss: 3.8890
0.6762715962135634
Model improve: 0.0000 -> 0.6763
Epoch: 2/100
Train loss: 4.5081, val loss: 2.9040
0.7745734660225594
Model improve: 0.6763 -> 0.7746
Epoch: 3/100
Train loss: 3.8147, val loss: 2.6774
0.8121931913777135
Model improve: 0.7746 -> 0.8122
Epoch: 4/100
Train loss: 3.4708, val loss: 2.2821
0.8400113370622907
Model improve: 0.8122 -> 0.8400
Epoch: 5/100
Train loss: 3.2904, val loss: 2.0969
0.8613790604362253
Model improve: 0.8400 -> 0.8614
Epoch: 6/100
Train loss: 3.1061, val loss: 2.0384
0.8590942503894539
Epoch: 7/100
Train loss: 2.8336, val loss: 1.9302
0.873021042190662
Model improve: 0.8614 -> 0.8730
Epoch: 8/100
Train loss: 2.7812, val loss: 1.7940
0.8796209151636821
Model improve: 0.8730 -> 0.8796
Epoch: 9/100
Train loss: 2.8444, val loss: 1.8200
0.8803411152776447
Model improve: 0.8796 -> 0.8803
Epoch: 10/100
Train loss: 2.6333, val loss: 1.7313
0.8789055835182833
Epoch: 11/100
Train loss: 2.5566, val loss: 1.8032
0.8815818642104865
Model improve: 0.8803 -> 0.8816
Epoch: 12/100
Train loss: 2.5189, val loss: 1.6990
0.8829231267794474
Model improve: 0.8816 -> 0.8829
Epoch: 13/100
Train loss: 2.4049, val loss: 1.6724
0.8835936371359904
Model improve: 0.8829 -> 0.8836
Epoch: 14/100
Train loss: 2.3255, val loss: 1.7191
0.8818451136525249
Epoch: 15/100
Train loss: 2.3697, val loss: 1.6981
0.8826443518331052
Epoch: 16/100
Date :04/14/2023, 03:51:47
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 8.5426, val loss: 4.0087
0.6582165823167183
Model improve: 0.0000 -> 0.6582
Epoch: 2/100
Train loss: 4.5436, val loss: 2.9406
0.7701149537862422
Model improve: 0.6582 -> 0.7701
Epoch: 3/100
Train loss: 3.8188, val loss: 2.6013
0.8145550335545637
Model improve: 0.7701 -> 0.8146
Epoch: 4/100
Date :04/14/2023, 04:09:12
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 8.4541, val loss: 3.8384
0.6838207910457472
Model improve: 0.0000 -> 0.6838
Epoch: 2/100
Train loss: 4.4647, val loss: 2.7976
0.7831321350341662
Model improve: 0.6838 -> 0.7831
Epoch: 3/100
Train loss: 3.7721, val loss: 2.5943
0.821416607484509
Model improve: 0.7831 -> 0.8214
Epoch: 4/100
Train loss: 3.4349, val loss: 2.1490
0.8511702271637017
Model improve: 0.8214 -> 0.8512
Epoch: 5/100
Train loss: 3.2224, val loss: 2.1367
0.8542785554769425
Model improve: 0.8512 -> 0.8543
Epoch: 6/100
Train loss: 3.0102, val loss: 1.9881
0.8616983703116152
Model improve: 0.8543 -> 0.8617
Epoch: 7/100
Train loss: 2.7057, val loss: 1.8950
0.8689896988453543
Model improve: 0.8617 -> 0.8690
Epoch: 8/100
Train loss: 2.6325, val loss: 1.6520
0.884769608348259
Model improve: 0.8690 -> 0.8848
Epoch: 9/100
Train loss: 2.6521, val loss: 1.6511
0.8921344035987382
Model improve: 0.8848 -> 0.8921
Epoch: 10/100
Train loss: 2.3826, val loss: 1.5416
0.8887555171241406
Epoch: 11/100
Train loss: 2.2799, val loss: 1.5240
0.8958055547485181
Model improve: 0.8921 -> 0.8958
Epoch: 12/100
Train loss: 2.1997, val loss: 1.4169
0.9007389214737863
Model improve: 0.8958 -> 0.9007
Epoch: 13/100
Train loss: 2.0722, val loss: 1.4072
0.9021851007544686
Model improve: 0.9007 -> 0.9022
Epoch: 14/100
Train loss: 1.9408, val loss: 1.3893
0.9049156397787101
Model improve: 0.9022 -> 0.9049
Epoch: 15/100
Train loss: 1.9881, val loss: 1.3279
0.9045248103198049
Epoch: 16/100
Train loss: 1.9080, val loss: 1.2884
0.9092043915029426
Model improve: 0.9049 -> 0.9092
Epoch: 17/100
Train loss: 1.7653, val loss: 1.2465
0.9109947480779955
Model improve: 0.9092 -> 0.9110
Epoch: 18/100
Train loss: 1.8132, val loss: 1.2625
0.9127166789243824
Model improve: 0.9110 -> 0.9127
Epoch: 19/100
Train loss: 1.8304, val loss: 1.3186
0.9124902075082286
Epoch: 20/100
Train loss: 1.9019, val loss: 1.2564
0.9123152163606594
Epoch: 21/100
Train loss: 2.8152, val loss: 1.8588
0.8752462294451968
Epoch: 22/100
Train loss: 2.6080, val loss: 1.7703
0.8803306023304998
Epoch: 23/100
Train loss: 2.3807, val loss: 1.7996
0.8805673071899186
Epoch: 24/100
Train loss: 2.4031, val loss: 1.6548
0.8895844091626018
Epoch: 25/100
Train loss: 2.2661, val loss: 1.6300
0.8882287189404084
Epoch: 26/100
Train loss: 2.1648, val loss: 1.5555
0.8931158918613475
Epoch: 27/100
Train loss: 2.1315, val loss: 1.5413
0.8987269416266985
Epoch: 28/100
Train loss: 2.0460, val loss: 1.5252
0.8969440650798446
Epoch: 29/100
Train loss: 1.9674, val loss: 1.4426
0.8996639285624906
Epoch: 30/100
Train loss: 1.8532, val loss: 1.3908
0.903106303132628
Epoch: 31/100
Date :04/14/2023, 07:09:48
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 3
19523
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/20
Train loss: 8.3636, val loss: 4.0786
0.6356240132698525
Model improve: 0.0000 -> 0.6356
Epoch: 2/20
Train loss: 4.6219, val loss: 2.8998
0.7626237675822135
Model improve: 0.6356 -> 0.7626
Epoch: 3/20
Train loss: 3.9101, val loss: 2.6054
0.808438513106121
Model improve: 0.7626 -> 0.8084
Epoch: 4/20
Train loss: 3.5268, val loss: 2.3295
0.8305786010170016
Model improve: 0.8084 -> 0.8306
Epoch: 5/20
Train loss: 3.2719, val loss: 2.1126
0.8473008423850278
Model improve: 0.8306 -> 0.8473
Epoch: 6/20
Train loss: 3.0413, val loss: 1.9456
0.8576196170297983
Model improve: 0.8473 -> 0.8576
Epoch: 7/20
Train loss: 2.7299, val loss: 1.9097
0.8579375132707112
Model improve: 0.8576 -> 0.8579
Epoch: 8/20
Train loss: 2.6590, val loss: 1.7375
0.8689317755088463
Model improve: 0.8579 -> 0.8689
Epoch: 9/20
Train loss: 2.6740, val loss: 1.7871
0.8749772463948229
Model improve: 0.8689 -> 0.8750
Epoch: 10/20
Train loss: 2.4242, val loss: 1.5972
0.8787659560465446
Model improve: 0.8750 -> 0.8788
Epoch: 11/20
Train loss: 2.2881, val loss: 1.6678
0.8790872575365603
Model improve: 0.8788 -> 0.8791
Epoch: 12/20
Train loss: 2.2293, val loss: 1.5286
0.8852489411670283
Model improve: 0.8791 -> 0.8852
Epoch: 13/20
Train loss: 2.0957, val loss: 1.5023
0.8866572013664366
Model improve: 0.8852 -> 0.8867
Epoch: 14/20
Train loss: 1.9690, val loss: 1.4793
0.8879495816527662
Model improve: 0.8867 -> 0.8879
Epoch: 15/20
Train loss: 1.9937, val loss: 1.3961
0.8931976460919075
Model improve: 0.8879 -> 0.8932
Epoch: 16/20
Train loss: 1.9191, val loss: 1.3983
0.8925269304238572
Epoch: 17/20
Train loss: 1.7794, val loss: 1.3528
0.8945333881302859
Model improve: 0.8932 -> 0.8945
Epoch: 18/20
Train loss: 1.8370, val loss: 1.3605
0.895552536855004
Model improve: 0.8945 -> 0.8956
Epoch: 19/20
Train loss: 1.8419, val loss: 1.3842
0.8957826107628536
Model improve: 0.8956 -> 0.8958
Epoch: 20/20
Train loss: 1.9237, val loss: 1.3491
0.8959583100980514
Model improve: 0.8958 -> 0.8960
Date :04/14/2023, 12:15:21
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/20
Train loss: 8.2110, val loss: 3.7159
0.6938603229888395
Model improve: 0.0000 -> 0.6939
Epoch: 2/20
Train loss: 3.7248, val loss: 2.6761
0.7914238900650473
Model improve: 0.6939 -> 0.7914
Epoch: 3/20
Train loss: 2.5452, val loss: 2.2336
0.8422033666465631
Model improve: 0.8284 -> 0.8422
Epoch: 5/20
Train loss: 2.3375, val loss: 1.9485
0.8582156532708769
Model improve: 0.8422 -> 0.8582
Epoch: 6/20
Train loss: 2.0618, val loss: 1.9589
0.8594489307849571
Model improve: 0.8582 -> 0.8594
Epoch: 7/20
Date :04/14/2023, 12:48:08
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/20
Date :04/14/2023, 12:49:41
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/20
Train loss: 8.9511, val loss: 4.1219
0.6498067955834944
Model improve: 0.0000 -> 0.6498
Epoch: 2/20
Date :04/14/2023, 12:55:13
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/20
Train loss: 8.8193, val loss: 4.0448
0.6537888552227632
Model improve: 0.0000 -> 0.6538
Epoch: 2/20
Date :04/14/2023, 13:00:31
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/20
Train loss: 8.3231, val loss: 3.9047
0.6671364121179661
Model improve: 0.0000 -> 0.6671
Epoch: 2/20
Train loss: 4.5149, val loss: 2.9929
0.7615244990255587
Model improve: 0.6671 -> 0.7615
Epoch: 3/20
Date :04/14/2023, 13:10:45
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Date :04/14/2023, 13:13:12
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/20
Train loss: 9.1380, val loss: 4.5580
0.6009242147871277
Model improve: 0.0000 -> 0.6009
Epoch: 2/20
Train loss: 5.3015, val loss: 3.5003
0.7067773326775222
Model improve: 0.6009 -> 0.7068
Epoch: 3/20
Train loss: 4.7350, val loss: 3.1900
0.7509983161272411
Model improve: 0.7068 -> 0.7510
Epoch: 4/20
Train loss: 4.4277, val loss: 2.7806
0.7845914304994343
Model improve: 0.7510 -> 0.7846
Epoch: 5/20
Train loss: 4.2230, val loss: 2.6292
0.8046730112193305
Model improve: 0.7846 -> 0.8047
Epoch: 6/20
Train loss: 3.9994, val loss: 2.3949
0.8238031858882302
Model improve: 0.8047 -> 0.8238
Epoch: 7/20
Train loss: 3.7266, val loss: 2.3004
0.8360804327379362
Model improve: 0.8238 -> 0.8361
Epoch: 8/20
Train loss: 3.6319, val loss: 2.0107
0.8503212128405383
Model improve: 0.8361 -> 0.8503
Epoch: 9/20
Train loss: 3.5949, val loss: 2.0391
0.8576883284598916
Model improve: 0.8503 -> 0.8577
Epoch: 10/20
Train loss: 3.4009, val loss: 1.8761
0.8590524896283226
Model improve: 0.8577 -> 0.8591
Epoch: 11/20
Train loss: 3.2352, val loss: 1.7832
0.8733374872278223
Model improve: 0.8591 -> 0.8733
Epoch: 12/20
Train loss: 3.1488, val loss: 1.7130
0.8755617979362009
Model improve: 0.8733 -> 0.8756
Epoch: 13/20
Train loss: 3.0001, val loss: 1.7053
0.8750530750177402
Epoch: 14/20
Train loss: 2.8559, val loss: 1.5870
0.8837540415247086
Model improve: 0.8756 -> 0.8838
Epoch: 15/20
Train loss: 2.8876, val loss: 1.5614
0.8901634709918778
Model improve: 0.8838 -> 0.8902
Epoch: 16/20
Train loss: 2.7684, val loss: 1.5370
0.891104274781643
Model improve: 0.8902 -> 0.8911
Epoch: 17/20
Train loss: 2.6388, val loss: 1.4632
0.8925149240154674
Model improve: 0.8911 -> 0.8925
Epoch: 18/20
Train loss: 2.6703, val loss: 1.4815
0.8943234247935936
Model improve: 0.8925 -> 0.8943
Epoch: 19/20
Train loss: 2.6519, val loss: 1.5409
0.8936000810583583
Epoch: 20/20
Date :04/14/2023, 20:08:46
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/20
Train loss: 9.2067, val loss: 3.9947
0.665044954294823
Model improve: 0.0000 -> 0.6650
Epoch: 2/20
Train loss: 4.7735, val loss: 3.2021
0.7444987628738243
Model improve: 0.6650 -> 0.7445
Epoch: 3/20
Train loss: 4.1704, val loss: 2.7871
0.8006461191003912
Model improve: 0.7445 -> 0.8006
Epoch: 4/20
Train loss: 3.8012, val loss: 2.4087
0.8259688594509881
Model improve: 0.8006 -> 0.8260
Epoch: 5/20
Train loss: 3.5837, val loss: 2.3098
0.8367637451266151
Model improve: 0.8260 -> 0.8368
Epoch: 6/20
Train loss: 3.3847, val loss: 2.1291
0.849104574305083
Model improve: 0.8368 -> 0.8491
Epoch: 7/20
Train loss: 3.0773, val loss: 2.0154
0.8587002180104831
Model improve: 0.8491 -> 0.8587
Epoch: 8/20
Train loss: 2.9893, val loss: 1.8092
0.8691824017662391
Model improve: 0.8587 -> 0.8692
Epoch: 9/20
Date :04/14/2023, 23:08:49
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/20
Train loss: 8.3741, val loss: 4.1469
0.6410209740450676
Model improve: 0.0000 -> 0.6410
Epoch: 2/20
Train loss: 4.8957, val loss: 3.2304
0.7391491396980764
Model improve: 0.6410 -> 0.7391
Epoch: 3/20
Train loss: 4.2642, val loss: 2.8984
0.7917971679021152
Model improve: 0.7391 -> 0.7918
Epoch: 4/20
Train loss: 3.8954, val loss: 2.4480
0.8181077289560452
Model improve: 0.7918 -> 0.8181
Epoch: 5/20
Train loss: 3.6681, val loss: 2.4281
0.830186679610073
Model improve: 0.8181 -> 0.8302
Epoch: 6/20
Train loss: 3.4352, val loss: 2.2776
0.8367761277330044
Model improve: 0.8302 -> 0.8368
Epoch: 7/20
Train loss: 3.1293, val loss: 2.0587
0.8517783814250938
Model improve: 0.8368 -> 0.8518
Epoch: 8/20
Train loss: 3.0386, val loss: 1.9122
0.8619395142449526
Model improve: 0.8518 -> 0.8619
Epoch: 9/20
Train loss: 3.0321, val loss: 1.8356
0.8744560540705945
Model improve: 0.8619 -> 0.8745
Epoch: 10/20
Train loss: 2.7938, val loss: 1.7305
0.8745883810071284
Model improve: 0.8745 -> 0.8746
Epoch: 11/20
Train loss: 2.6706, val loss: 1.6388
0.8821146248897543
Model improve: 0.8746 -> 0.8821
Epoch: 12/20
Train loss: 2.5581, val loss: 1.5624
0.888938183534961
Model improve: 0.8821 -> 0.8889
Epoch: 13/20
Train loss: 2.4244, val loss: 1.5721
0.8899838098236731
Model improve: 0.8889 -> 0.8900
Epoch: 14/20
Train loss: 2.2865, val loss: 1.4631
0.8953162890707057
Model improve: 0.8900 -> 0.8953
Epoch: 15/20
Train loss: 2.3135, val loss: 1.4477
0.8971260703405044
Model improve: 0.8953 -> 0.8971
Epoch: 16/20
Train loss: 2.2054, val loss: 1.4040
0.8989803341114015
Model improve: 0.8971 -> 0.8990
Epoch: 17/20
Train loss: 2.0865, val loss: 1.3446
0.9021694457271457
Model improve: 0.8990 -> 0.9022
Epoch: 18/20
Train loss: 2.1281, val loss: 1.3643
0.9036236295480496
Model improve: 0.9022 -> 0.9036
Epoch: 19/20
Train loss: 2.1402, val loss: 1.4280
0.9030159127975946
Epoch: 20/20
Train loss: 2.2178, val loss: 1.3640
0.903297765477514
Date :04/15/2023, 10:10:32
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :04/15/2023, 10:11:24
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 9.7342, val loss: 2.9820
0.7920593884081645
Model improve: 0.0000 -> 0.7921
Epoch: 2/60
Train loss: 3.7450, val loss: 2.0819
0.8633835851105626
Model improve: 0.7921 -> 0.8634
Epoch: 3/60
Train loss: 2.9841, val loss: 1.7577
0.8894777698512439
Model improve: 0.8634 -> 0.8895
Epoch: 4/60
Train loss: 2.7325, val loss: 1.5973
0.8937868078612734
Model improve: 0.8895 -> 0.8938
Epoch: 5/60
Train loss: 2.6104, val loss: 1.5244
0.904085945905158
Model improve: 0.8938 -> 0.9041
Epoch: 6/60
Train loss: 2.4439, val loss: 1.5409
0.905838422324746
Model improve: 0.9041 -> 0.9058
Epoch: 7/60
Train loss: 2.2276, val loss: 1.4608
0.9070042173346247
Model improve: 0.9058 -> 0.9070
Epoch: 8/60
Date :04/15/2023, 10:45:29
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 9.7440, val loss: 2.9834
0.7917240632617285
Model improve: 0.0000 -> 0.7917
Epoch: 2/60
Train loss: 3.7443, val loss: 2.0833
0.8629884556218377
Model improve: 0.7917 -> 0.8630
Epoch: 3/60
Train loss: 2.9846, val loss: 1.7556
0.889501479456722
Model improve: 0.8630 -> 0.8895
Epoch: 4/60
Train loss: 2.7369, val loss: 1.6089
0.8940135672996435
Model improve: 0.8895 -> 0.8940
Epoch: 5/60
Train loss: 2.6245, val loss: 1.5341
0.9030921178137256
Model improve: 0.8940 -> 0.9031
Epoch: 6/60
Train loss: 2.4818, val loss: 1.5538
0.9029682514238151
Epoch: 7/60
Train loss: 2.2668, val loss: 1.5184
0.9027166566005755
Epoch: 8/60
Train loss: 2.2352, val loss: 1.4124
0.9084061396585629
Model improve: 0.9031 -> 0.9084
Epoch: 9/60
Train loss: 2.3067, val loss: 1.4616
0.9084328098520557
Model improve: 0.9084 -> 0.9084
Epoch: 10/60
Train loss: 2.1279, val loss: 1.3934
0.9085507501057553
Model improve: 0.9084 -> 0.9086
Epoch: 11/60
Train loss: 2.0955, val loss: 1.3873
0.9097872314958847
Model improve: 0.9086 -> 0.9098
Epoch: 12/60
Train loss: 2.0759, val loss: 1.4196
0.9090393057861785
Epoch: 13/60
Train loss: 1.9937, val loss: 1.3390
0.9140904791033075
Model improve: 0.9098 -> 0.9141
Epoch: 14/60
Train loss: 1.9265, val loss: 1.3539
0.9133935348093108
Epoch: 15/60
Train loss: 1.9771, val loss: 1.3255
0.9142839504337607
Model improve: 0.9141 -> 0.9143
Epoch: 16/60
Train loss: 1.9019, val loss: 1.3270
0.9145675306842866
Model improve: 0.9143 -> 0.9146
Epoch: 17/60
Train loss: 1.7980, val loss: 1.3427
0.9116136925635435
Epoch: 18/60
Train loss: 1.8612, val loss: 1.3366
0.9111493859411909
Epoch: 19/60
Train loss: 1.8583, val loss: 1.3578
0.9179118857532067
Model improve: 0.9146 -> 0.9179
Epoch: 20/60
Train loss: 1.9221, val loss: 1.3159
0.912939950856367
Epoch: 21/60
Train loss: 1.8885, val loss: 1.3294
0.9141061484617203
Epoch: 22/60
Train loss: 1.7486, val loss: 1.2890
0.9169750401680422
Epoch: 23/60
Train loss: 1.6631, val loss: 1.3004
0.9183993313377807
Model improve: 0.9179 -> 0.9184
Epoch: 24/60
Train loss: 1.7745, val loss: 1.2568
0.9193890200460325
Model improve: 0.9184 -> 0.9194
Epoch: 25/60
Train loss: 1.6934, val loss: 1.2589
0.9186372095815631
Epoch: 26/60
Train loss: 1.6551, val loss: 1.2413
0.9176018735641156
Epoch: 27/60
Train loss: 1.7124, val loss: 1.2891
0.9186058389308542
Epoch: 28/60
Train loss: 1.6473, val loss: 1.2441
0.9183590125489344
Epoch: 29/60
Train loss: 1.6178, val loss: 1.2892
0.9140917529388345
Epoch: 30/60
Train loss: 1.5522, val loss: 1.2260
0.9204272398092278
Model improve: 0.9194 -> 0.9204
Epoch: 31/60
Train loss: 1.5834, val loss: 1.3064
0.9136221715156655
Epoch: 32/60
Train loss: 1.6156, val loss: 1.2500
0.920291346733274
Epoch: 33/60
Train loss: 1.4685, val loss: 1.2526
0.9198971406861408
Epoch: 34/60
Train loss: 1.4629, val loss: 1.2439
0.9187189353799565
Epoch: 35/60
Train loss: 1.5833, val loss: 1.2404
0.9189943853076795
Epoch: 36/60
Train loss: 1.4506, val loss: 1.2544
0.9189402840393652
Epoch: 37/60
Train loss: 1.5660, val loss: 1.2469
0.9196758212302419
Epoch: 38/60
Train loss: 1.4955, val loss: 1.2322
0.9199835553324358
Epoch: 39/60
Train loss: 1.4653, val loss: 1.2029
0.9226337886594996
Model improve: 0.9204 -> 0.9226
Epoch: 40/60
Train loss: 1.5148, val loss: 1.1942
0.9240470308949522
Model improve: 0.9226 -> 0.9240
Epoch: 41/60
Train loss: 1.3622, val loss: 1.2099
0.9225367536781803
Epoch: 42/60
Train loss: 1.5385, val loss: 1.2060
0.9223131570011526
Epoch: 43/60
Train loss: 1.4663, val loss: 1.1918
0.9244785434345596
Model improve: 0.9240 -> 0.9245
Epoch: 44/60
Train loss: 1.4867, val loss: 1.1888
0.9228675072770385
Epoch: 45/60
Train loss: 1.5010, val loss: 1.1793
0.924734068574062
Model improve: 0.9245 -> 0.9247
Epoch: 46/60
Train loss: 1.3378, val loss: 1.1785
0.9242597792328757
Epoch: 47/60
Train loss: 1.4324, val loss: 1.1751
0.9255237815497614
Model improve: 0.9247 -> 0.9255
Epoch: 48/60
Train loss: 1.5341, val loss: 1.1868
0.9249646931350716
Epoch: 49/60
Train loss: 1.4034, val loss: 1.1701
0.9247016252829225
Epoch: 50/60
Train loss: 1.4004, val loss: 1.1690
0.925698606370264
Model improve: 0.9255 -> 0.9257
Epoch: 51/60
Train loss: 1.3956, val loss: 1.1555
0.927002933169662
Model improve: 0.9257 -> 0.9270
Epoch: 52/60
Train loss: 1.4566, val loss: 1.1589
0.926554448565562
Epoch: 53/60
Train loss: 1.3714, val loss: 1.1638
0.9263703953060395
Epoch: 54/60
Train loss: 1.3128, val loss: 1.1655
0.9250666488566506
Epoch: 55/60
Train loss: 1.3636, val loss: 1.1689
0.9255449820910766
Epoch: 56/60
Train loss: 1.3665, val loss: 1.1634
0.9262232966515063
Epoch: 57/60
Train loss: 1.4349, val loss: 1.1626
0.9267162190419993
Epoch: 58/60
Train loss: 1.4441, val loss: 1.1712
0.9251853751043727
Epoch: 59/60
Train loss: 1.3605, val loss: 1.1642
0.9259518466215556
Epoch: 60/60
Train loss: 1.4702, val loss: 1.1664
0.9256236610307398
Date :04/15/2023, 19:44:17
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 9.9190, val loss: 2.9534
0.794003254102293
Model improve: 0.0000 -> 0.7940
Epoch: 2/60
Train loss: 3.7240, val loss: 2.0488
0.8650632091089853
Model improve: 0.7940 -> 0.8651
Epoch: 3/60
Date :04/15/2023, 19:56:56
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 9.9341, val loss: 2.9588
0.7938483150844091
Model improve: 0.0000 -> 0.7938
Epoch: 2/60
Train loss: 3.7243, val loss: 2.0459
0.8651385369577543
Model improve: 0.7938 -> 0.8651
Epoch: 3/60
Train loss: 2.9679, val loss: 1.7889
0.8828955903926148
Model improve: 0.8651 -> 0.8829
Epoch: 4/60
Train loss: 2.7229, val loss: 1.6242
0.8902762725893371
Model improve: 0.8829 -> 0.8903
Epoch: 5/60
Train loss: 2.6006, val loss: 1.5343
0.902237152684891
Model improve: 0.8903 -> 0.9022
Epoch: 6/60
Train loss: 2.4536, val loss: 1.5380
0.9011492405754156
Epoch: 7/60
Train loss: 2.2513, val loss: 1.4891
0.906077001520635
Model improve: 0.9022 -> 0.9061
Epoch: 8/60
Train loss: 2.2209, val loss: 1.4250
0.9081982767700562
Model improve: 0.9061 -> 0.9082
Epoch: 9/60
Train loss: 2.2922, val loss: 1.4739
0.9058906199766915
Epoch: 10/60
Train loss: 2.1113, val loss: 1.3890
0.9090300171826027
Model improve: 0.9082 -> 0.9090
Epoch: 11/60
Train loss: 2.0931, val loss: 1.3636
0.910165363932894
Model improve: 0.9090 -> 0.9102
Epoch: 12/60
Train loss: 2.0762, val loss: 1.3604
0.9115709922208214
Model improve: 0.9102 -> 0.9116
Epoch: 13/60
Train loss: 1.9799, val loss: 1.3746
0.9133498801448455
Model improve: 0.9116 -> 0.9133
Epoch: 14/60
Train loss: 1.9089, val loss: 1.3259
0.9143735837577063
Model improve: 0.9133 -> 0.9144
Epoch: 15/60
Train loss: 1.9624, val loss: 1.2948
0.9164713798621601
Model improve: 0.9144 -> 0.9165
Epoch: 16/60
Train loss: 1.8890, val loss: 1.3510
0.9112982145485972
Epoch: 17/60
Train loss: 1.7779, val loss: 1.3448
0.9145886659362299
Epoch: 18/60
Train loss: 1.8506, val loss: 1.2976
0.9159700940998438
Epoch: 19/60
Train loss: 1.8268, val loss: 1.3007
0.9209031223658178
Model improve: 0.9165 -> 0.9209
Epoch: 20/60
Train loss: 1.9128, val loss: 1.2848
0.9153871311997361
Epoch: 21/60
Train loss: 1.8732, val loss: 1.3226
0.9147833443934325
Epoch: 22/60
Train loss: 1.7353, val loss: 1.2764
0.9162911377872552
Epoch: 23/60
Train loss: 1.6380, val loss: 1.2606
0.9202615017163827
Epoch: 24/60
Train loss: 1.7607, val loss: 1.2738
0.9155792083898261
Epoch: 25/60
Train loss: 1.6789, val loss: 1.2487
0.9191761577771814
Epoch: 26/60
Train loss: 1.6431, val loss: 1.2744
0.9144571072559075
Epoch: 27/60
Train loss: 1.6921, val loss: 1.2432
0.9190337438543743
Epoch: 28/60
Train loss: 1.6296, val loss: 1.2199
0.9185297131543327
Epoch: 29/60
Train loss: 1.6001, val loss: 1.2781
0.9166417187920266
Epoch: 30/60
Train loss: 1.5334, val loss: 1.2265
0.917996154862196
Epoch: 31/60
Train loss: 1.5763, val loss: 1.2400
0.920660936185614
Epoch: 32/60
Train loss: 1.5991, val loss: 1.2460
0.9193334420303636
Epoch: 33/60
Train loss: 1.4496, val loss: 1.2269
0.9201060707367102
Epoch: 34/60
Train loss: 1.4518, val loss: 1.2347
0.9203359113431786
Epoch: 35/60
Train loss: 1.5614, val loss: 1.2021
0.9230274857051994
Model improve: 0.9209 -> 0.9230
Epoch: 36/60
Train loss: 1.4341, val loss: 1.2534
0.9189594498521604
Epoch: 37/60
Train loss: 1.5512, val loss: 1.2438
0.9186535832204689
Epoch: 38/60
Train loss: 1.4821, val loss: 1.2201
0.9197605514233913
Epoch: 39/60
Train loss: 1.4552, val loss: 1.2163
0.9199426026421165
Epoch: 40/60
Train loss: 1.4956, val loss: 1.2206
0.9202922254031772
Epoch: 41/60
Train loss: 1.3515, val loss: 1.2163
0.921103544148856
Epoch: 42/60
Train loss: 1.5199, val loss: 1.2055
0.9208310003576945
Epoch: 43/60
Train loss: 1.4538, val loss: 1.2105
0.9224151929249866
Epoch: 44/60
Train loss: 1.4749, val loss: 1.2019
0.9225258779222895
Epoch: 45/60
Train loss: 1.4842, val loss: 1.1895
0.9228642451328737
Epoch: 46/60
Train loss: 1.3204, val loss: 1.1781
0.9243229973703968
Model improve: 0.9230 -> 0.9243
Epoch: 47/60
Train loss: 1.4191, val loss: 1.1978
0.9235906932389064
Epoch: 48/60
Train loss: 1.5103, val loss: 1.1951
0.9244828434323132
Model improve: 0.9243 -> 0.9245
Epoch: 49/60
Train loss: 1.3885, val loss: 1.1839
0.9244440157667945
Epoch: 50/60
Train loss: 1.3823, val loss: 1.1736
0.9257636186048132
Model improve: 0.9245 -> 0.9258
Epoch: 51/60
Train loss: 1.3722, val loss: 1.1651
0.9264687760888162
Model improve: 0.9258 -> 0.9265
Epoch: 52/60
Train loss: 1.4422, val loss: 1.1617
0.9280679089522482
Model improve: 0.9265 -> 0.9281
Epoch: 53/60
Train loss: 1.3572, val loss: 1.1625
0.9280460520705166
Epoch: 54/60
Train loss: 1.2993, val loss: 1.1686
0.9264717432121142
Epoch: 55/60
Train loss: 1.3509, val loss: 1.1678
0.9269878628561232
Epoch: 56/60
Train loss: 1.3526, val loss: 1.1605
0.9271192281039111
Epoch: 57/60
Train loss: 1.4238, val loss: 1.1604
0.9272339535507537
Epoch: 58/60
Train loss: 1.4281, val loss: 1.1683
0.9264633251127053
Epoch: 59/60
Train loss: 1.3435, val loss: 1.1682
0.9270620910434524
Epoch: 60/60
Train loss: 1.4516, val loss: 1.1663
0.9266067599376284
Date :04/16/2023, 01:06:43
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 10.4269, val loss: 3.0900
0.7877007064244724
Model improve: 0.0000 -> 0.7877
Epoch: 2/60
Train loss: 4.0208, val loss: 2.1413
0.854697372197164
Model improve: 0.7877 -> 0.8547
Epoch: 3/60
Train loss: 3.3188, val loss: 1.8592
0.8825565437642896
Model improve: 0.8547 -> 0.8826
Epoch: 4/60
Train loss: 3.0718, val loss: 1.6623
0.8930882351171928
Model improve: 0.8826 -> 0.8931
Epoch: 5/60
Train loss: 2.9630, val loss: 1.6218
0.8956826819168847
Model improve: 0.8931 -> 0.8957
Epoch: 6/60
Train loss: 2.8025, val loss: 1.5211
0.9043089204663671
Model improve: 0.8957 -> 0.9043
Epoch: 7/60
Train loss: 2.5972, val loss: 1.5226
0.9056638503014148
Model improve: 0.9043 -> 0.9057
Epoch: 8/60
Train loss: 2.5995, val loss: 1.4565
0.9049031554972552
Epoch: 9/60
Date :04/16/2023, 01:48:29
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 10.3378, val loss: 3.6161
0.7336216178475252
Model improve: 0.0000 -> 0.7336
Epoch: 2/60
Train loss: 4.5229, val loss: 2.4997
0.8197372757037655
Model improve: 0.7336 -> 0.8197
Epoch: 3/60
Train loss: 3.8557, val loss: 2.1460
0.852719251974837
Model improve: 0.8197 -> 0.8527
Epoch: 4/60
Train loss: 3.6320, val loss: 1.9781
0.8633492226163846
Model improve: 0.8527 -> 0.8633
Epoch: 5/60
Train loss: 3.4968, val loss: 1.8939
0.8714648235998228
Model improve: 0.8633 -> 0.8715
Epoch: 6/60
Train loss: 3.3651, val loss: 1.8151
0.8795814293733272
Model improve: 0.8715 -> 0.8796
Epoch: 7/60
Train loss: 3.2013, val loss: 1.7791
0.8820052325916352
Model improve: 0.8796 -> 0.8820
Epoch: 8/60
Train loss: 3.1617, val loss: 1.7432
0.8822451603655902
Model improve: 0.8820 -> 0.8822
Epoch: 9/60
Train loss: 3.1784, val loss: 1.7200
0.8879590011110957
Model improve: 0.8822 -> 0.8880
Epoch: 10/60
Train loss: 3.0537, val loss: 1.6390
0.8867707523790522
Epoch: 11/60
Date :04/16/2023, 03:36:27
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 9.8923, val loss: 2.9586
0.7970984925719667
Model improve: 0.0000 -> 0.7971
Epoch: 2/60
Train loss: 3.6954, val loss: 2.0007
0.8670206887317209
Model improve: 0.7971 -> 0.8670
Epoch: 3/60
Train loss: 2.9478, val loss: 1.7602
0.8853693274591402
Model improve: 0.8670 -> 0.8854
Epoch: 4/60
Train loss: 2.6918, val loss: 1.6015
0.8954971899250579
Model improve: 0.8854 -> 0.8955
Epoch: 5/60
Train loss: 2.5427, val loss: 1.5315
0.9021802164100519
Model improve: 0.8955 -> 0.9022
Epoch: 6/60
Train loss: 2.4193, val loss: 1.4700
0.9067487156626988
Model improve: 0.9022 -> 0.9067
Epoch: 7/60
Train loss: 2.1916, val loss: 1.4776
0.9067175120938938
Epoch: 8/60
Train loss: 2.1744, val loss: 1.3900
0.9126021980648428
Model improve: 0.9067 -> 0.9126
Epoch: 9/60
Train loss: 2.2441, val loss: 1.4071
0.9110064027137459
Epoch: 10/60
Train loss: 2.0780, val loss: 1.3588
0.9108204945450289
Epoch: 11/60
Train loss: 2.0662, val loss: 1.4025
0.9117245199734898
Epoch: 12/60
Train loss: 2.0060, val loss: 1.3913
0.9122821263538288
Epoch: 13/60
Train loss: 1.9363, val loss: 1.3513
0.9125751450787862
Epoch: 14/60
Train loss: 1.8445, val loss: 1.3547
0.9131088678376
Model improve: 0.9126 -> 0.9131
Epoch: 15/60
Train loss: 1.9239, val loss: 1.3539
0.9104185698574281
Epoch: 16/60
Train loss: 1.8517, val loss: 1.3804
0.9113977644745841
Epoch: 17/60
Train loss: 1.7348, val loss: 1.3198
0.9130199157374267
Epoch: 18/60
Train loss: 1.8097, val loss: 1.3028
0.9137945372551495
Model improve: 0.9131 -> 0.9138
Epoch: 19/60
Train loss: 1.7999, val loss: 1.3173
0.9163196964126019
Model improve: 0.9138 -> 0.9163
Epoch: 20/60
Train loss: 1.8618, val loss: 1.2767
0.9185436434738116
Model improve: 0.9163 -> 0.9185
Epoch: 21/60
Train loss: 1.8155, val loss: 1.3735
0.9122295886258527
Epoch: 22/60
Train loss: 1.6824, val loss: 1.3015
0.9165365003641286
Epoch: 23/60
Train loss: 1.6142, val loss: 1.3125
0.9152527625563868
Epoch: 24/60
Train loss: 1.7162, val loss: 1.2547
0.9176768906636976
Epoch: 25/60
Train loss: 1.6519, val loss: 1.2743
0.918545743873963
Model improve: 0.9185 -> 0.9185
Epoch: 26/60
Train loss: 1.5740, val loss: 1.2958
0.9173574515142521
Epoch: 27/60
Train loss: 1.6251, val loss: 1.2478
0.921322338457742
Model improve: 0.9185 -> 0.9213
Epoch: 28/60
Train loss: 1.5857, val loss: 1.2511
0.9181167231217983
Epoch: 29/60
Train loss: 1.5478, val loss: 1.2530
0.9183810541993557
Epoch: 30/60
Train loss: 1.4774, val loss: 1.2490
0.9180945817416974
Epoch: 31/60
Train loss: 1.5417, val loss: 1.2566
0.9171743931612163
Epoch: 32/60
Train loss: 1.5399, val loss: 1.2505
0.919383662818127
Epoch: 33/60
Train loss: 1.4085, val loss: 1.2262
0.9190770465879049
Epoch: 34/60
Train loss: 1.4049, val loss: 1.2000
0.9195356490623234
Epoch: 35/60
Train loss: 1.5197, val loss: 1.2059
0.9205676505293341
Epoch: 36/60
Train loss: 1.3849, val loss: 1.2059
0.9191956326853684
Epoch: 37/60
Train loss: 1.5036, val loss: 1.2199
0.920158343737615
Epoch: 38/60
Train loss: 1.4404, val loss: 1.2046
0.9191890925561019
Epoch: 39/60
Train loss: 1.4237, val loss: 1.2163
0.9203613892714356
Epoch: 40/60
Train loss: 1.4513, val loss: 1.2194
0.9187386849515486
Epoch: 41/60
Train loss: 1.3171, val loss: 1.2074
0.9208883819069589
Epoch: 42/60
Train loss: 1.4703, val loss: 1.2019
0.921907089924704
Model improve: 0.9213 -> 0.9219
Epoch: 43/60
Date :04/16/2023, 06:59:49
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
Date :04/16/2023, 07:01:13
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
18770
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 9.7219, val loss: 3.0527
0.7819024355542845
Model improve: 0.0000 -> 0.7819
Epoch: 2/60
Train loss: 3.7611, val loss: 2.1017
0.8527024041233404
Model improve: 0.7819 -> 0.8527
Epoch: 3/60
Train loss: 2.9707, val loss: 1.8245
0.8746294449530815
Model improve: 0.8527 -> 0.8746
Epoch: 4/60
Train loss: 2.7344, val loss: 1.7358
0.8820234774100615
Model improve: 0.8746 -> 0.8820
Epoch: 5/60
Train loss: 2.5646, val loss: 1.6919
0.8862415292205611
Model improve: 0.8820 -> 0.8862
Epoch: 6/60
Train loss: 2.4801, val loss: 1.6493
0.8880139454255112
Model improve: 0.8862 -> 0.8880
Epoch: 7/60
Train loss: 2.2567, val loss: 1.6231
0.8923185902183327
Model improve: 0.8880 -> 0.8923
Epoch: 8/60
Train loss: 2.1950, val loss: 1.5892
0.8945971903301123
Model improve: 0.8923 -> 0.8946
Epoch: 9/60
Train loss: 2.1963, val loss: 1.5731
0.8938321390049457
Epoch: 10/60
Train loss: 2.1559, val loss: 1.6153
0.8972648434820685
Model improve: 0.8946 -> 0.8973
Epoch: 11/60
Train loss: 2.0665, val loss: 1.5863
0.8964949019284407
Epoch: 12/60
Train loss: 2.0044, val loss: 1.4889
0.8974188478255503
Model improve: 0.8973 -> 0.8974
Epoch: 13/60
Train loss: 1.9649, val loss: 1.5430
0.8962828011777512
Epoch: 14/60
Train loss: 1.8267, val loss: 1.5166
0.8977196106805272
Model improve: 0.8974 -> 0.8977
Epoch: 15/60
Train loss: 1.9830, val loss: 1.5660
0.8966099190562391
Epoch: 16/60
Train loss: 1.8777, val loss: 1.5525
0.8953782438661017
Epoch: 17/60
Train loss: 1.7559, val loss: 1.5506
0.8951682207691569
Epoch: 18/60
Train loss: 1.7106, val loss: 1.5249
0.8954983994465935
Epoch: 19/60
Train loss: 1.8715, val loss: 1.4840
0.9011756864408512
Model improve: 0.8977 -> 0.9012
Epoch: 20/60
Train loss: 1.8424, val loss: 1.5321
0.8976978721099371
Epoch: 21/60
Train loss: 1.7873, val loss: 1.5170
0.8981432953698849
Epoch: 22/60
Train loss: 1.8707, val loss: 1.5067
0.8996030665264475
Epoch: 23/60
Date :04/16/2023, 08:46:32
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
18770
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 9.7531, val loss: 3.0937
0.7812558190738923
Model improve: 0.0000 -> 0.7813
Epoch: 2/60
Train loss: 3.8500, val loss: 2.1408
0.8483061357346897
Model improve: 0.7813 -> 0.8483
Epoch: 3/60
Train loss: 3.0663, val loss: 1.8458
0.8732879588861153
Model improve: 0.8483 -> 0.8733
Epoch: 4/60
Train loss: 2.8300, val loss: 1.7324
0.8809824224277113
Model improve: 0.8733 -> 0.8810
Epoch: 5/60
Train loss: 2.6833, val loss: 1.7157
0.8831268969614327
Model improve: 0.8810 -> 0.8831
Epoch: 6/60
Train loss: 2.5976, val loss: 1.6772
0.8899982252819548
Model improve: 0.8831 -> 0.8900
Epoch: 7/60
Train loss: 2.3916, val loss: 1.6669
0.885852669183449
Epoch: 8/60
Train loss: 2.3324, val loss: 1.5968
0.8931778304458282
Model improve: 0.8900 -> 0.8932
Epoch: 9/60
Train loss: 2.3252, val loss: 1.5953
0.8927270952558313
Epoch: 10/60
Train loss: 2.2591, val loss: 1.5901
0.8966414077743394
Model improve: 0.8932 -> 0.8966
Epoch: 11/60
Train loss: 2.1858, val loss: 1.5556
0.8975907072423704
Model improve: 0.8966 -> 0.8976
Epoch: 12/60
Train loss: 2.1406, val loss: 1.5596
0.8948873212667044
Epoch: 13/60
Train loss: 2.0945, val loss: 1.5475
0.898577296537733
Model improve: 0.8976 -> 0.8986
Epoch: 14/60
Train loss: 1.9821, val loss: 1.5170
0.8971503650479855
Epoch: 15/60
Train loss: 2.0991, val loss: 1.5367
0.8984016438451601
Epoch: 16/60
Train loss: 2.0206, val loss: 1.5235
0.9004659356202626
Model improve: 0.8986 -> 0.9005
Epoch: 17/60
Train loss: 1.9024, val loss: 1.4789
0.9005197932721779
Model improve: 0.9005 -> 0.9005
Epoch: 18/60
Train loss: 1.8343, val loss: 1.4980
0.8966591659053019
Epoch: 19/60
Train loss: 1.9764, val loss: 1.4843
0.899537331950468
Epoch: 20/60
Date :04/16/2023, 10:22:51
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
18770
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 10.3465, val loss: 3.1253
0.7758833939907612
Model improve: 0.0000 -> 0.7759
Epoch: 2/60
Train loss: 3.7878, val loss: 2.1257
0.8497755504459058
Model improve: 0.7759 -> 0.8498
Epoch: 3/60
Train loss: 3.0116, val loss: 1.8845
0.8672371788884243
Model improve: 0.8498 -> 0.8672
Epoch: 4/60
Train loss: 2.7578, val loss: 1.7236
0.8814552388238924
Model improve: 0.8672 -> 0.8815
Epoch: 5/60
Train loss: 2.5984, val loss: 1.6343
0.8858872843458077
Model improve: 0.8815 -> 0.8859
Epoch: 6/60
Train loss: 2.4889, val loss: 1.6338
0.8934506579542414
Model improve: 0.8859 -> 0.8935
Epoch: 7/60
Train loss: 2.2985, val loss: 1.5522
0.8912136150328407
Epoch: 8/60
Train loss: 2.2375, val loss: 1.5551
0.894940236618948
Model improve: 0.8935 -> 0.8949
Epoch: 9/60
Train loss: 2.2215, val loss: 1.5616
0.8937005111644215
Epoch: 10/60
Train loss: 2.1864, val loss: 1.5866
0.8970122414010869
Model improve: 0.8949 -> 0.8970
Epoch: 11/60
Train loss: 2.1147, val loss: 1.5074
0.8999022156063209
Model improve: 0.8970 -> 0.8999
Epoch: 12/60
Train loss: 2.0499, val loss: 1.4758
0.9002264747212995
Model improve: 0.8999 -> 0.9002
Epoch: 13/60
Train loss: 2.0054, val loss: 1.5334
0.8978572917779797
Epoch: 14/60
Train loss: 1.8739, val loss: 1.4652
0.8995420440708809
Epoch: 15/60
Train loss: 2.0175, val loss: 1.4937
0.8995542807690957
Epoch: 16/60
Train loss: 1.9266, val loss: 1.5040
0.8997256673740219
Epoch: 17/60
Train loss: 1.7973, val loss: 1.5468
0.8954687924178127
Epoch: 18/60
Train loss: 1.7471, val loss: 1.4921
0.8978531086326488
Epoch: 19/60
Train loss: 1.8999, val loss: 1.5130
0.8985775559652488
Epoch: 20/60
Train loss: 1.8776, val loss: 1.5052
0.8989684556298888
Epoch: 21/60
Train loss: 1.8298, val loss: 1.4807
0.8989894967505468
Epoch: 22/60
Train loss: 1.8983, val loss: 1.4770
0.89938763414509
Epoch: 23/60
Train loss: 1.6346, val loss: 1.4654
0.9028246294829961
Model improve: 0.9002 -> 0.9028
Epoch: 24/60
Train loss: 1.6560, val loss: 1.4282
0.9051972935723893
Model improve: 0.9028 -> 0.9052
Epoch: 25/60
Train loss: 1.7270, val loss: 1.4682
0.9021147527248466
Epoch: 26/60
Train loss: 1.6732, val loss: 1.4489
0.9013854652433889
Epoch: 27/60
Train loss: 1.6563, val loss: 1.4544
0.9016254733837562
Epoch: 28/60
Date :04/16/2023, 12:32:09
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 3
18770
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 10.0759, val loss: 3.0816
0.7802001008644577
Model improve: 0.0000 -> 0.7802
Epoch: 2/60
Train loss: 3.8120, val loss: 2.1427
0.8498748619499201
Model improve: 0.7802 -> 0.8499
Epoch: 3/60
Train loss: 3.0072, val loss: 1.8799
0.8706484193932926
Model improve: 0.8499 -> 0.8706
Epoch: 4/60
Train loss: 2.7712, val loss: 1.7492
0.8824804813855602
Model improve: 0.8706 -> 0.8825
Epoch: 5/60
Train loss: 2.6268, val loss: 1.6987
0.8847052705626087
Model improve: 0.8825 -> 0.8847
Epoch: 6/60
Train loss: 2.5013, val loss: 1.6886
0.889117994336598
Model improve: 0.8847 -> 0.8891
Epoch: 7/60
Train loss: 2.3115, val loss: 1.6205
0.8890607648028634
Epoch: 8/60
Date :04/16/2023, 20:51:42
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 3
18770
Date :04/16/2023, 20:52:50
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 3
18770
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/20
Train loss: 10.0784, val loss: 3.0858
0.7800431215127149
Model improve: 0.0000 -> 0.7800
Epoch: 2/20
Train loss: 3.8126, val loss: 2.1445
0.8502287083363843
Model improve: 0.7800 -> 0.8502
Epoch: 3/20
Train loss: 3.0055, val loss: 1.8810
0.8706239446267351
Model improve: 0.8502 -> 0.8706
Epoch: 4/20
Train loss: 2.7643, val loss: 1.7224
0.8830464502171266
Model improve: 0.8706 -> 0.8830
Epoch: 5/20
Train loss: 2.6140, val loss: 1.6640
0.8855342146411772
Model improve: 0.8830 -> 0.8855
Epoch: 6/20
Train loss: 2.4794, val loss: 1.6426
0.8911201601458261
Model improve: 0.8855 -> 0.8911
Epoch: 7/20
Train loss: 2.2683, val loss: 1.5772
0.893696534402391
Model improve: 0.8911 -> 0.8937
Epoch: 8/20
Train loss: 2.1970, val loss: 1.5258
0.8983460335245906
Model improve: 0.8937 -> 0.8983
Epoch: 9/20
Train loss: 2.1818, val loss: 1.5288
0.8980564920821429
Epoch: 10/20
Train loss: 2.1276, val loss: 1.5412
0.8992344928054227
Model improve: 0.8983 -> 0.8992
Epoch: 11/20
Train loss: 2.0215, val loss: 1.4962
0.9039313920480049
Model improve: 0.8992 -> 0.9039
Epoch: 12/20
Train loss: 1.9546, val loss: 1.4527
0.9025882460185356
Epoch: 13/20
Train loss: 1.8950, val loss: 1.4451
0.9034295667969634
Epoch: 14/20
Train loss: 1.7552, val loss: 1.3929
0.9034049472430683
Epoch: 15/20
Train loss: 1.9074, val loss: 1.4236
0.9051545711550464
Model improve: 0.9039 -> 0.9052
Epoch: 16/20
Train loss: 1.8120, val loss: 1.4158
0.9052319579633623
Model improve: 0.9052 -> 0.9052
Epoch: 17/20
Train loss: 1.6978, val loss: 1.3845
0.9053119419318858
Model improve: 0.9052 -> 0.9053
Epoch: 18/20
Train loss: 1.6610, val loss: 1.3920
0.9049549151293019
Epoch: 19/20
Train loss: 1.8252, val loss: 1.4121
0.9060682053028106
Model improve: 0.9053 -> 0.9061
Epoch: 20/20
Train loss: 1.8182, val loss: 1.4002
0.9053234656117233
Date :04/16/2023, 23:23:37
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 70
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
23684
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/20
Train loss: 9.8061, val loss: 2.6958
0.8170377562740284
Model improve: 0.0000 -> 0.8170
Epoch: 2/20
Train loss: 3.5140, val loss: 1.8933
0.8738170679723066
Model improve: 0.8170 -> 0.8738
Epoch: 3/20
Train loss: 2.8996, val loss: 1.6687
0.88988707587289
Model improve: 0.8738 -> 0.8899
Epoch: 4/20
Train loss: 2.6290, val loss: 1.5911
0.8966246263081795
Model improve: 0.8899 -> 0.8966
Epoch: 5/20
Train loss: 2.4902, val loss: 1.4999
0.8995996756494693
Model improve: 0.8966 -> 0.8996
Epoch: 6/20
Train loss: 2.2372, val loss: 1.4539
0.9038081171400797
Model improve: 0.8996 -> 0.9038
Epoch: 7/20
Train loss: 2.1781, val loss: 1.3958
0.9067173660570148
Model improve: 0.9038 -> 0.9067
Epoch: 8/20
Train loss: 2.1000, val loss: 1.3362
0.9108765405002958
Model improve: 0.9067 -> 0.9109
Epoch: 9/20
Train loss: 2.0772, val loss: 1.3603
0.9127902478288219
Model improve: 0.9109 -> 0.9128
Epoch: 10/20
Train loss: 1.9900, val loss: 1.2920
0.9136435648705054
Model improve: 0.9128 -> 0.9136
Epoch: 11/20
Train loss: 1.8542, val loss: 1.2879
0.9124961975320702
Epoch: 12/20
Date :04/17/2023, 00:27:44
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 70
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
23684
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/20
Train loss: 9.2513, val loss: 2.6876
0.8181911078556
Model improve: 0.0000 -> 0.8182
Epoch: 2/20
Train loss: 3.4741, val loss: 1.8595
0.8732742520260403
Model improve: 0.8182 -> 0.8733
Epoch: 3/20
Train loss: 2.8590, val loss: 1.6761
0.888714295297899
Model improve: 0.8733 -> 0.8887
Epoch: 4/20
Train loss: 2.6065, val loss: 1.5874
0.897877042725979
Model improve: 0.8887 -> 0.8979
Epoch: 5/20
Train loss: 2.4827, val loss: 1.5095
0.8979907624351653
Model improve: 0.8979 -> 0.8980
Epoch: 6/20
Train loss: 2.2306, val loss: 1.4748
0.9044865801037894
Model improve: 0.8980 -> 0.9045
Epoch: 7/20
Train loss: 2.1795, val loss: 1.3984
0.905436962228739
Model improve: 0.9045 -> 0.9054
Epoch: 8/20
Date :04/17/2023, 01:10:39
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/20
Date :04/17/2023, 01:11:35
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
19629
Date :04/17/2023, 01:25:45
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/20
Train loss: 10.3759, val loss: 3.0759
0.7897486968788135
Model improve: 0.0000 -> 0.7897
Epoch: 2/20
Train loss: 3.7841, val loss: 2.0123
0.866865155957487
Model improve: 0.7897 -> 0.8669
Epoch: 3/20
Train loss: 3.0041, val loss: 1.7263
0.8868150749196708
Model improve: 0.8669 -> 0.8868
Epoch: 4/20
Train loss: 2.7185, val loss: 1.5577
0.8951984783674827
Model improve: 0.8868 -> 0.8952
Epoch: 5/20
Train loss: 2.5833, val loss: 1.4839
0.903522997789875
Model improve: 0.8952 -> 0.9035
Epoch: 6/20
Train loss: 2.4196, val loss: 1.4215
0.9062301168736205
Model improve: 0.9035 -> 0.9062
Epoch: 7/20
Train loss: 2.1711, val loss: 1.4164
0.9067983630173263
Model improve: 0.9062 -> 0.9068
Epoch: 8/20
Train loss: 2.1697, val loss: 1.3424
0.910042977586478
Model improve: 0.9068 -> 0.9100
Epoch: 9/20
Train loss: 2.2364, val loss: 1.3376
0.9122927184162384
Model improve: 0.9100 -> 0.9123
Epoch: 10/20
Train loss: 2.0186, val loss: 1.2703
0.912214481263779
Epoch: 11/20
Train loss: 1.9921, val loss: 1.2848
0.914275892885547
Model improve: 0.9123 -> 0.9143
Epoch: 12/20
Train loss: 1.9589, val loss: 1.2490
0.9159801521690804
Model improve: 0.9143 -> 0.9160
Epoch: 13/20
Train loss: 1.8469, val loss: 1.2392
0.9166145914570842
Model improve: 0.9160 -> 0.9166
Epoch: 14/20
Train loss: 1.7880, val loss: 1.2258
0.9182733078658866
Model improve: 0.9166 -> 0.9183
Epoch: 15/20
Train loss: 1.8479, val loss: 1.1927
0.9182492399424717
Epoch: 16/20
Train loss: 1.7766, val loss: 1.2129
0.918552288456629
Model improve: 0.9183 -> 0.9186
Epoch: 17/20
Train loss: 1.6758, val loss: 1.2009
0.9187229550299604
Model improve: 0.9186 -> 0.9187
Epoch: 18/20
Train loss: 1.7252, val loss: 1.2032
0.919720429238246
Model improve: 0.9187 -> 0.9197
Epoch: 19/20
Train loss: 1.8183, val loss: 1.1928
0.9193746669045147
Epoch: 20/20
Train loss: 1.8484, val loss: 1.2108
0.919808081648229
Model improve: 0.9197 -> 0.9198
Date :04/17/2023, 03:03:38
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Train loss: 10.5150, val loss: 3.0685
0.7904874624040416
Model improve: 0.0000 -> 0.7905
Epoch: 2/25
Train loss: 3.7811, val loss: 1.9965
0.8659295805731395
Model improve: 0.7905 -> 0.8659
Epoch: 3/25
Train loss: 3.0137, val loss: 1.7443
0.8855461965641711
Model improve: 0.8659 -> 0.8855
Epoch: 4/25
Train loss: 2.7154, val loss: 1.5416
0.8948753026309229
Model improve: 0.8855 -> 0.8949
Epoch: 5/25
Train loss: 2.5845, val loss: 1.4845
0.9027616742411195
Model improve: 0.8949 -> 0.9028
Epoch: 6/25
Train loss: 2.4259, val loss: 1.4224
0.9053438021767873
Model improve: 0.9028 -> 0.9053
Epoch: 7/25
Train loss: 2.1779, val loss: 1.4306
0.9067926364649305
Model improve: 0.9053 -> 0.9068
Epoch: 8/25
Train loss: 2.1788, val loss: 1.3628
0.9077868299377205
Model improve: 0.9068 -> 0.9078
Epoch: 9/25
Train loss: 2.2516, val loss: 1.3723
0.9120609165890882
Model improve: 0.9078 -> 0.9121
Epoch: 10/25
Train loss: 2.0433, val loss: 1.3348
0.9105766393183483
Epoch: 11/25
Train loss: 2.0121, val loss: 1.3284
0.9100876102575401
Epoch: 12/25
Train loss: 1.9764, val loss: 1.2894
0.9125824418849261
Model improve: 0.9121 -> 0.9126
Epoch: 13/25
Train loss: 1.8614, val loss: 1.3044
0.913746342073311
Model improve: 0.9126 -> 0.9137
Epoch: 14/25
Train loss: 1.8038, val loss: 1.2700
0.9158089623071032
Model improve: 0.9137 -> 0.9158
Epoch: 15/25
Train loss: 1.8603, val loss: 1.2189
0.91536063326933
Epoch: 16/25
Train loss: 1.7767, val loss: 1.2699
0.9154067323260922
Epoch: 17/25
Train loss: 1.6646, val loss: 1.2374
0.9161758084737677
Model improve: 0.9158 -> 0.9162
Epoch: 18/25
Train loss: 1.6998, val loss: 1.2094
0.9174809920899637
Model improve: 0.9162 -> 0.9175
Epoch: 19/25
Train loss: 1.7797, val loss: 1.2048
0.9180351462423058
Model improve: 0.9175 -> 0.9180
Epoch: 20/25
Train loss: 1.7925, val loss: 1.2221
0.9190927211131562
Model improve: 0.9180 -> 0.9191
Epoch: 21/25
Train loss: 1.7655, val loss: 1.1918
0.919709776841311
Model improve: 0.9191 -> 0.9197
Epoch: 22/25
Train loss: 1.5723, val loss: 1.1940
0.9193782535245686
Epoch: 23/25
Train loss: 1.6167, val loss: 1.2142
0.919933582582655
Model improve: 0.9197 -> 0.9199
Epoch: 24/25
Train loss: 1.6386, val loss: 1.2010
0.919160007895241
Epoch: 25/25
Train loss: 1.6453, val loss: 1.1827
0.9197841595855312
Date :04/17/2023, 05:45:29
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Train loss: 10.2188, val loss: 2.7818
0.8018937904564325
Model improve: 0.0000 -> 0.8019
Epoch: 2/25
Train loss: 3.2147, val loss: 1.8941
0.8695745834292912
Model improve: 0.8019 -> 0.8696
Epoch: 3/25
Train loss: 2.5949, val loss: 1.6240
0.8865824908124004
Model improve: 0.8696 -> 0.8866
Epoch: 4/25
Date :04/17/2023, 06:03:22
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Train loss: 10.1928, val loss: 2.7728
0.8012671653774048
Model improve: 0.0000 -> 0.8013
Epoch: 2/25
Train loss: 3.2176, val loss: 1.8919
0.869752961882403
Model improve: 0.8013 -> 0.8698
Epoch: 3/25
Train loss: 2.6006, val loss: 1.6382
0.8862365723247304
Model improve: 0.8698 -> 0.8862
Epoch: 4/25
Train loss: 2.3302, val loss: 1.5751
0.8924680371774898
Model improve: 0.8862 -> 0.8925
Epoch: 5/25
Train loss: 2.0356, val loss: 1.4719
0.9011661049497395
Model improve: 0.8925 -> 0.9012
Epoch: 6/25
Train loss: 2.0048, val loss: 1.4449
0.9025123220838005
Model improve: 0.9012 -> 0.9025
Epoch: 7/25
Train loss: 1.8373, val loss: 1.4176
0.903769153216423
Model improve: 0.9025 -> 0.9038
Epoch: 8/25
Train loss: 1.7253, val loss: 1.3970
0.9047539538880719
Model improve: 0.9038 -> 0.9048
Epoch: 9/25
Date :04/17/2023, 06:44:27
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Train loss: 10.2178, val loss: 2.7833
0.8021629848387514
Model improve: 0.0000 -> 0.8022
Epoch: 2/25
Train loss: 2.3278, val loss: 1.5524
0.89436135039894
Model improve: 0.8869 -> 0.8944
Epoch: 5/25
Train loss: 2.0270, val loss: 1.4563
0.901437569462232
Model improve: 0.8944 -> 0.9014
Epoch: 6/25
Train loss: 1.9985, val loss: 1.4404
0.9017018926570095
Model improve: 0.9014 -> 0.9017
Epoch: 7/25
Train loss: 1.8352, val loss: 1.4453
0.9029326887434784
Model improve: 0.9017 -> 0.9029
Epoch: 8/25
Train loss: 1.7271, val loss: 1.4045
0.904309986024566
Model improve: 0.9029 -> 0.9043
Epoch: 9/25
Train loss: 1.7707, val loss: 1.3997
0.9050141295970421
Model improve: 0.9043 -> 0.9050
Epoch: 10/25
Train loss: 1.6167, val loss: 1.3516
0.9103438193923127
Model improve: 0.9050 -> 0.9103
Epoch: 11/25
Train loss: 1.5111, val loss: 1.3466
0.907488334937241
Epoch: 12/25
Train loss: 1.4574, val loss: 1.3023
0.908534842021075
Epoch: 13/25
Train loss: 1.4368, val loss: 1.2820
0.9106560960103136
Model improve: 0.9103 -> 0.9107
Epoch: 14/25
Train loss: 1.3679, val loss: 1.2851
0.9118592607936966
Model improve: 0.9107 -> 0.9119
Epoch: 15/25
Train loss: 1.4639, val loss: 1.2742
0.9108038510476797
Epoch: 16/25
Train loss: 1.3851, val loss: 1.2675
0.9120673901310767
Model improve: 0.9119 -> 0.9121
Epoch: 17/25
Train loss: 1.4157, val loss: 1.2577
0.9144720904581921
Model improve: 0.9121 -> 0.9145
Epoch: 18/25
Train loss: 1.2642, val loss: 1.2315
0.9141781103245701
Epoch: 19/25
Train loss: 1.2006, val loss: 1.2501
0.9130467846503315
Epoch: 20/25
Date :04/17/2023, 08:46:31
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13524
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/60
Train loss: 12.1748, val loss: 3.3670
0.7259319683410262
Model improve: 0.0000 -> 0.7259
Epoch: 2/60
Train loss: 3.3855, val loss: 2.2994
0.8178670517016079
Model improve: 0.7259 -> 0.8179
Epoch: 3/60
Train loss: 2.8209, val loss: 1.9889
0.8444602287717048
Model improve: 0.8179 -> 0.8445
Epoch: 4/60
Train loss: 2.5486, val loss: 1.8412
0.8582199448922747
Model improve: 0.8445 -> 0.8582
Epoch: 5/60
Date :04/17/2023, 09:03:33
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13524
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 12.1463, val loss: 3.3586
0.7262899034535333
Model improve: 0.0000 -> 0.7263
Epoch: 2/60
Train loss: 3.3844, val loss: 2.3014
0.8178556844299301
Model improve: 0.7263 -> 0.8179
Epoch: 3/60
Train loss: 2.8242, val loss: 1.9907
0.8436334857594838
Model improve: 0.8179 -> 0.8436
Epoch: 4/60
Train loss: 2.5544, val loss: 1.8465
0.8580713127108065
Model improve: 0.8436 -> 0.8581
Epoch: 5/60
Train loss: 2.4426, val loss: 1.7150
0.8732894116522367
Model improve: 0.8581 -> 0.8733
Epoch: 6/60
Train loss: 2.1926, val loss: 1.6463
0.8762164230396131
Model improve: 0.8733 -> 0.8762
Epoch: 7/60
Train loss: 2.0997, val loss: 1.6452
0.8787986970031618
Model improve: 0.8762 -> 0.8788
Epoch: 8/60
Train loss: 2.0650, val loss: 1.5901
0.8837981239426969
Model improve: 0.8788 -> 0.8838
Epoch: 9/60
Train loss: 1.9597, val loss: 1.5710
0.8822799912999367
Epoch: 10/60
Train loss: 1.8558, val loss: 1.5560
0.8858401118771851
Model improve: 0.8838 -> 0.8858
Epoch: 11/60
Train loss: 1.7492, val loss: 1.6005
0.8845689144699777
Epoch: 12/60
Train loss: 1.8037, val loss: 1.5667
0.890476083974576
Model improve: 0.8858 -> 0.8905
Epoch: 13/60
Train loss: 1.7781, val loss: 1.5052
0.8894965986253571
Epoch: 14/60
Train loss: 1.6623, val loss: 1.5408
0.8916975606592761
Model improve: 0.8905 -> 0.8917
Epoch: 15/60
Train loss: 1.6742, val loss: 1.5564
0.8897890038174335
Epoch: 16/60
Train loss: 1.5603, val loss: 1.5106
0.8923583427940766
Model improve: 0.8917 -> 0.8924
Epoch: 17/60
Train loss: 1.5622, val loss: 1.5022
0.8930106144187525
Model improve: 0.8924 -> 0.8930
Epoch: 18/60
Train loss: 1.5073, val loss: 1.4749
0.8942935999689509
Model improve: 0.8930 -> 0.8943
Epoch: 19/60
Train loss: 1.4854, val loss: 1.5288
0.8922186035967209
Epoch: 20/60
Train loss: 1.4333, val loss: 1.5186
0.8934421552927757
Epoch: 21/60
Train loss: 1.4710, val loss: 1.4948
0.895472760540684
Model improve: 0.8943 -> 0.8955
Epoch: 22/60
Train loss: 1.4977, val loss: 1.4833
0.8922781744533104
Epoch: 23/60
Train loss: 1.4738, val loss: 1.4914
0.8935023128919197
Epoch: 24/60
Train loss: 1.4547, val loss: 1.5054
0.8931129201423413
Epoch: 25/60
Train loss: 1.4848, val loss: 1.4906
0.8955924327158565
Model improve: 0.8955 -> 0.8956
Epoch: 26/60
Train loss: 1.2867, val loss: 1.5052
0.8914471732450835
Epoch: 27/60
Train loss: 1.3045, val loss: 1.4943
0.8955037412559066
Epoch: 28/60
Train loss: 1.2052, val loss: 1.4903
0.8942136265208422
Epoch: 29/60
Train loss: 1.3201, val loss: 1.4652
0.8974674857359499
Model improve: 0.8956 -> 0.8975
Epoch: 30/60
Train loss: 1.2818, val loss: 1.4765
0.897760179253219
Model improve: 0.8975 -> 0.8978
Epoch: 31/60
Train loss: 1.1844, val loss: 1.4587
0.8969248273365129
Epoch: 32/60
Train loss: 1.3330, val loss: 1.4824
0.8970816890219389
Epoch: 33/60
Train loss: 1.1403, val loss: 1.4758
0.8973880093491413
Epoch: 34/60
Train loss: 1.3103, val loss: 1.4322
0.8991718382071626
Model improve: 0.8978 -> 0.8992
Epoch: 35/60
Train loss: 1.2552, val loss: 1.4799
0.8975285545526451
Epoch: 36/60
Train loss: 1.2079, val loss: 1.4513
0.897604558652811
Epoch: 37/60
Train loss: 1.1929, val loss: 1.4660
0.898914795818892
Epoch: 38/60
Train loss: 1.1610, val loss: 1.4450
0.8987699845201736
Epoch: 39/60
Train loss: 1.1814, val loss: 1.4424
0.9010940268585678
Model improve: 0.8992 -> 0.9011
Epoch: 40/60
Train loss: 1.2487, val loss: 1.4226
0.9024959538154691
Model improve: 0.9011 -> 0.9025
Epoch: 41/60
Train loss: 1.2332, val loss: 1.4210
0.9019002412241972
Epoch: 42/60
Train loss: 1.2281, val loss: 1.4286
0.90079526455173
Epoch: 43/60
Train loss: 1.1865, val loss: 1.4237
0.9006631708335748
Epoch: 44/60
Train loss: 1.1058, val loss: 1.4276
0.9016596121190364
Epoch: 45/60
Train loss: 1.2454, val loss: 1.4276
0.9020758652978105
Epoch: 46/60
Train loss: 1.0863, val loss: 1.4202
0.9016431042928734
Epoch: 47/60
Train loss: 1.0445, val loss: 1.4179
0.9015104009791062
Epoch: 48/60
Train loss: 1.2637, val loss: 1.4185
0.9015017687139482
Epoch: 49/60
Train loss: 1.1222, val loss: 1.4153
0.9007380185494726
Epoch: 50/60
Train loss: 1.2005, val loss: 1.4155
0.9025598572950705
Model improve: 0.9025 -> 0.9026
Epoch: 51/60
Train loss: 1.1509, val loss: 1.4176
0.9012137186463893
Epoch: 52/60
Train loss: 1.1554, val loss: 1.4188
0.9027588003080246
Model improve: 0.9026 -> 0.9028
Epoch: 53/60
Train loss: 1.0670, val loss: 1.4132
0.9024356366336791
Epoch: 54/60
Train loss: 1.1007, val loss: 1.4235
0.9012289240754732
Epoch: 55/60
Train loss: 1.0632, val loss: 1.4138
0.9019820566895801
Epoch: 56/60
Train loss: 1.0983, val loss: 1.4084
0.9020759996554724
Epoch: 57/60
Train loss: 1.0868, val loss: 1.4130
0.9016092270843933
Epoch: 58/60
Train loss: 1.1160, val loss: 1.4154
0.9018071595261985
Epoch: 59/60
Train loss: 1.0501, val loss: 1.4291
0.9006861492502187
Epoch: 60/60
Train loss: 1.0839, val loss: 1.4114
0.9025718539792742
Date :04/17/2023, 20:00:39
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13524
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 12.7016, val loss: 3.3700
0.7294121925212869
Model improve: 0.0000 -> 0.7294
Epoch: 2/60
Train loss: 3.3886, val loss: 2.3094
0.8134714808193041
Model improve: 0.7294 -> 0.8135
Epoch: 3/60
Train loss: 2.8525, val loss: 1.9950
0.8454821655486042
Model improve: 0.8135 -> 0.8455
Epoch: 4/60
Train loss: 2.5560, val loss: 1.8565
0.860376361285697
Model improve: 0.8455 -> 0.8604
Epoch: 5/60
Train loss: 2.4515, val loss: 1.7289
0.8723690263494244
Model improve: 0.8604 -> 0.8724
Epoch: 6/60
Train loss: 2.1797, val loss: 1.6832
0.8762152992259102
Model improve: 0.8724 -> 0.8762
Epoch: 7/60
Train loss: 2.1338, val loss: 1.6671
0.878437559869651
Model improve: 0.8762 -> 0.8784
Epoch: 8/60
Train loss: 2.0618, val loss: 1.6259
0.8820144473632548
Model improve: 0.8784 -> 0.8820
Epoch: 9/60
Train loss: 1.9720, val loss: 1.5971
0.882395814009231
Model improve: 0.8820 -> 0.8824
Epoch: 10/60
Train loss: 1.8479, val loss: 1.5975
0.8835345420236838
Model improve: 0.8824 -> 0.8835
Epoch: 11/60
Train loss: 1.7221, val loss: 1.5658
0.888147168824547
Model improve: 0.8835 -> 0.8881
Epoch: 12/60
Train loss: 1.8226, val loss: 1.5951
0.8860858884463696
Epoch: 13/60
Train loss: 1.7889, val loss: 1.5290
0.8892136057468669
Model improve: 0.8881 -> 0.8892
Epoch: 14/60
Train loss: 1.6451, val loss: 1.5430
0.8909088363732417
Model improve: 0.8892 -> 0.8909
Epoch: 15/60
Train loss: 1.6784, val loss: 1.5271
0.8925166125152404
Model improve: 0.8909 -> 0.8925
Epoch: 16/60
Train loss: 1.5171, val loss: 1.5487
0.8905410026874889
Epoch: 17/60
Train loss: 1.5291, val loss: 1.5194
0.8911074078208804
Epoch: 18/60
Train loss: 1.5241, val loss: 1.5469
0.8899256434462154
Epoch: 19/60
Train loss: 1.5182, val loss: 1.5458
0.890696515372029
Epoch: 20/60
Train loss: 1.4250, val loss: 1.5762
0.8904269856551019
Epoch: 21/60
Train loss: 1.4600, val loss: 1.4959
0.8951102506357856
Model improve: 0.8925 -> 0.8951
Epoch: 22/60
Train loss: 1.4982, val loss: 1.4745
0.8936852965678566
Epoch: 23/60
Train loss: 1.4903, val loss: 1.5087
0.8924311609605474
Epoch: 24/60
Train loss: 1.4612, val loss: 1.5099
0.8951139245517198
Model improve: 0.8951 -> 0.8951
Epoch: 25/60
Train loss: 1.4619, val loss: 1.5132
0.8955395189708582
Model improve: 0.8951 -> 0.8955
Epoch: 26/60
Train loss: 1.2643, val loss: 1.5336
0.892682653923199
Epoch: 27/60
Train loss: 1.3083, val loss: 1.5264
0.8962653708313606
Model improve: 0.8955 -> 0.8963
Epoch: 28/60
Train loss: 1.2091, val loss: 1.4813
0.8957547204023222
Epoch: 29/60
Train loss: 1.3173, val loss: 1.5251
0.8944866930179891
Epoch: 30/60
Train loss: 1.2904, val loss: 1.5144
0.8954845417091524
Epoch: 31/60
Train loss: 1.1889, val loss: 1.4915
0.8953467049287545
Epoch: 32/60
Train loss: 1.3266, val loss: 1.5091
0.8960941994633191
Epoch: 33/60
Train loss: 1.1306, val loss: 1.5324
0.8953555285195983
Epoch: 34/60
Train loss: 1.3093, val loss: 1.4848
0.8980440815866001
Model improve: 0.8963 -> 0.8980
Epoch: 35/60
Train loss: 1.2582, val loss: 1.4902
0.8969661243401003
Epoch: 36/60
Train loss: 1.1860, val loss: 1.5161
0.8951741282102977
Epoch: 37/60
Train loss: 1.2046, val loss: 1.4761
0.898396064624484
Model improve: 0.8980 -> 0.8984
Epoch: 38/60
Train loss: 1.1570, val loss: 1.4809
0.8967923364783595
Epoch: 39/60
Train loss: 1.1712, val loss: 1.4611
0.9005574101488166
Model improve: 0.8984 -> 0.9006
Epoch: 40/60
Train loss: 1.2285, val loss: 1.4565
0.8996669695515421
Epoch: 41/60
Train loss: 1.2341, val loss: 1.4573
0.9007253498753488
Model improve: 0.9006 -> 0.9007
Epoch: 42/60
Train loss: 1.2091, val loss: 1.4645
0.8994194037218448
Epoch: 43/60
Train loss: 1.1801, val loss: 1.4585
0.9008152047054221
Model improve: 0.9007 -> 0.9008
Epoch: 44/60
Train loss: 1.1171, val loss: 1.4453
0.9007290185377155
Epoch: 45/60
Train loss: 1.2500, val loss: 1.4365
0.9012815890402591
Model improve: 0.9008 -> 0.9013
Epoch: 46/60
Train loss: 1.1055, val loss: 1.4503
0.9016420427354559
Model improve: 0.9013 -> 0.9016
Epoch: 47/60
Train loss: 1.0379, val loss: 1.4553
0.9000022630925417
Epoch: 48/60
Train loss: 1.2538, val loss: 1.4465
0.9022896818532173
Model improve: 0.9016 -> 0.9023
Epoch: 49/60
Train loss: 1.1166, val loss: 1.4359
0.9015148049216127
Epoch: 50/60
Train loss: 1.2070, val loss: 1.4407
0.903077539619709
Model improve: 0.9023 -> 0.9031
Epoch: 51/60
Train loss: 1.1432, val loss: 1.4474
0.902633766064687
Epoch: 52/60
Train loss: 1.1495, val loss: 1.4452
0.9031160422044385
Model improve: 0.9031 -> 0.9031
Epoch: 53/60
Train loss: 1.0543, val loss: 1.4373
0.9026077511215085
Epoch: 54/60
Train loss: 1.1219, val loss: 1.4666
0.9019589335697412
Epoch: 55/60
Train loss: 1.0539, val loss: 1.4361
0.9032342201990057
Model improve: 0.9031 -> 0.9032
Epoch: 56/60
Train loss: 1.1159, val loss: 1.4300
0.9035503773640957
Model improve: 0.9032 -> 0.9036
Epoch: 57/60
Train loss: 1.0929, val loss: 1.4365
0.903214321735909
Epoch: 58/60
Date :04/17/2023, 23:30:29
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
19629
Date :04/17/2023, 23:30:48
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
19629
Date :04/17/2023, 23:31:24
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :04/17/2023, 23:32:01
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.0001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0001
    lr: 0.0001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 1.5351, val loss: 0.2436
0.9984435499939637
Model improve: 0.0000 -> 0.9984
Epoch: 2/60
Date :04/17/2023, 23:37:13
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.0001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0001
    lr: 0.0001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 1.1743, val loss: 1.1713
0.9282284782805028
Model improve: 0.0000 -> 0.9282
Epoch: 2/60
Train loss: 1.1222, val loss: 1.1900
0.9299307422234673
Model improve: 0.9282 -> 0.9299
Epoch: 3/60
Train loss: 1.2147, val loss: 1.1623
0.9282454142185959
Epoch: 4/60
Train loss: 1.2648, val loss: 1.1654
0.9278619360224126
Epoch: 5/60
Train loss: 1.1568, val loss: 1.1859
0.9284549966783306
Epoch: 6/60
Date :04/18/2023, 00:04:31
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.0001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0001
    lr: 0.0001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 1.2773, val loss: 1.2052
0.9247877998775688
Model improve: 0.0000 -> 0.9248
Epoch: 2/60
Train loss: 1.1947, val loss: 1.2229
0.9272169374141384
Model improve: 0.9248 -> 0.9272
Epoch: 3/60
Train loss: 1.2794, val loss: 1.1884
0.9265388852994647
Epoch: 4/60
Train loss: 1.3229, val loss: 1.1860
0.925918722995606
Epoch: 5/60
Train loss: 1.2118, val loss: 1.2070
0.9262363898002647
Epoch: 6/60
Date :04/18/2023, 00:31:33
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.0001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
50
Date :04/18/2023, 00:32:06
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.0001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
138
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0001
    lr: 0.0001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 183.1748, val loss: 182.8590
0.9993881101362161
Model improve: 0.0000 -> 0.9994
Epoch: 2/60
Date :04/18/2023, 00:33:31
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.0001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0001
    lr: 0.0001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :04/18/2023, 00:34:10
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 10.3037, val loss: 2.8249
0.7994108456428308
Model improve: 0.0000 -> 0.7994
Epoch: 2/60
Date :04/18/2023, 00:41:14
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :04/18/2023, 00:41:46
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
conv_stem has not been pruned
blocks.0.0.conv has not been pruned
blocks.0.1.conv has not been pruned
blocks.1.0.conv_exp has not been pruned
blocks.1.0.conv_pwl has not been pruned
blocks.1.1.conv_exp has not been pruned
blocks.1.1.conv_pwl has not been pruned
blocks.1.2.conv_exp has not been pruned
blocks.1.2.conv_pwl has not been pruned
blocks.1.3.conv_exp has not been pruned
blocks.1.3.conv_pwl has not been pruned
blocks.2.0.conv_exp has not been pruned
blocks.2.0.conv_pwl has not been pruned
blocks.2.1.conv_exp has not been pruned
blocks.2.1.conv_pwl has not been pruned
blocks.2.2.conv_exp has not been pruned
blocks.2.2.conv_pwl has not been pruned
blocks.2.3.conv_exp has not been pruned
blocks.2.3.conv_pwl has not been pruned
blocks.3.0.conv_pw has not been pruned
blocks.3.0.conv_dw has not been pruned
blocks.3.0.se.conv_reduce has not been pruned
blocks.3.0.se.conv_expand has not been pruned
blocks.3.0.conv_pwl has not been pruned
blocks.3.1.conv_pw has not been pruned
blocks.3.1.conv_dw has not been pruned
blocks.3.1.se.conv_reduce has not been pruned
blocks.3.1.se.conv_expand has not been pruned
blocks.3.1.conv_pwl has not been pruned
blocks.3.2.conv_pw has not been pruned
blocks.3.2.conv_dw has not been pruned
blocks.3.2.se.conv_reduce has not been pruned
blocks.3.2.se.conv_expand has not been pruned
blocks.3.2.conv_pwl has not been pruned
blocks.3.3.conv_pw has not been pruned
blocks.3.3.conv_dw has not been pruned
blocks.3.3.se.conv_reduce has not been pruned
blocks.3.3.se.conv_expand has not been pruned
blocks.3.3.conv_pwl has not been pruned
blocks.3.4.conv_pw has not been pruned
blocks.3.4.conv_dw has not been pruned
blocks.3.4.se.conv_reduce has not been pruned
blocks.3.4.se.conv_expand has not been pruned
blocks.3.4.conv_pwl has not been pruned
blocks.3.5.conv_pw has not been pruned
blocks.3.5.conv_dw has not been pruned
blocks.3.5.se.conv_reduce has not been pruned
blocks.3.5.se.conv_expand has not been pruned
blocks.3.5.conv_pwl has not been pruned
blocks.4.0.conv_pw has not been pruned
blocks.4.0.conv_dw has not been pruned
blocks.4.0.se.conv_reduce has not been pruned
blocks.4.0.se.conv_expand has not been pruned
blocks.4.0.conv_pwl has not been pruned
blocks.4.1.conv_pw has not been pruned
blocks.4.1.conv_dw has not been pruned
blocks.4.1.se.conv_reduce has not been pruned
blocks.4.1.se.conv_expand has not been pruned
blocks.4.1.conv_pwl has not been pruned
blocks.4.2.conv_pw has not been pruned
blocks.4.2.conv_dw has not been pruned
blocks.4.2.se.conv_reduce has not been pruned
blocks.4.2.se.conv_expand has not been pruned
blocks.4.2.conv_pwl has not been pruned
blocks.4.3.conv_pw has not been pruned
blocks.4.3.conv_dw has not been pruned
blocks.4.3.se.conv_reduce has not been pruned
blocks.4.3.se.conv_expand has not been pruned
blocks.4.3.conv_pwl has not been pruned
blocks.4.4.conv_pw has not been pruned
blocks.4.4.conv_dw has not been pruned
blocks.4.4.se.conv_reduce has not been pruned
blocks.4.4.se.conv_expand has not been pruned
blocks.4.4.conv_pwl has not been pruned
blocks.4.5.conv_pw has not been pruned
blocks.4.5.conv_dw has not been pruned
blocks.4.5.se.conv_reduce has not been pruned
blocks.4.5.se.conv_expand has not been pruned
blocks.4.5.conv_pwl has not been pruned
blocks.4.6.conv_pw has not been pruned
blocks.4.6.conv_dw has not been pruned
blocks.4.6.se.conv_reduce has not been pruned
blocks.4.6.se.conv_expand has not been pruned
blocks.4.6.conv_pwl has not been pruned
blocks.4.7.conv_pw has not been pruned
blocks.4.7.conv_dw has not been pruned
blocks.4.7.se.conv_reduce has not been pruned
blocks.4.7.se.conv_expand has not been pruned
blocks.4.7.conv_pwl has not been pruned
blocks.4.8.conv_pw has not been pruned
blocks.4.8.conv_dw has not been pruned
blocks.4.8.se.conv_reduce has not been pruned
blocks.4.8.se.conv_expand has not been pruned
blocks.4.8.conv_pwl has not been pruned
blocks.5.0.conv_pw has not been pruned
blocks.5.0.conv_dw has not been pruned
blocks.5.0.se.conv_reduce has not been pruned
blocks.5.0.se.conv_expand has not been pruned
blocks.5.0.conv_pwl has not been pruned
blocks.5.1.conv_pw has not been pruned
blocks.5.1.conv_dw has not been pruned
blocks.5.1.se.conv_reduce has not been pruned
blocks.5.1.se.conv_expand has not been pruned
blocks.5.1.conv_pwl has not been pruned
blocks.5.2.conv_pw has not been pruned
blocks.5.2.conv_dw has not been pruned
blocks.5.2.se.conv_reduce has not been pruned
blocks.5.2.se.conv_expand has not been pruned
blocks.5.2.conv_pwl has not been pruned
blocks.5.3.conv_pw has not been pruned
blocks.5.3.conv_dw has not been pruned
blocks.5.3.se.conv_reduce has not been pruned
blocks.5.3.se.conv_expand has not been pruned
blocks.5.3.conv_pwl has not been pruned
blocks.5.4.conv_pw has not been pruned
blocks.5.4.conv_dw has not been pruned
blocks.5.4.se.conv_reduce has not been pruned
blocks.5.4.se.conv_expand has not been pruned
blocks.5.4.conv_pwl has not been pruned
blocks.5.5.conv_pw has not been pruned
blocks.5.5.conv_dw has not been pruned
blocks.5.5.se.conv_reduce has not been pruned
blocks.5.5.se.conv_expand has not been pruned
blocks.5.5.conv_pwl has not been pruned
blocks.5.6.conv_pw has not been pruned
blocks.5.6.conv_dw has not been pruned
blocks.5.6.se.conv_reduce has not been pruned
blocks.5.6.se.conv_expand has not been pruned
blocks.5.6.conv_pwl has not been pruned
blocks.5.7.conv_pw has not been pruned
blocks.5.7.conv_dw has not been pruned
blocks.5.7.se.conv_reduce has not been pruned
blocks.5.7.se.conv_expand has not been pruned
blocks.5.7.conv_pwl has not been pruned
blocks.5.8.conv_pw has not been pruned
blocks.5.8.conv_dw has not been pruned
blocks.5.8.se.conv_reduce has not been pruned
blocks.5.8.se.conv_expand has not been pruned
blocks.5.8.conv_pwl has not been pruned
blocks.5.9.conv_pw has not been pruned
blocks.5.9.conv_dw has not been pruned
blocks.5.9.se.conv_reduce has not been pruned
blocks.5.9.se.conv_expand has not been pruned
blocks.5.9.conv_pwl has not been pruned
blocks.5.10.conv_pw has not been pruned
blocks.5.10.conv_dw has not been pruned
blocks.5.10.se.conv_reduce has not been pruned
blocks.5.10.se.conv_expand has not been pruned
blocks.5.10.conv_pwl has not been pruned
blocks.5.11.conv_pw has not been pruned
blocks.5.11.conv_dw has not been pruned
blocks.5.11.se.conv_reduce has not been pruned
blocks.5.11.se.conv_expand has not been pruned
blocks.5.11.conv_pwl has not been pruned
blocks.5.12.conv_pw has not been pruned
blocks.5.12.conv_dw has not been pruned
blocks.5.12.se.conv_reduce has not been pruned
blocks.5.12.se.conv_expand has not been pruned
blocks.5.12.conv_pwl has not been pruned
blocks.5.13.conv_pw has not been pruned
blocks.5.13.conv_dw has not been pruned
blocks.5.13.se.conv_reduce has not been pruned
blocks.5.13.se.conv_expand has not been pruned
blocks.5.13.conv_pwl has not been pruned
blocks.5.14.conv_pw has not been pruned
blocks.5.14.conv_dw has not been pruned
blocks.5.14.se.conv_reduce has not been pruned
blocks.5.14.se.conv_expand has not been pruned
blocks.5.14.conv_pwl has not been pruned
conv_head has not been pruned
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :04/18/2023, 00:56:56
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
conv_stem: 19.91% sparsity
blocks.0.0.conv: 29.21% sparsity
blocks.0.1.conv: 29.67% sparsity
blocks.1.0.conv_exp: 29.21% sparsity
blocks.1.0.conv_pwl: 23.87% sparsity
blocks.1.1.conv_exp: 27.91% sparsity
blocks.1.1.conv_pwl: 25.34% sparsity
blocks.1.2.conv_exp: 27.42% sparsity
blocks.1.2.conv_pwl: 26.07% sparsity
blocks.1.3.conv_exp: 27.48% sparsity
blocks.1.3.conv_pwl: 26.42% sparsity
blocks.2.0.conv_exp: 29.54% sparsity
blocks.2.0.conv_pwl: 26.31% sparsity
blocks.2.1.conv_exp: 27.91% sparsity
blocks.2.1.conv_pwl: 26.01% sparsity
blocks.2.2.conv_exp: 27.45% sparsity
blocks.2.2.conv_pwl: 26.33% sparsity
blocks.2.3.conv_exp: 27.74% sparsity
blocks.2.3.conv_pwl: 26.45% sparsity
blocks.3.0.conv_pw: 27.25% sparsity
blocks.3.0.conv_dw: 11.41% sparsity
blocks.3.0.se.conv_reduce: 70.68% sparsity
blocks.3.0.se.conv_expand: 91.63% sparsity
blocks.3.0.conv_pwl: 26.71% sparsity
blocks.3.1.conv_pw: 28.07% sparsity
blocks.3.1.conv_dw: 24.57% sparsity
blocks.3.1.se.conv_reduce: 36.74% sparsity
blocks.3.1.se.conv_expand: 28.64% sparsity
blocks.3.1.conv_pwl: 27.55% sparsity
blocks.3.2.conv_pw: 27.59% sparsity
blocks.3.2.conv_dw: 24.41% sparsity
blocks.3.2.se.conv_reduce: 36.02% sparsity
blocks.3.2.se.conv_expand: 28.12% sparsity
blocks.3.2.conv_pwl: 27.25% sparsity
blocks.3.3.conv_pw: 27.50% sparsity
blocks.3.3.conv_dw: 25.41% sparsity
blocks.3.3.se.conv_reduce: 31.86% sparsity
blocks.3.3.se.conv_expand: 25.91% sparsity
blocks.3.3.conv_pwl: 27.31% sparsity
blocks.3.4.conv_pw: 28.19% sparsity
blocks.3.4.conv_dw: 26.97% sparsity
blocks.3.4.se.conv_reduce: 34.72% sparsity
blocks.3.4.se.conv_expand: 32.12% sparsity
blocks.3.4.conv_pwl: 28.57% sparsity
blocks.3.5.conv_pw: 28.12% sparsity
blocks.3.5.conv_dw: 25.37% sparsity
blocks.3.5.se.conv_reduce: 34.26% sparsity
blocks.3.5.se.conv_expand: 28.19% sparsity
blocks.3.5.conv_pwl: 28.66% sparsity
blocks.4.0.conv_pw: 27.61% sparsity
blocks.4.0.conv_dw: 32.22% sparsity
blocks.4.0.se.conv_reduce: 31.84% sparsity
blocks.4.0.se.conv_expand: 26.24% sparsity
blocks.4.0.conv_pwl: 27.02% sparsity
blocks.4.1.conv_pw: 27.71% sparsity
blocks.4.1.conv_dw: 24.42% sparsity
blocks.4.1.se.conv_reduce: 34.77% sparsity
blocks.4.1.se.conv_expand: 28.62% sparsity
blocks.4.1.conv_pwl: 26.94% sparsity
blocks.4.2.conv_pw: 27.76% sparsity
blocks.4.2.conv_dw: 25.39% sparsity
blocks.4.2.se.conv_reduce: 34.76% sparsity
blocks.4.2.se.conv_expand: 28.61% sparsity
blocks.4.2.conv_pwl: 27.18% sparsity
blocks.4.3.conv_pw: 27.59% sparsity
blocks.4.3.conv_dw: 24.41% sparsity
blocks.4.3.se.conv_reduce: 33.60% sparsity
blocks.4.3.se.conv_expand: 28.95% sparsity
blocks.4.3.conv_pwl: 27.09% sparsity
blocks.4.4.conv_pw: 27.79% sparsity
blocks.4.4.conv_dw: 25.84% sparsity
blocks.4.4.se.conv_reduce: 36.05% sparsity
blocks.4.4.se.conv_expand: 29.89% sparsity
blocks.4.4.conv_pwl: 27.02% sparsity
blocks.4.5.conv_pw: 27.93% sparsity
blocks.4.5.conv_dw: 25.16% sparsity
blocks.4.5.se.conv_reduce: 36.91% sparsity
blocks.4.5.se.conv_expand: 32.22% sparsity
blocks.4.5.conv_pwl: 27.73% sparsity
blocks.4.6.conv_pw: 27.80% sparsity
blocks.4.6.conv_dw: 25.94% sparsity
blocks.4.6.se.conv_reduce: 35.83% sparsity
blocks.4.6.se.conv_expand: 28.65% sparsity
blocks.4.6.conv_pwl: 27.30% sparsity
blocks.4.7.conv_pw: 28.01% sparsity
blocks.4.7.conv_dw: 26.56% sparsity
blocks.4.7.se.conv_reduce: 33.10% sparsity
blocks.4.7.se.conv_expand: 27.68% sparsity
blocks.4.7.conv_pwl: 27.26% sparsity
blocks.4.8.conv_pw: 28.05% sparsity
blocks.4.8.conv_dw: 25.01% sparsity
blocks.4.8.se.conv_reduce: 33.24% sparsity
blocks.4.8.se.conv_expand: 26.84% sparsity
blocks.4.8.conv_pwl: 27.60% sparsity
blocks.5.0.conv_pw: 27.46% sparsity
blocks.5.0.conv_dw: 19.59% sparsity
blocks.5.0.se.conv_reduce: 85.33% sparsity
blocks.5.0.se.conv_expand: 87.12% sparsity
blocks.5.0.conv_pwl: 27.77% sparsity
blocks.5.1.conv_pw: 27.44% sparsity
blocks.5.1.conv_dw: 25.56% sparsity
blocks.5.1.se.conv_reduce: 31.90% sparsity
blocks.5.1.se.conv_expand: 27.13% sparsity
blocks.5.1.conv_pwl: 26.87% sparsity
blocks.5.2.conv_pw: 27.48% sparsity
blocks.5.2.conv_dw: 26.01% sparsity
blocks.5.2.se.conv_reduce: 31.63% sparsity
blocks.5.2.se.conv_expand: 26.85% sparsity
blocks.5.2.conv_pwl: 26.69% sparsity
blocks.5.3.conv_pw: 27.51% sparsity
blocks.5.3.conv_dw: 26.70% sparsity
blocks.5.3.se.conv_reduce: 31.88% sparsity
blocks.5.3.se.conv_expand: 27.23% sparsity
blocks.5.3.conv_pwl: 26.89% sparsity
blocks.5.4.conv_pw: 27.43% sparsity
blocks.5.4.conv_dw: 26.90% sparsity
blocks.5.4.se.conv_reduce: 31.45% sparsity
blocks.5.4.se.conv_expand: 27.02% sparsity
blocks.5.4.conv_pwl: 26.81% sparsity
blocks.5.5.conv_pw: 27.38% sparsity
blocks.5.5.conv_dw: 27.58% sparsity
blocks.5.5.se.conv_reduce: 30.34% sparsity
blocks.5.5.se.conv_expand: 27.02% sparsity
blocks.5.5.conv_pwl: 26.75% sparsity
blocks.5.6.conv_pw: 27.34% sparsity
blocks.5.6.conv_dw: 27.52% sparsity
blocks.5.6.se.conv_reduce: 30.40% sparsity
blocks.5.6.se.conv_expand: 26.98% sparsity
blocks.5.6.conv_pwl: 26.67% sparsity
blocks.5.7.conv_pw: 27.27% sparsity
blocks.5.7.conv_dw: 26.90% sparsity
blocks.5.7.se.conv_reduce: 29.72% sparsity
blocks.5.7.se.conv_expand: 26.55% sparsity
blocks.5.7.conv_pwl: 26.51% sparsity
blocks.5.8.conv_pw: 27.26% sparsity
blocks.5.8.conv_dw: 26.13% sparsity
blocks.5.8.se.conv_reduce: 28.77% sparsity
blocks.5.8.se.conv_expand: 26.22% sparsity
blocks.5.8.conv_pwl: 26.60% sparsity
blocks.5.9.conv_pw: 27.35% sparsity
blocks.5.9.conv_dw: 26.41% sparsity
blocks.5.9.se.conv_reduce: 29.28% sparsity
blocks.5.9.se.conv_expand: 26.11% sparsity
blocks.5.9.conv_pwl: 26.69% sparsity
blocks.5.10.conv_pw: 27.19% sparsity
blocks.5.10.conv_dw: 26.25% sparsity
blocks.5.10.se.conv_reduce: 29.30% sparsity
blocks.5.10.se.conv_expand: 26.23% sparsity
blocks.5.10.conv_pwl: 26.64% sparsity
blocks.5.11.conv_pw: 27.53% sparsity
blocks.5.11.conv_dw: 25.95% sparsity
blocks.5.11.se.conv_reduce: 30.28% sparsity
blocks.5.11.se.conv_expand: 26.09% sparsity
blocks.5.11.conv_pwl: 26.83% sparsity
blocks.5.12.conv_pw: 27.70% sparsity
blocks.5.12.conv_dw: 24.85% sparsity
blocks.5.12.se.conv_reduce: 30.31% sparsity
blocks.5.12.se.conv_expand: 25.98% sparsity
blocks.5.12.conv_pwl: 26.81% sparsity
blocks.5.13.conv_pw: 27.93% sparsity
blocks.5.13.conv_dw: 25.01% sparsity
blocks.5.13.se.conv_reduce: 32.02% sparsity
blocks.5.13.se.conv_expand: 26.24% sparsity
blocks.5.13.conv_pwl: 27.40% sparsity
blocks.5.14.conv_pw: 27.91% sparsity
blocks.5.14.conv_dw: 25.17% sparsity
blocks.5.14.se.conv_reduce: 31.95% sparsity
blocks.5.14.se.conv_expand: 24.27% sparsity
blocks.5.14.conv_pwl: 27.03% sparsity
conv_head: 26.54% sparsity
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 10.5140, val loss: 2.7811
0.8099141385926745
Model improve: 0.0000 -> 0.8099
Epoch: 2/60
Train loss: 3.2204, val loss: 1.9431
0.8711014455057821
Model improve: 0.8099 -> 0.8711
Epoch: 3/60
Train loss: 2.6232, val loss: 1.6828
0.891295546803688
Model improve: 0.8711 -> 0.8913
Epoch: 4/60
Train loss: 2.3354, val loss: 1.5539
0.8994464237416422
Model improve: 0.8913 -> 0.8994
Epoch: 5/60
Train loss: 2.0670, val loss: 1.4760
0.904194413143058
Model improve: 0.8994 -> 0.9042
Epoch: 6/60
Train loss: 2.0133, val loss: 1.5216
0.9070662289564785
Model improve: 0.9042 -> 0.9071
Epoch: 7/60
Train loss: 1.8640, val loss: 1.4520
0.9096492860300852
Model improve: 0.9071 -> 0.9096
Epoch: 8/60
Train loss: 1.7287, val loss: 1.4073
0.9080696415325071
Epoch: 9/60
Train loss: 1.8015, val loss: 1.3311
0.911992750055543
Model improve: 0.9096 -> 0.9120
Epoch: 10/60
Train loss: 1.6764, val loss: 1.3837
0.9083557654915889
Epoch: 11/60
Train loss: 1.5820, val loss: 1.3614
0.911773732618661
Epoch: 12/60
Train loss: 1.5375, val loss: 1.3457
0.9135103543191097
Model improve: 0.9120 -> 0.9135
Epoch: 13/60
Train loss: 1.5169, val loss: 1.3663
0.911702032394317
Epoch: 14/60
Train loss: 1.4449, val loss: 1.3203
0.9130839685871698
Epoch: 15/60
Train loss: 1.5701, val loss: 1.3594
0.9156016417400696
Model improve: 0.9135 -> 0.9156
Epoch: 16/60
Train loss: 1.4636, val loss: 1.3366
0.9128581300233606
Epoch: 17/60
Train loss: 1.5219, val loss: 1.3171
0.9157352474580055
Model improve: 0.9156 -> 0.9157
Epoch: 18/60
Train loss: 1.3659, val loss: 1.3625
0.9128933860533374
Epoch: 19/60
Train loss: 1.2959, val loss: 1.2976
0.9157051922360159
Epoch: 20/60
Train loss: 1.3533, val loss: 1.3868
0.9115031665054302
Epoch: 21/60
Train loss: 1.2943, val loss: 1.3170
0.9110920781567524
Epoch: 22/60
Train loss: 1.3603, val loss: 1.2901
0.9153372950226347
Epoch: 23/60
Train loss: 1.2684, val loss: 1.2898
0.9115166551613892
Epoch: 24/60
Train loss: 1.3468, val loss: 1.2912
0.916333873875544
Model improve: 0.9157 -> 0.9163
Epoch: 25/60
Train loss: 1.3228, val loss: 1.2753
0.9145734429574089
Epoch: 26/60
Train loss: 1.2715, val loss: 1.2582
0.9174969634208328
Model improve: 0.9163 -> 0.9175
Epoch: 27/60
Train loss: 1.2630, val loss: 1.2669
0.9166597095251322
Epoch: 28/60
Train loss: 1.3501, val loss: 1.2770
0.9152215139036968
Epoch: 29/60
Train loss: 1.2797, val loss: 1.3127
0.9167496955291334
Epoch: 30/60
Train loss: 1.2787, val loss: 1.3112
0.9143200474853009
Epoch: 31/60
Train loss: 1.2345, val loss: 1.2618
0.915437668493043
Epoch: 32/60
Train loss: 1.2480, val loss: 1.3032
0.9143606118187877
Epoch: 33/60
Train loss: 1.1909, val loss: 1.2801
0.9160676946015993
Epoch: 34/60
Train loss: 1.2917, val loss: 1.3041
0.9160299581695239
Epoch: 35/60
Train loss: 1.2273, val loss: 1.2126
0.9186912017478143
Model improve: 0.9175 -> 0.9187
Epoch: 36/60
Train loss: 1.2254, val loss: 1.2718
0.9166289098297807
Epoch: 37/60
Train loss: 1.1294, val loss: 1.2878
0.9144458035108475
Epoch: 38/60
Train loss: 1.1272, val loss: 1.2539
0.915570912797486
Epoch: 39/60
Train loss: 1.1493, val loss: 1.2364
0.9172157271671277
Epoch: 40/60
Train loss: 1.1350, val loss: 1.2223
0.9172809135194385
Epoch: 41/60
Train loss: 1.1258, val loss: 1.2319
0.9180335637610467
Epoch: 42/60
Train loss: 1.0955, val loss: 1.2440
0.9184846513230907
Epoch: 43/60
Train loss: 1.0995, val loss: 1.2206
0.9180516638187405
Epoch: 44/60
Train loss: 1.0201, val loss: 1.2429
0.9172736040330657
Epoch: 45/60
Train loss: 1.1274, val loss: 1.2426
0.9192146964996132
Model improve: 0.9187 -> 0.9192
Epoch: 46/60
Train loss: 0.9622, val loss: 1.2335
0.9172684105770155
Epoch: 47/60
Train loss: 1.0870, val loss: 1.2201
0.9177212099828338
Epoch: 48/60
Train loss: 1.1135, val loss: 1.2223
0.9193966698760794
Model improve: 0.9192 -> 0.9194
Epoch: 49/60
Train loss: 1.1324, val loss: 1.2219
0.9191685576585918
Epoch: 50/60
Train loss: 1.0987, val loss: 1.2169
0.9194410538260681
Model improve: 0.9194 -> 0.9194
Epoch: 51/60
Train loss: 0.9689, val loss: 1.2122
0.9186204870668595
Epoch: 52/60
Train loss: 1.1343, val loss: 1.2072
0.9201776933646394
Model improve: 0.9194 -> 0.9202
Epoch: 53/60
Train loss: 1.0400, val loss: 1.2070
0.9198472411498055
Epoch: 54/60
Train loss: 1.0400, val loss: 1.2085
0.9199867353835542
Epoch: 55/60
Train loss: 1.0762, val loss: 1.2110
0.9186915827321955
Epoch: 56/60
Train loss: 0.9780, val loss: 1.2233
0.9197590820719295
Epoch: 57/60
Train loss: 0.9438, val loss: 1.2042
0.920009395814681
Epoch: 58/60
Train loss: 1.0387, val loss: 1.2048
0.919950738264468
Epoch: 59/60
Train loss: 1.0638, val loss: 1.2143
0.9193109358148306
Epoch: 60/60
Train loss: 1.1241, val loss: 1.2118
0.9194320667873063
Date :04/18/2023, 07:08:10
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :04/18/2023, 07:08:40
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/25
Train loss: 10.1389, val loss: 2.7190
0.809961091382942
Model improve: 0.0000 -> 0.8100
Epoch: 2/25
Train loss: 3.1843, val loss: 1.9556
0.8673543749901046
Model improve: 0.8100 -> 0.8674
Epoch: 3/25
Train loss: 2.5991, val loss: 1.7054
0.8899800096692512
Model improve: 0.8674 -> 0.8900
Epoch: 4/25
Train loss: 2.3209, val loss: 1.5606
0.8980220066186341
Model improve: 0.8900 -> 0.8980
Epoch: 5/25
Train loss: 2.0652, val loss: 1.5165
0.9018891604877267
Model improve: 0.8980 -> 0.9019
Epoch: 6/25
Train loss: 2.0063, val loss: 1.5566
0.9069960822741336
Model improve: 0.9019 -> 0.9070
Epoch: 7/25
Train loss: 1.8452, val loss: 1.4439
0.9083861884164357
Model improve: 0.9070 -> 0.9084
Epoch: 8/25
Train loss: 1.7090, val loss: 1.3797
0.910626645825059
Model improve: 0.9084 -> 0.9106
Epoch: 9/25
Date :04/18/2023, 07:51:14
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/25
Train loss: 10.2786, val loss: 2.7251
0.8098349798241368
Model improve: 0.0000 -> 0.8098
Epoch: 2/25
Train loss: 3.1833, val loss: 1.9605
0.868274838655346
Model improve: 0.8098 -> 0.8683
Epoch: 3/25
Train loss: 2.5915, val loss: 1.6892
0.8895627298526377
Model improve: 0.8683 -> 0.8896
Epoch: 4/25
Train loss: 2.3186, val loss: 1.5794
0.8957073250909375
Model improve: 0.8896 -> 0.8957
Epoch: 5/25
Date :04/18/2023, 08:13:44
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 16
validbs: 64
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Train loss: 5.6157, val loss: 2.1566
0.856512632647534
Model improve: 0.0000 -> 0.8565
Epoch: 2/25
Train loss: 2.7525, val loss: 1.9038
0.8766239531204748
Model improve: 0.8565 -> 0.8766
Epoch: 3/25
Train loss: 2.3961, val loss: 1.6757
0.89116414295746
Model improve: 0.8766 -> 0.8912
Epoch: 4/25
Train loss: 2.1932, val loss: 1.7269
0.8930113679535167
Model improve: 0.8912 -> 0.8930
Epoch: 5/25
Train loss: 2.0362, val loss: 1.7556
0.8959924553360796
Model improve: 0.8930 -> 0.8960
Epoch: 6/25
Train loss: 1.9280, val loss: 1.6189
0.9022620147242829
Model improve: 0.8960 -> 0.9023
Epoch: 7/25
Date :04/18/2023, 09:18:34
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Train loss: 7.4714, val loss: 2.1948
0.849825399274562
Model improve: 0.0000 -> 0.8498
Epoch: 2/25
Date :04/18/2023, 09:26:06
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/60
Train loss: 7.4723, val loss: 2.1971
0.8494083065714412
Model improve: 0.0000 -> 0.8494
Epoch: 2/60
Train loss: 2.8470, val loss: 1.8477
0.8786473041000202
Model improve: 0.8494 -> 0.8786
Epoch: 3/60
Train loss: 2.4107, val loss: 1.7147
0.8931700129353451
Model improve: 0.8786 -> 0.8932
Epoch: 4/60
Train loss: 2.1584, val loss: 1.6067
0.8951700610851204
Model improve: 0.8932 -> 0.8952
Epoch: 5/60
Train loss: 2.0913, val loss: 1.5455
0.9010603450974538
Model improve: 0.8952 -> 0.9011
Epoch: 6/60
Train loss: 1.8918, val loss: 1.5121
0.900404150862683
Epoch: 7/60
Train loss: 1.7807, val loss: 1.5572
0.9019310525572763
Model improve: 0.9011 -> 0.9019
Epoch: 8/60
Train loss: 1.8258, val loss: 1.5255
0.9045203173407917
Model improve: 0.9019 -> 0.9045
Epoch: 9/60
Train loss: 1.7453, val loss: 1.4938
0.9089680731208185
Model improve: 0.9045 -> 0.9090
Epoch: 10/60
Date :04/18/2023, 10:25:50
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/60
Train loss: 7.1619, val loss: 2.1838
0.8506092195122441
Model improve: 0.0000 -> 0.8506
Epoch: 2/60
Train loss: 2.8768, val loss: 1.7954
0.8813717893514051
Model improve: 0.8506 -> 0.8814
Epoch: 3/60
Train loss: 2.4333, val loss: 1.6929
0.8953684802564398
Model improve: 0.8814 -> 0.8954
Epoch: 4/60
Train loss: 2.1535, val loss: 1.5739
0.8966464908282674
Model improve: 0.8954 -> 0.8966
Epoch: 5/60
Train loss: 2.0753, val loss: 1.5924
0.9016991344636608
Model improve: 0.8966 -> 0.9017
Epoch: 6/60
Train loss: 1.8888, val loss: 1.5528
0.903694119788291
Model improve: 0.9017 -> 0.9037
Epoch: 7/60
Train loss: 1.7998, val loss: 1.5199
0.9037804676573263
Model improve: 0.9037 -> 0.9038
Epoch: 8/60
Train loss: 1.8305, val loss: 1.5580
0.902258324171851
Epoch: 9/60
Train loss: 1.7443, val loss: 1.5148
0.9045321213037697
Model improve: 0.9038 -> 0.9045
Epoch: 10/60
Train loss: 1.6134, val loss: 1.5031
0.9050500173637546
Model improve: 0.9045 -> 0.9051
Epoch: 11/60
Date :04/18/2023, 11:28:18
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/60
Date :04/18/2023, 11:28:35
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/60
Train loss: 7.1603, val loss: 2.1858
0.85113629487834
Model improve: 0.0000 -> 0.8511
Epoch: 2/60
Train loss: 2.8723, val loss: 1.8283
0.8810760654296745
Model improve: 0.8511 -> 0.8811
Epoch: 3/60
Train loss: 2.4259, val loss: 1.7042
0.8949983752159408
Model improve: 0.8811 -> 0.8950
Epoch: 4/60
Train loss: 2.1440, val loss: 1.5771
0.8976944737227008
Model improve: 0.8950 -> 0.8977
Epoch: 5/60
Train loss: 2.0795, val loss: 1.6479
0.8998760945469705
Model improve: 0.8977 -> 0.8999
Epoch: 6/60
Train loss: 1.8866, val loss: 1.5558
0.9036537037923805
Model improve: 0.8999 -> 0.9037
Epoch: 7/60
Train loss: 1.7911, val loss: 1.4898
0.9054386975050952
Model improve: 0.9037 -> 0.9054
Epoch: 8/60
Train loss: 1.8237, val loss: 1.6150
0.9004974505711445
Epoch: 9/60
Train loss: 1.7503, val loss: 1.5114
0.9045487109159398
Epoch: 10/60
Train loss: 1.6153, val loss: 1.5693
0.903854994288989
Epoch: 11/60
Train loss: 1.6038, val loss: 1.4671
0.9073640237660221
Model improve: 0.9054 -> 0.9074
Epoch: 12/60
Train loss: 1.5972, val loss: 1.4170
0.9071708582678252
Epoch: 13/60
Train loss: 1.5887, val loss: 1.4213
0.9072680350741753
Epoch: 14/60
Train loss: 1.5958, val loss: 1.4533
0.9082076131988472
Model improve: 0.9074 -> 0.9082
Epoch: 15/60
Train loss: 1.5624, val loss: 1.4856
0.9064796589211364
Epoch: 16/60
Train loss: 1.5055, val loss: 1.4577
0.9103382739512703
Model improve: 0.9082 -> 0.9103
Epoch: 17/60
Train loss: 1.5338, val loss: 1.4000
0.9104048377561309
Model improve: 0.9103 -> 0.9104
Epoch: 18/60
Train loss: 1.5111, val loss: 1.5233
0.9031672932488641
Epoch: 19/60
Train loss: 1.4294, val loss: 1.4197
0.9093949252360244
Epoch: 20/60
Train loss: 1.4332, val loss: 1.3985
0.9092476706545445
Epoch: 21/60
Train loss: 1.3505, val loss: 1.4371
0.9065668217563746
Epoch: 22/60
Train loss: 1.3378, val loss: 1.3839
0.9106265413499552
Model improve: 0.9104 -> 0.9106
Epoch: 23/60
Train loss: 1.3222, val loss: 1.4068
0.9090950570142097
Epoch: 24/60
Train loss: 1.3631, val loss: 1.4357
0.9102563813183653
Epoch: 25/60
Train loss: 1.3848, val loss: 1.4183
0.9095839697698959
Epoch: 26/60
Train loss: 1.2697, val loss: 1.3937
0.9093739390288492
Epoch: 27/60
Train loss: 1.2639, val loss: 1.3920
0.9134619720779334
Model improve: 0.9106 -> 0.9135
Epoch: 28/60
Train loss: 1.2252, val loss: 1.3699
0.9126181242274879
Epoch: 29/60
Train loss: 1.2046, val loss: 1.3843
0.9109457496147557
Epoch: 30/60
Train loss: 1.2702, val loss: 1.3622
0.9106112894406321
Epoch: 31/60
Train loss: 1.2595, val loss: 1.4096
0.9109314397946288
Epoch: 32/60
Train loss: 1.2032, val loss: 1.3766
0.9125198505125459
Epoch: 33/60
Train loss: 1.2293, val loss: 1.3736
0.915171302324846
Model improve: 0.9135 -> 0.9152
Epoch: 34/60
Train loss: 1.2380, val loss: 1.3275
0.9170457680638463
Model improve: 0.9152 -> 0.9170
Epoch: 35/60
Train loss: 1.1307, val loss: 1.3525
0.9133919592343303
Epoch: 36/60
Train loss: 1.1247, val loss: 1.3706
0.9137869193247312
Epoch: 37/60
Train loss: 1.1540, val loss: 1.3271
0.9155806637678852
Epoch: 38/60
Train loss: 1.0977, val loss: 1.3989
0.9130640141998078
Epoch: 39/60
Train loss: 1.1050, val loss: 1.3553
0.9165261032814673
Epoch: 40/60
Train loss: 1.1587, val loss: 1.3313
0.9158807178730332
Epoch: 41/60
Train loss: 1.0543, val loss: 1.3337
0.9163014371059219
Epoch: 42/60
Train loss: 1.0529, val loss: 1.3022
0.9155411530454016
Epoch: 43/60
Train loss: 1.0995, val loss: 1.2884
0.9169407463150049
Epoch: 44/60
Train loss: 1.0358, val loss: 1.3380
0.9153269949145132
Epoch: 45/60
Train loss: 1.1025, val loss: 1.3432
0.9164101275873406
Epoch: 46/60
Train loss: 1.0534, val loss: 1.3268
0.9171917153378245
Model improve: 0.9170 -> 0.9172
Epoch: 47/60
Train loss: 1.1023, val loss: 1.3505
0.9148469984081774
Epoch: 48/60
Train loss: 1.0648, val loss: 1.3097
0.9173906219428453
Model improve: 0.9172 -> 0.9174
Epoch: 49/60
Train loss: 1.0636, val loss: 1.3032
0.9162708234157648
Epoch: 50/60
Train loss: 0.9871, val loss: 1.3430
0.9171063064456892
Epoch: 51/60
Train loss: 1.0262, val loss: 1.3181
0.9181996585444661
Model improve: 0.9174 -> 0.9182
Epoch: 52/60
Train loss: 1.0605, val loss: 1.2918
0.9185632704529093
Model improve: 0.9182 -> 0.9186
Epoch: 53/60
Train loss: 1.0054, val loss: 1.3178
0.9175954164903164
Epoch: 54/60
Train loss: 1.0452, val loss: 1.2921
0.9192938690582719
Model improve: 0.9186 -> 0.9193
Epoch: 55/60
Train loss: 1.0433, val loss: 1.2955
0.9186997342931942
Epoch: 56/60
Train loss: 1.0242, val loss: 1.3122
0.9180968809180184
Epoch: 57/60
Train loss: 0.9625, val loss: 1.3040
0.91821188686253
Epoch: 58/60
Train loss: 1.0081, val loss: 1.3043
0.9186908576437798
Epoch: 59/60
Train loss: 0.9787, val loss: 1.2895
0.9189075873501545
Epoch: 60/60
Train loss: 1.0342, val loss: 1.2885
0.9190394786294377
Date :04/18/2023, 19:48:39
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
Date :04/18/2023, 19:49:09
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
Date :04/18/2023, 19:49:43
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
Date :04/18/2023, 19:51:26
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
Val loss: 195.9663
Best metric at: 0.4647
Date :04/18/2023, 19:52:18
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
Val loss: 1.1958
Best metric at: 0.9290
Val loss: 1.1944
Best metric at: 0.9290
Val loss: 1.1820
Best metric at: 0.9289
Val loss: 1.1833
Best metric at: 0.9291
Date :04/18/2023, 19:55:21
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
Val loss: 1.1796
Best metric at: 0.9290
Date :04/18/2023, 19:56:29
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
Val loss: 1.2863
Best metric at: 0.9236
Date :04/18/2023, 19:57:25
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
Val loss: 1.1775
Best metric at: 0.9293
Date :04/18/2023, 19:59:19
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
a1: 0.15198443381740748
a2: 0.25672863401172846
Date :04/18/2023, 19:59:59
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
a1: 0.15198443381740748
a2: 0.25672863401172846
a2: 0.591286932170864
Val loss: 1.1932
Best metric at: 0.9293
a1: 0.06235401415270182
a2: 0.43121246871095986
a2: 0.5064335171363383
Val loss: 1.1918
Best metric at: 0.9293
a1: 0.8270655972088143
a2: 0.15944838645109546
a2: 0.013486016340090196
Val loss: 1.1797
Best metric at: 0.9290
a1: 0.7199921054915603
a2: 0.21462486621604113
a2: 0.06538302829239853
Val loss: 1.1809
Best metric at: 0.9291
a1: 0.2672438107409991
a2: 0.4715927941741754
a2: 0.2611633950848255
Val loss: 1.1870
Best metric at: 0.9293
a1: 0.09334615100410686
a2: 0.07299611171763565
a2: 0.8336577372782575
Val loss: 1.2019
Best metric at: 0.9290
a1: 0.5841280021678498
a2: 0.1430333685445009
a2: 0.27283862928764935
Val loss: 1.1833
Best metric at: 0.9294
a1: 0.9789985109384551
a2: 0.01286657109614868
a2: 0.008134917965396185
Val loss: 1.1777
Best metric at: 0.9292
a1: 0.6752797077033628
a2: 0.1791797060334395
a2: 0.14554058626319774
Val loss: 1.1815
Best metric at: 0.9293
a1: 0.2669025972514883
a2: 0.27382629391412533
a2: 0.45927110883438643
Val loss: 1.1893
Best metric at: 0.9293
a1: 0.5115030478579079
a2: 0.11267029369678376
a2: 0.3758266584453083
Val loss: 1.1854
Best metric at: 0.9293
a1: 0.44789500720865805
a2: 0.33020664548058476
a2: 0.2218983473107572
Val loss: 1.1846
Best metric at: 0.9294
a1: 0.49347319354088803
a2: 0.3850166066960303
a2: 0.1215101997630817
Val loss: 1.1840
Best metric at: 0.9292
a1: 0.47266420144868604
a2: 0.3297865654482751
a2: 0.19754923310303885
Val loss: 1.1842
Best metric at: 0.9293
a1: 0.3930754464832773
a2: 0.5859019143445474
a2: 0.021022639172175284
Val loss: 1.1861
Best metric at: 0.9291
a1: 0.6196615168494861
a2: 0.13558810504186886
a2: 0.24475037810864503
Val loss: 1.1826
Best metric at: 0.9295
a1: 0.6164568637635519
a2: 0.12562046745824462
a2: 0.25792266877820347
Val loss: 1.1828
Best metric at: 0.9294
a1: 0.60490359123934
a2: 0.07992244399007106
a2: 0.3151739647705889
Val loss: 1.1835
Best metric at: 0.9294
a1: 0.771425813824802
a2: 0.11843509436354932
a2: 0.11013909181164866
Val loss: 1.1802
Best metric at: 0.9292
a1: 0.5928042892176116
a2: 0.19261474433207101
a2: 0.21458096645031735
Val loss: 1.1828
Best metric at: 0.9294
a1: 0.8861674738233349
a2: 0.01876285825438264
a2: 0.09506966792228241
Val loss: 1.1787
Best metric at: 0.9292
a1: 0.6439297209383917
a2: 0.13960193212321131
a2: 0.21646834693839695
Val loss: 1.1821
Best metric at: 0.9294
a1: 0.5632627794918339
a2: 0.15200828020570947
a2: 0.28472894030245666
Val loss: 1.1837
Best metric at: 0.9294
a1: 0.7129052502781625
a2: 0.15236734620711695
a2: 0.1347274035147205
Val loss: 1.1810
Best metric at: 0.9293
a1: 0.5464911667775783
a2: 0.19833134303265398
a2: 0.25517749018976776
Val loss: 1.1836
Best metric at: 0.9295
a1: 0.5476648136075143
a2: 0.20223470429328555
a2: 0.25010048209920016
Val loss: 1.1836
Best metric at: 0.9294
a1: 0.4121811105587424
a2: 0.24480578874213935
a2: 0.3430131006991183
Val loss: 1.1861
Best metric at: 0.9295
a1: 0.3991321207127344
a2: 0.24029120099439163
a2: 0.360576678292874
Val loss: 1.1864
Best metric at: 0.9294
a1: 0.346665963489489
a2: 0.26361215588918663
a2: 0.3897218806213243
Val loss: 1.1874
Best metric at: 0.9293
a1: 0.5313048226918315
a2: 0.2213747643004123
a2: 0.24732041300775615
Val loss: 1.1837
Best metric at: 0.9294
a1: 0.22294797135329114
a2: 0.735926054778326
a2: 0.041125973868382903
Val loss: 1.1882
Best metric at: 0.9292
a1: 0.5545659499663426
a2: 0.16532442204213807
a2: 0.2801096279915194
Val loss: 1.1837
Best metric at: 0.9295
a1: 0.43287689854810757
a2: 0.1911904822972235
a2: 0.375932619154669
Val loss: 1.1863
Best metric at: 0.9293
a1: 0.511090786197315
a2: 0.23550948399754895
a2: 0.2533997298051361
Val loss: 1.1840
Best metric at: 0.9294
a1: 0.6763598679204943
a2: 0.17321469231671655
a2: 0.15042543976278916
Val loss: 1.1815
Best metric at: 0.9293
a1: 0.0014064481955573527
a2: 0.3095429466640451
a2: 0.6890506051403975
Val loss: 1.1968
Best metric at: 0.9292
a1: 0.37609398648826164
a2: 0.21818874934676707
a2: 0.4057172641649713
Val loss: 1.1873
Best metric at: 0.9293
a1: 0.4657107575805287
a2: 0.17237721595541217
a2: 0.3619120264640591
Val loss: 1.1857
Best metric at: 0.9294
a1: 0.5717835098194496
a2: 0.2170261320748806
a2: 0.2111903581056698
Val loss: 1.1830
Best metric at: 0.9294
a1: 0.6584777428116122
a2: 0.1599042393695258
a2: 0.181618017818862
Val loss: 1.1818
Best metric at: 0.9295
a1: 0.33747259795846385
a2: 0.2743025058572944
a2: 0.3882248961842418
Val loss: 1.1875
Best metric at: 0.9293
a1: 0.6511031572511133
a2: 0.16449627436815034
a2: 0.18440056838073632
Val loss: 1.1819
Best metric at: 0.9294
a1: 0.5337196496344744
a2: 0.1957280741352606
a2: 0.270552276230265
Val loss: 1.1839
Best metric at: 0.9295
a1: 0.7167929385993692
a2: 0.13577037971772665
a2: 0.14743668168290414
Val loss: 1.1809
Best metric at: 0.9293
a1: 0.6169452125168311
a2: 0.17675338548780817
a2: 0.20630140199536073
Val loss: 1.1824
Best metric at: 0.9295
a1: 0.47887972703038134
a2: 0.09968861516856778
a2: 0.42143165780105096
Val loss: 1.1866
Best metric at: 0.9294
a1: 0.5672085707543884
a2: 0.17277124941394614
a2: 0.2600201798316655
Date :04/18/2023, 20:30:25
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
a1: 0.15198443381740748
a2: 0.8480155661825926
Val loss: 1.2024
Best metric at: 0.9288
a1: 0.3000306870007385
a2: 0.6999693129992615
Val loss: 1.1957
Best metric at: 0.9293
a1: 0.06235401415270182
a2: 0.9376459858472982
Val loss: 1.2068
Best metric at: 0.9288
a1: 0.4558018778512702
a2: 0.5441981221487298
Val loss: 1.1896
Best metric at: 0.9292
a1: 0.8270655972088143
a2: 0.17293440279118566
Val loss: 1.1797
Best metric at: 0.9293
a1: 0.9178000802246458
a2: 0.08219991977535424
Val loss: 1.1783
Best metric at: 0.9293
a1: 0.7199921054915603
a2: 0.28000789450843966
Val loss: 1.1818
Best metric at: 0.9294
a1: 0.7610427634795177
a2: 0.23895723652048229
Val loss: 1.1809
Best metric at: 0.9294
a1: 0.2672438107409991
a2: 0.7327561892590009
Val loss: 1.1971
Best metric at: 0.9291
a1: 0.6379449698582085
a2: 0.3620550301417915
Val loss: 1.1839
Best metric at: 0.9294
a1: 0.5909330241832139
a2: 0.4090669758167861
Val loss: 1.1852
Best metric at: 0.9295
a1: 0.5876348656545988
a2: 0.41236513434540123
Val loss: 1.1853
Best metric at: 0.9294
a1: 0.5877650762827198
a2: 0.4122349237172802
Val loss: 1.1853
Best metric at: 0.9294
a1: 0.49738572492717237
a2: 0.5026142750728276
Val loss: 1.1881
Best metric at: 0.9292
a1: 0.49928681553395504
a2: 0.500713184466045
Val loss: 1.1881
Best metric at: 0.9292
a1: 0.9562184105938507
a2: 0.04378158940614929
Val loss: 1.1779
Best metric at: 0.9292
a1: 0.6626046299316218
a2: 0.33739537006837816
Val loss: 1.1832
Best metric at: 0.9293
a1: 0.8291381647120306
a2: 0.1708618352879694
Val loss: 1.1797
Best metric at: 0.9294
a1: 0.5704812681907496
a2: 0.4295187318092504
Val loss: 1.1858
Best metric at: 0.9293
a1: 0.40497933791797625
a2: 0.5950206620820238
Val loss: 1.1914
Best metric at: 0.9292
a1: 0.6846190100415803
a2: 0.3153809899584197
Val loss: 1.1827
Best metric at: 0.9293
a1: 0.5889070172065249
a2: 0.41109298279347506
Val loss: 1.1852
Best metric at: 0.9294
a1: 0.5838064724703699
a2: 0.4161935275296301
Val loss: 1.1854
Best metric at: 0.9294
a1: 0.3889226858088205
a2: 0.6110773141911795
Val loss: 1.1921
Best metric at: 0.9292
a1: 0.5480404836889315
a2: 0.45195951631106845
Val loss: 1.1865
Best metric at: 0.9293
a1: 0.6369998001381671
a2: 0.36300019986183285
Val loss: 1.1839
Best metric at: 0.9294
a1: 0.7485265060307102
a2: 0.25147349396928975
Val loss: 1.1812
Best metric at: 0.9294
a1: 0.5122142209338854
a2: 0.48778577906611464
Val loss: 1.1876
Best metric at: 0.9293
a1: 0.6296431554982573
a2: 0.37035684450174267
Val loss: 1.1841
Best metric at: 0.9294
a1: 0.4388101092034311
a2: 0.5611898907965689
Val loss: 1.1902
Best metric at: 0.9292
a1: 0.5424493550073325
a2: 0.4575506449926675
Val loss: 1.1867
Best metric at: 0.9293
a1: 0.6232344132774523
a2: 0.3767655867225477
Val loss: 1.1843
Best metric at: 0.9294
a1: 0.6871250247623127
a2: 0.31287497523768726
Val loss: 1.1826
Best metric at: 0.9293
a1: 0.6049806620539402
a2: 0.3950193379460598
Val loss: 1.1848
Best metric at: 0.9294
a1: 0.465072330688251
a2: 0.5349276693117491
Val loss: 1.1892
Best metric at: 0.9292
a1: 0.5542063398877782
a2: 0.44579366011222177
Val loss: 1.1863
Best metric at: 0.9293
a1: 0.6990609030789545
a2: 0.3009390969210455
Val loss: 1.1823
Best metric at: 0.9294
a1: 0.6486834009103563
a2: 0.35131659908964374
Val loss: 1.1836
Best metric at: 0.9293
a1: 0.7840012236450524
a2: 0.21599877635494757
Val loss: 1.1805
Best metric at: 0.9294
a1: 0.7185592932610312
a2: 0.2814407067389688
Val loss: 1.1819
Best metric at: 0.9294
a1: 0.32259565650771316
a2: 0.6774043434922868
Val loss: 1.1947
Best metric at: 0.9292
a1: 0.5991939536954314
a2: 0.4008060463045686
Val loss: 1.1849
Best metric at: 0.9295
a1: 0.6104057666948237
a2: 0.3895942333051763
Val loss: 1.1846
Best metric at: 0.9294
a1: 0.5239694926339673
a2: 0.4760305073660327
Val loss: 1.1873
Best metric at: 0.9293
a1: 0.5981805056923897
a2: 0.4018194943076103
Val loss: 1.1850
Best metric at: 0.9294
a1: 0.5092774859869581
a2: 0.4907225140130419
Val loss: 1.1877
Best metric at: 0.9293
a1: 0.5776224937941548
a2: 0.42237750620584524
Val loss: 1.1856
Best metric at: 0.9293
a1: 0.474365249433361
a2: 0.525634750566639
Val loss: 1.1889
Best metric at: 0.9292
a1: 0.6503861305093835
a2: 0.34961386949061646
Val loss: 1.1835
Best metric at: 0.9293
a1: 0.5393200049795615
a2: 0.46067999502043855
Val loss: 1.1868
Best metric at: 0.9294
a1: 0.6756065356095926
a2: 0.3243934643904074
Val loss: 1.1829
Best metric at: 0.9293
a1: 0.6233792102006486
a2: 0.37662078979935143
Val loss: 1.1843
Best metric at: 0.9294
a1: 0.5876114819620628
a2: 0.4123885180379372
Val loss: 1.1853
Best metric at: 0.9294
a1: 0.5673108126450448
a2: 0.4326891873549552
Val loss: 1.1859
Best metric at: 0.9293
a1: 0.6028981259425477
a2: 0.39710187405745234
Val loss: 1.1848
Best metric at: 0.9294
a1: 0.494648781453547
a2: 0.505351218546453
Date :04/18/2023, 21:06:43
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
a1: 0.41343476265084567
Val loss: nan
Date :04/18/2023, 21:07:46
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
a1: 0.41343476265084567
a2: 0.4220486909267687
a2: 0.1645165464223856
Val loss: 1.1895
Best metric at: 0.9295
a1: 0.0011131166943540928
a2: 0.30232160930999574
a2: 0.6965652739956502
Val loss: 1.1816
Best metric at: 0.9289
a1: 0.1461415760181248
a2: 0.079568643671926
a2: 0.7742897803099492
Val loss: 1.1802
Best metric at: 0.9292
a1: 0.18521134905251652
a2: 0.2818023932264547
a2: 0.5329862577210289
Val loss: 1.1834
Best metric at: 0.9293
a1: 0.39340303201413257
a2: 0.32672084535187756
a2: 0.2798761226339898
Val loss: 1.1881
Best metric at: 0.9293
a1: 0.41558337474485857
a2: 0.40005175093013445
a2: 0.18436487432500703
Val loss: 1.1893
Best metric at: 0.9294
a1: 0.20320327498447074
a2: 0.6989126743661949
a2: 0.09788405064933436
Val loss: 1.1886
Best metric at: 0.9291
a1: 0.028086329672748975
a2: 0.6512626503833255
a2: 0.32065101994392553
Val loss: 1.1871
Best metric at: 0.9291
a1: 0.41371444954108855
a2: 0.3273902629320654
a2: 0.2588952875268461
Val loss: 1.1886
Best metric at: 0.9294
a1: 0.1398426822706862
a2: 0.17092205266017102
a2: 0.6892352650691428
Val loss: 1.1813
Best metric at: 0.9293
a1: 0.7633264157020823
a2: 0.029391505379986096
a2: 0.20728207891793166
Val loss: 1.1986
Best metric at: 0.9291
a1: 0.6235141412616442
a2: 0.34151787898159336
a2: 0.03496797975676247
Val loss: 1.1947
Best metric at: 0.9293
a1: 0.9347808426927353
a2: 0.017364599752478643
a2: 0.04785455755478604
Val loss: 1.2067
Best metric at: 0.9288
a1: 0.5200839460172855
a2: 0.4139858954213158
a2: 0.06593015856139872
Val loss: 1.1921
Best metric at: 0.9293
a1: 0.34536833288640423
a2: 0.4641422562841355
a2: 0.19048941082946025
Val loss: 1.1884
Best metric at: 0.9295
a1: 0.3356737621662871
a2: 0.5058541714650134
a2: 0.15847206636869948
Val loss: 1.1886
Best metric at: 0.9295
a1: 0.5647269630829721
a2: 0.21150398571543905
a2: 0.22376905120158885
Val loss: 1.1920
Best metric at: 0.9293
a1: 0.2985629642682369
a2: 0.5030297380195566
a2: 0.19840729771220644
Val loss: 1.1879
Best metric at: 0.9294
a1: 0.29125436758009116
a2: 0.48664908416909
a2: 0.22209654825081887
Val loss: 1.1876
Best metric at: 0.9295
a1: 0.46681695445793986
a2: 0.2385003353588024
a2: 0.29468271018325765
Val loss: 1.1892
Best metric at: 0.9293
a1: 0.6550170227677194
a2: 0.13507343188568854
a2: 0.209909545346592
Val loss: 1.1947
Best metric at: 0.9293
a1: 0.2819520621797304
a2: 0.48102798790333273
a2: 0.2370199499169368
Val loss: 1.1874
Best metric at: 0.9294
a1: 0.3327531935710586
a2: 0.5617079773621082
a2: 0.1055388290668332
Val loss: 1.1890
Best metric at: 0.9295
a1: 0.4799297015467964
a2: 0.4352537034993197
a2: 0.08481659495388394
Val loss: 1.1912
Best metric at: 0.9294
a1: 0.34943944546963
a2: 0.5399017677546374
a2: 0.1106587867757326
Val loss: 1.1892
Best metric at: 0.9295
a1: 0.3506954520309434
a2: 0.5632185923772391
a2: 0.08608595559181753
Val loss: 1.1894
Best metric at: 0.9294
a1: 0.24785989637822894
a2: 0.5930547115179945
a2: 0.1590853921037766
Val loss: 1.1881
Best metric at: 0.9293
a1: 0.23821884449880726
a2: 0.7494561785184833
a2: 0.012324976982709401
Val loss: 1.1894
Best metric at: 0.9291
a1: 0.39598938263442085
a2: 0.5393348133189084
a2: 0.06467580404667073
Val loss: 1.1900
Best metric at: 0.9294
a1: 0.3429334233883917
a2: 0.5694313701337205
a2: 0.0876352064778878
Val loss: 1.1893
Best metric at: 0.9295
a1: 0.44346279521843135
a2: 0.45332467710828855
a2: 0.1032125276732801
Val loss: 1.1904
Best metric at: 0.9295
a1: 0.35116601091059896
a2: 0.5418748978791209
a2: 0.10695909121028013
Val loss: 1.1892
Best metric at: 0.9295
a1: 0.3460347545585689
a2: 0.589002643281916
a2: 0.0649626021595151
Val loss: 1.1895
Best metric at: 0.9294
a1: 0.2920433626041656
a2: 0.6216858446802505
a2: 0.08627079271558391
Val loss: 1.1889
Best metric at: 0.9294
a1: 0.4950170445621605
a2: 0.45218460484990575
a2: 0.05279835058793381
Val loss: 1.1916
Best metric at: 0.9294
a1: 0.17012753714327114
a2: 0.6639732970591881
a2: 0.16589916579754083
Val loss: 1.1880
Best metric at: 0.9292
a1: 0.40166688241658727
a2: 0.5249500039052174
a2: 0.07338311367819539
Val loss: 1.1900
Best metric at: 0.9293
a1: 0.20888478797865828
a2: 0.38481545777328985
a2: 0.40629975424805187
Val loss: 1.1851
Best metric at: 0.9294
a1: 0.12813048691019016
a2: 0.5763625197969895
a2: 0.2955069932928204
Val loss: 1.1866
Best metric at: 0.9292
a1: 0.3680605700020101
a2: 0.5499229147937781
a2: 0.08201651520421183
Val loss: 1.1896
Best metric at: 0.9294
a1: 0.43168969324233647
a2: 0.5137767982065459
a2: 0.054533508551117715
Val loss: 1.1906
Best metric at: 0.9294
a1: 0.3948873135901063
a2: 0.48628165719365873
a2: 0.11883102921623495
Val loss: 1.1896
Best metric at: 0.9294
a1: 0.3224274236467422
a2: 0.37594511647237583
a2: 0.301627459880882
Val loss: 1.1870
Best metric at: 0.9295
a1: 0.3325404965342619
a2: 0.3914665450119807
a2: 0.27599295845375743
Val loss: 1.1874
Best metric at: 0.9295
a1: 0.25204330561982946
a2: 0.6175362435395222
a2: 0.13042045084064835
Val loss: 1.1884
Best metric at: 0.9293
a1: 0.4355175499469669
a2: 0.467224877936101
a2: 0.09725757211693209
Val loss: 1.1903
Best metric at: 0.9294
a1: 0.3126189939454033
a2: 0.36542480397550575
a2: 0.32195620207909087
Val loss: 1.1867
Best metric at: 0.9295
a1: 0.20210999975079103
a2: 0.37188260839947107
a2: 0.4260073918497379
Val loss: 1.1849
Best metric at: 0.9294
a1: 0.3106069151030083
a2: 0.4200194666897667
a2: 0.269373618207225
Val loss: 1.1873
Best metric at: 0.9295
a1: 0.26468837220495034
a2: 0.3503933127327903
a2: 0.3849183150622594
Val loss: 1.1856
Best metric at: 0.9295
a1: 0.31206970830413255
a2: 0.3225889547163824
a2: 0.365341336979485
Val loss: 1.1862
Best metric at: 0.9296
a1: 0.3113528894477349
a2: 0.30168270553474835
a2: 0.3869644050175168
Val loss: 1.1860
Best metric at: 0.9295
a1: 0.30879954198199316
a2: 0.3101604657227053
a2: 0.3810399922953016
Val loss: 1.1860
Best metric at: 0.9295
a1: 0.23236896597188117
a2: 0.2762638078441091
a2: 0.49136722618400974
Val loss: 1.1841
Best metric at: 0.9294
a1: 0.38184257661039345
a2: 0.3583270575094744
a2: 0.25983036588013214
Val loss: 1.1881
Best metric at: 0.9295
a1: 0.2759364212604031
a2: 0.32146235694156605
a2: 0.4026012217980309
Val loss: 1.1855
Best metric at: 0.9294
a1: 0.3201674146990103
a2: 0.2872490317260924
a2: 0.39258355357489727
Val loss: 1.1860
Best metric at: 0.9295
a1: 0.37690253226812287
a2: 0.26723262975270334
a2: 0.3558648379791738
Val loss: 1.1871
Best metric at: 0.9294
a1: 0.45708872407914
a2: 0.30030755551453264
a2: 0.24260372040632744
Val loss: 1.1895
Best metric at: 0.9293
a1: 0.41655626169534354
a2: 0.29023016677981933
a2: 0.2932135715248371
Val loss: 1.1883
Best metric at: 0.9293
a1: 0.1669285474394061
a2: 0.33187674863839595
a2: 0.501194703922198
Val loss: 1.1838
Best metric at: 0.9293
a1: 0.32925055036015877
a2: 0.4046787669946703
a2: 0.26607068264517086
Val loss: 1.1875
Best metric at: 0.9295
a1: 0.3184132779339092
a2: 0.36141708096937386
a2: 0.32016964109671686
Val loss: 1.1868
Best metric at: 0.9295
a1: 0.28449902643464436
a2: 0.24805001738465363
a2: 0.4674509561807021
Val loss: 1.1848
Best metric at: 0.9295
a1: 0.2839139546501822
a2: 0.25627434161374674
a2: 0.45981170373607105
Val loss: 1.1849
Best metric at: 0.9296
a1: 0.2714481099795176
a2: 0.23810405205191262
a2: 0.4904478379685697
Val loss: 1.1844
Best metric at: 0.9295
a1: 0.27184299871627654
a2: 0.24110537148492728
a2: 0.48705162979879624
Val loss: 1.1845
Best metric at: 0.9295
a1: 0.271908742514903
a2: 0.24481954696672437
a2: 0.4832717105183727
Val loss: 1.1845
Best metric at: 0.9295
a1: 0.2238374883887888
a2: 0.2008585854254685
a2: 0.5753039261857427
Val loss: 1.1830
Best metric at: 0.9295
a1: 0.2495127303210533
a2: 0.22943300983030768
a2: 0.521054259848639
Val loss: 1.1839
Best metric at: 0.9294
a1: 0.1088411646396922
a2: 0.1790041866279295
a2: 0.7121546487323783
Val loss: 1.1810
Best metric at: 0.9291
a1: 0.25554599771305747
a2: 0.255396883274709
a2: 0.4890571190122336
Val loss: 1.1843
Best metric at: 0.9295
a1: 0.27899229031710815
a2: 0.2608875202930105
a2: 0.46012018938988136
Val loss: 1.1848
Best metric at: 0.9296
a1: 0.19874373155191427
a2: 0.22766985699311987
a2: 0.5735864114549658
Val loss: 1.1829
Best metric at: 0.9294
a1: 0.2667122692542828
a2: 0.2750613610467619
a2: 0.4582263696989553
Val loss: 1.1848
Best metric at: 0.9295
a1: 0.28305421675238907
a2: 0.2890680330779451
a2: 0.42787775016966584
Val loss: 1.1853
Best metric at: 0.9295
a1: 0.22828141630582516
a2: 0.32224691805588457
a2: 0.44947166563829033
Val loss: 1.1847
Best metric at: 0.9294
a1: 0.18205237226209292
a2: 0.24296691062908127
a2: 0.5749807171088258
Val loss: 1.1829
Best metric at: 0.9294
a1: 0.370708942494997
a2: 0.27206547587237495
a2: 0.3572255816326281
Val loss: 1.1870
Best metric at: 0.9294
a1: 0.2872441065517892
a2: 0.21622011821882972
a2: 0.4965357752293811
Val loss: 1.1845
Best metric at: 0.9295
a1: 0.2223311091318687
a2: 0.2620881177457846
a2: 0.5155807731223467
Val loss: 1.1838
Best metric at: 0.9294
a1: 0.3611194525369889
a2: 0.28905705577508123
a2: 0.3498234916879298
Val loss: 1.1870
Best metric at: 0.9294
a1: 0.287952061091581
a2: 0.33844895230636035
a2: 0.37359898660205865
Val loss: 1.1859
Best metric at: 0.9295
a1: 0.2678732711716753
a2: 0.25164661486182227
a2: 0.48048011396650236
Val loss: 1.1845
Best metric at: 0.9295
a1: 0.3309065134962669
a2: 0.2888558136743773
a2: 0.38023767282935583
Val loss: 1.1863
Best metric at: 0.9295
a1: 0.3016773673967635
a2: 0.3161980800995604
a2: 0.38212455250367616
Val loss: 1.1860
Best metric at: 0.9295
a1: 0.23709850860071158
a2: 0.19774045567778054
a2: 0.5651610357215079
Val loss: 1.1833
Best metric at: 0.9295
a1: 0.3516378706054444
a2: 0.23606953711287895
a2: 0.4122925922816767
Val loss: 1.1862
Best metric at: 0.9294
a1: 0.2756004222431697
a2: 0.3406668592905017
a2: 0.38373271846632856
Val loss: 1.1857
Best metric at: 0.9295
a1: 0.21305404567119604
a2: 0.30361214432508266
a2: 0.48333381000372133
Val loss: 1.1842
Best metric at: 0.9294
a1: 0.3895305394089799
a2: 0.2596071353742881
a2: 0.35086232521673205
Val loss: 1.1873
Best metric at: 0.9293
a1: 0.29903219325184605
a2: 0.3174393798597779
a2: 0.3835284268883761
Val loss: 1.1859
Best metric at: 0.9295
a1: 0.3070820670265034
a2: 0.28511043124482405
a2: 0.40780750172867253
Val loss: 1.1857
Best metric at: 0.9295
a1: 0.25052623939382623
a2: 0.3113143990816206
a2: 0.43815936152455315
Val loss: 1.1849
Best metric at: 0.9295
a1: 0.23775957765359362
a2: 0.2227625237449014
a2: 0.539477898601505
Val loss: 1.1836
Best metric at: 0.9294
a1: 0.2609881228553457
a2: 0.27355985333968413
a2: 0.4654520238049702
Val loss: 1.1846
Best metric at: 0.9295
a1: 0.19167756011296205
a2: 0.37907137241305366
a2: 0.4292510674739842
Val loss: 1.1848
Best metric at: 0.9294
a1: 0.3251606924135945
a2: 0.24432513093065233
a2: 0.4305141766557531
Val loss: 1.1856
Best metric at: 0.9295
a1: 0.345260899253935
a2: 0.30109236299089626
a2: 0.35364673775516875
Val loss: 1.1867
Best metric at: 0.9295
a1: 0.25201104277982367
a2: 0.34548239051957264
a2: 0.4025065667006037
Val loss: 1.1854
Best metric at: 0.9295
a1: 0.2860445573995834
a2: 0.3275969786183787
a2: 0.38635846398203794
Val loss: 1.1858
Best metric at: 0.9295
a1: 0.29534279754727977
a2: 0.29124734799382673
a2: 0.4134098544588935
Val loss: 1.1855
Best metric at: 0.9295
a1: 0.3277267827218651
a2: 0.26656842139524906
a2: 0.4057047958828859
Val loss: 1.1860
Best metric at: 0.9295
a1: 0.3280577890586248
a2: 0.2524321587611078
a2: 0.4195100521802674
Val loss: 1.1858
Best metric at: 0.9295
a1: 0.37350828143468895
a2: 0.235835975140078
a2: 0.390655743425233
Val loss: 1.1867
Best metric at: 0.9293
a1: 0.2128806646108251
a2: 0.20814287811240326
a2: 0.5789764572767716
Val loss: 1.1829
Best metric at: 0.9294
a1: 0.31323290907624396
a2: 0.2701071923997179
a2: 0.41665989852403806
Val loss: 1.1857
Best metric at: 0.9295
a1: 0.40974413621361044
a2: 0.2795878321811855
a2: 0.310668031605204
Val loss: 1.1881
Best metric at: 0.9293
a1: 0.3591587878755786
a2: 0.26295451505760115
a2: 0.3778866970668202
Val loss: 1.1866
Best metric at: 0.9294
a1: 0.24606584625955075
a2: 0.3024033303771615
a2: 0.45153082336328776
Val loss: 1.1847
Best metric at: 0.9295
a1: 0.2683683015427237
a2: 0.21708954442922118
a2: 0.5145421540280553
Val loss: 1.1841
Best metric at: 0.9295
a1: 0.29027551394881074
a2: 0.2886768826143635
a2: 0.42104760343682585
Val loss: 1.1854
Best metric at: 0.9295
a1: 0.3389315173198515
a2: 0.3094991013203234
a2: 0.35156938135982513
Val loss: 1.1867
Best metric at: 0.9295
a1: 0.27589675412391207
a2: 0.23865042190464503
a2: 0.48545282397144285
Val loss: 1.1845
Best metric at: 0.9295
a1: 0.3144389806254281
a2: 0.2385553684526346
a2: 0.44700565092193734
Val loss: 1.1853
Best metric at: 0.9294
a1: 0.25698949734063886
a2: 0.25561652287901304
a2: 0.48739397978034815
Val loss: 1.1843
Best metric at: 0.9295
a1: 0.2997981391759843
a2: 0.19315495974588892
a2: 0.5070469010781268
Val loss: 1.1845
Best metric at: 0.9294
a1: 0.22860957711139093
a2: 0.1761428593961209
a2: 0.5952475634924881
Val loss: 1.1828
Best metric at: 0.9294
a1: 0.2770294848484953
a2: 0.3282279984210523
a2: 0.39474251673045235
Val loss: 1.1856
Best metric at: 0.9295
a1: 0.3300622510913126
a2: 0.22628404422355683
a2: 0.44365370468513055
Val loss: 1.1856
Best metric at: 0.9294
a1: 0.20683321513451453
a2: 0.20927391503204984
a2: 0.5838928698334357
Val loss: 1.1828
Best metric at: 0.9295
a1: 0.28476993232013903
a2: 0.28424641752945606
a2: 0.4309836501504049
Val loss: 1.1852
Best metric at: 0.9295
a1: 0.24341085864236373
a2: 0.2682113186022065
a2: 0.48837782275542974
Val loss: 1.1842
Best metric at: 0.9295
a1: 0.2701212968696756
a2: 0.24297166859070043
a2: 0.48690703453962403
Val loss: 1.1845
Best metric at: 0.9295
a1: 0.26869947223796126
a2: 0.24429112298848552
a2: 0.48700940477355326
Val loss: 1.1844
Best metric at: 0.9295
a1: 0.2909762699477692
a2: 0.24509826568536458
a2: 0.46392546436686627
Val loss: 1.1849
Best metric at: 0.9295
a1: 0.2768620956999793
a2: 0.24505372813218895
a2: 0.4780841761678317
Val loss: 1.1846
Best metric at: 0.9295
a1: 0.2244048781302843
a2: 0.22368614243484883
a2: 0.5519089794348668
Val loss: 1.1833
Best metric at: 0.9295
a1: 0.17123657832072725
a2: 0.1896309945535266
a2: 0.6391324271257461
Val loss: 1.1820
Best metric at: 0.9294
a1: 0.26571260531951235
a2: 0.16213254610728287
a2: 0.5721548485732048
Val loss: 1.1834
Best metric at: 0.9295
a1: 0.3617487236372542
a2: 0.23705123997214192
a2: 0.40120003639060386
Val loss: 1.1864
Best metric at: 0.9294
a1: 0.3048632980161337
a2: 0.25833202769482155
a2: 0.4368046742890448
Val loss: 1.1854
Best metric at: 0.9295
a1: 0.29484741933415304
a2: 0.28153403942764493
a2: 0.42361854123820203
Val loss: 1.1854
Best metric at: 0.9295
a1: 0.3145067027648981
a2: 0.24729823695806497
a2: 0.4381950602770369
Val loss: 1.1854
Best metric at: 0.9295
a1: 0.275478681217605
a2: 0.21495938984355895
a2: 0.509561928938836
Val loss: 1.1842
Best metric at: 0.9295
a1: 0.33477540221893104
a2: 0.2699562406536925
a2: 0.3952683571273764
Val loss: 1.1861
Best metric at: 0.9296
a1: 0.3419551383242382
a2: 0.232832046483767
a2: 0.42521281519199483
Val loss: 1.1859
Best metric at: 0.9295
a1: 0.3913592385014466
a2: 0.26426699837704626
a2: 0.34437376312150714
Val loss: 1.1874
Best metric at: 0.9293
a1: 0.24366377221915403
a2: 0.20704573585287803
a2: 0.549290491927968
Val loss: 1.1835
Best metric at: 0.9295
a1: 0.32765097670621557
a2: 0.25093061083641205
a2: 0.4214184124573724
Val loss: 1.1858
Best metric at: 0.9295
a1: 0.26436846845882517
a2: 0.2728597325568806
a2: 0.46277179898429416
Val loss: 1.1847
Best metric at: 0.9295
a1: 0.29490559659976423
a2: 0.2838341270740079
a2: 0.42126027632622787
Val loss: 1.1854
Best metric at: 0.9295
a1: 0.3475353046547188
a2: 0.2926424017511106
a2: 0.35982229359417056
Val loss: 1.1867
Best metric at: 0.9295
a1: 0.29544762850445316
a2: 0.23156360111816898
a2: 0.47298877037737785
Val loss: 1.1848
Best metric at: 0.9295
a1: 0.31601487856808796
a2: 0.26093832513495696
a2: 0.4230467962969551
Date :04/18/2023, 22:42:01
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
a1: 0.41343476265084567
a2: 0.4220486909267687
a2: 0.1645165464223856
Val loss: 1.1775
Best metric at: 0.9293
a1: 0.0011131166943540928
a2: 0.30232160930999574
a2: 0.6965652739956502
Date :04/18/2023, 22:42:57
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
a1: 0.41343476265084567
a2: 0.4220486909267687
a2: 0.1645165464223856
Val loss: 1.1912
Best metric at: 0.9290
a1: 0.0011131166943540928
a2: 0.30232160930999574
a2: 0.6965652739956502
Val loss: 1.1912
Best metric at: 0.9290
a1: 0.1461415760181248
a2: 0.079568643671926
a2: 0.7742897803099492
Val loss: 1.1912
Best metric at: 0.9290
a1: 0.18521134905251652
a2: 0.2818023932264547
a2: 0.5329862577210289
Date :04/18/2023, 22:46:38
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
19629
a1: 0.41343476265084567
a2: 0.4220486909267687
a2: 0.1645165464223856
Val loss: 1.2186
Best metric at: 0.9199
a1: 0.0011131166943540928
a2: 0.30232160930999574
a2: 0.6965652739956502
Date :04/18/2023, 22:50:22
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
19629
a1: 0.41343476265084567
a2: 0.4220486909267687
a2: 0.1645165464223856
Val loss: 1.2152
Best metric at: 0.9193
a1: 0.0011131166943540928
a2: 0.30232160930999574
a2: 0.6965652739956502
Val loss: 1.2152
Best metric at: 0.9193
a1: 0.1461415760181248
a2: 0.079568643671926
a2: 0.7742897803099492
Val loss: 1.2152
Best metric at: 0.9193
a1: 0.18521134905251652
a2: 0.2818023932264547
a2: 0.5329862577210289
Val loss: 1.2152
Best metric at: 0.9193
a1: 0.39340303201413257
a2: 0.32672084535187756
a2: 0.2798761226339898
Val loss: 1.2152
Best metric at: 0.9193
a1: 0.41558337474485857
a2: 0.40005175093013445
a2: 0.18436487432500703
Val loss: 1.2152
Best metric at: 0.9193
a1: 0.20320327498447074
a2: 0.6989126743661949
a2: 0.09788405064933436
Val loss: 1.2152
Best metric at: 0.9193
a1: 0.028086329672748975
a2: 0.6512626503833255
a2: 0.32065101994392553
Val loss: 1.2152
Best metric at: 0.9193
a1: 0.41371444954108855
a2: 0.3273902629320654
a2: 0.2588952875268461
Val loss: 1.2152
Best metric at: 0.9193
a1: 0.1398426822706862
a2: 0.17092205266017102
a2: 0.6892352650691428
Val loss: 1.2152
Best metric at: 0.9193
a1: 0.7633264157020823
a2: 0.029391505379986096
a2: 0.20728207891793166
Val loss: 1.2152
Best metric at: 0.9193
a1: 0.7145501609330792
a2: 0.20757768025384976
a2: 0.07787215881307102
Val loss: 1.2152
Best metric at: 0.9193
a1: 0.5897269914059355
a2: 0.3710957249920963
a2: 0.03917728360196815
Date :04/18/2023, 22:58:15
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
19629
a1: 0.41343476265084567
a2: 0.4220486909267687
a2: 0.1645165464223856
Val loss: 1.2152
Best metric at: 0.9193
a1: 0.0011131166943540928
a2: 0.30232160930999574
a2: 0.6965652739956502
Date :04/18/2023, 22:59:09
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
19629
a1: 0.41343476265084567
a2: 0.4220486909267687
a2: 0.1645165464223856
Val loss: 1.2108
Best metric at: 0.9198
a1: 0.0011131166943540928
a2: 0.30232160930999574
a2: 0.6965652739956502
Date :04/18/2023, 23:00:28
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
a1: 0.41343476265084567
a2: 0.4220486909267687
a2: 0.1645165464223856
Val loss: 1.1775
Best metric at: 0.9293
a1: 0.0011131166943540928
a2: 0.30232160930999574
a2: 0.6965652739956502




        Date :04/18/2023, 23:12:18
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnet_b1_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
78014
a1: 0.41343476265084567
a2: 0.4220486909267687
a2: 0.1645165464223856
Val loss: 0.9263
Best metric at: 0.9473
a1: 0.0011131166943540928
a2: 0.30232160930999574
a2: 0.6965652739956502
Val loss: 0.9783
Best metric at: 0.9464
a1: 0.1461415760181248
a2: 0.079568643671926
a2: 0.7742897803099492
Val loss: 0.9805
Best metric at: 0.9444
a1: 0.18521134905251652
a2: 0.2818023932264547
a2: 0.5329862577210289
Val loss: 0.9434
Best metric at: 0.9496
a1: 0.39340303201413257
a2: 0.32672084535187756
a2: 0.2798761226339898
Val loss: 0.9172
Best metric at: 0.9493
a1: 0.41558337474485857
a2: 0.40005175093013445
a2: 0.18436487432500703
Val loss: 0.9266
Best metric at: 0.9477
a1: 0.20320327498447074
a2: 0.6989126743661949
a2: 0.09788405064933436
Val loss: 0.9337
Best metric at: 0.9482
a1: 0.028086329672748975
a2: 0.6512626503833255
a2: 0.32065101994392553
Val loss: 0.9438
Best metric at: 0.9478
a1: 0.41371444954108855
a2: 0.3273902629320654
a2: 0.2588952875268461
Val loss: 0.9152
Best metric at: 0.9499
a1: 0.1398426822706862
a2: 0.17092205266017102
a2: 0.6892352650691428
Val loss: 0.9573
Best metric at: 0.9470
a1: 0.7634852386193365
a2: 0.029372231048016645
a2: 0.20714253033264687
Val loss: 0.9185
Best metric at: 0.9483
a1: 0.6608359199920092
a2: 0.21481442746265622
a2: 0.12434965254533459
Val loss: 0.9093
Best metric at: 0.9505
a1: 0.6596327368252984
a2: 0.16092916934709106
a2: 0.1794380938276105
Val loss: 0.9174
Best metric at: 0.9474
a1: 0.5853082956539262
a2: 0.22436066648200928
a2: 0.19033103786406455
Val loss: 0.9317
Best metric at: 0.9465
a1: 0.8270172300547429
a2: 0.005940991856799388
a2: 0.1670417780884577
Val loss: 0.9047
Best metric at: 0.9486
a1: 0.9865107677786839
a2: 0.0011870909893129025
a2: 0.012302141232003247
Val loss: 0.9202
Best metric at: 0.9480
a1: 0.5316867094089024
a2: 0.216066141279184
a2: 0.2522471493119136
Val loss: 0.9361
Best metric at: 0.9466
a1: 0.3277552858781888
a2: 0.40477092360451367
a2: 0.2674737905172975
Val loss: 0.9140
Best metric at: 0.9501
a1: 0.29990159686554707
a2: 0.45270828615825787
a2: 0.24739011697619506
Val loss: 0.9286
Best metric at: 0.9492
a1: 0.6328586347268174
a2: 0.11346291353386467
a2: 0.25367845173931797
Val loss: 0.9197
Best metric at: 0.9480
a1: 0.5199961102258168
a2: 0.18462703155660648
a2: 0.2953768582175767
Val loss: 0.9512
Best metric at: 0.9458
a1: 0.33171935852003653
a2: 0.3581846994024682
a2: 0.31009594207749525
Val loss: 0.9446
Best metric at: 0.9463
a1: 0.5235521243896778
a2: 0.2650784463882166
a2: 0.2113694292221056
Val loss: 0.9153
Best metric at: 0.9505
a1: 0.49851940904168657
a2: 0.26307300803899253
a2: 0.2384075829193209
Val loss: 0.9316
Best metric at: 0.9485
a1: 0.6663747112784971
a2: 0.23837486950717154
a2: 0.09525041921433133
Date :04/19/2023, 00:48:36
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnet_b1_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/60
Train loss: 8.0366, val loss: 2.1462
0.8575820927699593
Model improve: 0.0000 -> 0.8576
Epoch: 2/60
Train loss: 2.8081, val loss: 1.7927
0.8855408552895698
Model improve: 0.8576 -> 0.8855
Epoch: 3/60
Train loss: 2.3913, val loss: 1.6937
0.895822570108537
Model improve: 0.8855 -> 0.8958
Epoch: 4/60
Train loss: 2.1324, val loss: 1.6065
0.8975061011450923
Model improve: 0.8958 -> 0.8975
Epoch: 5/60
Train loss: 2.0532, val loss: 1.5341
0.9052535964143185
Model improve: 0.8975 -> 0.9053
Epoch: 6/60
Train loss: 1.8808, val loss: 1.5290
0.9054627869236821
Model improve: 0.9053 -> 0.9055
Epoch: 7/60
Train loss: 1.7888, val loss: 1.5476
0.9029990518797375
Epoch: 8/60
Train loss: 1.8412, val loss: 1.5083
0.9054710746866745
Model improve: 0.9055 -> 0.9055
Epoch: 9/60
Train loss: 1.7514, val loss: 1.4972
0.9058593665718464
Model improve: 0.9055 -> 0.9059
Epoch: 10/60
Train loss: 1.6588, val loss: 1.4965
0.9095996922807068
Model improve: 0.9059 -> 0.9096
Epoch: 11/60
Train loss: 1.6239, val loss: 1.4308
0.9091937638201488
Epoch: 12/60
Train loss: 1.5970, val loss: 1.4622
0.9082481837034406
Epoch: 13/60
Train loss: 1.6107, val loss: 1.4464
0.9068781336669096
Epoch: 14/60
Train loss: 1.6293, val loss: 1.4247
0.9104627967905087
Model improve: 0.9096 -> 0.9105
Epoch: 15/60
Train loss: 1.6018, val loss: 1.4649
0.9091038004993895
Epoch: 16/60
Train loss: 1.5460, val loss: 1.4274
0.9103372568699213
Epoch: 17/60
Train loss: 1.5663, val loss: 1.4331
0.9107522189524755
Model improve: 0.9105 -> 0.9108
Epoch: 18/60
Train loss: 1.5637, val loss: 1.4442
0.909865208314872
Epoch: 19/60
Date :04/19/2023, 02:27:39
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnet_b1_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Train loss: 8.0380, val loss: 2.1479
0.8579689009289435
Model improve: 0.0000 -> 0.8580
Epoch: 2/25
Train loss: 2.8080, val loss: 1.7898
0.885474774237728
Model improve: 0.8580 -> 0.8855
Epoch: 3/25
Train loss: 2.3890, val loss: 1.6948
0.8967408230311746
Model improve: 0.8855 -> 0.8967
Epoch: 4/25
Train loss: 2.1260, val loss: 1.5995
0.8991461199722234
Model improve: 0.8967 -> 0.8991
Epoch: 5/25
Train loss: 2.0443, val loss: 1.5195
0.9066918555470481
Model improve: 0.8991 -> 0.9067
Epoch: 6/25
Train loss: 1.8614, val loss: 1.4976
0.9059049557741607
Epoch: 7/25
Train loss: 1.7573, val loss: 1.5094
0.9069709897101991
Model improve: 0.9067 -> 0.9070
Epoch: 8/25
Train loss: 1.8060, val loss: 1.4657
0.9077303502376226
Model improve: 0.9070 -> 0.9077
Epoch: 9/25
Train loss: 1.7076, val loss: 1.4485
0.9084985361992317
Model improve: 0.9077 -> 0.9085
Epoch: 10/25
Train loss: 1.6040, val loss: 1.4396
0.9142087443655981
Model improve: 0.9085 -> 0.9142
Epoch: 11/25
Train loss: 1.5662, val loss: 1.3648
0.9139430050986579
Epoch: 12/25
Train loss: 1.5322, val loss: 1.3981
0.9116814602329899
Epoch: 13/25
Train loss: 1.5282, val loss: 1.3762
0.9123241249320635
Epoch: 14/25
Train loss: 1.5445, val loss: 1.3304
0.9147105015764581
Model improve: 0.9142 -> 0.9147
Epoch: 15/25
Train loss: 1.5050, val loss: 1.3817
0.9156297622557279
Model improve: 0.9147 -> 0.9156
Epoch: 16/25
Train loss: 1.4504, val loss: 1.3397
0.9148028960797593
Epoch: 17/25
Train loss: 1.4723, val loss: 1.3403
0.9172704393369815
Model improve: 0.9156 -> 0.9173
Epoch: 18/25
Train loss: 1.4652, val loss: 1.3520
0.9171711190714379
Epoch: 19/25
Train loss: 1.3574, val loss: 1.3111
0.9173835718246054
Model improve: 0.9173 -> 0.9174
Epoch: 20/25
Train loss: 1.3779, val loss: 1.3284
0.9176103691484185
Model improve: 0.9174 -> 0.9176
Epoch: 21/25
Train loss: 1.3144, val loss: 1.3179
0.9188283921930427
Model improve: 0.9176 -> 0.9188
Epoch: 22/25
Train loss: 1.2936, val loss: 1.3228
0.9189216949503485
Model improve: 0.9188 -> 0.9189
Epoch: 23/25
Train loss: 1.2727, val loss: 1.3283
0.9195313251566911
Model improve: 0.9189 -> 0.9195
Epoch: 24/25
Train loss: 1.3486, val loss: 1.3076
0.9192213265243494
Epoch: 25/25
Train loss: 1.3905, val loss: 1.3123
0.9193713883994961
Date :04/19/2023, 04:42:43
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnet_b1_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Date :04/19/2023, 04:46:50
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnet_b1_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 2
19479
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Train loss: 8.2484, val loss: 2.2151
0.8452968886921313
Model improve: 0.0000 -> 0.8453
Epoch: 2/25
Train loss: 2.7752, val loss: 1.8564
0.8788246188427391
Model improve: 0.8453 -> 0.8788
Epoch: 3/25
Train loss: 2.3707, val loss: 1.8056
0.8876303086130287
Model improve: 0.8788 -> 0.8876
Epoch: 4/25
Train loss: 2.1141, val loss: 1.6117
0.8941522358148803
Model improve: 0.8876 -> 0.8942
Epoch: 5/25
Train loss: 2.0487, val loss: 1.6271
0.8888102411944274
Epoch: 6/25
Train loss: 1.8586, val loss: 1.5415
0.8984528380661252
Model improve: 0.8942 -> 0.8985
Epoch: 7/25
Train loss: 1.7674, val loss: 1.5444
0.8990129821145506
Model improve: 0.8985 -> 0.8990
Epoch: 8/25
Train loss: 1.7867, val loss: 1.5450
0.8990666351165709
Model improve: 0.8990 -> 0.8991
Epoch: 9/25
Train loss: 1.7120, val loss: 1.5117
0.8993217172543045
Model improve: 0.8991 -> 0.8993
Epoch: 10/25
Train loss: 1.5567, val loss: 1.4880
0.9010562067867162
Model improve: 0.8993 -> 0.9011
Epoch: 11/25
Train loss: 1.5799, val loss: 1.5243
0.9023820201474541
Model improve: 0.9011 -> 0.9024
Epoch: 12/25
Train loss: 1.5417, val loss: 1.4455
0.9052864293090783
Model improve: 0.9024 -> 0.9053
Epoch: 13/25
Train loss: 1.5306, val loss: 1.4934
0.9043029277503211
Epoch: 14/25
Train loss: 1.5395, val loss: 1.4621
0.9036179049878258
Epoch: 15/25
Train loss: 1.4826, val loss: 1.4342
0.906634985653982
Model improve: 0.9053 -> 0.9066
Epoch: 16/25
Train loss: 1.4522, val loss: 1.4337
0.9073509522239619
Model improve: 0.9066 -> 0.9074
Epoch: 17/25
Train loss: 1.4359, val loss: 1.4160
0.9076947761272337
Model improve: 0.9074 -> 0.9077
Epoch: 18/25
Train loss: 1.4604, val loss: 1.4115
0.9076831733803485
Epoch: 19/25
Train loss: 1.3743, val loss: 1.4030
0.908001184727023
Model improve: 0.9077 -> 0.9080
Epoch: 20/25
Train loss: 1.3711, val loss: 1.4000
0.9088870938806518
Model improve: 0.9080 -> 0.9089
Epoch: 21/25
Train loss: 1.3263, val loss: 1.3976
0.9084506026157554
Epoch: 22/25
Train loss: 1.2793, val loss: 1.3935
0.9084839494122936
Epoch: 23/25
Train loss: 1.3171, val loss: 1.4077
0.9084394679316682
Epoch: 24/25
Train loss: 1.2873, val loss: 1.3914
0.9087390251594595
Epoch: 25/25
Train loss: 1.3725, val loss: 1.4012
0.9081946376715986
Fold: 3
19523
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Train loss: 7.9975, val loss: 2.0921
0.852660025860485
Model improve: 0.0000 -> 0.8527
Epoch: 2/25
Train loss: 2.8178, val loss: 1.7457
0.8739237646807086
Model improve: 0.8527 -> 0.8739
Epoch: 3/25
Train loss: 2.3601, val loss: 1.6083
0.8853291809751327
Model improve: 0.8739 -> 0.8853
Epoch: 4/25
Train loss: 2.1098, val loss: 1.6319
0.883797564160412
Epoch: 5/25
Train loss: 2.0370, val loss: 1.6003
0.8886605275998237
Model improve: 0.8853 -> 0.8887
Epoch: 6/25
Train loss: 1.9211, val loss: 1.5720
0.8901547197565541
Model improve: 0.8887 -> 0.8902
Epoch: 7/25
Train loss: 1.8376, val loss: 1.5516
0.8918904749728763
Model improve: 0.8902 -> 0.8919
Epoch: 8/25
Train loss: 1.7626, val loss: 1.5523
0.8917936134196491
Epoch: 9/25
Train loss: 1.7345, val loss: 1.5265
0.8946258532235428
Model improve: 0.8919 -> 0.8946
Epoch: 10/25
Train loss: 1.6318, val loss: 1.5009
0.8965413321310324
Model improve: 0.8946 -> 0.8965
Epoch: 11/25
Train loss: 1.5406, val loss: 1.4888
0.8962132251847776
Epoch: 12/25
Train loss: 1.5315, val loss: 1.4449
0.898184180860162
Model improve: 0.8965 -> 0.8982
Epoch: 13/25
Train loss: 1.5091, val loss: 1.4500
0.8970764644677693
Epoch: 14/25
Train loss: 1.4850, val loss: 1.4416
0.8980422463029585
Epoch: 15/25
Train loss: 1.4359, val loss: 1.4371
0.8992535514839838
Model improve: 0.8982 -> 0.8993
Epoch: 16/25
Train loss: 1.4112, val loss: 1.4237
0.9014260883297456
Model improve: 0.8993 -> 0.9014
Epoch: 17/25
Train loss: 1.3134, val loss: 1.4408
0.9000829018571692
Epoch: 18/25
Train loss: 1.4245, val loss: 1.4020
0.9018139702197802
Model improve: 0.9014 -> 0.9018
Epoch: 19/25
Train loss: 1.3185, val loss: 1.4219
0.9003231465451097
Epoch: 20/25
Train loss: 1.3385, val loss: 1.4219
0.9006861621335691
Epoch: 21/25
Train loss: 1.3678, val loss: 1.3974
0.9011706262804198
Epoch: 22/25
Train loss: 1.3459, val loss: 1.4018
0.9013097361084981
Epoch: 23/25
Train loss: 1.3461, val loss: 1.3987
0.9012795807187675
Epoch: 24/25
Train loss: 1.3409, val loss: 1.4001
0.9015343592027338
Epoch: 25/25
Train loss: 1.2488, val loss: 1.4030
0.9017313858911412
Fold: 4
19574
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Train loss: 8.3701, val loss: 2.1842
0.8555284826450908
Model improve: 0.0000 -> 0.8555
Epoch: 2/25
Train loss: 2.7883, val loss: 1.8154
0.8904441781259493
Model improve: 0.8555 -> 0.8904
Epoch: 3/25
Train loss: 2.3599, val loss: 1.6580
0.9005632328093295
Model improve: 0.8904 -> 0.9006
Epoch: 4/25
Train loss: 2.1495, val loss: 1.5872
0.9064883454173029
Model improve: 0.9006 -> 0.9065
Epoch: 5/25
Train loss: 1.9616, val loss: 1.4962
0.909381282801427
Model improve: 0.9065 -> 0.9094
Epoch: 6/25
Train loss: 1.9369, val loss: 1.5160
0.9079550155666559
Epoch: 7/25
Train loss: 1.7697, val loss: 1.5104
0.9074623105939891
Epoch: 8/25
Train loss: 1.7351, val loss: 1.5084
0.9088364201085988
Epoch: 9/25
Train loss: 1.6158, val loss: 1.4615
0.9112391881988099
Model improve: 0.9094 -> 0.9112
Epoch: 10/25
Train loss: 1.6822, val loss: 1.4547
0.9112142471287338
Epoch: 11/25
Train loss: 1.5317, val loss: 1.4271
0.9133643769383594
Model improve: 0.9112 -> 0.9134
Epoch: 12/25
Train loss: 1.5249, val loss: 1.4195
0.9133453137253735
Epoch: 13/25
Train loss: 1.4754, val loss: 1.4254
0.9147095783069847
Model improve: 0.9134 -> 0.9147
Epoch: 14/25
Train loss: 1.3297, val loss: 1.3990
0.9134184868767861
Epoch: 15/25
Train loss: 1.4943, val loss: 1.3929
0.9149113157584831
Model improve: 0.9147 -> 0.9149
Epoch: 16/25
Train loss: 1.3965, val loss: 1.3697
0.9178603819020923
Model improve: 0.9149 -> 0.9179
Epoch: 17/25
Train loss: 1.4191, val loss: 1.3751
0.916049243863375
Epoch: 18/25
Train loss: 1.3739, val loss: 1.3986
0.9152647980707282
Epoch: 19/25
Train loss: 1.3799, val loss: 1.3884
0.9163934998127509
Epoch: 20/25
Train loss: 1.3222, val loss: 1.3686
0.9167336366599675
Epoch: 21/25
Train loss: 1.3391, val loss: 1.3587
0.9171311122937367
Epoch: 22/25
Train loss: 1.3069, val loss: 1.3634
0.9174498943539704
Epoch: 23/25
Train loss: 1.3086, val loss: 1.3622
0.9172226673998991
Epoch: 24/25
Train loss: 1.3304, val loss: 1.3828
0.9169425222967259
Epoch: 25/25
Train loss: 1.2805, val loss: 1.3639
0.9170273177258952
Date :04/19/2023, 11:58:24
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnet_b1_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Train loss: 7.9477, val loss: 2.1634
0.8546960210302407
Model improve: 0.0000 -> 0.8547
Epoch: 2/25
Train loss: 2.7861, val loss: 1.7750
0.8837044251662477
Model improve: 0.8547 -> 0.8837
Epoch: 3/25
Train loss: 2.3791, val loss: 1.6483
0.8938836898291969
Model improve: 0.8837 -> 0.8939
Epoch: 4/25
Train loss: 2.1213, val loss: 1.5320
0.8973682793037051
Model improve: 0.8939 -> 0.8974
Epoch: 5/25
Train loss: 2.0226, val loss: 1.5140
0.901190027032414
Model improve: 0.8974 -> 0.9012
Epoch: 6/25
Train loss: 1.8678, val loss: 1.5004
0.9037540054163378
Model improve: 0.9012 -> 0.9038
Epoch: 7/25
Train loss: 1.7866, val loss: 1.4759
0.9038458367818181
Model improve: 0.9038 -> 0.9038
Epoch: 8/25
Train loss: 1.7994, val loss: 1.4323
0.9037066093052907
Epoch: 9/25
Train loss: 1.7185, val loss: 1.4356
0.9048996018774894
Model improve: 0.9038 -> 0.9049
Epoch: 10/25
Train loss: 1.5980, val loss: 1.4178
0.90805982930189
Model improve: 0.9049 -> 0.9081
Epoch: 11/25
Train loss: 1.5749, val loss: 1.4232
0.9080238654440589
Epoch: 12/25
Train loss: 1.5801, val loss: 1.4253
0.9089292948255421
Model improve: 0.9081 -> 0.9089
Epoch: 13/25
Train loss: 1.5187, val loss: 1.4114
0.9076145952513496
Epoch: 14/25
Train loss: 1.5215, val loss: 1.3829
0.9110108626271177
Model improve: 0.9089 -> 0.9110
Epoch: 15/25
Train loss: 1.5038, val loss: 1.3713
0.9113857338919313
Model improve: 0.9110 -> 0.9114
Epoch: 16/25
Train loss: 1.4417, val loss: 1.3641
0.9135706686421365
Model improve: 0.9114 -> 0.9136
Epoch: 17/25
Train loss: 1.4613, val loss: 1.3759
0.9144023686215058
Model improve: 0.9136 -> 0.9144
Epoch: 18/25
Train loss: 1.4554, val loss: 1.3510
0.9142895586869018
Epoch: 19/25
Train loss: 1.3707, val loss: 1.3487
0.9138741813916631
Epoch: 20/25
Train loss: 1.3720, val loss: 1.3545
0.914923054984601
Model improve: 0.9144 -> 0.9149
Epoch: 21/25
Train loss: 1.3194, val loss: 1.3335
0.9141200834442488
Epoch: 22/25
Train loss: 1.3378, val loss: 1.3262
0.9138540019812016
Epoch: 23/25
Train loss: 1.2564, val loss: 1.3399
0.9140609110365514
Epoch: 24/25
Train loss: 1.3675, val loss: 1.3260
0.9142687701545158
Epoch: 25/25
Train loss: 1.3439, val loss: 1.3243
0.9145406978680667
Date :04/19/2023, 23:37:04
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: eca_nfnet_l0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Train loss: 7.3282, val loss: 2.0018
0.8669165533576336
Model improve: 0.0000 -> 0.8669
Epoch: 2/25
Train loss: 2.5067, val loss: 1.7029
0.8963643104827306
Model improve: 0.8669 -> 0.8964
Epoch: 3/25
Train loss: 2.0961, val loss: 1.6360
0.901385598246247
Model improve: 0.8964 -> 0.9014
Epoch: 4/25
Train loss: 1.8655, val loss: 1.5518
0.9038838906867488
Model improve: 0.9014 -> 0.9039
Epoch: 5/25
Train loss: 1.8157, val loss: 1.6300
0.9023614480490255
Epoch: 6/25
Train loss: 1.6355, val loss: 1.5210
0.9095498987215692
Model improve: 0.9039 -> 0.9095
Epoch: 7/25
Train loss: 1.5638, val loss: 1.6480
0.9061865083452336
Epoch: 8/25
Train loss: 1.6110, val loss: 1.5820
0.9075342286297743
Epoch: 9/25
Train loss: 1.5279, val loss: 1.6191
0.9057703189479106
Epoch: 10/25
Train loss: 1.4040, val loss: 1.5427
0.9077120381446377
Epoch: 11/25
Train loss: 1.4132, val loss: 1.5889
0.9083022387232976
Epoch: 12/25
Train loss: 1.3883, val loss: 1.4921
0.9105956805266062
Model improve: 0.9095 -> 0.9106
Epoch: 13/25
Date :04/20/2023, 00:20:14
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: eca_nfnet_l0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/25
Train loss: 7.3014, val loss: 2.0168
0.8666116068893892
Model improve: 0.0000 -> 0.8666
Epoch: 2/25
Date :04/20/2023, 00:23:47
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 25
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: eca_nfnet_l0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/25
Train loss: 15.6935, val loss: 3.2775
0.7629387811934919
Model improve: 0.0000 -> 0.7629
Epoch: 2/25
Train loss: 3.3964, val loss: 2.1118
0.862878564180496
Model improve: 0.7629 -> 0.8629
Epoch: 3/25
Train loss: 2.7096, val loss: 1.8104
0.8868041634295818
Model improve: 0.8629 -> 0.8868
Epoch: 4/25
Train loss: 2.3799, val loss: 1.6681
0.8949350902606805
Model improve: 0.8868 -> 0.8949
Epoch: 5/25
Train loss: 2.0979, val loss: 1.6239
0.8990753830093228
Model improve: 0.8949 -> 0.8991
Epoch: 6/25
Train loss: 2.0560, val loss: 1.6019
0.9015314396947067
Model improve: 0.8991 -> 0.9015
Epoch: 7/25
Train loss: 1.8779, val loss: 1.5935
0.9034451116308992
Model improve: 0.9015 -> 0.9034
Epoch: 8/25
Train loss: 1.7425, val loss: 1.5404
0.9070680117577689
Model improve: 0.9034 -> 0.9071
Epoch: 9/25
Date :04/20/2023, 00:51:52
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: eca_nfnet_l0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/25
Date :04/20/2023, 00:52:24
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: eca_nfnet_l0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
SAM (
Parameter Group 0
    adaptive: False
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    rho: 0.05
    weight_decay: 0.01
)
Epoch: 1/25
Date :04/20/2023, 00:53:12
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: eca_nfnet_l0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
SAM (
Parameter Group 0
    adaptive: False
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    rho: 0.05
    weight_decay: 0.01
)
Epoch: 1/25
Date :04/20/2023, 00:55:44
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: eca_nfnet_l0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
SAM (
Parameter Group 0
    adaptive: False
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    rho: 0.05
    weight_decay: 0.01
)
Epoch: 1/25
Date :04/20/2023, 00:56:41
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: eca_nfnet_l0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
SAM (
Parameter Group 0
    adaptive: False
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    rho: 0.05
    weight_decay: 0.01
)
Epoch: 1/25
Date :04/20/2023, 00:56:58
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: eca_nfnet_l0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
SAM (
Parameter Group 0
    adaptive: False
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    rho: 0.05
    weight_decay: 0.01
)
Epoch: 1/25
Train loss: 9.0125, val loss: 2.3782
0.8340731819893618
Model improve: 0.0000 -> 0.8341
Epoch: 2/25
Train loss: 3.1990, val loss: 1.8291
0.8858555303913078
Model improve: 0.8341 -> 0.8859
Epoch: 3/25
Train loss: 2.6194, val loss: 1.6812
0.8949029221123895
Model improve: 0.8859 -> 0.8949
Epoch: 4/25
Date :04/20/2023, 01:09:01
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: eca_nfnet_l0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
SAM (
Parameter Group 0
    adaptive: False
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    rho: 0.05
    weight_decay: 0.01
)
Epoch: 1/25
Train loss: 6.7217, val loss: 2.1042
0.8597847944759237
Model improve: 0.0000 -> 0.8598
Epoch: 2/25
Train loss: 2.9125, val loss: 1.7546
0.8931228768282082
Model improve: 0.8598 -> 0.8931
Epoch: 3/25
Train loss: 2.5866, val loss: 1.6371
0.8993843950926528
Model improve: 0.8931 -> 0.8994
Epoch: 4/25
Train loss: 2.2880, val loss: 1.5413
0.9016573443299868
Model improve: 0.8994 -> 0.9017
Epoch: 5/25
Train loss: 2.2780, val loss: 1.6122
0.9039266178978611
Model improve: 0.9017 -> 0.9039
Epoch: 6/25
Train loss: 2.1628, val loss: 1.5374
0.9094345000010752
Model improve: 0.9039 -> 0.9094
Epoch: 7/25
Train loss: 2.0107, val loss: 1.5462
0.9080795295473852
Epoch: 8/25
Date :04/20/2023, 01:35:47
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 16
validbs: 64
epochwarmup: 0
totalepoch: 25
learningrate: 0.0007
weightdecay: 0.0
thrupsample: 50
model_name: eca_nfnet_l0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0007
    lr: 0.0007
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Train loss: 6.4638, val loss: 2.0037
0.8719659944812755
Model improve: 0.0000 -> 0.8720
Epoch: 2/25
Train loss: 2.8089, val loss: 1.7326
0.8912085390548374
Model improve: 0.8720 -> 0.8912
Epoch: 3/25
Train loss: 2.4879, val loss: 1.6098
0.9006593657508363
Model improve: 0.8912 -> 0.9007
Epoch: 4/25
Train loss: 2.2698, val loss: 1.6290
0.9042029964365468
Model improve: 0.9007 -> 0.9042
Epoch: 5/25
Train loss: 2.1888, val loss: 1.4955
0.9089731014686786
Model improve: 0.9042 -> 0.9090
Epoch: 6/25
Train loss: 2.0698, val loss: 1.4993
0.9098723078531902
Model improve: 0.9090 -> 0.9099
Epoch: 7/25
Train loss: 1.9796, val loss: 1.4753
0.9106529228619159
Model improve: 0.9099 -> 0.9107
Epoch: 8/25
Train loss: 1.9036, val loss: 1.4675
0.9124076298993348
Model improve: 0.9107 -> 0.9124
Epoch: 9/25
Train loss: 1.7931, val loss: 1.5077
0.9110336383364395
Epoch: 10/25
Train loss: 1.8431, val loss: 1.4251
0.9159932070461783
Model improve: 0.9124 -> 0.9160
Epoch: 11/25
Train loss: 1.7754, val loss: 1.4778
0.9146776142666947
Epoch: 12/25
Train loss: 1.7916, val loss: 1.4633
0.9128891056707688
Epoch: 13/25
Train loss: 1.7112, val loss: 1.4540
0.9165438743685135
Model improve: 0.9160 -> 0.9165
Epoch: 14/25
Train loss: 1.6454, val loss: 1.5028
0.9152994654906327
Epoch: 15/25
Train loss: 1.6819, val loss: 1.5158
0.9172985893177833
Model improve: 0.9165 -> 0.9173
Epoch: 16/25
Train loss: 1.6205, val loss: 1.4846
0.9151804423171791
Epoch: 17/25
Train loss: 1.6319, val loss: 1.4819
0.9176632439492847
Model improve: 0.9173 -> 0.9177
Epoch: 18/25
Train loss: 1.5910, val loss: 1.4384
0.9182029877964346
Model improve: 0.9177 -> 0.9182
Epoch: 19/25
Train loss: 1.5350, val loss: 1.4210
0.9194165599487049
Model improve: 0.9182 -> 0.9194
Epoch: 20/25
Train loss: 1.5317, val loss: 1.4809
0.9184775867903381
Epoch: 21/25
Train loss: 1.5894, val loss: 1.4429
0.917978827655363
Epoch: 22/25
Train loss: 1.5332, val loss: 1.4499
0.918387458192924
Epoch: 23/25
Train loss: 1.5023, val loss: 1.4512
0.9187758319763641
Epoch: 24/25
Train loss: 1.5604, val loss: 1.4505
0.9187092315951084
Epoch: 25/25
Date :04/20/2023, 00:00:48
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnet_b0_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Train loss: 9.0164, val loss: 4.2659
0.6236718262376362
Model improve: 0.0000 -> 0.6237
Epoch: 2/25
Train loss: 4.7135, val loss: 3.2034
0.7560054107683807
Model improve: 0.6237 -> 0.7560
Epoch: 3/25
Train loss: 3.9572, val loss: 2.6666
0.803596275485646
Model improve: 0.7560 -> 0.8036
Epoch: 4/25
Train loss: 3.5997, val loss: 2.3978
0.8341555072185217
Model improve: 0.8036 -> 0.8342
Epoch: 5/25
Train loss: 3.3981, val loss: 2.1669
0.8500356305817048
Model improve: 0.8342 -> 0.8500
Epoch: 6/25
Train loss: 3.1668, val loss: 2.0954
0.8578262864151506
Model improve: 0.8500 -> 0.8578
Epoch: 7/25
Train loss: 2.8832, val loss: 1.9460
0.8671862956747945
Model improve: 0.8578 -> 0.8672
Epoch: 8/25
Train loss: 2.7966, val loss: 1.8540
0.8735480925762883
Model improve: 0.8672 -> 0.8735
Epoch: 9/25
Train loss: 2.8392, val loss: 1.8757
0.8770812373998456
Model improve: 0.8735 -> 0.8771
Epoch: 10/25
Train loss: 2.6167, val loss: 1.7768
0.8789394948491468
Model improve: 0.8771 -> 0.8789
Epoch: 11/25
Train loss: 2.5476, val loss: 1.7104
0.885200075667839
Model improve: 0.8789 -> 0.8852
Epoch: 12/25
Train loss: 2.4785, val loss: 1.6220
0.8889344459418351
Model improve: 0.8852 -> 0.8889
Epoch: 13/25
Train loss: 2.3627, val loss: 1.5984
0.8899137499805111
Model improve: 0.8889 -> 0.8899
Epoch: 14/25
Train loss: 2.2369, val loss: 1.5594
0.8940411115209972
Model improve: 0.8899 -> 0.8940
Epoch: 15/25
Train loss: 2.2791, val loss: 1.5323
0.8956951806331045
Model improve: 0.8940 -> 0.8957
Epoch: 16/25
Train loss: 2.1824, val loss: 1.5030
0.8983908675969264
Model improve: 0.8957 -> 0.8984
Epoch: 17/25
Train loss: 2.0444, val loss: 1.4470
0.8985400457761185
Model improve: 0.8984 -> 0.8985
Epoch: 18/25
Train loss: 2.0935, val loss: 1.4653
0.9000076576139762
Model improve: 0.8985 -> 0.9000
Epoch: 19/25
Train loss: 2.0795, val loss: 1.4480
0.9016210811358102
Model improve: 0.9000 -> 0.9016
Epoch: 20/25
Train loss: 2.1340, val loss: 1.4431
0.9017173663088724
Model improve: 0.9016 -> 0.9017
Epoch: 21/25
Train loss: 2.0962, val loss: 1.4415
0.9024622936815674
Model improve: 0.9017 -> 0.9025
Epoch: 22/25
Train loss: 1.9536, val loss: 1.3952
0.9030611923302552
Model improve: 0.9025 -> 0.9031
Epoch: 23/25
Train loss: 1.8523, val loss: 1.4128
0.9030373094602356
Epoch: 24/25
Train loss: 2.0029, val loss: 1.4156
0.9033311609983288
Model improve: 0.9031 -> 0.9033
Epoch: 25/25
Train loss: 1.9526, val loss: 1.3843
0.904515844218996
Model improve: 0.9033 -> 0.9045
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Train loss: 9.5760, val loss: 4.4547
0.6154695143869073
Model improve: 0.0000 -> 0.6155
Epoch: 2/25
Train loss: 4.7406, val loss: 3.1201
0.7476133443246297
Model improve: 0.6155 -> 0.7476
Epoch: 3/25
Train loss: 4.0162, val loss: 2.6998
0.795598681571381
Model improve: 0.7476 -> 0.7956
Epoch: 4/25
Train loss: 3.5641, val loss: 2.3977
0.8240304791750366
Model improve: 0.7956 -> 0.8240
Epoch: 5/25
Train loss: 3.2788, val loss: 2.1156
0.8422066346889625
Model improve: 0.8240 -> 0.8422
Epoch: 6/25
Train loss: 3.1017, val loss: 2.0633
0.8525730145115679
Model improve: 0.8422 -> 0.8526
Epoch: 7/25
Train loss: 2.9315, val loss: 1.9888
0.8571290603991212
Model improve: 0.8526 -> 0.8571
Epoch: 8/25
Train loss: 2.6902, val loss: 1.8872
0.8639835762491889
Model improve: 0.8571 -> 0.8640
Epoch: 9/25
Train loss: 2.5919, val loss: 1.7453
0.8735696404522304
Model improve: 0.8640 -> 0.8736
Epoch: 10/25
Train loss: 2.6475, val loss: 1.7777
0.8755259207237709
Model improve: 0.8736 -> 0.8755
Epoch: 11/25
Train loss: 2.4176, val loss: 1.7188
0.8765912688735049
Model improve: 0.8755 -> 0.8766
Epoch: 12/25
Train loss: 2.4246, val loss: 1.6802
0.8817610325777537
Model improve: 0.8766 -> 0.8818
Epoch: 13/25
Train loss: 2.3248, val loss: 1.6343
0.8820481153065669
Model improve: 0.8818 -> 0.8820
Epoch: 14/25
Train loss: 2.2690, val loss: 1.5982
0.8847513247044366
Model improve: 0.8820 -> 0.8848
Epoch: 15/25
Train loss: 2.2631, val loss: 1.6173
0.8857707253704394
Model improve: 0.8848 -> 0.8858
Epoch: 16/25
Train loss: 1.9943, val loss: 1.5296
0.8887088265754349
Model improve: 0.8858 -> 0.8887
Epoch: 17/25
Train loss: 2.1967, val loss: 1.5121
0.8892160374360635
Model improve: 0.8887 -> 0.8892
Epoch: 18/25
Train loss: 2.0694, val loss: 1.5214
0.8888785476132574
Epoch: 19/25
Train loss: 1.9009, val loss: 1.4813
0.8920378455458097
Model improve: 0.8916 -> 0.8920
Epoch: 22/25
Train loss: 1.9537, val loss: 1.4774
0.8919674689905022
Epoch: 23/25
Train loss: 2.0490, val loss: 1.4564
0.8927709931692238
Model improve: 0.8920 -> 0.8928
Epoch: 24/25
Train loss: 1.9160, val loss: 1.4492
0.8933945441784927
Model improve: 0.8928 -> 0.8934
Epoch: 25/25
Train loss: 1.9484, val loss: 1.4702
0.8930725750482155
Fold: 2
19479
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Train loss: 9.1917, val loss: 4.2815
0.6305529497023539
Model improve: 0.0000 -> 0.6306
Epoch: 2/25
Train loss: 4.7079, val loss: 3.1136
0.74950644628242
Model improve: 0.6306 -> 0.7495
Epoch: 3/25
Date :04/20/2023, 03:42:52
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 224
fmax: 16000
trainbs: 64
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnet_b0_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Train loss: 10.3876, val loss: 4.1999
0.6315664582217007
Model improve: 0.0000 -> 0.6316
Epoch: 2/25
Train loss: 4.7414, val loss: 3.1580
0.7514880559430868
Model improve: 0.6316 -> 0.7515
Epoch: 3/25
Train loss: 4.0055, val loss: 2.6160
0.802647030824353
Model improve: 0.7515 -> 0.8026
Epoch: 4/25
Date :04/20/2023, 14:56:44
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Date :04/20/2023, 15:00:20
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 48
validbs: 96
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnet_b0_ns
mix_up: 0.5
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Train loss: 8.2303, val loss: 4.0834
0.6557769978541261
Model improve: 0.0000 -> 0.6558
Epoch: 2/25
Train loss: 4.8255, val loss: 3.1255
0.7646628420812823
Model improve: 0.6558 -> 0.7647
Epoch: 3/25
Train loss: 4.2108, val loss: 2.6351
0.811696397334978
Model improve: 0.7647 -> 0.8117
Epoch: 4/25
Train loss: 3.8918, val loss: 2.4474
0.8269427907756718
Model improve: 0.8117 -> 0.8269
Epoch: 5/25
Train loss: 3.6717, val loss: 2.3202
0.8428361121552509
Model improve: 0.8269 -> 0.8428
Epoch: 6/25
Date :04/20/2023, 15:15:19
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 48
validbs: 96
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnet_b0_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: False
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Date :04/20/2023, 15:16:16
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 48
validbs: 96
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnet_b0_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: False
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Train loss: 7.6236, val loss: 3.4725
0.7027716466001243
Model improve: 0.0000 -> 0.7028
Epoch: 2/25
Train loss: 3.4205, val loss: 2.8238
0.7829888951925665
Model improve: 0.7028 -> 0.7830
Epoch: 3/25
Train loss: 2.6748, val loss: 2.4651
0.8222674844034148
Model improve: 0.7830 -> 0.8223
Epoch: 4/25
Train loss: 2.2586, val loss: 2.2907
0.8318579165221235
Model improve: 0.8223 -> 0.8319
Epoch: 5/25
Train loss: 1.9470, val loss: 2.2388
0.8395662481607123
Model improve: 0.8319 -> 0.8396
Epoch: 6/25
Date :04/20/2023, 15:32:02
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 48
validbs: 96
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnet_b0_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Train loss: 8.0802, val loss: 3.9184
0.664890706976627
Model improve: 0.0000 -> 0.6649
Epoch: 2/25
Train loss: 4.4856, val loss: 3.0838
0.7696675403209028
Model improve: 0.6649 -> 0.7697
Epoch: 3/25
Train loss: 3.8791, val loss: 2.5857
0.8178261134635547
Model improve: 0.7697 -> 0.8178
Epoch: 4/25
Train loss: 3.5297, val loss: 2.4174
0.8255549934476655
Model improve: 0.8178 -> 0.8256
Epoch: 5/25
Train loss: 3.2700, val loss: 2.2106
0.8455302359304143
Model improve: 0.8256 -> 0.8455
Epoch: 6/25
Train loss: 3.0032, val loss: 2.0382
0.8595061631915704
Model improve: 0.8455 -> 0.8595
Epoch: 7/25
Train loss: 2.9825, val loss: 1.9530
0.865667277255796
Model improve: 0.8595 -> 0.8657
Epoch: 8/25
Train loss: 2.8252, val loss: 1.9509
0.8697411680344171
Model improve: 0.8657 -> 0.8697
Epoch: 9/25
Train loss: 2.6846, val loss: 1.8553
0.8731467134292306
Model improve: 0.8697 -> 0.8731
Epoch: 10/25
Train loss: 2.5308, val loss: 1.7905
0.877889841850261
Model improve: 0.8731 -> 0.8779
Epoch: 11/25
Train loss: 2.5125, val loss: 1.7363
0.8798258640063326
Model improve: 0.8779 -> 0.8798
Epoch: 12/25
Date :04/20/2023, 16:05:15
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 48
validbs: 96
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnet_b0_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Date :04/20/2023, 16:05:43
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 8
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnet_b0_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/25
Train loss: 6.2332, val loss: 4.1025
0.694572561554336
Model improve: 0.0000 -> 0.6946
Epoch: 2/25
Train loss: 4.5620, val loss: 3.1758
0.7702857180566349
Model improve: 0.6946 -> 0.7703
Epoch: 3/25
Date :04/20/2023, 16:25:55
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnet_b0_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/25
Train loss: 9.6015, val loss: 4.1348
0.6389717831813587
Model improve: 0.0000 -> 0.6390
Epoch: 2/25
Date :04/20/2023, 16:28:52
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnet_b0_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/25
Train loss: 9.6267, val loss: 4.0924
0.6423652571985464
Model improve: 0.0000 -> 0.6424
Epoch: 2/25
Train loss: 4.6525, val loss: 3.1850
0.7574400020095201
Model improve: 0.6424 -> 0.7574
Epoch: 3/25
Train loss: 3.9409, val loss: 2.7051
0.7998222391826912
Model improve: 0.7574 -> 0.7998
Epoch: 4/25
Train loss: 3.5926, val loss: 2.3831
0.8306646521636127
Model improve: 0.7998 -> 0.8307
Epoch: 5/25
Train loss: 3.4101, val loss: 2.2573
0.8452174506930008
Model improve: 0.8307 -> 0.8452
Epoch: 6/25
Train loss: 3.1882, val loss: 2.1234
0.8536049861746434
Model improve: 0.8452 -> 0.8536
Epoch: 7/25
Train loss: 2.9041, val loss: 1.9574
0.8617202200121672
Model improve: 0.8536 -> 0.8617
Epoch: 8/25
Train loss: 2.8236, val loss: 1.8876
0.8671819182727019
Model improve: 0.8617 -> 0.8672
Epoch: 9/25
Date :04/20/2023, 16:50:20
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnet_b0_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Train loss: 9.0844, val loss: 4.0986
0.6494679804711535
Model improve: 0.0000 -> 0.6495
Epoch: 2/25
Train loss: 4.5620, val loss: 3.1524
0.7587282417298259
Model improve: 0.6495 -> 0.7587
Epoch: 3/25
Train loss: 3.8247, val loss: 2.6282
0.8052638030296129
Model improve: 0.7587 -> 0.8053
Epoch: 4/25
Train loss: 3.4736, val loss: 2.3945
0.8321761817319483
Model improve: 0.8053 -> 0.8322
Epoch: 5/25
Train loss: 3.2849, val loss: 2.1639
0.8478930180019707
Model improve: 0.8322 -> 0.8479
Epoch: 6/25
Train loss: 3.0620, val loss: 2.0978
0.8578990813756388
Model improve: 0.8479 -> 0.8579
Epoch: 7/25
Train loss: 2.7702, val loss: 1.9805
0.8617208384450611
Model improve: 0.8579 -> 0.8617
Epoch: 8/25
Train loss: 2.6845, val loss: 1.8649
0.8708355248479901
Model improve: 0.8617 -> 0.8708
Epoch: 9/25
Train loss: 2.7363, val loss: 1.8864
0.872315954588947
Model improve: 0.8708 -> 0.8723
Epoch: 10/25
Train loss: 2.5108, val loss: 1.7835
0.8783238426405074
Model improve: 0.8723 -> 0.8783
Epoch: 11/25
Train loss: 2.4478, val loss: 1.7643
0.8783517242531244
Model improve: 0.8783 -> 0.8784
Epoch: 12/25
Train loss: 2.3739, val loss: 1.6583
0.8869838281533687
Model improve: 0.8784 -> 0.8870
Epoch: 13/25
Train loss: 2.2501, val loss: 1.6628
0.8856798778606878
Epoch: 14/25
Train loss: 2.1398, val loss: 1.6267
0.8863653337662587
Epoch: 15/25
Train loss: 2.1869, val loss: 1.5618
0.8905880172283881
Model improve: 0.8870 -> 0.8906
Epoch: 16/25
Train loss: 2.0867, val loss: 1.5624
0.8932167894306079
Model improve: 0.8906 -> 0.8932
Epoch: 17/25
Train loss: 1.9512, val loss: 1.4774
0.8937027033153928
Model improve: 0.8932 -> 0.8937
Epoch: 18/25
Train loss: 1.9976, val loss: 1.5265
0.8947646346047706
Model improve: 0.8937 -> 0.8948
Epoch: 19/25
Date :04/20/2023, 17:39:11
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnet_b0_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Train loss: 10.5302, val loss: 4.5185
0.6076623044016255
Model improve: 0.0000 -> 0.6077
Epoch: 2/25
Train loss: 5.1165, val loss: 3.4855
0.7210930808752885
Model improve: 0.6077 -> 0.7211
Epoch: 3/25
Date :04/20/2023, 17:45:06
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnet_b0_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Train loss: 9.7299, val loss: 4.3431
0.6181066558221262
Model improve: 0.0000 -> 0.6181
Epoch: 2/25
Date :04/20/2023, 17:49:09
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnet_b0_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Train loss: 9.5981, val loss: 4.1916
0.6361576983128066
Model improve: 0.0000 -> 0.6362
Epoch: 2/25
Train loss: 4.6319, val loss: 3.2133
0.759739186304416
Model improve: 0.6362 -> 0.7597
Epoch: 3/25
Train loss: 3.9036, val loss: 2.7699
0.7986515520094333
Model improve: 0.7597 -> 0.7987
Epoch: 4/25
Date :04/20/2023, 17:57:19
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 160
fmax: 16000
trainbs: 64
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnet_b0_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Train loss: 9.6249, val loss: 4.1196
0.6382810909280123
Model improve: 0.0000 -> 0.6383
Epoch: 2/25
Train loss: 4.6470, val loss: 3.3702
0.7517478779084907
Model improve: 0.6383 -> 0.7517
Epoch: 3/25
Train loss: 3.8881, val loss: 2.5611
0.8097851582715047
Model improve: 0.7517 -> 0.8098
Epoch: 4/25
Train loss: 3.5814, val loss: 2.5398
0.8260301361905307
Model improve: 0.8098 -> 0.8260
Epoch: 5/25
Train loss: 3.2834, val loss: 2.2556
0.8409797490501124
Model improve: 0.8260 -> 0.8410
Epoch: 6/25
Train loss: 3.0994, val loss: 2.1109
0.8559054107578133
Model improve: 0.8410 -> 0.8559
Epoch: 7/25
Train loss: 2.8787, val loss: 1.9059
0.8708508064867251
Model improve: 0.8559 -> 0.8709
Epoch: 8/25
Train loss: 2.7295, val loss: 1.8574
0.8693131946236693
Epoch: 9/25
Train loss: 2.6203, val loss: 1.8249
0.8750926572853478
Model improve: 0.8709 -> 0.8751
Epoch: 10/25
Train loss: 2.4467, val loss: 1.7145
0.8830532321356024
Model improve: 0.8751 -> 0.8831
Epoch: 11/25
Train loss: 2.5188, val loss: 1.7306
0.8832087707824691
Model improve: 0.8831 -> 0.8832
Epoch: 12/25
Train loss: 2.4446, val loss: 1.6322
0.888673674381062
Model improve: 0.8832 -> 0.8887
Epoch: 13/25
Train loss: 2.4010, val loss: 1.6417
0.8882567494451948
Epoch: 14/25
Train loss: 2.3377, val loss: 1.6821
0.8891256351109373
Model improve: 0.8887 -> 0.8891
Epoch: 15/25
Date :04/20/2023, 19:05:02
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 96
fmax: 16000
trainbs: 64
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnet_b0_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Train loss: 9.1068, val loss: 4.1668
0.632642347283954
Model improve: 0.0000 -> 0.6326
Epoch: 2/25
Train loss: 4.6101, val loss: 3.1239
0.7531735749268583
Model improve: 0.6326 -> 0.7532
Epoch: 3/25
Train loss: 3.9389, val loss: 2.6437
0.7994956746064305
Model improve: 0.7532 -> 0.7995
Epoch: 4/25
Train loss: 3.6029, val loss: 2.4856
0.8241197935262361
Model improve: 0.7995 -> 0.8241
Epoch: 5/25
Train loss: 3.3185, val loss: 2.2035
0.8416463352471285
Model improve: 0.8241 -> 0.8416
Epoch: 6/25
Train loss: 3.1978, val loss: 2.2078
0.8434807435440232
Model improve: 0.8416 -> 0.8435
Epoch: 7/25
Date :04/20/2023, 19:19:56
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnet_b0_ns
mix_up: 0.2
hop_length: 192
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Train loss: 9.8945, val loss: 4.2582
0.6237665784085684
Model improve: 0.0000 -> 0.6238
Epoch: 2/25
Train loss: 4.7012, val loss: 3.1918
0.7514607248313069
Model improve: 0.6238 -> 0.7515
Epoch: 3/25
Train loss: 3.9673, val loss: 2.6442
0.8015741238841261
Model improve: 0.7515 -> 0.8016
Epoch: 4/25
Train loss: 3.5955, val loss: 2.4432
0.8292060897178957
Model improve: 0.8016 -> 0.8292
Epoch: 5/25
Train loss: 3.2549, val loss: 2.2601
0.8410872883921366
Model improve: 0.8292 -> 0.8411
Epoch: 6/25
Train loss: 3.1175, val loss: 2.0376
0.8602990239178371
Model improve: 0.8411 -> 0.8603
Epoch: 7/25
Train loss: 2.8451, val loss: 1.9223
0.8694696551809067
Model improve: 0.8603 -> 0.8695
Epoch: 8/25
Train loss: 2.7484, val loss: 1.9379
0.8705320856338515
Model improve: 0.8695 -> 0.8705
Epoch: 9/25
Train loss: 2.7660, val loss: 1.8178
0.8756827611936007
Model improve: 0.8705 -> 0.8757
Epoch: 10/25
Train loss: 2.5492, val loss: 1.7721
0.8772146838067052
Model improve: 0.8757 -> 0.8772
Epoch: 11/25
Train loss: 2.3933, val loss: 1.7188
0.8826387834433043
Model improve: 0.8772 -> 0.8826
Epoch: 12/25
Train loss: 2.3602, val loss: 1.6678
0.8884767381416127
Model improve: 0.8826 -> 0.8885
Epoch: 13/25
Train loss: 2.1819, val loss: 1.7131
0.885224148607418
Epoch: 14/25
Train loss: 2.1791, val loss: 1.5872
0.8911246509529726
Model improve: 0.8885 -> 0.8911
Epoch: 15/25
Train loss: 2.2307, val loss: 1.6045
0.8931492448587254
Model improve: 0.8911 -> 0.8931
Epoch: 16/25
Train loss: 2.2171, val loss: 1.5799
0.8950432784455943
Model improve: 0.8931 -> 0.8950
Epoch: 17/25
Train loss: 2.0760, val loss: 1.4862
0.8981145197815205
Model improve: 0.8950 -> 0.8981
Epoch: 18/25
Train loss: 2.0686, val loss: 1.4805
0.9007200331295645
Model improve: 0.8981 -> 0.9007
Epoch: 19/25
Train loss: 2.0022, val loss: 1.4990
0.9013777215701793
Model improve: 0.9007 -> 0.9014
Epoch: 20/25
Date :04/20/2023, 20:20:29
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnet_b0_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 9.5648, val loss: 3.9934
0.6566237593598329
Model improve: 0.0000 -> 0.6566
Epoch: 2/200
Train loss: 4.5793, val loss: 3.0615
0.7606445143518661
Model improve: 0.6566 -> 0.7606
Epoch: 3/200
Train loss: 3.8426, val loss: 2.6029
0.8088570533494065
Model improve: 0.7606 -> 0.8089
Epoch: 4/200
Train loss: 3.4650, val loss: 2.3407
0.8377405026934763
Model improve: 0.8089 -> 0.8377
Epoch: 5/200
Train loss: 3.2464, val loss: 2.1967
0.8478961475572298
Model improve: 0.8377 -> 0.8479
Epoch: 6/200
Train loss: 2.9836, val loss: 1.9787
0.8618525005960321
Model improve: 0.8479 -> 0.8619
Epoch: 7/200
Train loss: 2.6617, val loss: 1.7918
0.8753640203921225
Model improve: 0.8619 -> 0.8754
Epoch: 8/200
Train loss: 2.5581, val loss: 1.7069
0.8783918065633566
Model improve: 0.8754 -> 0.8784
Epoch: 9/200
Train loss: 2.6155, val loss: 1.7129
0.8810752588493636
Model improve: 0.8784 -> 0.8811
Epoch: 10/200
Train loss: 2.4333, val loss: 1.6471
0.8826372930739956
Model improve: 0.8811 -> 0.8826
Epoch: 11/200
Train loss: 3.0614, val loss: 2.1698
0.8545680669904452
Epoch: 12/200
Train loss: 2.9379, val loss: 2.0538
0.862239419978373
Epoch: 13/200
Date :04/20/2023, 20:54:02
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 64
epochwarmup: 0
totalepoch: 200
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnet_b0_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Date :04/20/2023, 20:55:52
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 64
epochwarmup: 0
totalepoch: 200
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnet_b0_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 7.5140, val loss: 3.5498
0.705562358089497
Model improve: 0.0000 -> 0.7056
Epoch: 2/200
Train loss: 4.3718, val loss: 2.8267
0.7892330060906101
Model improve: 0.7056 -> 0.7892
Epoch: 3/200
Train loss: 3.8910, val loss: 2.5436
0.8169905357164482
Model improve: 0.7892 -> 0.8170
Epoch: 4/200
Train loss: 3.4511, val loss: 2.3081
0.8379549523356467
Model improve: 0.8170 -> 0.8380
Epoch: 5/200
Train loss: 3.3469, val loss: 2.1640
0.8471212663807474
Model improve: 0.8380 -> 0.8471
Epoch: 6/200
Train loss: 3.1255, val loss: 2.1408
0.8518770641313428
Model improve: 0.8471 -> 0.8519
Epoch: 7/200
Train loss: 2.9406, val loss: 1.9628
0.863280499543301
Model improve: 0.8519 -> 0.8633
Epoch: 8/200
Train loss: 2.8949, val loss: 1.9515
0.8639350193090694
Model improve: 0.8633 -> 0.8639
Epoch: 9/200
Train loss: 2.7560, val loss: 1.8729
0.8687464328456751
Model improve: 0.8639 -> 0.8687
Epoch: 10/200
Train loss: 2.7689, val loss: 1.9479
0.8643447839555796
Epoch: 11/200
Train loss: 2.6494, val loss: 1.8308
0.87285847364446
Model improve: 0.8687 -> 0.8729
Epoch: 12/200
Train loss: 2.5722, val loss: 1.8413
0.8768577064610424
Model improve: 0.8729 -> 0.8769
Epoch: 13/200
Train loss: 2.5135, val loss: 1.7833
0.8800773670363823
Model improve: 0.8769 -> 0.8801
Epoch: 14/200
Train loss: 2.4671, val loss: 1.7018
0.8836908963301674
Model improve: 0.8801 -> 0.8837
Epoch: 15/200
Train loss: 2.4021, val loss: 1.7825
0.8798200163918946
Epoch: 16/200
Train loss: 2.4027, val loss: 1.7619
0.8817646582525304
Epoch: 17/200
Train loss: 2.2150, val loss: 1.7280
0.8824363522227813
Epoch: 18/200
Train loss: 2.3023, val loss: 1.6883
0.889493763794773
Model improve: 0.8837 -> 0.8895
Epoch: 19/200
Train loss: 2.3112, val loss: 1.7004
0.8850629768038827
Epoch: 20/200
Train loss: 2.2672, val loss: 1.6583
0.8870475790826688
Epoch: 21/200
Train loss: 2.2200, val loss: 1.6255
0.8905256471121695
Model improve: 0.8895 -> 0.8905
Epoch: 22/200
Train loss: 2.2159, val loss: 1.6766
0.887794185904633
Epoch: 23/200
Train loss: 2.1948, val loss: 1.6626
0.8872077909376604
Epoch: 24/200
Train loss: 2.2125, val loss: 1.6245
0.8937019813120645
Model improve: 0.8905 -> 0.8937
Epoch: 25/200
Train loss: 2.1557, val loss: 1.6426
0.8934105255931134
Epoch: 26/200
Train loss: 2.1451, val loss: 1.5853
0.893037594940238
Epoch: 27/200
Train loss: 2.0450, val loss: 1.7293
0.8860005973620306
Epoch: 28/200
Train loss: 2.0731, val loss: 1.6212
0.8934776121437927
Epoch: 29/200
Train loss: 2.1138, val loss: 1.6049
0.8937840817166598
Model improve: 0.8937 -> 0.8938
Epoch: 30/200
Train loss: 2.0938, val loss: 1.6673
0.8910097034585381
Epoch: 31/200
Train loss: 2.0194, val loss: 1.5562
0.8972017747343473
Model improve: 0.8938 -> 0.8972
Epoch: 32/200
Train loss: 2.0404, val loss: 1.6454
0.8904925404807628
Epoch: 33/200
Train loss: 2.0890, val loss: 1.6489
0.8908129540854054
Epoch: 34/200
Date :04/20/2023, 22:54:02
Duration: 5
Sample rate: 32000
nfft: 1024
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 64
epochwarmup: 0
totalepoch: 200
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnet_b0_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 7.5594, val loss: 3.6382
0.7021034008782547
Model improve: 0.0000 -> 0.7021
Epoch: 2/200
Train loss: 4.4206, val loss: 2.9113
0.7824802516904423
Model improve: 0.7021 -> 0.7825
Epoch: 3/200
Train loss: 3.9324, val loss: 2.5673
0.8191581020716637
Model improve: 0.7825 -> 0.8192
Epoch: 4/200
Train loss: 3.4760, val loss: 2.3180
0.8379027447966814
Model improve: 0.8192 -> 0.8379
Epoch: 5/200
Train loss: 3.3590, val loss: 2.1398
0.8519154347692437
Model improve: 0.8379 -> 0.8519
Epoch: 6/200
Train loss: 3.1392, val loss: 2.1045
0.8557627621934906
Model improve: 0.8519 -> 0.8558
Epoch: 7/200
Train loss: 2.9516, val loss: 1.9236
0.8680221540606968
Model improve: 0.8558 -> 0.8680
Epoch: 8/200
Train loss: 2.8996, val loss: 1.9636
0.8675907994558848
Epoch: 9/200
Train loss: 2.7608, val loss: 1.7956
0.8791071193882404
Model improve: 0.8680 -> 0.8791
Epoch: 10/200
Train loss: 2.7728, val loss: 1.9205
0.8718539330549004
Epoch: 11/200
Train loss: 2.6667, val loss: 1.7758
0.8766631602292305
Epoch: 12/200
Train loss: 2.5935, val loss: 1.7662
0.879882673708618
Model improve: 0.8791 -> 0.8799
Epoch: 13/200
Train loss: 2.5340, val loss: 1.7655
0.8808912065401321
Model improve: 0.8799 -> 0.8809
Epoch: 14/200
Train loss: 2.4906, val loss: 1.6340
0.8887050191766891
Model improve: 0.8809 -> 0.8887
Epoch: 15/200
Train loss: 2.4236, val loss: 1.7355
0.8828525774955686
Epoch: 16/200
Train loss: 2.4102, val loss: 1.7255
0.8835893972068296
Epoch: 17/200
Train loss: 2.2463, val loss: 1.7127
0.8823004165104729
Epoch: 18/200
Train loss: 2.3206, val loss: 1.7113
0.8855758720748567
Epoch: 19/200
Train loss: 2.3258, val loss: 1.6449
0.8892065502904802
Model improve: 0.8887 -> 0.8892
Epoch: 20/200
Train loss: 2.2809, val loss: 1.6367
0.8867677657871843
Epoch: 21/200
Train loss: 2.2338, val loss: 1.6713
0.8853952053472761
Epoch: 22/200
Train loss: 2.2341, val loss: 1.6217
0.8914418650093295
Model improve: 0.8892 -> 0.8914
Epoch: 23/200
Train loss: 2.2068, val loss: 1.6110
0.8949010689323338
Model improve: 0.8914 -> 0.8949
Epoch: 24/200
Train loss: 2.2319, val loss: 1.6000
0.8944168731150092
Epoch: 25/200
Train loss: 2.1691, val loss: 1.6341
0.8879600852059917
Epoch: 26/200
Train loss: 2.1521, val loss: 1.5611
0.8945093380575967
Epoch: 27/200
Train loss: 2.0609, val loss: 1.6751
0.8884599854485354
Epoch: 28/200
Train loss: 2.0740, val loss: 1.6178
0.8917523420970689
Epoch: 29/200
Train loss: 2.1249, val loss: 1.5959
0.8911432122977218
Epoch: 30/200
Train loss: 2.1024, val loss: 1.5690
0.8954995776972583
Model improve: 0.8949 -> 0.8955
Epoch: 31/200
Train loss: 2.0447, val loss: 1.5457
0.8962536951332822
Model improve: 0.8955 -> 0.8963
Epoch: 32/200
Train loss: 2.0488, val loss: 1.5903
0.8932606773945546
Epoch: 33/200
Train loss: 2.0942, val loss: 1.5862
0.8942542411396999
Epoch: 34/200
Train loss: 1.9780, val loss: 1.6923
0.8891693188958404
Epoch: 35/200
Train loss: 1.9930, val loss: 1.5520
0.8961693033510253
Epoch: 36/200
Train loss: 1.9946, val loss: 1.5371
0.8962962975862909
Model improve: 0.8963 -> 0.8963
Epoch: 37/200
Train loss: 1.9013, val loss: 1.5925
0.895140315802534
Epoch: 38/200
Train loss: 1.9264, val loss: 1.5569
0.8985480352134887
Model improve: 0.8963 -> 0.8985
Epoch: 39/200
Train loss: 1.9121, val loss: 1.5672
0.8967744605845146
Epoch: 40/200
Train loss: 1.9336, val loss: 1.5799
0.8965234855079922
Epoch: 41/200
Train loss: 1.9983, val loss: 1.5456
0.8965580305506585
Epoch: 42/200
Train loss: 1.9163, val loss: 1.5862
0.8940946093767906
Epoch: 43/200
Train loss: 1.8686, val loss: 1.6063
0.8916729594934442
Epoch: 44/200
Train loss: 1.9436, val loss: 1.5472
0.8985948899459599
Model improve: 0.8985 -> 0.8986
Epoch: 45/200
Train loss: 1.8821, val loss: 1.5242
0.9022407824661307
Model improve: 0.8986 -> 0.9022
Epoch: 46/200
Train loss: 1.7956, val loss: 1.5773
0.8968236804395371
Epoch: 47/200
Train loss: 1.8922, val loss: 1.5862
0.8980767827706394
Epoch: 48/200
Train loss: 1.9088, val loss: 1.5802
0.8966427457668846
Epoch: 49/200
Train loss: 1.8279, val loss: 1.5468
0.8974759796846268
Epoch: 50/200
Train loss: 1.9102, val loss: 1.5463
0.8974501274369654
Epoch: 51/200
Train loss: 1.7612, val loss: 1.6185
0.8922313545709842
Epoch: 52/200
Train loss: 1.9627, val loss: 1.5885
0.8975824827030443
Epoch: 53/200
Train loss: 1.7749, val loss: 1.5481
0.9010786990334946
Epoch: 54/200
Train loss: 1.8042, val loss: 1.5763
0.8995501990259966
Epoch: 55/200
Train loss: 1.7926, val loss: 1.6136
0.8956031851075542
Epoch: 56/200
Train loss: 1.8452, val loss: 1.5313
0.8992569593890372
Epoch: 57/200
Train loss: 1.8175, val loss: 1.5709
0.8977638731207873
Epoch: 58/200
Train loss: 1.8226, val loss: 1.5289
0.9006059819172236
Epoch: 59/200
Train loss: 1.7863, val loss: 1.5742
0.8959934034153773
Epoch: 60/200
Train loss: 1.7671, val loss: 1.5548
0.8971185578220379
Epoch: 61/200
Train loss: 1.8276, val loss: 1.5594
0.8996559631719714
Epoch: 62/200
Train loss: 1.8411, val loss: 1.5383
0.8976710598740157
Epoch: 63/200
Train loss: 1.7870, val loss: 1.5239
0.8995558172319019
Epoch: 64/200
Train loss: 1.7961, val loss: 1.5428
0.8988770348666142
Epoch: 65/200
Train loss: 1.6621, val loss: 1.5909
0.8953021805927054
Epoch: 66/200
Train loss: 1.7673, val loss: 1.5635
0.8982267334485865
Epoch: 67/200
Train loss: 1.7369, val loss: 1.5050
0.9015590989363973
Epoch: 68/200
Train loss: 1.7402, val loss: 1.5069
0.902181372521802
Epoch: 69/200
Train loss: 1.7143, val loss: 1.5332
0.9003435631120804
Epoch: 70/200
Train loss: 1.6896, val loss: 1.6079
0.8960233305518663
Epoch: 71/200
Train loss: 1.7352, val loss: 1.4891
0.9033667288379987
Model improve: 0.9022 -> 0.9034
Epoch: 72/200
Train loss: 1.6380, val loss: 1.5005
0.903478939989278
Model improve: 0.9034 -> 0.9035
Epoch: 73/200
Train loss: 1.6272, val loss: 1.5267
0.9001310686329309
Epoch: 74/200
Train loss: 1.6567, val loss: 1.4998
0.8994396270458058
Epoch: 75/200
Train loss: 1.6745, val loss: 1.5167
0.9010883855210156
Epoch: 76/200
Train loss: 1.7753, val loss: 1.5229
0.9017944729453086
Epoch: 77/200
Train loss: 1.5743, val loss: 1.5498
0.9011509917415441
Epoch: 78/200
Train loss: 1.6470, val loss: 1.5285
0.9031121413177255
Epoch: 79/200
Train loss: 1.6915, val loss: 1.5528
0.9028573732829531
Epoch: 80/200
Train loss: 1.6331, val loss: 1.5420
0.9014247059277647
Epoch: 81/200
Train loss: 1.7645, val loss: 1.5014
0.9029741168598359
Epoch: 82/200
Train loss: 1.6386, val loss: 1.5648
0.9013616827849591
Epoch: 83/200
Train loss: 1.5616, val loss: 1.5507
0.9034656673990547
Epoch: 84/200
Train loss: 1.6924, val loss: 1.4561
0.9073446059603631
Model improve: 0.9035 -> 0.9073
Epoch: 85/200
Train loss: 1.6228, val loss: 1.4799
0.9050752096617151
Epoch: 86/200
Train loss: 1.6129, val loss: 1.5105
0.9024967068136984
Epoch: 87/200
Train loss: 1.6400, val loss: 1.4988
0.9033688088234763
Epoch: 88/200
Train loss: 1.6373, val loss: 1.5326
0.9044186801729299
Epoch: 89/200
Train loss: 1.7250, val loss: 1.5068
0.9010300043713552
Epoch: 90/200
Train loss: 1.5860, val loss: 1.5045
0.9023373073697659
Epoch: 91/200
Train loss: 1.6493, val loss: 1.5742
0.8997381983339029
Epoch: 92/200
Train loss: 1.6420, val loss: 1.5238
0.9033274085206533
Epoch: 93/200
Train loss: 1.5819, val loss: 1.5117
0.9056366299617271
Epoch: 94/200
Train loss: 1.6163, val loss: 1.4994
0.9046147358901709
Epoch: 95/200
Train loss: 1.5486, val loss: 1.4781
0.9038721550810528
Epoch: 96/200
Train loss: 1.5803, val loss: 1.5534
0.9020677741442953
Epoch: 97/200
Train loss: 1.6103, val loss: 1.5325
0.903211356574052
Epoch: 98/200
Train loss: 1.5470, val loss: 1.5293
0.9023313372358057
Epoch: 99/200
Train loss: 1.5774, val loss: 1.4870
0.9070538446250167
Epoch: 100/200
Train loss: 1.5859, val loss: 1.4511
0.905426976385789
Epoch: 101/200
Train loss: 1.6019, val loss: 1.5129
0.9025851485470068
Epoch: 102/200
Train loss: 1.4638, val loss: 1.4558
0.9095933582868925
Model improve: 0.9073 -> 0.9096
Epoch: 103/200
Train loss: 1.5904, val loss: 1.4963
0.905035495923698
Epoch: 104/200
Train loss: 1.5658, val loss: 1.5042
0.9053526457927028
Epoch: 105/200
Train loss: 1.5835, val loss: 1.4619
0.9072603468848275
Epoch: 106/200
Train loss: 1.5310, val loss: 1.4895
0.9075854343442874
Epoch: 107/200
Train loss: 1.5905, val loss: 1.5274
0.9049625393141283
Epoch: 108/200
Train loss: 1.4868, val loss: 1.4320
0.9093864703564413
Epoch: 109/200
Train loss: 1.5819, val loss: 1.5028
0.9070481437486714
Epoch: 110/200
Train loss: 1.4995, val loss: 1.5046
0.9081359291629605
Epoch: 111/200
Train loss: 1.5490, val loss: 1.4931
0.9072921512111473
Epoch: 112/200
Train loss: 1.5369, val loss: 1.4823
0.9057342741483532
Epoch: 113/200
Train loss: 1.5141, val loss: 1.5283
0.9056625706641593
Epoch: 114/200
Train loss: 1.5998, val loss: 1.4547
0.9076137341405908
Epoch: 115/200
Train loss: 1.5441, val loss: 1.5387
0.9013541350601837
Epoch: 116/200
Train loss: 1.5389, val loss: 1.4670
0.9044059115217438
Epoch: 117/200
Train loss: 1.4903, val loss: 1.4763
0.9068959960699202
Epoch: 118/200
Train loss: 1.6660, val loss: 1.5490
0.9031422064357769
Epoch: 119/200
Train loss: 1.5122, val loss: 1.5371
0.9042441978929059
Epoch: 120/200
Train loss: 1.4604, val loss: 1.4564
0.9072343473304965
Epoch: 121/200
Train loss: 1.4639, val loss: 1.4536
0.9094163955609301
Epoch: 122/200
Train loss: 1.4720, val loss: 1.4425
0.9104817091516643
Model improve: 0.9096 -> 0.9105
Epoch: 123/200
Train loss: 1.3918, val loss: 1.4793
0.9096223998074424
Epoch: 124/200
Train loss: 1.4480, val loss: 1.4724
0.9081588764577031
Epoch: 125/200
Train loss: 1.5164, val loss: 1.5237
0.9058959678082494
Epoch: 126/200
Train loss: 1.4850, val loss: 1.4676
0.9083751085119134
Epoch: 127/200
Train loss: 1.4477, val loss: 1.4675
0.9075353310114446
Epoch: 128/200
Train loss: 1.4800, val loss: 1.4915
0.906975926637309
Epoch: 129/200
Train loss: 1.4789, val loss: 1.4652
0.9089738077939546
Epoch: 130/200
Train loss: 1.4832, val loss: 1.4707
0.9080701948482259
Epoch: 131/200
Train loss: 1.4780, val loss: 1.4756
0.9078522454392939
Epoch: 132/200
Train loss: 1.4181, val loss: 1.5227
0.9077728683811459
Epoch: 133/200
Train loss: 1.4703, val loss: 1.4821
0.9094030232846926
Epoch: 134/200
Train loss: 1.4870, val loss: 1.4703
0.9086678737420232
Epoch: 135/200
Train loss: 1.4712, val loss: 1.4538
0.9107164767298795
Model improve: 0.9105 -> 0.9107
Epoch: 136/200
Train loss: 1.4657, val loss: 1.4878
0.9098781832894431
Epoch: 137/200
Train loss: 1.5118, val loss: 1.4943
0.9086160098448752
Epoch: 138/200
Train loss: 1.4396, val loss: 1.4437
0.9105318914377699
Epoch: 139/200
Train loss: 1.4192, val loss: 1.4766
0.9098911747063038
Epoch: 140/200
Train loss: 1.5066, val loss: 1.4819
0.9098139490373046
Epoch: 141/200
Train loss: 1.4841, val loss: 1.4575
0.9092792598728415
Epoch: 142/200
Train loss: 1.4441, val loss: 1.4617
0.9093308968738707
Epoch: 143/200
Train loss: 1.3840, val loss: 1.4513
0.9098702601463199
Epoch: 144/200
Train loss: 1.4659, val loss: 1.4313
0.9098931090898352
Epoch: 145/200
Train loss: 1.4103, val loss: 1.4579
0.9105606185301214
Epoch: 146/200
Train loss: 1.4592, val loss: 1.4562
0.9111667096112629
Model improve: 0.9107 -> 0.9112
Epoch: 147/200
Train loss: 1.5328, val loss: 1.4596
0.9114571457232209
Model improve: 0.9112 -> 0.9115
Epoch: 148/200
Train loss: 1.4748, val loss: 1.4378
0.9119963694821148
Model improve: 0.9115 -> 0.9120
Epoch: 149/200
Train loss: 1.4932, val loss: 1.4509
0.9119998043668132
Model improve: 0.9120 -> 0.9120
Epoch: 150/200
Date :04/21/2023, 07:25:25
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnet_b0_ns
mix_up: 0.4
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 7.4616, val loss: 3.8829
0.6738932203509531
Model improve: 0.0000 -> 0.6739
Epoch: 2/200
Train loss: 4.6836, val loss: 3.0220
0.7729954459710674
Model improve: 0.6739 -> 0.7730
Epoch: 3/200
Train loss: 4.1484, val loss: 2.6869
0.8048783193355656
Model improve: 0.7730 -> 0.8049
Epoch: 4/200
Train loss: 3.7391, val loss: 2.4309
0.8299747370527086
Model improve: 0.8049 -> 0.8300
Epoch: 5/200
Train loss: 3.6075, val loss: 2.2601
0.8463463479026082
Model improve: 0.8300 -> 0.8463
Epoch: 6/200
Train loss: 3.4662, val loss: 2.1767
0.8507793349962073
Model improve: 0.8463 -> 0.8508
Epoch: 7/200
Train loss: 3.2470, val loss: 2.0625
0.859456835252944
Model improve: 0.8508 -> 0.8595
Epoch: 8/200
Train loss: 3.1675, val loss: 1.9391
0.8669901196028762
Model improve: 0.8595 -> 0.8670
Epoch: 9/200
Train loss: 3.1345, val loss: 1.9160
0.8708004640242205
Model improve: 0.8670 -> 0.8708
Epoch: 10/200
Train loss: 3.0602, val loss: 2.1470
0.8574029597789318
Epoch: 11/200
Train loss: 2.8830, val loss: 1.9107
0.873299234727032
Model improve: 0.8708 -> 0.8733
Epoch: 12/200
Train loss: 2.9529, val loss: 1.8454
0.8793231515035014
Model improve: 0.8733 -> 0.8793
Epoch: 13/200
Train loss: 2.9419, val loss: 1.8521
0.879900575049477
Model improve: 0.8793 -> 0.8799
Epoch: 14/200
Train loss: 2.7669, val loss: 1.7835
0.884084638178791
Model improve: 0.8799 -> 0.8841
Epoch: 15/200
Train loss: 2.7973, val loss: 1.8303
0.8810637433461707
Epoch: 16/200
Train loss: 2.7060, val loss: 1.7934
0.8804435392780987
Epoch: 17/200
Train loss: 2.7720, val loss: 1.8572
0.8839760750830427
Epoch: 18/200
Train loss: 2.6486, val loss: 1.7086
0.888305197961996
Model improve: 0.8841 -> 0.8883
Epoch: 19/200
Train loss: 2.5984, val loss: 1.6376
0.8914746190700594
Model improve: 0.8883 -> 0.8915
Epoch: 20/200
Train loss: 2.6261, val loss: 1.7133
0.8879796498282994
Epoch: 21/200
Train loss: 2.6308, val loss: 1.7073
0.8915257662123922
Model improve: 0.8915 -> 0.8915
Epoch: 22/200
Train loss: 2.5395, val loss: 1.6642
0.8923186182308304
Model improve: 0.8915 -> 0.8923
Epoch: 23/200
Train loss: 2.5044, val loss: 1.6130
0.8968244613671297
Model improve: 0.8923 -> 0.8968
Epoch: 24/200
Train loss: 2.5074, val loss: 1.6435
0.8941882416312311
Epoch: 25/200
Train loss: 2.5000, val loss: 1.6189
0.8960308198494558
Epoch: 26/200
Train loss: 2.4186, val loss: 1.6202
0.8960859162868199
Epoch: 27/200
Train loss: 2.4844, val loss: 1.7294
0.8939330012667347
Epoch: 28/200
Train loss: 2.3878, val loss: 1.5906
0.8950490001468727
Epoch: 29/200
Train loss: 2.4741, val loss: 1.6327
0.8962294306937696
Epoch: 30/200
Train loss: 2.3506, val loss: 1.5383
0.8987789201334649
Model improve: 0.8968 -> 0.8988
Epoch: 31/200
Train loss: 2.3452, val loss: 1.5962
0.8944441528179216
Epoch: 32/200
Train loss: 2.3855, val loss: 1.7265
0.8917804861302328
Epoch: 33/200
Train loss: 2.3002, val loss: 1.6296
0.8964820020655137
Epoch: 34/200
Train loss: 2.3021, val loss: 1.6125
0.8969450134100905
Epoch: 35/200
Train loss: 2.2948, val loss: 1.6340
0.8945741438338064
Epoch: 36/200
Train loss: 2.3398, val loss: 1.5813
0.8963905454407147
Epoch: 37/200
Train loss: 2.3021, val loss: 1.5832
0.8981206452124783
Epoch: 38/200
Train loss: 2.3344, val loss: 1.6096
0.8975399409795859
Epoch: 39/200
Train loss: 2.2316, val loss: 1.5860
0.8966558128891433
Epoch: 40/200
Train loss: 2.2546, val loss: 1.5280
0.9021841077237241
Model improve: 0.8988 -> 0.9022
Epoch: 41/200
Train loss: 2.2872, val loss: 1.5222
0.9039361336055194
Model improve: 0.9022 -> 0.9039
Epoch: 42/200
Train loss: 2.2996, val loss: 1.6097
0.8990519857920655
Epoch: 43/200
Train loss: 2.1785, val loss: 1.5751
0.8960090853727302
Epoch: 44/200
Train loss: 2.2491, val loss: 1.6263
0.8944849571967782
Epoch: 45/200
Train loss: 2.2291, val loss: 1.5934
0.8972955919779166
Epoch: 46/200
Train loss: 2.1859, val loss: 1.6288
0.9001729722316226
Epoch: 47/200
Date :04/21/2023, 16:13:07
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.001
weightdecay: 0.01
thrupsample: 60
model_name: tf_efficientnet_b0_ns
mix_up: 0.4
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
21533
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 7.5612, val loss: 2.4694
0.8402868671121929
Model improve: 0.0000 -> 0.8403
Epoch: 2/200
Train loss: 3.6170, val loss: 2.0696
0.8727258443454808
Model improve: 0.8403 -> 0.8727
Epoch: 3/200
Train loss: 3.1946, val loss: 1.8279
0.8882583421361263
Model improve: 0.8727 -> 0.8883
Epoch: 4/200
Train loss: 3.0126, val loss: 1.7586
0.8927426313234171
Model improve: 0.8883 -> 0.8927
Epoch: 5/200
Train loss: 2.9013, val loss: 1.7130
0.8976758751191172
Model improve: 0.8927 -> 0.8977
Epoch: 6/200
Train loss: 2.8328, val loss: 1.6788
0.8976686222095436
Epoch: 7/200
Train loss: 2.6933, val loss: 1.7205
0.8987871887564369
Model improve: 0.8977 -> 0.8988
Epoch: 8/200
Date :04/21/2023, 16:40:15
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.001
weightdecay: 0.01
thrupsample: 40
model_name: tf_efficientnet_b0_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
17727
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 8.0023, val loss: 2.5093
0.8221777488060636
Model improve: 0.0000 -> 0.8222
Epoch: 2/200
Train loss: 3.4737, val loss: 1.9796
0.8725256161473112
Model improve: 0.8222 -> 0.8725
Epoch: 3/200
Train loss: 3.0904, val loss: 1.7938
0.886422808527758
Model improve: 0.8725 -> 0.8864
Epoch: 4/200
Train loss: 2.8863, val loss: 1.7337
0.8951320887778601
Model improve: 0.8864 -> 0.8951
Epoch: 5/200
Train loss: 2.7187, val loss: 1.6168
0.8946122688519209
Epoch: 6/200
Train loss: 2.6085, val loss: 1.6098
0.8964556143313385
Model improve: 0.8951 -> 0.8965
Epoch: 7/200
Train loss: 2.5450, val loss: 1.6007
0.8994390102736137
Model improve: 0.8965 -> 0.8994
Epoch: 8/200
Train loss: 2.4341, val loss: 1.6511
0.8969619588821401
Epoch: 9/200
Train loss: 2.3491, val loss: 1.4991
0.905263902037252
Model improve: 0.8994 -> 0.9053
Epoch: 10/200
Train loss: 2.3034, val loss: 1.5062
0.9053876594394616
Model improve: 0.9053 -> 0.9054
Epoch: 11/200
Train loss: 2.3931, val loss: 1.5245
0.9017149064133128
Epoch: 12/200
Train loss: 2.3186, val loss: 1.5080
0.9050840127634523
Epoch: 13/200
Train loss: 2.1766, val loss: 1.5298
0.8995571781946119
Epoch: 14/200
Train loss: 2.2363, val loss: 1.5367
0.9035311486979112
Epoch: 15/200
Train loss: 2.1582, val loss: 1.4768
0.9045156495588729
Epoch: 16/200
Train loss: 2.1707, val loss: 1.4777
0.9059144602277902
Model improve: 0.9054 -> 0.9059
Epoch: 17/200
Train loss: 2.1114, val loss: 1.4766
0.9064977278121353
Model improve: 0.9059 -> 0.9065
Epoch: 18/200
Train loss: 2.0657, val loss: 1.4687
0.9066886661513553
Model improve: 0.9065 -> 0.9067
Epoch: 19/200
Train loss: 2.0369, val loss: 1.5070
0.9046329482804273
Epoch: 20/200
Train loss: 2.0697, val loss: 1.4498
0.9058539821784763
Epoch: 21/200
Train loss: 2.0558, val loss: 1.4132
0.9086323686063517
Model improve: 0.9067 -> 0.9086
Epoch: 22/200
Train loss: 2.0621, val loss: 1.4363
0.9082193978625746
Epoch: 23/200
Train loss: 1.9774, val loss: 1.5021
0.9059107323532806
Epoch: 24/200
Train loss: 2.0466, val loss: 1.5506
0.9056309330396132
Epoch: 25/200
Train loss: 2.0317, val loss: 1.4948
0.9058754472245355
Epoch: 26/200
Train loss: 1.9552, val loss: 1.5104
0.9052656403489214
Epoch: 27/200
Train loss: 2.0325, val loss: 1.5122
0.9037362187453737
Epoch: 28/200
Train loss: 1.9680, val loss: 1.4396
0.9049011392043934
Epoch: 29/200
Train loss: 1.9803, val loss: 1.4080
0.9080058478068627
Epoch: 30/200
Train loss: 1.8583, val loss: 1.4127
0.9083275436751538
Epoch: 31/200
Train loss: 1.9271, val loss: 1.4188
0.9099502825903387
Model improve: 0.9086 -> 0.9100
Epoch: 32/200
Train loss: 1.9944, val loss: 1.3915
0.910807623924368
Model improve: 0.9100 -> 0.9108
Epoch: 33/200
Train loss: 1.9363, val loss: 1.4867
0.9075505285645221
Epoch: 34/200
Date :04/21/2023, 18:25:22
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.001
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13830
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 8.2480, val loss: 2.8277
0.7952177309416104
Model improve: 0.0000 -> 0.7952
Epoch: 2/200
Train loss: 3.6081, val loss: 2.1567
0.8451360055908476
Model improve: 0.7952 -> 0.8451
Epoch: 3/200
Train loss: 3.1565, val loss: 1.8989
0.8641089301575864
Model improve: 0.8451 -> 0.8641
Epoch: 4/200
Train loss: 3.0313, val loss: 1.7673
0.8813017282296015
Model improve: 0.8641 -> 0.8813
Epoch: 5/200
Train loss: 2.8057, val loss: 1.7247
0.8857996630895401
Model improve: 0.8813 -> 0.8858
Epoch: 6/200
Train loss: 2.7644, val loss: 1.6962
0.8903034500253846
Model improve: 0.8858 -> 0.8903
Epoch: 7/200
Train loss: 2.6892, val loss: 1.6185
0.8941542531640305
Model improve: 0.8903 -> 0.8942
Epoch: 8/200
Train loss: 2.5860, val loss: 1.5963
0.8965255453000227
Model improve: 0.8942 -> 0.8965
Epoch: 9/200
Train loss: 2.5365, val loss: 1.6258
0.8976258741118334
Model improve: 0.8965 -> 0.8976
Epoch: 10/200
Train loss: 2.4180, val loss: 1.5608
0.9000900798596048
Model improve: 0.8976 -> 0.9001
Epoch: 11/200
Train loss: 2.4240, val loss: 1.4877
0.8992494116170128
Epoch: 12/200
Train loss: 2.3286, val loss: 1.5025
0.9003219557207672
Model improve: 0.9001 -> 0.9003
Epoch: 13/200
Train loss: 2.3560, val loss: 1.5654
0.9001682105183302
Epoch: 14/200
Train loss: 2.3644, val loss: 1.5325
0.8993562020643484
Epoch: 15/200
Train loss: 2.3472, val loss: 1.4741
0.903512147230218
Model improve: 0.9003 -> 0.9035
Epoch: 16/200
Train loss: 2.1298, val loss: 1.5022
0.9008649798663773
Epoch: 17/200
Date :04/21/2023, 19:07:07
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13830
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 13.2671, val loss: 3.9044
0.6937721863225911
Model improve: 0.0000 -> 0.6938
Epoch: 2/200
Train loss: 4.5364, val loss: 2.9194
0.7911007470252548
Model improve: 0.6938 -> 0.7911
Epoch: 3/200
Train loss: 4.1173, val loss: 2.4593
0.8280383927624483
Model improve: 0.7911 -> 0.8280
Epoch: 4/200
Train loss: 3.8730, val loss: 2.2087
0.8467632195780014
Model improve: 0.8280 -> 0.8468
Epoch: 5/200
Train loss: 3.6637, val loss: 2.0381
0.8606907791922536
Model improve: 0.8468 -> 0.8607
Epoch: 6/200
Train loss: 3.4725, val loss: 1.9610
0.8696407062625772
Model improve: 0.8607 -> 0.8696
Epoch: 7/200
Train loss: 3.4249, val loss: 1.7945
0.8810724360915778
Model improve: 0.8696 -> 0.8811
Epoch: 8/200
Train loss: 3.3307, val loss: 1.7211
0.8869989271225384
Model improve: 0.8811 -> 0.8870
Epoch: 9/200
Train loss: 3.2180, val loss: 1.6987
0.8907097902323873
Model improve: 0.8870 -> 0.8907
Epoch: 10/200
Train loss: 3.2223, val loss: 1.6813
0.8947416515176567
Model improve: 0.8907 -> 0.8947
Epoch: 11/200
Train loss: 3.1610, val loss: 1.6838
0.8965974138590849
Model improve: 0.8947 -> 0.8966
Epoch: 12/200
Train loss: 3.0605, val loss: 1.5454
0.9015396383245462
Model improve: 0.8966 -> 0.9015
Epoch: 13/200
Train loss: 2.9617, val loss: 1.5838
0.9024904778203567
Model improve: 0.9015 -> 0.9025
Epoch: 14/200
Train loss: 2.9749, val loss: 1.6464
0.9040199460432934
Model improve: 0.9025 -> 0.9040
Epoch: 15/200
Train loss: 2.9520, val loss: 1.5781
0.9061156270304968
Model improve: 0.9040 -> 0.9061
Epoch: 16/200
Train loss: 2.8814, val loss: 1.5043
0.9084626811130242
Model improve: 0.9061 -> 0.9085
Epoch: 17/200
Train loss: 2.8974, val loss: 1.4957
0.9099593734421675
Model improve: 0.9085 -> 0.9100
Epoch: 18/200
Train loss: 2.8526, val loss: 1.4578
0.9116848612334605
Model improve: 0.9100 -> 0.9117
Epoch: 19/200
Train loss: 2.8848, val loss: 1.5929
0.9091883576966732
Epoch: 20/200
Train loss: 2.7365, val loss: 1.4972
0.9117541583007391
Model improve: 0.9117 -> 0.9118
Epoch: 21/200
Train loss: 2.7340, val loss: 1.4398
0.9121554441450934
Model improve: 0.9118 -> 0.9122
Epoch: 22/200
Train loss: 2.7700, val loss: 1.4731
0.9130219874177318
Model improve: 0.9122 -> 0.9130
Epoch: 23/200
Train loss: 2.7750, val loss: 1.4599
0.9145474011722807
Model improve: 0.9130 -> 0.9145
Epoch: 24/200
Train loss: 2.7770, val loss: 1.5372
0.9147770921250196
Model improve: 0.9145 -> 0.9148
Epoch: 25/200
Train loss: 2.7308, val loss: 1.4108
0.9170363933471593
Model improve: 0.9148 -> 0.9170
Epoch: 26/200
Train loss: 2.6328, val loss: 1.3996
0.9162560742460963
Epoch: 27/200
Train loss: 2.6987, val loss: 1.4892
0.9163618491634649
Epoch: 28/200
Train loss: 2.6429, val loss: 1.4784
0.9160575135926378
Epoch: 29/200
Train loss: 2.6623, val loss: 1.3832
0.9182377899324815
Model improve: 0.9170 -> 0.9182
Epoch: 30/200
Train loss: 2.6287, val loss: 1.4286
0.9172461865186765
Epoch: 31/200
Train loss: 2.7187, val loss: 1.4435
0.9169034756521699
Epoch: 32/200
Train loss: 2.6068, val loss: 1.3425
0.9182398008692857
Model improve: 0.9182 -> 0.9182
Epoch: 33/200
Train loss: 2.5755, val loss: 1.4742
0.9147603791561613
Epoch: 34/200
Train loss: 2.5423, val loss: 1.3882
0.9178161647541316
Epoch: 35/200
Train loss: 2.5378, val loss: 1.4472
0.917281670697596
Epoch: 36/200
Train loss: 2.5800, val loss: 1.3778
0.918537533188827
Model improve: 0.9182 -> 0.9185
Epoch: 37/200
Train loss: 2.5845, val loss: 1.3809
0.9188618102560044
Model improve: 0.9185 -> 0.9189
Epoch: 38/200
Train loss: 2.4653, val loss: 1.4251
0.916941482189008
Epoch: 39/200
Train loss: 2.5106, val loss: 1.3531
0.9196362677603124
Model improve: 0.9189 -> 0.9196
Epoch: 40/200
Train loss: 2.4563, val loss: 1.3723
0.9186545201901151
Epoch: 41/200
Train loss: 2.5402, val loss: 1.4669
0.9178260092673012
Epoch: 42/200
Train loss: 2.5722, val loss: 1.3717
0.9206705257923152
Model improve: 0.9196 -> 0.9207
Epoch: 43/200
Train loss: 2.5151, val loss: 1.3938
0.9188766388840669
Epoch: 44/200
Train loss: 2.4779, val loss: 1.2863
0.9203872531764256
Epoch: 45/200
Train loss: 2.4596, val loss: 1.4589
0.919268008729597
Epoch: 46/200
Train loss: 2.4561, val loss: 1.4456
0.9183609357295014
Epoch: 47/200
Train loss: 2.4953, val loss: 1.4202
0.9186897301901361
Epoch: 48/200
Train loss: 2.4967, val loss: 1.4143
0.9183211952814525
Epoch: 49/200
Train loss: 2.5107, val loss: 1.4437
0.9178994005268679
Epoch: 50/200
Train loss: 2.4397, val loss: 1.3816
0.9197447160840139
Epoch: 51/200
Train loss: 2.4954, val loss: 1.3533
0.9200143898890173
Epoch: 52/200
Train loss: 2.3580, val loss: 1.3396
0.9193336169783071
Epoch: 53/200
Train loss: 2.4768, val loss: 1.4299
0.9161726850322033
Epoch: 54/200
Train loss: 2.3994, val loss: 1.4056
0.9192848944899895
Epoch: 55/200
Train loss: 2.4193, val loss: 1.4510
0.9185512652099675
Epoch: 56/200
Train loss: 2.4037, val loss: 1.4135
0.9194478360741991
Epoch: 57/200
Train loss: 2.4420, val loss: 1.3742
0.9207386300245856
Model improve: 0.9207 -> 0.9207
Epoch: 58/200
Train loss: 2.3288, val loss: 1.3306
0.9210588058235686
Model improve: 0.9207 -> 0.9211
Epoch: 59/200
Train loss: 2.3571, val loss: 1.3854
0.9199922278291834
Epoch: 60/200
Train loss: 2.3599, val loss: 1.3432
0.9206754182927313
Epoch: 61/200
Train loss: 2.3714, val loss: 1.4128
0.9205394977526103
Epoch: 62/200
Train loss: 2.4128, val loss: 1.4407
0.9194627082374159
Epoch: 63/200
Train loss: 2.3778, val loss: 1.4367
0.918309394300947
Epoch: 64/200
Train loss: 2.3596, val loss: 1.4507
0.9194555496480721
Epoch: 65/200
Train loss: 2.3780, val loss: 1.3805
0.9208703440217254
Epoch: 66/200
Train loss: 2.3656, val loss: 1.3513
0.9216376081040916
Model improve: 0.9211 -> 0.9216
Epoch: 67/200
Train loss: 2.2894, val loss: 1.3053
0.9219297982379434
Model improve: 0.9216 -> 0.9219
Epoch: 68/200
Train loss: 2.3631, val loss: 1.3374
0.9218368955275448
Epoch: 69/200
Train loss: 2.3471, val loss: 1.3456
0.9201114828412188
Epoch: 70/200
Train loss: 2.3660, val loss: 1.3847
0.921651975942876
Epoch: 71/200
Train loss: 2.3535, val loss: 1.3704
0.9207425618368832
Epoch: 72/200
Train loss: 2.3872, val loss: 1.3820
0.9209691334309814
Epoch: 73/200
Train loss: 2.3716, val loss: 1.4368
0.9201828493791134
Epoch: 74/200
Train loss: 2.3602, val loss: 1.4123
0.9205058780995368
Epoch: 75/200
Train loss: 2.2826, val loss: 1.3443
0.9224680135071416
Model improve: 0.9219 -> 0.9225
Epoch: 76/200
Train loss: 2.2650, val loss: 1.3447
0.9227151133911122
Model improve: 0.9225 -> 0.9227
Epoch: 77/200
Train loss: 2.2450, val loss: 1.3383
0.9226319824960972
Epoch: 78/200
Train loss: 2.3389, val loss: 1.3693
0.9220743915500955
Epoch: 79/200
Train loss: 2.2754, val loss: 1.3459
0.9229158676516792
Model improve: 0.9227 -> 0.9229
Epoch: 80/200
Train loss: 2.2932, val loss: 1.2876
0.9227204387686375
Epoch: 81/200
Train loss: 2.3481, val loss: 1.3113
0.923384720740793
Model improve: 0.9229 -> 0.9234
Epoch: 82/200
Train loss: 2.2636, val loss: 1.3262
0.9231005438789935
Epoch: 83/200
Train loss: 2.2719, val loss: 1.3434
0.9233807825881912
Epoch: 84/200
Train loss: 2.3248, val loss: 1.3782
0.9214766778230588
Epoch: 85/200
Train loss: 2.3000, val loss: 1.2964
0.9215393137455211
Epoch: 86/200
Train loss: 2.2507, val loss: 1.4078
0.9189427502402492
Epoch: 87/200
Train loss: 2.2796, val loss: 1.3406
0.9209678805822769
Epoch: 88/200
Train loss: 2.2216, val loss: 1.3529
0.9216077799771868
Epoch: 89/200
Train loss: 2.2459, val loss: 1.3050
0.922886836144665
Epoch: 90/200
Train loss: 2.2452, val loss: 1.3451
0.9222218971938684
Epoch: 91/200
Train loss: 2.3024, val loss: 1.3190
0.9243728872446937
Model improve: 0.9234 -> 0.9244
Epoch: 92/200
Train loss: 2.2104, val loss: 1.3118
0.922111966869426
Epoch: 93/200
Train loss: 2.3009, val loss: 1.3594
0.9214423983994281
Epoch: 94/200
Train loss: 2.2856, val loss: 1.3521
0.921014131411787
Epoch: 95/200
Train loss: 2.3374, val loss: 1.3277
0.9220719855228123
Epoch: 96/200
Train loss: 2.2513, val loss: 1.3616
0.9217380256326896
Epoch: 97/200
Train loss: 2.2188, val loss: 1.3385
0.9212040018103742
Epoch: 98/200
Train loss: 2.1885, val loss: 1.3517
0.9200843724008575
Epoch: 99/200
Train loss: 2.1861, val loss: 1.3628
0.9205561971980671
Epoch: 100/200
Train loss: 2.2209, val loss: 1.3656
0.9211342955555051
Epoch: 101/200
Train loss: 2.2971, val loss: 1.3354
0.9217542883883414
Epoch: 102/200
Train loss: 2.1736, val loss: 1.3598
0.9214469459190833
Epoch: 103/200
Train loss: 2.1173, val loss: 1.3707
0.9216903687430007
Epoch: 104/200
Train loss: 2.2234, val loss: 1.3526
0.9229987171099279
Epoch: 105/200
Train loss: 2.2048, val loss: 1.3123
0.923224422200367
Epoch: 106/200
Train loss: 2.2515, val loss: 1.3402
0.9223311408458801
Epoch: 107/200
Train loss: 2.2528, val loss: 1.3093
0.9227048571135749
Epoch: 108/200
Train loss: 2.1609, val loss: 1.2427
0.9246032178853196
Model improve: 0.9244 -> 0.9246
Epoch: 109/200
Train loss: 2.2354, val loss: 1.3527
0.9216837001992677
Epoch: 110/200
Train loss: 2.1544, val loss: 1.2558
0.9246852550808158
Model improve: 0.9246 -> 0.9247
Epoch: 111/200
Train loss: 2.1914, val loss: 1.3156
0.9243618813840634
Epoch: 112/200
Train loss: 2.2173, val loss: 1.3226
0.9247234519982122
Model improve: 0.9247 -> 0.9247
Epoch: 113/200
Train loss: 2.2272, val loss: 1.3603
0.9226381180465335
Epoch: 114/200
Train loss: 2.1678, val loss: 1.3392
0.9227429403312244
Epoch: 115/200
Train loss: 2.2105, val loss: 1.4417
0.9206215086059615
Epoch: 116/200
Train loss: 2.1586, val loss: 1.2620
0.9257278255448699
Model improve: 0.9247 -> 0.9257
Epoch: 117/200
Train loss: 2.2063, val loss: 1.3705
0.9213765017643643
Epoch: 118/200
Train loss: 2.2061, val loss: 1.3038
0.9237452243924065
Epoch: 119/200
Train loss: 2.1840, val loss: 1.3439
0.922647054024921
Epoch: 120/200
Train loss: 2.2197, val loss: 1.3374
0.9228951074978472
Epoch: 121/200
Train loss: 2.0891, val loss: 1.3393
0.9225159878009038
Epoch: 122/200
Train loss: 2.1308, val loss: 1.2855
0.9241476330724697
Epoch: 123/200
Train loss: 2.1664, val loss: 1.3115
0.9246921752107916
Epoch: 124/200
Train loss: 2.1965, val loss: 1.3869
0.9226511291666707
Epoch: 125/200
Train loss: 2.1602, val loss: 1.2863
0.9253761293866569
Epoch: 126/200
Train loss: 2.1620, val loss: 1.3167
0.9237275720820517
Epoch: 127/200
Train loss: 2.1855, val loss: 1.2839
0.92490478958828
Epoch: 128/200
Train loss: 2.1122, val loss: 1.2747
0.9249278194873983
Epoch: 129/200
Train loss: 2.1179, val loss: 1.3357
0.9242046834459016
Epoch: 130/200
Train loss: 2.1501, val loss: 1.3383
0.9231394097076632
Epoch: 131/200
Train loss: 2.1301, val loss: 1.3025
0.9249696539701813
Epoch: 132/200
Train loss: 2.1624, val loss: 1.2834
0.9256855862266915
Epoch: 133/200
Train loss: 2.1522, val loss: 1.2791
0.9260395221942288
Model improve: 0.9257 -> 0.9260
Epoch: 134/200
Train loss: 2.0971, val loss: 1.2753
0.9259452919611734
Epoch: 135/200
Train loss: 2.1329, val loss: 1.2896
0.9258859262990313
Epoch: 136/200
Train loss: 2.1490, val loss: 1.2805
0.9250800591781412
Epoch: 137/200
Train loss: 2.1562, val loss: 1.2993
0.9250523102073709
Epoch: 138/200
Train loss: 2.0962, val loss: 1.2990
0.925086826477815
Epoch: 139/200
Train loss: 2.0898, val loss: 1.2820
0.9252035462801789
Epoch: 140/200
Train loss: 2.0996, val loss: 1.3349
0.9239467306032046
Epoch: 141/200
Train loss: 2.1611, val loss: 1.3001
0.9253435777896072
Epoch: 142/200
Train loss: 2.1390, val loss: 1.3342
0.9254382979363426
Epoch: 143/200
Train loss: 2.1083, val loss: 1.3372
0.9247479680389591
Epoch: 144/200
Train loss: 2.1533, val loss: 1.3469
0.9249751693961532
Epoch: 145/200
Train loss: 2.1498, val loss: 1.3465
0.9232047792000362
Epoch: 146/200
Train loss: 2.0872, val loss: 1.2931
0.9241478321878732
Epoch: 147/200
Train loss: 2.1767, val loss: 1.3340
0.9230257508872233
Epoch: 148/200
Train loss: 2.0907, val loss: 1.3805
0.922519926925937
Epoch: 149/200
Train loss: 2.1118, val loss: 1.3056
0.9250109128164967
Epoch: 150/200
Train loss: 2.1295, val loss: 1.3044
0.924431997202531
Epoch: 151/200
Train loss: 2.1740, val loss: 1.3733
0.9229586130257789
Epoch: 152/200
Train loss: 2.0961, val loss: 1.3243
0.9251778799717801
Epoch: 153/200
Train loss: 2.1578, val loss: 1.3653
0.9228816826293125
Epoch: 154/200
Train loss: 2.1140, val loss: 1.3307
0.9243464171491869
Epoch: 155/200
Train loss: 2.1243, val loss: 1.3056
0.9247593338657812
Epoch: 156/200
Train loss: 2.0943, val loss: 1.3103
0.9241089384958298
Epoch: 157/200
Train loss: 2.0869, val loss: 1.3359
0.9233215044229467
Epoch: 158/200
Train loss: 2.1214, val loss: 1.2994
0.9244931415034733
Epoch: 159/200
Train loss: 2.0846, val loss: 1.3153
0.9240843787618772
Epoch: 160/200
Train loss: 2.0859, val loss: 1.3024
0.9244337474822525
Epoch: 161/200
Train loss: 2.1287, val loss: 1.3273
0.9249178149679258
Epoch: 162/200
Train loss: 2.1315, val loss: 1.3715
0.923632171734059
Epoch: 163/200
Train loss: 2.1507, val loss: 1.3306
0.923844511963619
Epoch: 164/200
Train loss: 2.0917, val loss: 1.3142
0.9250197001305567
Epoch: 165/200
Train loss: 2.1506, val loss: 1.3374
0.9245761953008689
Epoch: 166/200
Train loss: 2.0685, val loss: 1.3573
0.9231858779239492
Epoch: 167/200
Train loss: 2.1111, val loss: 1.3567
0.9233138839668424
Epoch: 168/200
Train loss: 2.2083, val loss: 1.3422
0.9236850393677779
Epoch: 169/200
Train loss: 2.1057, val loss: 1.3191
0.924285383157935
Epoch: 170/200
Train loss: 2.0844, val loss: 1.3069
0.9250424574011727
Epoch: 171/200
Train loss: 2.0323, val loss: 1.2909
0.9252093453115026
Epoch: 172/200
Train loss: 2.1134, val loss: 1.3161
0.9250156201875327
Epoch: 173/200
Train loss: 2.0737, val loss: 1.3254
0.9247703049721745
Epoch: 174/200
Train loss: 2.0361, val loss: 1.2880
0.9255554550566977
Epoch: 175/200
Train loss: 2.1236, val loss: 1.3303
0.9242851921536893
Epoch: 176/200
Train loss: 2.0863, val loss: 1.2778
0.9260809355504561
Model improve: 0.9260 -> 0.9261
Epoch: 177/200
Train loss: 2.1591, val loss: 1.2967
0.9251551822000544
Epoch: 178/200
Train loss: 2.0625, val loss: 1.3359
0.9245398004291946
Epoch: 179/200
Train loss: 2.0234, val loss: 1.2943
0.9257117869800595
Epoch: 180/200
Train loss: 2.1275, val loss: 1.2961
0.9255568843725707
Epoch: 181/200
Train loss: 2.0388, val loss: 1.2945
0.9257314772889732
Epoch: 182/200
Train loss: 2.0854, val loss: 1.2963
0.9255481190739264
Epoch: 183/200
Train loss: 2.0986, val loss: 1.2763
0.926761217648661
Model improve: 0.9261 -> 0.9268
Epoch: 184/200
Train loss: 2.1028, val loss: 1.3169
0.925170304276617
Epoch: 185/200
Train loss: 2.0350, val loss: 1.2774
0.9268303354992088
Model improve: 0.9268 -> 0.9268
Epoch: 186/200
Train loss: 2.0981, val loss: 1.2806
0.9260481465987361
Epoch: 187/200
Train loss: 2.1120, val loss: 1.2724
0.9261386566678351
Epoch: 188/200
Train loss: 2.1086, val loss: 1.3009
0.9253601776134655
Epoch: 189/200
Train loss: 2.0784, val loss: 1.3171
0.9250006511863931
Epoch: 190/200
Train loss: 2.0659, val loss: 1.2796
0.9262782349044476
Epoch: 191/200
Train loss: 2.0944, val loss: 1.3028
0.9258033443752548
Epoch: 192/200
Train loss: 2.0533, val loss: 1.2737
0.9259631554317086
Epoch: 193/200
Train loss: 2.0688, val loss: 1.3011
0.9254284614754357
Epoch: 194/200
Train loss: 2.1282, val loss: 1.2602
0.9268942738387455
Model improve: 0.9268 -> 0.9269
Epoch: 195/200
Train loss: 2.0644, val loss: 1.3175
0.9250978947734516
Epoch: 196/200
Train loss: 2.0580, val loss: 1.3061
0.9257938017531292
Epoch: 197/200
Train loss: 2.0423, val loss: 1.3089
0.9253909245438607
Epoch: 198/200
Train loss: 2.1105, val loss: 1.3287
0.9247324116760673
Epoch: 199/200
Train loss: 2.0259, val loss: 1.3106
0.9252082363483165
Epoch: 200/200
Train loss: 2.0882, val loss: 1.2961
0.9254297383186819
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 13.8334, val loss: 3.9137
0.6903660844071653
Model improve: 0.0000 -> 0.6904
Epoch: 2/200
Train loss: 4.5828, val loss: 2.9443
0.7868272867286155
Model improve: 0.6904 -> 0.7868
Epoch: 3/200
Train loss: 4.1320, val loss: 2.4818
0.8251075625845506
Model improve: 0.7868 -> 0.8251
Epoch: 4/200
Train loss: 3.8663, val loss: 2.2063
0.8483036242327373
Model improve: 0.8251 -> 0.8483
Epoch: 5/200
Train loss: 3.6465, val loss: 2.0142
0.86249749860629
Model improve: 0.8483 -> 0.8625
Epoch: 6/200
Train loss: 3.5033, val loss: 1.9029
0.8720479069599959
Model improve: 0.8625 -> 0.8720
Epoch: 7/200
Train loss: 3.4867, val loss: 1.8490
0.8780930500802364
Model improve: 0.8720 -> 0.8781
Epoch: 8/200
Train loss: 3.2849, val loss: 1.7723
0.8825897710714848
Model improve: 0.8781 -> 0.8826
Epoch: 9/200
Train loss: 3.2679, val loss: 1.6758
0.890118225488794
Model improve: 0.8826 -> 0.8901
Epoch: 10/200
Train loss: 3.1084, val loss: 1.6538
0.893330715199399
Model improve: 0.8901 -> 0.8933
Epoch: 11/200
Train loss: 3.0954, val loss: 1.5814
0.8952190651363064
Model improve: 0.8933 -> 0.8952
Epoch: 12/200
Train loss: 3.0113, val loss: 1.4981
0.8995835006693337
Model improve: 0.8952 -> 0.8996
Epoch: 13/200
Train loss: 3.0988, val loss: 1.5410
0.8979835110465518
Epoch: 14/200
Train loss: 3.0274, val loss: 1.6440
0.899883000705975
Model improve: 0.8996 -> 0.8999
Epoch: 15/200
Train loss: 2.9277, val loss: 1.5858
0.9032965313828906
Model improve: 0.8999 -> 0.9033
Epoch: 16/200
Train loss: 2.9252, val loss: 1.5343
0.9040686516240588
Model improve: 0.9033 -> 0.9041
Epoch: 17/200
Train loss: 2.9163, val loss: 1.5220
0.9035461412634725
Epoch: 18/200
Train loss: 2.8984, val loss: 1.4866
0.9050309495749828
Model improve: 0.9041 -> 0.9050
Epoch: 19/200
Train loss: 2.8679, val loss: 1.4660
0.9066678189462964
Model improve: 0.9050 -> 0.9067
Epoch: 20/200
Train loss: 2.7851, val loss: 1.4272
0.9080691013696732
Model improve: 0.9067 -> 0.9081
Epoch: 21/200
Train loss: 2.7950, val loss: 1.4078
0.9096093465135608
Model improve: 0.9081 -> 0.9096
Epoch: 22/200
Train loss: 2.7667, val loss: 1.5649
0.9060403686531018
Epoch: 23/200
Train loss: 2.8196, val loss: 1.4601
0.9098864864575789
Model improve: 0.9096 -> 0.9099
Epoch: 24/200
Train loss: 2.6967, val loss: 1.3933
0.9103338518132671
Model improve: 0.9099 -> 0.9103
Epoch: 25/200
Train loss: 2.7532, val loss: 1.4146
0.9133862859987847
Model improve: 0.9103 -> 0.9134
Epoch: 26/200
Train loss: 2.6727, val loss: 1.4422
0.9118361980089211
Epoch: 27/200
Train loss: 2.6975, val loss: 1.3966
0.9115366856003826
Epoch: 28/200
Train loss: 2.7225, val loss: 1.4655
0.9132161760360324
Epoch: 29/200
Train loss: 2.6404, val loss: 1.3851
0.9134769817853852
Model improve: 0.9134 -> 0.9135
Epoch: 30/200
Train loss: 2.7165, val loss: 1.4347
0.9126571622049373
Epoch: 31/200
Train loss: 2.6531, val loss: 1.4027
0.9125170462802
Epoch: 32/200
Train loss: 2.6467, val loss: 1.4620
0.9116785045108479
Epoch: 33/200
Train loss: 2.5667, val loss: 1.4537
0.9093088186578665
Epoch: 34/200
Train loss: 2.6194, val loss: 1.3485
0.9121043406039255
Epoch: 35/200
Train loss: 2.6517, val loss: 1.4078
0.911282076849268
Epoch: 36/200
Train loss: 2.5610, val loss: 1.4045
0.9109350761840973
Epoch: 37/200
Train loss: 2.5727, val loss: 1.3587
0.9119378757642369
Epoch: 38/200
Train loss: 2.5362, val loss: 1.4228
0.9125126910147199
Epoch: 39/200
Train loss: 2.5585, val loss: 1.3885
0.9134693131366172
Epoch: 40/200
Train loss: 2.4904, val loss: 1.3589
0.9119883974310548
Epoch: 41/200
Train loss: 2.6076, val loss: 1.4635
0.9115681773235423
Epoch: 42/200
Train loss: 2.5149, val loss: 1.4050
0.9117685146387496
Epoch: 43/200
Train loss: 2.4843, val loss: 1.3897
0.9133735404685303
Epoch: 44/200
Train loss: 2.4702, val loss: 1.3944
0.9131695685627862
Epoch: 45/200
Train loss: 2.4353, val loss: 1.4155
0.9131985273681991
Epoch: 46/200
Train loss: 2.4412, val loss: 1.3587
0.914303706428614
Model improve: 0.9135 -> 0.9143
Epoch: 47/200
Train loss: 2.4602, val loss: 1.4227
0.9132654364882781
Epoch: 48/200
Train loss: 2.4454, val loss: 1.3858
0.9116105664009377
Epoch: 49/200
Train loss: 2.4446, val loss: 1.4185
0.9115333491159346
Epoch: 50/200
Train loss: 2.3775, val loss: 1.3433
0.9124824340073228
Epoch: 51/200
Train loss: 2.4584, val loss: 1.4640
0.9121958767507019
Epoch: 52/200
Train loss: 2.4656, val loss: 1.3507
0.9133348975462299
Epoch: 53/200
Train loss: 2.4555, val loss: 1.3221
0.914615087681141
Model improve: 0.9143 -> 0.9146
Epoch: 54/200
Train loss: 2.4824, val loss: 1.3925
0.9152997394375841
Model improve: 0.9146 -> 0.9153
Epoch: 55/200
Train loss: 2.4289, val loss: 1.3869
0.9138163963642575
Epoch: 56/200
Train loss: 2.4811, val loss: 1.3407
0.9139951911051012
Epoch: 57/200
Train loss: 2.3729, val loss: 1.3886
0.9133337748783514
Epoch: 58/200
Train loss: 2.3732, val loss: 1.2996
0.9155420536635316
Model improve: 0.9153 -> 0.9155
Epoch: 59/200
Train loss: 2.3107, val loss: 1.3350
0.9149931220241101
Epoch: 60/200
Train loss: 2.3779, val loss: 1.3547
0.9147997466841354
Epoch: 61/200
Train loss: 2.4099, val loss: 1.4040
0.9146491285765687
Epoch: 62/200
Train loss: 2.3963, val loss: 1.3669
0.9153109556119342
Epoch: 63/200
Train loss: 2.4212, val loss: 1.4195
0.9143141015911055
Epoch: 64/200
Train loss: 2.3943, val loss: 1.4188
0.9138768341337439
Epoch: 65/200
Train loss: 2.4613, val loss: 1.3919
0.9140878933619062
Epoch: 66/200
Train loss: 2.4174, val loss: 1.3855
0.914085271038903
Epoch: 67/200
Train loss: 2.3206, val loss: 1.3342
0.916027145592316
Model improve: 0.9155 -> 0.9160
Epoch: 68/200
Train loss: 2.3882, val loss: 1.3766
0.9155919106684428
Epoch: 69/200
Train loss: 2.3746, val loss: 1.3969
0.9134291910541378
Epoch: 70/200
Train loss: 2.3504, val loss: 1.3489
0.9161060654579779
Model improve: 0.9160 -> 0.9161
Epoch: 71/200
Train loss: 2.3714, val loss: 1.3528
0.9146032464977231
Epoch: 72/200
Train loss: 2.3427, val loss: 1.3651
0.9149128809853999
Epoch: 73/200
Train loss: 2.2554, val loss: 1.3759
0.9157718961200616
Epoch: 74/200
Train loss: 2.3442, val loss: 1.3821
0.9166012994764299
Model improve: 0.9161 -> 0.9166
Epoch: 75/200
Train loss: 2.3111, val loss: 1.3523
0.9157618089159932
Epoch: 76/200
Train loss: 2.3716, val loss: 1.3550
0.9155368504095518
Epoch: 77/200
Train loss: 2.3271, val loss: 1.3526
0.9148835028182037
Epoch: 78/200
Train loss: 2.2798, val loss: 1.3339
0.9180897278134189
Model improve: 0.9166 -> 0.9181
Epoch: 79/200
Train loss: 2.3659, val loss: 1.3516
0.9158054219514732
Epoch: 80/200
Train loss: 2.3044, val loss: 1.3936
0.9135377158747384
Epoch: 81/200
Train loss: 2.2159, val loss: 1.3112
0.9171773580320177
Epoch: 82/200
Train loss: 2.2689, val loss: 1.3811
0.9160038941901095
Epoch: 83/200
Train loss: 2.2862, val loss: 1.3905
0.9144341792878424
Epoch: 84/200
Train loss: 2.3356, val loss: 1.3467
0.916499932316347
Epoch: 85/200
Train loss: 2.2741, val loss: 1.3854
0.916960133771563
Epoch: 86/200
Train loss: 2.2437, val loss: 1.3835
0.9151882282390075
Epoch: 87/200
Train loss: 2.2894, val loss: 1.4013
0.9162781621535029
Epoch: 88/200
Train loss: 2.2893, val loss: 1.3314
0.9168914868171552
Epoch: 89/200
Train loss: 2.2293, val loss: 1.3296
0.9175203355217141
Epoch: 90/200
Train loss: 2.2901, val loss: 1.3432
0.9172978813952923
Epoch: 91/200
Train loss: 2.2264, val loss: 1.3387
0.9175542131817115
Epoch: 92/200
Train loss: 2.1926, val loss: 1.4093
0.9147979153338806
Epoch: 93/200
Train loss: 2.2570, val loss: 1.3296
0.9169292708107092
Epoch: 94/200
Train loss: 2.3099, val loss: 1.3811
0.9153811395442932
Epoch: 95/200
Train loss: 2.2583, val loss: 1.3156
0.9176313572748102
Epoch: 96/200
Train loss: 2.2118, val loss: 1.3938
0.9166472099704824
Epoch: 97/200
Train loss: 2.3336, val loss: 1.3224
0.9178151216882913
Epoch: 98/200
Train loss: 2.2258, val loss: 1.3090
0.9177841570324051
Epoch: 99/200
Train loss: 2.2197, val loss: 1.3689
0.9161153472979466
Epoch: 100/200
Train loss: 2.2954, val loss: 1.3534
0.9165393132325935
Epoch: 101/200
Train loss: 2.2102, val loss: 1.3713
0.9163070857955908
Epoch: 102/200
Train loss: 2.2444, val loss: 1.3375
0.916986747376764
Epoch: 103/200
Train loss: 2.2174, val loss: 1.3830
0.9158317375165463
Epoch: 104/200
Train loss: 2.1866, val loss: 1.3994
0.9164831540323712
Epoch: 105/200
Train loss: 2.2304, val loss: 1.3148
0.9173081019209082
Epoch: 106/200
Train loss: 2.1127, val loss: 1.2911
0.9181267933459175
Model improve: 0.9181 -> 0.9181
Epoch: 107/200
Train loss: 2.2525, val loss: 1.3847
0.9156441974857216
Epoch: 108/200
Train loss: 2.2398, val loss: 1.3834
0.9169495824789934
Epoch: 109/200
Train loss: 2.1896, val loss: 1.3321
0.9185571498423238
Model improve: 0.9181 -> 0.9186
Epoch: 110/200
Train loss: 2.1984, val loss: 1.3066
0.9177079471123566
Epoch: 111/200
Train loss: 2.1736, val loss: 1.3182
0.9179973017123947
Epoch: 112/200
Train loss: 2.1906, val loss: 1.3675
0.9182130349474908
Epoch: 113/200
Train loss: 2.1597, val loss: 1.3662
0.9168082294705548
Epoch: 114/200
Train loss: 2.2727, val loss: 1.3348
0.9179720611678447
Epoch: 115/200
Train loss: 2.2038, val loss: 1.3649
0.9169888400087705
Epoch: 116/200
Train loss: 2.1882, val loss: 1.3185
0.9189822340147833
Model improve: 0.9186 -> 0.9190
Epoch: 117/200
Train loss: 2.1615, val loss: 1.3110
0.9183251386108453
Epoch: 118/200
Train loss: 2.1234, val loss: 1.3723
0.9178546460563864
Epoch: 119/200
Train loss: 2.1437, val loss: 1.3368
0.9190902288788899
Model improve: 0.9190 -> 0.9191
Epoch: 120/200
Train loss: 2.2314, val loss: 1.3615
0.9179689203983677
Epoch: 121/200
Train loss: 2.1736, val loss: 1.3554
0.917114999769687
Epoch: 122/200
Train loss: 2.1785, val loss: 1.3175
0.9197548912184579
Model improve: 0.9191 -> 0.9198
Epoch: 123/200
Train loss: 2.1282, val loss: 1.3618
0.9179838610796075
Epoch: 124/200
Train loss: 2.1874, val loss: 1.3295
0.9191424543818163
Epoch: 125/200
Train loss: 2.1571, val loss: 1.3249
0.9180618262182121
Epoch: 126/200
Train loss: 2.1944, val loss: 1.3131
0.919753590487873
Epoch: 127/200
Train loss: 2.1863, val loss: 1.3165
0.9198549159135022
Model improve: 0.9198 -> 0.9199
Epoch: 128/200
Train loss: 2.1377, val loss: 1.3358
0.9203815070682985
Model improve: 0.9199 -> 0.9204
Epoch: 129/200
Train loss: 2.1400, val loss: 1.3538
0.920081931283745
Epoch: 130/200
Train loss: 2.1331, val loss: 1.3242
0.9203475720037994
Epoch: 131/200
Train loss: 2.1999, val loss: 1.3012
0.9196709858039251
Epoch: 132/200
Train loss: 2.1861, val loss: 1.3532
0.918469845076696
Epoch: 133/200
Train loss: 2.1493, val loss: 1.2802
0.920718171495462
Model improve: 0.9204 -> 0.9207
Epoch: 134/200
Train loss: 2.1359, val loss: 1.2871
0.9197540590711925
Epoch: 135/200
Train loss: 2.1067, val loss: 1.3057
0.9192977691628459
Epoch: 136/200
Train loss: 2.1226, val loss: 1.3463
0.9184876479796956
Epoch: 137/200
Train loss: 2.0922, val loss: 1.3673
0.9171026622487682
Epoch: 138/200
Train loss: 2.1166, val loss: 1.3163
0.9186963474234101
Epoch: 139/200
Train loss: 2.1459, val loss: 1.2886
0.9199624179155923
Epoch: 140/200
Train loss: 2.1190, val loss: 1.2867
0.9201174290264698
Epoch: 141/200
Train loss: 2.0866, val loss: 1.3694
0.9173857664388226
Epoch: 142/200
Train loss: 2.1163, val loss: 1.3463
0.9192219971099392
Epoch: 143/200
Train loss: 2.1536, val loss: 1.3773
0.917432032784728
Epoch: 144/200
Train loss: 2.1294, val loss: 1.3004
0.9198777982890354
Epoch: 145/200
Train loss: 2.1285, val loss: 1.3352
0.9186939159090048
Epoch: 146/200
Train loss: 2.0580, val loss: 1.3525
0.9185900267471181
Epoch: 147/200
Train loss: 2.1931, val loss: 1.3718
0.9182198023645849
Epoch: 148/200
Train loss: 2.1511, val loss: 1.2868
0.919932200963828
Epoch: 149/200
Train loss: 2.1263, val loss: 1.3236
0.9190583992775706
Epoch: 150/200
Train loss: 2.0642, val loss: 1.3064
0.9197953373863063
Epoch: 151/200
Train loss: 2.1077, val loss: 1.3342
0.9188514929667251
Epoch: 152/200
Train loss: 2.1546, val loss: 1.2976
0.9195137767960996
Epoch: 153/200
Train loss: 2.1351, val loss: 1.3189
0.9191826826342132
Epoch: 154/200
Train loss: 2.1220, val loss: 1.3348
0.918634435899432
Epoch: 155/200
Train loss: 2.1272, val loss: 1.3371
0.9187064027452464
Epoch: 156/200
Train loss: 2.1282, val loss: 1.2960
0.9204554182636268
Epoch: 157/200
Train loss: 2.1527, val loss: 1.2926
0.9203697417357005
Epoch: 158/200
Train loss: 2.1365, val loss: 1.3732
0.9183455786091399
Epoch: 159/200
Train loss: 2.0622, val loss: 1.3140
0.9201602377109694
Epoch: 160/200
Train loss: 2.0936, val loss: 1.3217
0.9194008879385587
Epoch: 161/200
Train loss: 2.1537, val loss: 1.3323
0.919486021288637
Epoch: 162/200
Train loss: 2.1302, val loss: 1.2769
0.9208046745859144
Model improve: 0.9207 -> 0.9208
Epoch: 163/200
Train loss: 2.1012, val loss: 1.2932
0.9203200429339056
Epoch: 164/200
Train loss: 2.1378, val loss: 1.2969
0.9207732082216809
Epoch: 165/200
Train loss: 2.1727, val loss: 1.3301
0.9196991384793631
Epoch: 166/200
Train loss: 2.1188, val loss: 1.3423
0.9197448346014098
Epoch: 167/200
Train loss: 2.0832, val loss: 1.2956
0.9208232908430843
Model improve: 0.9208 -> 0.9208
Epoch: 168/200
Train loss: 2.1313, val loss: 1.3106
0.9208904450717197
Model improve: 0.9208 -> 0.9209
Epoch: 169/200
Train loss: 2.0962, val loss: 1.2809
0.9215449467726797
Model improve: 0.9209 -> 0.9215
Epoch: 170/200
Train loss: 2.1484, val loss: 1.3195
0.9206638670719108
Epoch: 171/200
Train loss: 2.1673, val loss: 1.3602
0.9197586491106441
Epoch: 172/200
Train loss: 2.1420, val loss: 1.3234
0.9209356042699545
Epoch: 173/200
Train loss: 2.1589, val loss: 1.3625
0.9201677573092084
Epoch: 174/200
Train loss: 2.0594, val loss: 1.3088
0.9209484086498059
Epoch: 175/200
Train loss: 2.0777, val loss: 1.3428
0.9203890733643889
Epoch: 176/200
Train loss: 2.1124, val loss: 1.3263
0.9205682876875335
Epoch: 177/200
Train loss: 2.1265, val loss: 1.3363
0.9197481198849133
Epoch: 178/200
Train loss: 2.1189, val loss: 1.3653
0.9199393939152286
Epoch: 179/200
Train loss: 2.1135, val loss: 1.2966
0.9212290416372783
Epoch: 180/200
Train loss: 2.0491, val loss: 1.2842
0.9213572024169521
Epoch: 181/200
Train loss: 2.0496, val loss: 1.2943
0.9215761949681004
Model improve: 0.9215 -> 0.9216
Epoch: 182/200
Train loss: 2.1086, val loss: 1.3200
0.920324727020968
Epoch: 183/200
Train loss: 2.1223, val loss: 1.2919
0.9211455812912852
Epoch: 184/200
Train loss: 2.1515, val loss: 1.3671
0.9196865812795871
Epoch: 185/200
Train loss: 2.0834, val loss: 1.3302
0.9196723866796462
Epoch: 186/200
Train loss: 2.1640, val loss: 1.2982
0.9206256307571207
Epoch: 187/200
Train loss: 2.0888, val loss: 1.3037
0.9211425874170069
Epoch: 188/200
Train loss: 2.0834, val loss: 1.3049
0.9208203235503556
Epoch: 189/200
Train loss: 2.1029, val loss: 1.3454
0.9196530524053111
Epoch: 190/200
Train loss: 2.0890, val loss: 1.3129
0.9203376076299763
Epoch: 191/200
Train loss: 2.1258, val loss: 1.2929
0.9211783861488106
Epoch: 192/200
Train loss: 2.0429, val loss: 1.3406
0.9198807659513937
Epoch: 193/200
Train loss: 2.0357, val loss: 1.3173
0.9204620901617127
Epoch: 194/200
Train loss: 2.0854, val loss: 1.3040
0.9206325458789324
Epoch: 195/200
Train loss: 2.0725, val loss: 1.3470
0.9194707613455769
Epoch: 196/200
Train loss: 2.0939, val loss: 1.3699
0.919224357682954
Epoch: 197/200
Train loss: 2.1342, val loss: 1.3586
0.9193220402898655
Epoch: 198/200
Train loss: 2.1273, val loss: 1.3427
0.9201395393750449
Epoch: 199/200
Train loss: 2.1382, val loss: 1.3112
0.9204937248509214
Epoch: 200/200
Train loss: 2.1025, val loss: 1.2925
0.9209785551154314
Fold: 2
13807
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 14.0569, val loss: 3.9639
0.6868860328716099
Model improve: 0.0000 -> 0.6869
Epoch: 2/200
Train loss: 4.5606, val loss: 3.0034
0.7818755497241361
Model improve: 0.6869 -> 0.7819
Epoch: 3/200
Train loss: 4.1167, val loss: 2.5009
0.8192717978167717
Model improve: 0.7819 -> 0.8193
Epoch: 4/200
Train loss: 3.8417, val loss: 2.2697
0.8402720005762214
Model improve: 0.8193 -> 0.8403
Epoch: 5/200
Train loss: 3.6947, val loss: 2.0849
0.8539438645050333
Model improve: 0.8403 -> 0.8539
Epoch: 6/200
Train loss: 3.4861, val loss: 1.9763
0.8641863592963711
Model improve: 0.8539 -> 0.8642
Epoch: 7/200
Train loss: 3.3736, val loss: 1.8890
0.8709068472464923
Model improve: 0.8642 -> 0.8709
Epoch: 8/200
Train loss: 3.3805, val loss: 1.8033
0.8795796698152466
Model improve: 0.8709 -> 0.8796
Epoch: 9/200
Train loss: 3.1686, val loss: 1.7605
0.885011094949779
Model improve: 0.8796 -> 0.8850
Epoch: 10/200
Train loss: 3.2277, val loss: 1.7362
0.8853849277570657
Model improve: 0.8850 -> 0.8854
Epoch: 11/200
Train loss: 3.1772, val loss: 1.7215
0.8882028133520374
Model improve: 0.8854 -> 0.8882
Epoch: 12/200
Train loss: 3.0619, val loss: 1.6571
0.8924072748744553
Model improve: 0.8882 -> 0.8924
Epoch: 13/200
Train loss: 2.9776, val loss: 1.6414
0.8944167669808935
Model improve: 0.8924 -> 0.8944
Epoch: 14/200
Train loss: 2.9822, val loss: 1.5886
0.8983220423723628
Model improve: 0.8944 -> 0.8983
Epoch: 15/200
Train loss: 2.9656, val loss: 1.5798
0.8983268532243525
Model improve: 0.8983 -> 0.8983
Epoch: 16/200
Train loss: 2.9614, val loss: 1.5408
0.899705327131302
Model improve: 0.8983 -> 0.8997
Epoch: 17/200
Train loss: 2.8517, val loss: 1.5394
0.9019214287256393
Model improve: 0.8997 -> 0.9019
Epoch: 18/200
Train loss: 2.8768, val loss: 1.6151
0.8993457093477245
Epoch: 19/200
Train loss: 2.8255, val loss: 1.4978
0.9028160354195939
Model improve: 0.9019 -> 0.9028
Epoch: 20/200
Train loss: 2.7991, val loss: 1.5285
0.9037394285505335
Model improve: 0.9028 -> 0.9037
Epoch: 21/200
Train loss: 2.8055, val loss: 1.5510
0.9032298162804304
Epoch: 22/200
Train loss: 2.7688, val loss: 1.5241
0.9028138282960925
Epoch: 23/200
Train loss: 2.7624, val loss: 1.4735
0.9043330192510686
Model improve: 0.9037 -> 0.9043
Epoch: 24/200
Train loss: 2.7980, val loss: 1.5335
0.9054715771030183
Model improve: 0.9043 -> 0.9055
Epoch: 25/200
Train loss: 2.6541, val loss: 1.4333
0.9066762168569275
Model improve: 0.9055 -> 0.9067
Epoch: 26/200
Train loss: 2.6213, val loss: 1.4249
0.9081981836485538
Model improve: 0.9067 -> 0.9082
Epoch: 27/200
Train loss: 2.7104, val loss: 1.4579
0.9086235387967749
Model improve: 0.9082 -> 0.9086
Epoch: 28/200
Train loss: 2.6578, val loss: 1.5242
0.9072874250356863
Epoch: 29/200
Train loss: 2.7140, val loss: 1.4511
0.9077389764092362
Epoch: 30/200
Train loss: 2.6451, val loss: 1.5964
0.9059557938173565
Epoch: 31/200
Train loss: 2.5735, val loss: 1.4281
0.9097417254851236
Model improve: 0.9086 -> 0.9097
Epoch: 32/200
Train loss: 2.6537, val loss: 1.5000
0.9062971964698296
Epoch: 33/200
Train loss: 2.5659, val loss: 1.4894
0.9064434652945371
Epoch: 34/200
Train loss: 2.5721, val loss: 1.5034
0.9078746660373382
Epoch: 35/200
Train loss: 2.5515, val loss: 1.4665
0.9087951684660959
Epoch: 36/200
Train loss: 2.4668, val loss: 1.4744
0.9084164480395598
Epoch: 37/200
Train loss: 2.5970, val loss: 1.5399
0.9077246013999801
Epoch: 38/200
Train loss: 2.5684, val loss: 1.4179
0.908359155843798
Epoch: 39/200
Train loss: 2.5184, val loss: 1.4572
0.9086061770596724
Epoch: 40/200
Train loss: 2.5008, val loss: 1.4191
0.90913276124841
Epoch: 41/200
Train loss: 2.5198, val loss: 1.4050
0.9100471469022726
Model improve: 0.9097 -> 0.9100
Epoch: 42/200
Train loss: 2.5376, val loss: 1.4635
0.9093571629145379
Epoch: 43/200
Train loss: 2.4961, val loss: 1.4058
0.9102807802000302
Model improve: 0.9100 -> 0.9103
Epoch: 44/200
Train loss: 2.5424, val loss: 1.4869
0.9103345777274999
Model improve: 0.9103 -> 0.9103
Epoch: 45/200
Train loss: 2.5062, val loss: 1.4572
0.9110639314663289
Model improve: 0.9103 -> 0.9111
Epoch: 46/200
Train loss: 2.4725, val loss: 1.4886
0.9106025201833913
Epoch: 47/200
Train loss: 2.4523, val loss: 1.5184
0.9082217482595997
Epoch: 48/200
Train loss: 2.4528, val loss: 1.4410
0.9112559286768529
Model improve: 0.9111 -> 0.9113
Epoch: 49/200
Train loss: 2.5211, val loss: 1.5264
0.910687470163967
Epoch: 50/200
Train loss: 2.4773, val loss: 1.4245
0.911689088978045
Model improve: 0.9113 -> 0.9117
Epoch: 51/200
Train loss: 2.4399, val loss: 1.4896
0.9107222972151426
Epoch: 52/200
Train loss: 2.4565, val loss: 1.4241
0.9124189187685636
Model improve: 0.9117 -> 0.9124
Epoch: 53/200
Train loss: 2.4224, val loss: 1.4553
0.9109508969218052
Epoch: 54/200
Train loss: 2.3743, val loss: 1.4907
0.9105536733953096
Epoch: 55/200
Train loss: 2.4556, val loss: 1.4294
0.9122097394852855
Epoch: 56/200
Train loss: 2.4013, val loss: 1.4891
0.9102003506535034
Epoch: 57/200
Train loss: 2.4384, val loss: 1.4317
0.9124000101804833
Epoch: 58/200
Train loss: 2.3468, val loss: 1.4359
0.9127507660084256
Model improve: 0.9124 -> 0.9128
Epoch: 59/200
Train loss: 2.4063, val loss: 1.4586
0.9114870334663079
Epoch: 60/200
Train loss: 2.3520, val loss: 1.4914
0.9115722751879165
Epoch: 61/200
Train loss: 2.3259, val loss: 1.4591
0.911228185550286
Epoch: 62/200
Train loss: 2.3929, val loss: 1.4445
0.9119178509950105
Epoch: 63/200
Train loss: 2.3764, val loss: 1.4418
0.9129495100231935
Model improve: 0.9128 -> 0.9129
Epoch: 64/200
Train loss: 2.3506, val loss: 1.4546
0.913165065684502
Model improve: 0.9129 -> 0.9132
Epoch: 65/200
Train loss: 2.3143, val loss: 1.4115
0.9130506976579198
Epoch: 66/200
Train loss: 2.3371, val loss: 1.4333
0.9130888601864603
Epoch: 67/200
Train loss: 2.3333, val loss: 1.3982
0.9127983990957339
Epoch: 68/200
Train loss: 2.3828, val loss: 1.4082
0.913433141132344
Model improve: 0.9132 -> 0.9134
Epoch: 69/200
Train loss: 2.3759, val loss: 1.4463
0.9113217377586045
Epoch: 70/200
Train loss: 2.3666, val loss: 1.4305
0.9122129887169108
Epoch: 71/200
Train loss: 2.3498, val loss: 1.4057
0.9117318191678085
Epoch: 72/200
Train loss: 2.3708, val loss: 1.4716
0.9115418265049822
Epoch: 73/200
Train loss: 2.3166, val loss: 1.4893
0.9111227310345543
Epoch: 74/200
Train loss: 2.2675, val loss: 1.4156
0.9123091259369341
Epoch: 75/200
Train loss: 2.3610, val loss: 1.4919
0.9105921918397598
Epoch: 76/200
Train loss: 2.3652, val loss: 1.4668
0.9106076766610893
Epoch: 77/200
Train loss: 2.3360, val loss: 1.3694
0.9123859476649822
Epoch: 78/200
Train loss: 2.3354, val loss: 1.4395
0.9115848371783033
Epoch: 79/200
Train loss: 2.3114, val loss: 1.3658
0.9117222943715442
Epoch: 80/200
Train loss: 2.3060, val loss: 1.4655
0.9097207408429447
Epoch: 81/200
Train loss: 2.2865, val loss: 1.5226
0.9095074155906485
Epoch: 82/200
Train loss: 2.3119, val loss: 1.4638
0.9109495117827977
Epoch: 83/200
Train loss: 2.1972, val loss: 1.4026
0.9129476528829891
Epoch: 84/200
Train loss: 2.2414, val loss: 1.4479
0.9128940379236835
Epoch: 85/200
Train loss: 2.2224, val loss: 1.3934
0.912782924111732
Epoch: 86/200
Train loss: 2.2497, val loss: 1.4754
0.911312957223694
Epoch: 87/200
Train loss: 2.2909, val loss: 1.4649
0.9122200503793589
Epoch: 88/200
Train loss: 2.3111, val loss: 1.4171
0.913678490176595
Model improve: 0.9134 -> 0.9137
Epoch: 89/200
Train loss: 2.2858, val loss: 1.4465
0.9122796879209087
Epoch: 90/200
Train loss: 2.3218, val loss: 1.4042
0.9137167727090816
Model improve: 0.9137 -> 0.9137
Epoch: 91/200
Train loss: 2.2377, val loss: 1.4001
0.9136178427067628
Epoch: 92/200
Train loss: 2.2757, val loss: 1.4644
0.911035776051504
Epoch: 93/200
Train loss: 2.2497, val loss: 1.4060
0.912605357271346
Epoch: 94/200
Train loss: 2.2751, val loss: 1.4274
0.9135223755805575
Epoch: 95/200
Train loss: 2.2491, val loss: 1.4158
0.9140961617506861
Model improve: 0.9137 -> 0.9141
Epoch: 96/200
Train loss: 2.1793, val loss: 1.4463
0.9139288086536748
Epoch: 97/200
Train loss: 2.1862, val loss: 1.4231
0.9141388433619584
Model improve: 0.9141 -> 0.9141
Epoch: 98/200
Train loss: 2.2430, val loss: 1.4322
0.9131599376242395
Epoch: 99/200
Train loss: 2.1880, val loss: 1.4480
0.9132299277392136
Epoch: 100/200
Train loss: 2.2498, val loss: 1.4719
0.9122252666743564
Epoch: 101/200
Train loss: 2.2280, val loss: 1.4436
0.913320010112526
Epoch: 102/200
Train loss: 2.1745, val loss: 1.4294
0.9129227876967516
Epoch: 103/200
Train loss: 2.2477, val loss: 1.4924
0.9103160666556952
Epoch: 104/200
Train loss: 2.2688, val loss: 1.4566
0.9122207795138655
Epoch: 105/200
Train loss: 2.1519, val loss: 1.4052
0.9137574738902184
Epoch: 106/200
Train loss: 2.1700, val loss: 1.4080
0.9131241350530166
Epoch: 107/200
Train loss: 2.1926, val loss: 1.3998
0.9131582663140843
Epoch: 108/200
Train loss: 2.2110, val loss: 1.4423
0.9123130121909417
Epoch: 109/200
Train loss: 2.2263, val loss: 1.4508
0.9127098271750053
Epoch: 110/200
Train loss: 2.2513, val loss: 1.4708
0.9110552583423414
Epoch: 111/200
Train loss: 2.1682, val loss: 1.4490
0.9125167492773472
Epoch: 112/200
Train loss: 2.1973, val loss: 1.4359
0.9133114819982526
Epoch: 113/200
Train loss: 2.1349, val loss: 1.4605
0.9133157701226323
Epoch: 114/200
Train loss: 2.1902, val loss: 1.4678
0.9121311810857688
Epoch: 115/200
Train loss: 2.2371, val loss: 1.4034
0.9139046312997056
Epoch: 116/200
Train loss: 2.2311, val loss: 1.3954
0.9144105395230081
Model improve: 0.9141 -> 0.9144
Epoch: 117/200
Train loss: 2.2012, val loss: 1.4619
0.9115955536319079
Epoch: 118/200
Train loss: 2.1647, val loss: 1.4804
0.9105034350774945
Epoch: 119/200
Train loss: 2.1650, val loss: 1.4219
0.9131296688779446
Epoch: 120/200
Train loss: 2.1434, val loss: 1.4133
0.9142592172332089
Epoch: 121/200
Train loss: 2.2105, val loss: 1.4318
0.9127393450408411
Epoch: 122/200
Train loss: 2.1639, val loss: 1.4483
0.9124582419524683
Epoch: 123/200
Train loss: 2.1731, val loss: 1.3421
0.9150603670189282
Model improve: 0.9144 -> 0.9151
Epoch: 124/200
Train loss: 2.1049, val loss: 1.4305
0.9142340187417531
Epoch: 125/200
Train loss: 2.1764, val loss: 1.4211
0.9130522381255705
Epoch: 126/200
Train loss: 2.1138, val loss: 1.4105
0.9132915160283452
Epoch: 127/200
Train loss: 2.1063, val loss: 1.4197
0.9139980273063342
Epoch: 128/200
Train loss: 2.1357, val loss: 1.4630
0.9122007168444467
Epoch: 129/200
Train loss: 2.1389, val loss: 1.4151
0.9136144995105259
Epoch: 130/200
Train loss: 2.1437, val loss: 1.4570
0.9125427971992154
Epoch: 131/200
Train loss: 2.1415, val loss: 1.3458
0.9148273089377594
Epoch: 132/200
Train loss: 2.1972, val loss: 1.4291
0.9133652010580128
Epoch: 133/200
Train loss: 2.1450, val loss: 1.4551
0.9126059471503445
Epoch: 134/200
Train loss: 2.1076, val loss: 1.3749
0.9135372223304227
Epoch: 135/200
Train loss: 2.1029, val loss: 1.3916
0.9140332173497243
Epoch: 136/200
Train loss: 2.1489, val loss: 1.4133
0.9137005728397686
Epoch: 137/200
Train loss: 2.1640, val loss: 1.4066
0.9139352610836905
Epoch: 138/200
Train loss: 2.1091, val loss: 1.4042
0.9139472941690135
Epoch: 139/200
Train loss: 2.1087, val loss: 1.3609
0.9149348473037284
Epoch: 140/200
Train loss: 2.1588, val loss: 1.4287
0.9128740673895512
Epoch: 141/200
Train loss: 2.0835, val loss: 1.4445
0.9125854515895414
Epoch: 142/200
Train loss: 2.1739, val loss: 1.3982
0.9141804368099132
Epoch: 143/200
Train loss: 2.1713, val loss: 1.4324
0.9133076589611868
Epoch: 144/200
Train loss: 2.1427, val loss: 1.3834
0.9146505420346017
Epoch: 145/200
Train loss: 2.1293, val loss: 1.3680
0.9153033993998693
Model improve: 0.9151 -> 0.9153
Epoch: 146/200
Train loss: 2.1350, val loss: 1.4410
0.9139544835352736
Epoch: 147/200
Train loss: 2.1409, val loss: 1.4108
0.9137715244025261
Epoch: 148/200
Train loss: 2.1093, val loss: 1.3752
0.9147490144218974
Epoch: 149/200
Train loss: 2.0619, val loss: 1.3914
0.9147729395991832
Epoch: 150/200
Train loss: 2.0928, val loss: 1.4101
0.9137126064067562
Epoch: 151/200
Train loss: 2.1570, val loss: 1.4647
0.9128971960128948
Epoch: 152/200
Train loss: 2.1495, val loss: 1.4131
0.9136194197107639
Epoch: 153/200
Train loss: 2.1364, val loss: 1.3692
0.915337689565976
Model improve: 0.9153 -> 0.9153
Epoch: 154/200
Train loss: 2.1444, val loss: 1.4433
0.9120784263661627
Epoch: 155/200
Train loss: 2.1896, val loss: 1.4040
0.9134131660716155
Epoch: 156/200
Train loss: 2.1275, val loss: 1.3738
0.9141919930295856
Epoch: 157/200
Train loss: 2.0670, val loss: 1.4399
0.912636281152404
Epoch: 158/200
Train loss: 2.1965, val loss: 1.4075
0.913376288809023
Epoch: 159/200
Train loss: 2.1417, val loss: 1.4111
0.9135396983921049
Epoch: 160/200
Train loss: 2.0572, val loss: 1.4494
0.9132580616864386
Epoch: 161/200
Train loss: 2.0892, val loss: 1.3833
0.9140403015383712
Epoch: 162/200
Train loss: 2.0990, val loss: 1.4425
0.9132509307585459
Epoch: 163/200
Train loss: 2.0648, val loss: 1.4084
0.9131718760696399
Epoch: 164/200
Train loss: 2.1021, val loss: 1.4163
0.9139911786189748
Epoch: 165/200
Train loss: 2.1037, val loss: 1.3688
0.9148526174822406
Epoch: 166/200
Train loss: 2.1015, val loss: 1.4460
0.9133024535625545
Epoch: 167/200
Train loss: 2.0327, val loss: 1.4354
0.9142338156652742
Epoch: 168/200
Train loss: 2.0715, val loss: 1.4271
0.9137541895025628
Epoch: 169/200
Train loss: 2.0479, val loss: 1.4003
0.9139678986609043
Epoch: 170/200
Train loss: 2.1556, val loss: 1.4099
0.913815934731701
Epoch: 171/200
Train loss: 2.0700, val loss: 1.3944
0.9140581289198755
Epoch: 172/200
Train loss: 2.0476, val loss: 1.3512
0.915541121239723
Model improve: 0.9153 -> 0.9155
Epoch: 173/200
Train loss: 2.1202, val loss: 1.4604
0.9123872258009643
Epoch: 174/200
Train loss: 2.0684, val loss: 1.3744
0.914517105986923
Epoch: 175/200
Train loss: 2.0988, val loss: 1.3550
0.9143220354983207
Epoch: 176/200
Train loss: 2.1310, val loss: 1.4361
0.9130834051345069
Epoch: 177/200
Train loss: 2.1033, val loss: 1.4270
0.9134356234094885
Epoch: 178/200
Train loss: 2.0071, val loss: 1.4265
0.9136712397061583
Epoch: 179/200
Train loss: 2.0982, val loss: 1.4213
0.9141836819017346
Epoch: 180/200
Train loss: 2.1076, val loss: 1.4141
0.9141252732856326
Epoch: 181/200
Train loss: 2.0477, val loss: 1.3960
0.913765267532427
Epoch: 182/200
Train loss: 2.0451, val loss: 1.4394
0.9134551526719114
Epoch: 183/200
Train loss: 2.1273, val loss: 1.3844
0.9148638729918418
Epoch: 184/200
Train loss: 2.0861, val loss: 1.3890
0.9141573205674879
Epoch: 185/200
Train loss: 2.0971, val loss: 1.4106
0.9136930424105709
Epoch: 186/200
Train loss: 2.1209, val loss: 1.4060
0.9142429650914438
Epoch: 187/200
Train loss: 2.0991, val loss: 1.4074
0.9142335755746153
Epoch: 188/200
Train loss: 2.0758, val loss: 1.3769
0.9148742611614822
Epoch: 189/200
Train loss: 2.1010, val loss: 1.4277
0.9135782073037779
Epoch: 190/200
Train loss: 2.0803, val loss: 1.4416
0.9133588610920587
Epoch: 191/200
Train loss: 2.1036, val loss: 1.4224
0.9136763045805313
Epoch: 192/200
Train loss: 2.0984, val loss: 1.4247
0.9135190427768957
Epoch: 193/200
Train loss: 2.1385, val loss: 1.3944
0.9145045254198143
Epoch: 194/200
Train loss: 2.0519, val loss: 1.4376
0.913021722877602
Epoch: 195/200
Train loss: 2.0553, val loss: 1.4370
0.9136245004287518
Epoch: 196/200
Train loss: 2.0342, val loss: 1.4387
0.913426014296259
Epoch: 197/200
Train loss: 2.1109, val loss: 1.4302
0.9134092942782441
Epoch: 198/200
Train loss: 2.0783, val loss: 1.4171
0.9142472664835832
Epoch: 199/200
Train loss: 2.1040, val loss: 1.3829
0.9141398272958738
Epoch: 200/200
Train loss: 2.0540, val loss: 1.3545
0.9152746731829405
Fold: 3
13816
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 13.8549, val loss: 3.9167
0.6897993684077527
Model improve: 0.0000 -> 0.6898
Epoch: 2/200
Train loss: 4.5681, val loss: 2.9080
0.7898255499436967
Model improve: 0.6898 -> 0.7898
Epoch: 3/200
Train loss: 4.1766, val loss: 2.4758
0.8260306434320251
Model improve: 0.7898 -> 0.8260
Epoch: 4/200
Train loss: 3.8609, val loss: 2.1894
0.8461588674736837
Model improve: 0.8260 -> 0.8462
Epoch: 5/200
Train loss: 3.6459, val loss: 1.9889
0.85665112870918
Model improve: 0.8462 -> 0.8567
Epoch: 6/200
Train loss: 3.4807, val loss: 1.8915
0.8647134177122098
Model improve: 0.8567 -> 0.8647
Epoch: 7/200
Train loss: 3.3822, val loss: 1.8136
0.872469769976979
Model improve: 0.8647 -> 0.8725
Epoch: 8/200
Train loss: 3.3410, val loss: 1.7466
0.8774591821807826
Model improve: 0.8725 -> 0.8775
Epoch: 9/200
Train loss: 3.2339, val loss: 1.6454
0.8844019947611866
Model improve: 0.8775 -> 0.8844
Epoch: 10/200
Train loss: 3.2086, val loss: 1.7175
0.8841164224452689
Epoch: 11/200
Train loss: 3.0915, val loss: 1.6165
0.8865788161914837
Model improve: 0.8844 -> 0.8866
Epoch: 12/200
Train loss: 3.0271, val loss: 1.6006
0.8906577359253639
Model improve: 0.8866 -> 0.8907
Epoch: 13/200
Train loss: 2.9707, val loss: 1.6200
0.8909924523355583
Model improve: 0.8907 -> 0.8910
Epoch: 14/200
Train loss: 3.0111, val loss: 1.6035
0.8931545579206003
Model improve: 0.8910 -> 0.8932
Epoch: 15/200
Train loss: 2.9562, val loss: 1.5672
0.8941617839499216
Model improve: 0.8932 -> 0.8942
Epoch: 16/200
Train loss: 2.9191, val loss: 1.5336
0.8947305561396206
Model improve: 0.8942 -> 0.8947
Epoch: 17/200
Train loss: 2.8761, val loss: 1.5469
0.8966929873171311
Model improve: 0.8947 -> 0.8967
Epoch: 18/200
Train loss: 2.8333, val loss: 1.5097
0.8944688684166927
Epoch: 19/200
Train loss: 2.7876, val loss: 1.5100
0.8975388976072418
Model improve: 0.8967 -> 0.8975
Epoch: 20/200
Train loss: 2.8888, val loss: 1.5280
0.897131659103457
Epoch: 21/200
Train loss: 2.8495, val loss: 1.5166
0.8991692004169178
Model improve: 0.8975 -> 0.8992
Epoch: 22/200
Train loss: 2.7822, val loss: 1.4681
0.8999222116843549
Model improve: 0.8992 -> 0.8999
Epoch: 23/200
Train loss: 2.7324, val loss: 1.4929
0.8985092529709819
Epoch: 24/200
Train loss: 2.7092, val loss: 1.5073
0.9003106122133372
Model improve: 0.8999 -> 0.9003
Epoch: 25/200
Train loss: 2.6878, val loss: 1.5460
0.899267206147136
Epoch: 26/200
Train loss: 2.7449, val loss: 1.5338
0.8992799221879721
Epoch: 27/200
Train loss: 2.7377, val loss: 1.4914
0.9008402223835874
Model improve: 0.9003 -> 0.9008
Epoch: 28/200
Train loss: 2.6577, val loss: 1.5180
0.8994497977178928
Epoch: 29/200
Train loss: 2.6896, val loss: 1.4553
0.9017653443412171
Model improve: 0.9008 -> 0.9018
Epoch: 30/200
Train loss: 2.6609, val loss: 1.4771
0.899598252388329
Epoch: 31/200
Train loss: 2.6281, val loss: 1.4713
0.900478576765036
Epoch: 32/200
Train loss: 2.6663, val loss: 1.4736
0.9023332219214524
Model improve: 0.9018 -> 0.9023
Epoch: 33/200
Train loss: 2.6216, val loss: 1.4273
0.9017964571590271
Epoch: 34/200
Train loss: 2.5875, val loss: 1.4446
0.9031242644379177
Model improve: 0.9023 -> 0.9031
Epoch: 35/200
Train loss: 2.5711, val loss: 1.5064
0.9028190545910865
Epoch: 36/200
Train loss: 2.5785, val loss: 1.4060
0.9039378089732839
Model improve: 0.9031 -> 0.9039
Epoch: 37/200
Train loss: 2.5495, val loss: 1.4429
0.9049760398920628
Model improve: 0.9039 -> 0.9050
Epoch: 38/200
Train loss: 2.5368, val loss: 1.4676
0.9059887751145066
Model improve: 0.9050 -> 0.9060
Epoch: 39/200
Train loss: 2.4924, val loss: 1.4687
0.9030060934277241
Epoch: 40/200
Train loss: 2.5397, val loss: 1.4146
0.9055131400644421
Epoch: 41/200
Train loss: 2.5224, val loss: 1.4482
0.904507016303939
Epoch: 42/200
Train loss: 2.5342, val loss: 1.4528
0.9054681132463074
Epoch: 43/200
Train loss: 2.5184, val loss: 1.4537
0.9056553004836946
Epoch: 44/200
Train loss: 2.5764, val loss: 1.5235
0.904111349593294
Epoch: 45/200
Train loss: 2.5204, val loss: 1.4157
0.9066078771768826
Model improve: 0.9060 -> 0.9066
Epoch: 46/200
Train loss: 2.5051, val loss: 1.4589
0.9042384377940462
Epoch: 47/200
Train loss: 2.4864, val loss: 1.4599
0.9046502289426964
Epoch: 48/200
Train loss: 2.4376, val loss: 1.4688
0.9047694231342182
Epoch: 49/200
Train loss: 2.5045, val loss: 1.4647
0.9052978558887674
Epoch: 50/200
Train loss: 2.4906, val loss: 1.4282
0.9051549875906559
Epoch: 51/200
Train loss: 2.4622, val loss: 1.4466
0.9049070897322308
Epoch: 52/200
Train loss: 2.4824, val loss: 1.4630
0.9053167068724566
Epoch: 53/200
Train loss: 2.4644, val loss: 1.4591
0.9042947302103902
Epoch: 54/200
Train loss: 2.4690, val loss: 1.4744
0.9034314915640206
Epoch: 55/200
Train loss: 2.4011, val loss: 1.3826
0.9048216251867911
Epoch: 56/200
Train loss: 2.3645, val loss: 1.4771
0.9030556974317597
Epoch: 57/200
Train loss: 2.3909, val loss: 1.4000
0.9059277423573917
Epoch: 58/200
Train loss: 2.3810, val loss: 1.4738
0.9062699101056971
Epoch: 59/200
Train loss: 2.4044, val loss: 1.4101
0.9052079404416007
Epoch: 60/200
Train loss: 2.3757, val loss: 1.5062
0.9017322587051788
Epoch: 61/200
Train loss: 2.3508, val loss: 1.4312
0.9060172246317983
Epoch: 62/200
Train loss: 2.2855, val loss: 1.4304
0.9071313851449879
Model improve: 0.9066 -> 0.9071
Epoch: 63/200
Train loss: 2.4307, val loss: 1.4764
0.9053683315976802
Epoch: 64/200
Train loss: 2.4371, val loss: 1.4013
0.9059621748326314
Epoch: 65/200
Train loss: 2.3045, val loss: 1.3930
0.9071944185429031
Model improve: 0.9071 -> 0.9072
Epoch: 66/200
Train loss: 2.3928, val loss: 1.4630
0.9054611853691672
Epoch: 67/200
Train loss: 2.3349, val loss: 1.4466
0.9049928312326052
Epoch: 68/200
Train loss: 2.3407, val loss: 1.3744
0.9068733056463955
Epoch: 69/200
Train loss: 2.4051, val loss: 1.4347
0.9061907303188264
Epoch: 70/200
Train loss: 2.3725, val loss: 1.4265
0.9057923499760756
Epoch: 71/200
Train loss: 2.3416, val loss: 1.3956
0.9068849370413274
Epoch: 72/200
Train loss: 2.3584, val loss: 1.5494
0.9023075512631523
Epoch: 73/200
Train loss: 2.3188, val loss: 1.4485
0.9051217664856825
Epoch: 74/200
Train loss: 2.2271, val loss: 1.3930
0.9052165651666317
Epoch: 75/200
Train loss: 2.2896, val loss: 1.4754
0.9051906943852468
Epoch: 76/200
Train loss: 2.3175, val loss: 1.4248
0.9063328991375639
Epoch: 77/200
Train loss: 2.3321, val loss: 1.4052
0.9067108069089834
Epoch: 78/200
Train loss: 2.3663, val loss: 1.4330
0.9052129788388961
Epoch: 79/200
Train loss: 2.3166, val loss: 1.4613
0.9048520155234835
Epoch: 80/200
Train loss: 2.3242, val loss: 1.4499
0.905317718303865
Epoch: 81/200
Train loss: 2.2650, val loss: 1.4681
0.905852275104045
Epoch: 82/200
Train loss: 2.3268, val loss: 1.4071
0.9076598209261602
Model improve: 0.9072 -> 0.9077
Epoch: 83/200
Train loss: 2.3633, val loss: 1.4369
0.9049999473689665
Epoch: 84/200
Train loss: 2.3410, val loss: 1.4439
0.9053454360958976
Epoch: 85/200
Train loss: 2.2299, val loss: 1.4417
0.9049138511198205
Epoch: 86/200
Train loss: 2.2526, val loss: 1.5041
0.9041927803274006
Epoch: 87/200
Train loss: 2.2241, val loss: 1.4760
0.9044792384525645
Epoch: 88/200
Train loss: 2.2272, val loss: 1.4658
0.9046733908113292
Epoch: 89/200
Train loss: 2.2382, val loss: 1.4023
0.9066854460611847
Epoch: 90/200
Train loss: 2.2420, val loss: 1.4448
0.9046052710583878
Epoch: 91/200
Train loss: 2.2147, val loss: 1.4265
0.9064006670069542
Epoch: 92/200
Train loss: 2.2851, val loss: 1.3687
0.9078243059741735
Model improve: 0.9077 -> 0.9078
Epoch: 93/200
Train loss: 2.2816, val loss: 1.4193
0.9059014409234054
Epoch: 94/200
Train loss: 2.2222, val loss: 1.4401
0.9048911254340273
Epoch: 95/200
Train loss: 2.1964, val loss: 1.4031
0.9061518915839877
Epoch: 96/200
Train loss: 2.1847, val loss: 1.4717
0.9051243724137861
Epoch: 97/200
Train loss: 2.2408, val loss: 1.4259
0.9075513038140729
Epoch: 98/200
Train loss: 2.2975, val loss: 1.4472
0.9050861206625366
Epoch: 99/200
Train loss: 2.2570, val loss: 1.4400
0.9057868521073288
Epoch: 100/200
Train loss: 2.2534, val loss: 1.4538
0.9046366948104586
Epoch: 101/200
Train loss: 2.2438, val loss: 1.4245
0.9068044603900601
Epoch: 102/200
Train loss: 2.3022, val loss: 1.4416
0.9055908001690561
Epoch: 103/200
Train loss: 2.2574, val loss: 1.4387
0.9048630657286683
Epoch: 104/200
Train loss: 2.2336, val loss: 1.4860
0.905094302326077
Epoch: 105/200
Train loss: 2.1485, val loss: 1.4624
0.9040364775354615
Epoch: 106/200
Train loss: 2.2489, val loss: 1.4480
0.9050252355855554
Epoch: 107/200
Train loss: 2.2144, val loss: 1.3834
0.9063015956427674
Epoch: 108/200
Train loss: 2.1489, val loss: 1.3923
0.9065666289570077
Epoch: 109/200
Train loss: 2.2047, val loss: 1.4133
0.9067408630121041
Epoch: 110/200
Train loss: 2.1561, val loss: 1.3793
0.9076457203877772
Epoch: 111/200
Train loss: 2.2276, val loss: 1.4112
0.9067228273677852
Epoch: 112/200
Train loss: 2.2279, val loss: 1.4530
0.9047715586670404
Epoch: 113/200
Train loss: 2.1817, val loss: 1.4450
0.9051049373148317
Epoch: 114/200
Train loss: 2.1651, val loss: 1.3962
0.9074204163250675
Epoch: 115/200
Train loss: 2.2265, val loss: 1.4225
0.9062746228896089
Epoch: 116/200
Train loss: 2.2573, val loss: 1.3961
0.9069490119389894
Epoch: 117/200
Train loss: 2.2454, val loss: 1.4179
0.9074081808916665
Epoch: 118/200
Train loss: 2.1845, val loss: 1.3599
0.9087341854491523
Model improve: 0.9078 -> 0.9087
Epoch: 119/200
Train loss: 2.2031, val loss: 1.4442
0.906215350852927
Epoch: 120/200
Train loss: 2.1598, val loss: 1.4154
0.9080313549009099
Epoch: 121/200
Train loss: 2.2093, val loss: 1.3982
0.9074482166643643
Epoch: 122/200
Train loss: 2.0619, val loss: 1.3625
0.9079197077175594
Epoch: 123/200
Train loss: 2.1946, val loss: 1.4597
0.9060537139324087
Epoch: 124/200
Train loss: 2.1633, val loss: 1.4246
0.9066017989236869
Epoch: 125/200
Train loss: 2.1525, val loss: 1.4355
0.9072169967035745
Epoch: 126/200
Train loss: 2.1035, val loss: 1.4302
0.9072158142032095
Epoch: 127/200
Train loss: 2.1416, val loss: 1.4193
0.9068539058443932
Epoch: 128/200
Train loss: 2.2132, val loss: 1.4225
0.9075664733159825
Epoch: 129/200
Train loss: 2.1512, val loss: 1.4267
0.9063045552182822
Epoch: 130/200
Train loss: 2.1722, val loss: 1.4398
0.9062880454591413
Epoch: 131/200
Train loss: 2.1681, val loss: 1.3845
0.9082728716879457
Epoch: 132/200
Train loss: 2.1387, val loss: 1.4134
0.908113285575618
Epoch: 133/200
Train loss: 2.1169, val loss: 1.4282
0.9076376196109648
Epoch: 134/200
Train loss: 2.2006, val loss: 1.4204
0.9079855126675214
Epoch: 135/200
Train loss: 2.1452, val loss: 1.4209
0.9078810431762605
Epoch: 136/200
Train loss: 2.1278, val loss: 1.3889
0.9076681234254361
Epoch: 137/200
Train loss: 2.1079, val loss: 1.4433
0.9069174542225832
Epoch: 138/200
Train loss: 2.1293, val loss: 1.3885
0.9084801558670214
Epoch: 139/200
Train loss: 2.1469, val loss: 1.4141
0.907877701968099
Epoch: 140/200
Train loss: 2.1548, val loss: 1.4270
0.9075257690293803
Epoch: 141/200
Train loss: 2.1167, val loss: 1.4143
0.9081091605877258
Epoch: 142/200
Train loss: 2.0935, val loss: 1.4472
0.9066933812943428
Epoch: 143/200
Train loss: 2.0810, val loss: 1.4204
0.9073948491021548
Epoch: 144/200
Train loss: 2.1520, val loss: 1.4276
0.9069113771651602
Epoch: 145/200
Train loss: 2.1616, val loss: 1.4090
0.9079331802675299
Epoch: 146/200
Train loss: 2.1345, val loss: 1.3651
0.9084048759306441
Epoch: 147/200
Train loss: 2.0989, val loss: 1.3500
0.9096300773967858
Model improve: 0.9087 -> 0.9096
Epoch: 148/200
Train loss: 2.0747, val loss: 1.4123
0.908758099770988
Epoch: 149/200
Train loss: 2.0955, val loss: 1.3985
0.9082865564657667
Epoch: 150/200
Train loss: 2.1271, val loss: 1.4095
0.9081044900325227
Epoch: 151/200
Train loss: 2.0373, val loss: 1.4070
0.9083588245749409
Epoch: 152/200
Train loss: 2.1473, val loss: 1.4011
0.9087796689821578
Epoch: 153/200
Train loss: 2.0886, val loss: 1.3621
0.9088345609842974
Epoch: 154/200
Train loss: 2.1584, val loss: 1.3991
0.908203105048708
Epoch: 155/200
Train loss: 2.1380, val loss: 1.3966
0.9089523702882277
Epoch: 156/200
Train loss: 2.1270, val loss: 1.3858
0.9088471786879775
Epoch: 157/200
Train loss: 2.1022, val loss: 1.4160
0.9085248507259238
Epoch: 158/200
Train loss: 2.1178, val loss: 1.4153
0.9085800567112432
Epoch: 159/200
Train loss: 2.1337, val loss: 1.3466
0.9102414608926902
Model improve: 0.9096 -> 0.9102
Epoch: 160/200
Train loss: 2.0932, val loss: 1.4138
0.9082058484811538
Epoch: 161/200
Train loss: 2.1558, val loss: 1.4005
0.9084726663274121
Epoch: 162/200
Train loss: 1.9910, val loss: 1.3566
0.909064172323992
Epoch: 163/200
Train loss: 2.0893, val loss: 1.4169
0.9092161095474965
Epoch: 164/200
Train loss: 2.1034, val loss: 1.3837
0.9091264103162017
Epoch: 165/200
Train loss: 2.0629, val loss: 1.4062
0.9083887302512683
Epoch: 166/200
Train loss: 2.0467, val loss: 1.3617
0.909426855776972
Epoch: 167/200
Train loss: 2.0371, val loss: 1.3518
0.9094309052090686
Epoch: 168/200
Train loss: 2.1413, val loss: 1.3847
0.9090829101403404
Epoch: 169/200
Train loss: 2.0449, val loss: 1.3924
0.9091066908938847
Epoch: 170/200
Train loss: 2.0468, val loss: 1.3705
0.9094122683322864
Epoch: 171/200
Train loss: 2.0936, val loss: 1.4098
0.9082915150005684
Epoch: 172/200
Train loss: 2.1347, val loss: 1.4195
0.907885691612012
Epoch: 173/200
Train loss: 2.1260, val loss: 1.4188
0.9083649216744097
Epoch: 174/200
Train loss: 2.1494, val loss: 1.3666
0.9097044285398015
Epoch: 175/200
Train loss: 2.0063, val loss: 1.4213
0.9082260382878351
Epoch: 176/200
Train loss: 2.0704, val loss: 1.3802
0.9097286220567021
Epoch: 177/200
Train loss: 2.0627, val loss: 1.4203
0.9080840835831392
Epoch: 178/200
Train loss: 2.1720, val loss: 1.4022
0.908984075727497
Epoch: 179/200
Train loss: 2.0842, val loss: 1.3864
0.9092279174040883
Epoch: 180/200
Train loss: 2.0340, val loss: 1.4106
0.9083030791640393
Epoch: 181/200
Train loss: 2.0925, val loss: 1.4256
0.9081453641420305
Epoch: 182/200
Train loss: 2.0195, val loss: 1.3665
0.9096239628354635
Epoch: 183/200
Train loss: 2.0408, val loss: 1.3789
0.9093781539976848
Epoch: 184/200
Train loss: 2.0628, val loss: 1.3592
0.9094227663027805
Epoch: 185/200
Train loss: 2.0817, val loss: 1.3758
0.9090959447346559
Epoch: 186/200
Train loss: 2.0913, val loss: 1.4359
0.908388469792961
Epoch: 187/200
Train loss: 2.1326, val loss: 1.3649
0.9097790329618989
Epoch: 188/200
Train loss: 2.1393, val loss: 1.4301
0.9083347440900211
Epoch: 189/200
Train loss: 2.1368, val loss: 1.3895
0.9091818575817441
Epoch: 190/200
Train loss: 2.1651, val loss: 1.4077
0.9090223608763339
Epoch: 191/200
Train loss: 2.1040, val loss: 1.3946
0.9089778913618947
Epoch: 192/200
Train loss: 2.1336, val loss: 1.4134
0.9091192741206253
Epoch: 193/200
Train loss: 2.0208, val loss: 1.3385
0.9099884012966274
Epoch: 194/200
Train loss: 2.0796, val loss: 1.3939
0.9091803595125132
Epoch: 195/200
Train loss: 2.1621, val loss: 1.4292
0.9085202236168985
Epoch: 196/200
Train loss: 2.0588, val loss: 1.4180
0.9087028225846777
Epoch: 197/200
Train loss: 2.0339, val loss: 1.3892
0.9097734177810406
Epoch: 198/200
Train loss: 2.0879, val loss: 1.3540
0.9095052441248236
Epoch: 199/200
Train loss: 2.0762, val loss: 1.3777
0.9098790952192386
Epoch: 200/200
Train loss: 2.1027, val loss: 1.3975
0.9091382639571408
Fold: 4
13830
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 13.8473, val loss: 3.9203
0.6990544221495154
Model improve: 0.0000 -> 0.6991
Epoch: 2/200
Train loss: 4.5749, val loss: 2.9548
0.7912437041206641
Model improve: 0.6991 -> 0.7912
Epoch: 3/200
Train loss: 4.1239, val loss: 2.4948
0.8285869551126933
Model improve: 0.7912 -> 0.8286
Epoch: 4/200
Train loss: 3.8553, val loss: 2.2528
0.8506706847683136
Model improve: 0.8286 -> 0.8507
Epoch: 5/200
Train loss: 3.6255, val loss: 2.0523
0.8662949909702667
Model improve: 0.8507 -> 0.8663
Epoch: 6/200
Train loss: 3.5388, val loss: 1.9607
0.8757357813158924
Model improve: 0.8663 -> 0.8757
Epoch: 7/200
Train loss: 3.4114, val loss: 1.8758
0.8829190889159273
Model improve: 0.8757 -> 0.8829
Epoch: 8/200
Train loss: 3.3072, val loss: 1.8040
0.8868456636027849
Model improve: 0.8829 -> 0.8868
Epoch: 9/200
Train loss: 3.2895, val loss: 1.7400
0.8934306601917209
Model improve: 0.8868 -> 0.8934
Epoch: 10/200
Train loss: 3.2067, val loss: 1.6521
0.8960988738296674
Model improve: 0.8934 -> 0.8961
Epoch: 11/200
Train loss: 3.1628, val loss: 1.6545
0.899180496665382
Model improve: 0.8961 -> 0.8992
Epoch: 12/200
Train loss: 3.0495, val loss: 1.5998
0.9036343171022189
Model improve: 0.8992 -> 0.9036
Epoch: 13/200
Train loss: 3.0125, val loss: 1.5719
0.905474933724072
Model improve: 0.9036 -> 0.9055
Epoch: 14/200
Train loss: 2.9361, val loss: 1.5277
0.9060011531569879
Model improve: 0.9055 -> 0.9060
Epoch: 15/200
Train loss: 2.9215, val loss: 1.5876
0.9075576400670124
Model improve: 0.9060 -> 0.9076
Epoch: 16/200
Train loss: 2.8643, val loss: 1.5130
0.9093525013906957
Model improve: 0.9076 -> 0.9094
Epoch: 17/200
Train loss: 2.8488, val loss: 1.5845
0.9110342108895837
Model improve: 0.9094 -> 0.9110
Epoch: 18/200
Train loss: 2.8913, val loss: 1.5646
0.9111411675392338
Model improve: 0.9110 -> 0.9111
Epoch: 19/200
Train loss: 2.9296, val loss: 1.5277
0.9108017534380182
Epoch: 20/200
Train loss: 2.7937, val loss: 1.5039
0.9136087668396506
Model improve: 0.9111 -> 0.9136
Epoch: 21/200
Train loss: 2.7970, val loss: 1.5180
0.9132621851967299
Epoch: 22/200
Train loss: 2.7504, val loss: 1.4375
0.9136517305967978
Model improve: 0.9136 -> 0.9137
Epoch: 23/200
Train loss: 2.6618, val loss: 1.4036
0.9150942379441781
Model improve: 0.9137 -> 0.9151
Epoch: 24/200
Train loss: 2.7282, val loss: 1.4526
0.9143436433793741
Epoch: 25/200
Train loss: 2.6967, val loss: 1.5030
0.9136321281358426
Epoch: 26/200
Train loss: 2.6749, val loss: 1.4951
0.9140509263425891
Epoch: 27/200
Train loss: 2.6304, val loss: 1.4758
0.912856778276284
Epoch: 28/200
Train loss: 2.7120, val loss: 1.4676
0.9155493252731571
Model improve: 0.9151 -> 0.9155
Epoch: 29/200
Train loss: 2.5876, val loss: 1.4070
0.9153981451141889
Epoch: 30/200
Train loss: 2.6868, val loss: 1.4132
0.917046587275129
Model improve: 0.9155 -> 0.9170
Epoch: 31/200
Train loss: 2.5828, val loss: 1.4207
0.9151184927325833
Epoch: 32/200
Train loss: 2.5697, val loss: 1.4869
0.9141562828591433
Epoch: 33/200
Train loss: 2.6145, val loss: 1.4846
0.9159167920875503
Epoch: 34/200
Train loss: 2.6078, val loss: 1.4911
0.9181359511672798
Model improve: 0.9170 -> 0.9181
Epoch: 35/200
Train loss: 2.5744, val loss: 1.4343
0.9170192133038619
Epoch: 36/200
Train loss: 2.5114, val loss: 1.4499
0.9163193389446712
Epoch: 37/200
Train loss: 2.6100, val loss: 1.4435
0.9172425239568788
Epoch: 38/200
Train loss: 2.5691, val loss: 1.4695
0.9163006354047418
Epoch: 39/200
Train loss: 2.5343, val loss: 1.5081
0.9154276115981601
Epoch: 40/200
Train loss: 2.5625, val loss: 1.4194
0.9185909502766333
Model improve: 0.9181 -> 0.9186
Epoch: 41/200
Train loss: 2.4952, val loss: 1.4119
0.9183853814057694
Epoch: 42/200
Train loss: 2.5111, val loss: 1.3856
0.9190424230261404
Model improve: 0.9186 -> 0.9190
Epoch: 43/200
Train loss: 2.5155, val loss: 1.4670
0.9154551930786126
Epoch: 44/200
Train loss: 2.5120, val loss: 1.4949
0.9176416232603913
Epoch: 45/200
Train loss: 2.4336, val loss: 1.4030
0.9182542547770912
Epoch: 46/200
Train loss: 2.4818, val loss: 1.4261
0.9169452705056151
Epoch: 47/200
Train loss: 2.4603, val loss: 1.4491
0.9173842319116575
Epoch: 48/200
Train loss: 2.5082, val loss: 1.4028
0.9183549254558101
Epoch: 49/200
Train loss: 2.4375, val loss: 1.3968
0.9172642910324301
Epoch: 50/200
Train loss: 2.4568, val loss: 1.4236
0.9175825710902465
Epoch: 51/200
Train loss: 2.4314, val loss: 1.4244
0.91711817112363
Epoch: 52/200
Train loss: 2.4573, val loss: 1.3900
0.9183191555937575
Epoch: 53/200
Train loss: 2.4723, val loss: 1.4124
0.9168523346600476
Epoch: 54/200
Train loss: 2.4614, val loss: 1.3562
0.9192441335293361
Model improve: 0.9190 -> 0.9192
Epoch: 55/200
Train loss: 2.3993, val loss: 1.4687
0.9174825034835822
Epoch: 56/200
Train loss: 2.4542, val loss: 1.4787
0.9163375261119875
Epoch: 57/200
Train loss: 2.4096, val loss: 1.4407
0.9172689194912597
Epoch: 58/200
Train loss: 2.4128, val loss: 1.3899
0.9189002209678679
Epoch: 59/200
Train loss: 2.4181, val loss: 1.4150
0.9172813014388517
Epoch: 60/200
Train loss: 2.3939, val loss: 1.4611
0.9162929658725681
Epoch: 61/200
Train loss: 2.4059, val loss: 1.3883
0.9181960685772931
Epoch: 62/200
Train loss: 2.4570, val loss: 1.4504
0.916957968082632
Epoch: 63/200
Train loss: 2.3752, val loss: 1.3825
0.9177159216059403
Epoch: 64/200
Train loss: 2.3317, val loss: 1.4095
0.9166764395017082
Epoch: 65/200
Train loss: 2.3778, val loss: 1.4308
0.9172433632285288
Epoch: 66/200
Train loss: 2.3579, val loss: 1.3836
0.919460604096502
Model improve: 0.9192 -> 0.9195
Epoch: 67/200
Train loss: 2.4247, val loss: 1.3873
0.9190027097671201
Epoch: 68/200
Train loss: 2.3979, val loss: 1.4109
0.9195979086978395
Model improve: 0.9195 -> 0.9196
Epoch: 69/200
Train loss: 2.3801, val loss: 1.3729
0.9191012025606885
Epoch: 70/200
Train loss: 2.3935, val loss: 1.4309
0.9167478136339701
Epoch: 71/200
Train loss: 2.3731, val loss: 1.4151
0.9190393227930809
Epoch: 72/200
Train loss: 2.2888, val loss: 1.4515
0.9176873911569228
Epoch: 73/200
Train loss: 2.3625, val loss: 1.3978
0.9185401431107838
Epoch: 74/200
Train loss: 2.3527, val loss: 1.3719
0.9179273555676262
Epoch: 75/200
Train loss: 2.3681, val loss: 1.4279
0.9178121973794736
Epoch: 76/200
Train loss: 2.3309, val loss: 1.4444
0.915315651975748
Epoch: 77/200
Train loss: 2.2867, val loss: 1.3851
0.9184073454730374
Epoch: 78/200
Train loss: 2.3266, val loss: 1.4345
0.9185301580211814
Epoch: 79/200
Train loss: 2.2665, val loss: 1.4167
0.9194781764157153
Epoch: 80/200
Train loss: 2.3215, val loss: 1.4071
0.9184913427194393
Epoch: 81/200
Train loss: 2.3442, val loss: 1.3860
0.9188224374648267
Epoch: 82/200
Train loss: 2.2143, val loss: 1.4083
0.9184637025416931
Epoch: 83/200
Train loss: 2.3218, val loss: 1.4303
0.9184210207918335
Epoch: 84/200
Train loss: 2.2539, val loss: 1.3958
0.9190407652024493
Epoch: 85/200
Train loss: 2.2612, val loss: 1.3448
0.9206424959331159
Model improve: 0.9196 -> 0.9206
Epoch: 86/200
Train loss: 2.2783, val loss: 1.4030
0.9184340412511433
Epoch: 87/200
Train loss: 2.2701, val loss: 1.4055
0.9172642609131728
Epoch: 88/200
Train loss: 2.2669, val loss: 1.4020
0.918428780616512
Epoch: 89/200
Train loss: 2.3105, val loss: 1.4101
0.9185064118135557
Epoch: 90/200
Train loss: 2.2653, val loss: 1.3242
0.9195588024233143
Epoch: 91/200
Train loss: 2.2365, val loss: 1.4367
0.9154657169759219
Epoch: 92/200
Train loss: 2.2624, val loss: 1.3774
0.9188308626155015
Epoch: 93/200
Train loss: 2.2148, val loss: 1.4155
0.9175606457248439
Epoch: 94/200
Train loss: 2.1912, val loss: 1.3868
0.9179936260633658
Epoch: 95/200
Train loss: 2.1312, val loss: 1.4367
0.9163951822602839
Epoch: 96/200
Train loss: 2.2588, val loss: 1.4076
0.9183819065315132
Epoch: 97/200
Train loss: 2.2120, val loss: 1.3972
0.9189518273020206
Epoch: 98/200
Train loss: 2.2616, val loss: 1.4214
0.9181899389241591
Epoch: 99/200
Train loss: 2.2040, val loss: 1.3913
0.9189121906242932
Epoch: 100/200
Train loss: 2.2248, val loss: 1.4639
0.915904840112135
Epoch: 101/200
Train loss: 2.2070, val loss: 1.3828
0.9185890465485185
Epoch: 102/200
Train loss: 2.1768, val loss: 1.3974
0.9192097708937209
Epoch: 103/200
Train loss: 2.1972, val loss: 1.3607
0.9206909684601922
Model improve: 0.9206 -> 0.9207
Epoch: 104/200
Train loss: 2.1741, val loss: 1.3874
0.9200765304152343
Epoch: 105/200
Train loss: 2.2781, val loss: 1.4122
0.919061409201032
Epoch: 106/200
Train loss: 2.2360, val loss: 1.4387
0.9176669287641879
Epoch: 107/200
Train loss: 2.1879, val loss: 1.4323
0.9176504893171308
Epoch: 108/200
Train loss: 2.2336, val loss: 1.3878
0.9185395028110716
Epoch: 109/200
Train loss: 2.2625, val loss: 1.3997
0.918393733878978
Epoch: 110/200
Train loss: 2.1970, val loss: 1.3747
0.92053544957361
Epoch: 111/200
Train loss: 2.1126, val loss: 1.3278
0.9214857465739168
Model improve: 0.9207 -> 0.9215
Epoch: 112/200
Train loss: 2.1935, val loss: 1.4222
0.9190309366830718
Epoch: 113/200
Train loss: 2.1674, val loss: 1.4093
0.918995631026297
Epoch: 114/200
Train loss: 2.1274, val loss: 1.4128
0.919182442890021
Epoch: 115/200
Train loss: 2.1260, val loss: 1.3435
0.9213170216901084
Epoch: 116/200
Train loss: 2.2649, val loss: 1.3956
0.9197683250712319
Epoch: 117/200
Train loss: 2.2687, val loss: 1.3781
0.9204058223038976
Epoch: 118/200
Train loss: 2.1989, val loss: 1.4025
0.9181552693844264
Epoch: 119/200
Train loss: 2.2215, val loss: 1.4232
0.9183936807247784
Epoch: 120/200
Train loss: 2.2370, val loss: 1.3967
0.9197445687140035
Epoch: 121/200
Train loss: 2.2295, val loss: 1.3838
0.9190010814012955
Epoch: 122/200
Train loss: 2.1707, val loss: 1.3676
0.9197511178871811
Epoch: 123/200
Train loss: 2.1112, val loss: 1.3744
0.9197855447039913
Epoch: 124/200
Train loss: 2.1950, val loss: 1.3678
0.9197395791025763
Epoch: 125/200
Train loss: 2.1380, val loss: 1.3345
0.9208495522570241
Epoch: 126/200
Train loss: 2.1682, val loss: 1.3787
0.9202269520678731
Epoch: 127/200
Train loss: 2.1258, val loss: 1.3901
0.9195510871058301
Epoch: 128/200
Train loss: 2.1785, val loss: 1.3997
0.9187635567562535
Epoch: 129/200
Train loss: 2.0952, val loss: 1.3641
0.9194616788656761
Epoch: 130/200
Train loss: 2.1342, val loss: 1.3680
0.9196292496072807
Epoch: 131/200
Train loss: 2.2313, val loss: 1.3625
0.9197198571196865
Epoch: 132/200
Train loss: 2.0940, val loss: 1.3658
0.9205462489955623
Epoch: 133/200
Train loss: 2.1780, val loss: 1.3441
0.9216995676515402
Model improve: 0.9215 -> 0.9217
Epoch: 134/200
Train loss: 2.1314, val loss: 1.3882
0.9210429497151429
Epoch: 135/200
Train loss: 2.1323, val loss: 1.3480
0.9215454739431018
Epoch: 136/200
Train loss: 2.0883, val loss: 1.3357
0.9200496087357575
Epoch: 137/200
Train loss: 2.1125, val loss: 1.3125
0.9210465003769024
Epoch: 138/200
Train loss: 2.1208, val loss: 1.3651
0.9211674159422643
Epoch: 139/200
Train loss: 2.1073, val loss: 1.3957
0.9194162105571708
Epoch: 140/200
Train loss: 2.1658, val loss: 1.3986
0.920654451130396
Epoch: 141/200
Train loss: 2.1056, val loss: 1.3957
0.9201173518575261
Epoch: 142/200
Train loss: 2.1218, val loss: 1.4394
0.9187683912221656
Epoch: 143/200
Train loss: 2.1217, val loss: 1.3715
0.9211615138764624
Epoch: 144/200
Train loss: 2.0581, val loss: 1.3599
0.9203550046166585
Epoch: 145/200
Train loss: 2.1279, val loss: 1.3283
0.9202787507642166
Epoch: 146/200
Train loss: 2.0652, val loss: 1.3642
0.9208807177865713
Epoch: 147/200
Train loss: 2.1032, val loss: 1.3594
0.9204538410156314
Epoch: 148/200
Train loss: 2.1100, val loss: 1.3582
0.9207133581757518
Epoch: 149/200
Train loss: 2.1077, val loss: 1.3796
0.9202888227029464
Epoch: 150/200
Train loss: 2.0944, val loss: 1.3810
0.9203209230697976
Epoch: 151/200
Train loss: 2.1568, val loss: 1.3969
0.9204085074380083
Epoch: 152/200
Train loss: 2.1288, val loss: 1.3591
0.9209017260196731
Epoch: 153/200
Train loss: 2.1376, val loss: 1.3969
0.9194745996697187
Epoch: 154/200
Train loss: 2.1238, val loss: 1.3670
0.9204781742801622
Epoch: 155/200
Train loss: 2.0585, val loss: 1.3953
0.9200357024853837
Epoch: 156/200
Train loss: 2.1096, val loss: 1.3652
0.9205432721997953
Epoch: 157/200
Train loss: 2.1403, val loss: 1.3868
0.9204125365041287
Epoch: 158/200
Train loss: 2.1487, val loss: 1.4197
0.9195280041140415
Epoch: 159/200
Train loss: 2.0558, val loss: 1.4097
0.9195807750868235
Epoch: 160/200
Train loss: 2.0726, val loss: 1.3988
0.919940492331007
Epoch: 161/200
Train loss: 2.1217, val loss: 1.4063
0.9193208295685528
Epoch: 162/200
Train loss: 2.0821, val loss: 1.3251
0.9217948823743044
Model improve: 0.9217 -> 0.9218
Epoch: 163/200
Train loss: 2.1130, val loss: 1.3891
0.9204186774938343
Epoch: 164/200
Train loss: 2.0586, val loss: 1.3759
0.9209433282250077
Epoch: 165/200
Train loss: 2.0526, val loss: 1.3466
0.921325133222942
Epoch: 166/200
Train loss: 2.1040, val loss: 1.3556
0.9210492867689676
Epoch: 167/200
Train loss: 2.0868, val loss: 1.4055
0.9189758050436675
Epoch: 168/200
Train loss: 2.0915, val loss: 1.3789
0.919949676399541
Epoch: 169/200
Train loss: 2.1224, val loss: 1.3315
0.9208539106631567
Epoch: 170/200
Train loss: 2.0355, val loss: 1.3681
0.9201818048513175
Epoch: 171/200
Train loss: 2.0256, val loss: 1.3724
0.9210735761558863
Epoch: 172/200
Train loss: 2.0979, val loss: 1.3762
0.9206210462607858
Epoch: 173/200
Train loss: 2.0862, val loss: 1.3842
0.9200147531555096
Epoch: 174/200
Train loss: 2.0322, val loss: 1.3431
0.9206979857792552
Epoch: 175/200
Train loss: 2.1050, val loss: 1.3699
0.9206344288998307
Epoch: 176/200
Train loss: 2.0508, val loss: 1.3259
0.9218652638904495
Model improve: 0.9218 -> 0.9219
Epoch: 177/200
Train loss: 2.1116, val loss: 1.4058
0.9198246625380095
Epoch: 178/200
Train loss: 2.1278, val loss: 1.4051
0.9198657617146294
Epoch: 179/200
Train loss: 2.1523, val loss: 1.3665
0.9210560793392801
Epoch: 180/200
Train loss: 2.0840, val loss: 1.3334
0.9215752370307518
Epoch: 181/200
Train loss: 2.1761, val loss: 1.3939
0.9205363586285473
Epoch: 182/200
Train loss: 2.0859, val loss: 1.3293
0.9217459920967551
Epoch: 183/200
Train loss: 2.1008, val loss: 1.3486
0.9211588084226117
Epoch: 184/200
Train loss: 2.0690, val loss: 1.3687
0.9211216265691166
Epoch: 185/200
Train loss: 2.0865, val loss: 1.3643
0.9209514727811007
Epoch: 186/200
Train loss: 2.0660, val loss: 1.3967
0.9206638413645204
Epoch: 187/200
Train loss: 2.1089, val loss: 1.3786
0.9209101816652749
Epoch: 188/200
Train loss: 2.0385, val loss: 1.4080
0.9202152876514761
Epoch: 189/200
Train loss: 2.0895, val loss: 1.3918
0.9204889445257562
Epoch: 190/200
Train loss: 2.0623, val loss: 1.3203
0.9225322166377433
Model improve: 0.9219 -> 0.9225
Epoch: 191/200
Train loss: 2.1448, val loss: 1.3424
0.9224198511776766
Epoch: 192/200
Train loss: 2.0810, val loss: 1.3889
0.9200269802865743
Epoch: 193/200
Train loss: 2.0725, val loss: 1.3276
0.9222999363611195
Epoch: 194/200
Train loss: 2.1301, val loss: 1.3950
0.9202925421903795
Epoch: 195/200
Train loss: 2.1301, val loss: 1.3179
0.9219611669392931
Epoch: 196/200
Train loss: 2.0910, val loss: 1.4083
0.9205036639383076
Epoch: 197/200
Train loss: 2.0659, val loss: 1.3728
0.9217237850369411
Epoch: 198/200
Train loss: 2.0460, val loss: 1.3461
0.9219498930534772
Epoch: 199/200
Train loss: 2.0352, val loss: 1.3659
0.9210259319244138
Epoch: 200/200
Train loss: 2.1019, val loss: 1.3801
0.9208200104756848
Date :04/24/2023, 00:48:47
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
Epoch: 1/200
Date :04/24/2023, 00:51:05
Date :04/24/2023, 00:51:05
Duration: 5
Duration: 5
Sample rate: 32000
Sample rate: 32000
nfft: 768
nfft: 768
fmin: 20
fmin: 20
nmels: 128
nmels: 128
fmax: 16000
fmax: 16000
trainbs: 32
trainbs: 32
validbs: 128
validbs: 128
epochwarmup: 0
epochwarmup: 0
totalepoch: 200
totalepoch: 200
learningrate: 0.0003
learningrate: 0.0003
weightdecay: 0.01
weightdecay: 0.01
thrupsample: 10
thrupsample: 10
model_name: tf_efficientnet_b0_ns
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
mix_up: 0.8
hop_length: 256
hop_length: 256
train_with_mixup: True
train_with_mixup: True
num_channels: 1
num_channels: 1
use_spec_augmenter: False
use_spec_augmenter: False
use_drop_path: True
use_drop_path: True
Fold: 0
Fold: 0
Date :04/24/2023, 00:52:05
Date :04/24/2023, 00:52:05
Duration: 5
Duration: 5
Sample rate: 32000
Sample rate: 32000
nfft: 768
nfft: 768
fmin: 20
fmin: 20
nmels: 128
nmels: 128
fmax: 16000
fmax: 16000
trainbs: 32
trainbs: 32
validbs: 128
validbs: 128
epochwarmup: 0
epochwarmup: 0
totalepoch: 200
totalepoch: 200
learningrate: 0.0003
learningrate: 0.0003
weightdecay: 0.01
weightdecay: 0.01
thrupsample: 10
thrupsample: 10
model_name: tf_efficientnet_b0_ns
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
mix_up: 0.8
hop_length: 256
hop_length: 256
train_with_mixup: True
train_with_mixup: True
num_channels: 1
num_channels: 1
use_spec_augmenter: False
use_spec_augmenter: False
use_drop_path: True
use_drop_path: True
Fold: 0
Fold: 0
Epoch: 1/200
Epoch: 1/200
Date :04/24/2023, 00:54:21
Date :04/24/2023, 00:54:21
Duration: 5
Duration: 5
Sample rate: 32000
Sample rate: 32000
nfft: 768
nfft: 768
fmin: 20
fmin: 20
nmels: 128
nmels: 128
fmax: 16000
fmax: 16000
trainbs: 32
trainbs: 32
validbs: 128
validbs: 128
epochwarmup: 0
epochwarmup: 0
totalepoch: 200
totalepoch: 200
learningrate: 0.0003
learningrate: 0.0003
weightdecay: 0.01
weightdecay: 0.01
thrupsample: 10
thrupsample: 10
model_name: tf_efficientnet_b0_ns
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
mix_up: 0.8
hop_length: 256
hop_length: 256
train_with_mixup: True
train_with_mixup: True
num_channels: 1
num_channels: 1
use_spec_augmenter: False
use_spec_augmenter: False
use_drop_path: True
use_drop_path: True
Fold: 0
Fold: 0
Date :04/24/2023, 00:54:41
Date :04/24/2023, 00:54:41
Duration: 5
Duration: 5
Sample rate: 32000
Sample rate: 32000
nfft: 768
nfft: 768
fmin: 20
fmin: 20
nmels: 128
nmels: 128
fmax: 16000
fmax: 16000
trainbs: 32
trainbs: 32
validbs: 128
validbs: 128
epochwarmup: 0
epochwarmup: 0
totalepoch: 200
totalepoch: 200
learningrate: 0.0003
learningrate: 0.0003
weightdecay: 0.01
weightdecay: 0.01
thrupsample: 10
thrupsample: 10
model_name: tf_efficientnet_b0_ns
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
mix_up: 0.8
hop_length: 256
hop_length: 256
train_with_mixup: True
train_with_mixup: True
num_channels: 1
num_channels: 1
use_spec_augmenter: False
use_spec_augmenter: False
use_drop_path: True
use_drop_path: True
Fold: 0
Fold: 0
Date :04/24/2023, 00:57:13
Date :04/24/2023, 00:57:13
Duration: 5
Duration: 5
Sample rate: 32000
Sample rate: 32000
nfft: 768
nfft: 768
fmin: 20
fmin: 20
nmels: 128
nmels: 128
fmax: 16000
fmax: 16000
trainbs: 32
trainbs: 32
validbs: 128
validbs: 128
epochwarmup: 0
epochwarmup: 0
totalepoch: 200
totalepoch: 200
learningrate: 0.0003
learningrate: 0.0003
weightdecay: 0.01
weightdecay: 0.01
thrupsample: 10
thrupsample: 10
model_name: tf_efficientnet_b0_ns
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
mix_up: 0.8
hop_length: 256
hop_length: 256
train_with_mixup: True
train_with_mixup: True
num_channels: 1
num_channels: 1
use_spec_augmenter: False
use_spec_augmenter: False
use_drop_path: True
use_drop_path: True
Fold: 0
Fold: 0
Date :04/24/2023, 00:57:47
Date :04/24/2023, 00:57:47
Duration: 5
Duration: 5
Sample rate: 32000
Sample rate: 32000
nfft: 768
nfft: 768
fmin: 20
fmin: 20
nmels: 128
nmels: 128
fmax: 16000
fmax: 16000
trainbs: 32
trainbs: 32
validbs: 128
validbs: 128
epochwarmup: 0
epochwarmup: 0
totalepoch: 200
totalepoch: 200
learningrate: 0.0003
learningrate: 0.0003
weightdecay: 0.01
weightdecay: 0.01
thrupsample: 10
thrupsample: 10
model_name: tf_efficientnet_b0_ns
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
mix_up: 0.8
hop_length: 256
hop_length: 256
train_with_mixup: True
train_with_mixup: True
num_channels: 1
num_channels: 1
use_spec_augmenter: False
use_spec_augmenter: False
use_drop_path: True
use_drop_path: True
Fold: 0
Fold: 0
Date :04/24/2023, 00:57:54
Date :04/24/2023, 00:57:54
Duration: 5
Duration: 5
Sample rate: 32000
Sample rate: 32000
nfft: 768
nfft: 768
fmin: 20
fmin: 20
nmels: 128
nmels: 128
fmax: 16000
fmax: 16000
trainbs: 32
trainbs: 32
validbs: 128
validbs: 128
epochwarmup: 0
epochwarmup: 0
totalepoch: 200
totalepoch: 200
learningrate: 0.0003
learningrate: 0.0003
weightdecay: 0.01
weightdecay: 0.01
thrupsample: 10
thrupsample: 10
model_name: tf_efficientnet_b0_ns
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
mix_up: 0.8
hop_length: 256
hop_length: 256
train_with_mixup: True
train_with_mixup: True
num_channels: 1
num_channels: 1
use_spec_augmenter: False
use_spec_augmenter: False
use_drop_path: True
use_drop_path: True
Fold: 0
Fold: 0
Epoch: 1/200
Epoch: 1/200
Date :04/24/2023, 03:22:49
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13830
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Date :04/24/2023, 03:27:32
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13830
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Date :04/24/2023, 03:30:01
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13830
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Date :04/24/2023, 03:31:33
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
1013
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 123.0535
Date :04/24/2023, 03:31:54
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
1013
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 123.0700
Date :04/24/2023, 03:32:18
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13830
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 14.1391
0.24706605592639935
Model improve: 0.0000 -> 0.2471
Epoch: 2/200
Train loss: 4.5167
0.24706605592639935
Epoch: 3/200
Train loss: 4.0617
0.24706605592639935
Epoch: 4/200
Train loss: 3.8078
0.24706605592639935
Epoch: 5/200
Train loss: 3.6301
0.24706605592639935
Epoch: 6/200
Train loss: 3.4344
0.24706605592639935
Epoch: 7/200
Date :04/24/2023, 04:00:43
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13830
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 14.1435
0.23198398610272067
Model improve: 0.0000 -> 0.2320
Epoch: 2/200
Date :04/24/2023, 04:13:43
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
512
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 163.1026
Date :04/24/2023, 04:14:17
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
512
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 163.1168
Date :04/24/2023, 04:14:37
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
512
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 162.7786
Date :04/24/2023, 04:15:18
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
1013
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 123.2478
Date :04/24/2023, 04:16:47
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
512
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 162.7625
Date :04/24/2023, 04:17:09
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
512
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 162.7556
Date :04/24/2023, 04:18:49
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13830
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Date :04/24/2023, 04:19:20
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13830
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 14.2380
With concurrent ThreadPoolExecutor, time cost reduced to 612.8279423713684 for processing 15 audios
0.231504265960449
Model improve: 0.0000 -> 0.2315
Epoch: 2/200
Train loss: 4.5077
With concurrent ThreadPoolExecutor, time cost reduced to 646.2746317386627 for processing 15 audios
0.23175864311299915
Model improve: 0.2315 -> 0.2318
Epoch: 3/200
Train loss: 4.0729
With concurrent ThreadPoolExecutor, time cost reduced to 668.2210865020752 for processing 15 audios
0.23072054173474213
Epoch: 4/200
Train loss: 3.7979
With concurrent ThreadPoolExecutor, time cost reduced to 691.4029579162598 for processing 15 audios
0.23077302238616654
Epoch: 5/200
Train loss: 3.6112
With concurrent ThreadPoolExecutor, time cost reduced to 711.1253690719604 for processing 15 audios
0.2295087813356745
Epoch: 6/200
Date :04/24/2023, 05:27:43
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
512
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 162.7581
Date :04/24/2023, 05:28:17
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
512
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 163.1005
Date :04/24/2023, 05:28:41
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
512
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 163.1077
Date :04/24/2023, 05:29:15
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13830
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 14.1983
Date :04/24/2023, 06:14:57
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
1013
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 122.4751
With concurrent ThreadPoolExecutor, time cost reduced to 9.376794576644897 for processing 15 audios
Date :04/24/2023, 06:16:10
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
1013
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 122.4709
With concurrent ThreadPoolExecutor, time cost reduced to 29.85867738723755 for processing 15 audios
Date :04/24/2023, 06:17:09
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
1013
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 122.5421
Date :04/24/2023, 13:20:18
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
1013
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 122.4885
Date :04/24/2023, 13:20:52
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
1013
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 122.5319
With concurrent ThreadPoolExecutor, time cost reduced to 67.51762008666992 for processing 15 audios
Date :04/24/2023, 13:22:53
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
1013
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 122.4616
Date :04/24/2023, 13:25:53
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
1013
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 122.5116
With concurrent ThreadPoolExecutor, time cost reduced to 20.008601188659668 for processing 15 audios
Date :04/24/2023, 13:26:53
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
1013
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 122.4659
With concurrent ThreadPoolExecutor, time cost reduced to 36.60513663291931 for processing 15 audios
Date :04/24/2023, 08:15:50
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
1013
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
With concurrent ThreadPoolExecutor, time cost reduced to 29.002737760543823 for processing 15 audios
Date :04/24/2023, 08:18:37
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
1013
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Date :04/24/2023, 08:19:37
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
1013
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
0.7857270642228692
Model improve: 0.0000 -> 0.7857
Epoch: 2/200
Date :04/24/2023, 08:33:29
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
1013
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
With concurrent ThreadPoolExecutor, time cost reduced to 61.135719299316406 for processing 15 audios
0.7857270642228692
Model improve: 0.0000 -> 0.7857
Epoch: 2/200
Date :04/24/2023, 08:35:54
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13830
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
With concurrent ThreadPoolExecutor, time cost reduced to 231.54045796394348 for processing 15 audios
0.23490247947740583
Model improve: 0.0000 -> 0.2349
Epoch: 2/200
Date :04/24/2023, 08:41:41
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13830
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
With concurrent ThreadPoolExecutor, time cost reduced to 219.6675841808319 for processing 15 audios
0.4535681718625786
Model improve: 0.0000 -> 0.4536
Epoch: 2/200
With concurrent ThreadPoolExecutor, time cost reduced to 217.46877026557922 for processing 15 audios
0.5748539269585522
Model improve: 0.4536 -> 0.5749
Epoch: 3/200
With concurrent ThreadPoolExecutor, time cost reduced to 217.39675855636597 for processing 15 audios
0.6232680146937669
Model improve: 0.5749 -> 0.6233
Epoch: 4/200
With concurrent ThreadPoolExecutor, time cost reduced to 217.21215915679932 for processing 15 audios
0.6532290672305199
Model improve: 0.6233 -> 0.6532
Epoch: 5/200
With concurrent ThreadPoolExecutor, time cost reduced to 216.9792320728302 for processing 15 audios
0.6716633191697066
Model improve: 0.6532 -> 0.6717
Epoch: 6/200
With concurrent ThreadPoolExecutor, time cost reduced to 218.0819993019104 for processing 15 audios
0.6859979707665843
Model improve: 0.6717 -> 0.6860
Epoch: 7/200
With concurrent ThreadPoolExecutor, time cost reduced to 218.20335030555725 for processing 15 audios
0.6971691233902941
Model improve: 0.6860 -> 0.6972
Epoch: 8/200
With concurrent ThreadPoolExecutor, time cost reduced to 217.18390369415283 for processing 15 audios
0.7110042566429504
Model improve: 0.6972 -> 0.7110
Epoch: 9/200
With concurrent ThreadPoolExecutor, time cost reduced to 218.63340187072754 for processing 15 audios
0.7145019432030537
Model improve: 0.7110 -> 0.7145
Epoch: 10/200
With concurrent ThreadPoolExecutor, time cost reduced to 217.25445818901062 for processing 15 audios
0.7205282316337321
Model improve: 0.7145 -> 0.7205
Epoch: 11/200
With concurrent ThreadPoolExecutor, time cost reduced to 220.95694303512573 for processing 15 audios
0.7255865886303238
Model improve: 0.7205 -> 0.7256
Epoch: 12/200
With concurrent ThreadPoolExecutor, time cost reduced to 217.24064564704895 for processing 15 audios
0.7368446590033397
Model improve: 0.7256 -> 0.7368
Epoch: 13/200
With concurrent ThreadPoolExecutor, time cost reduced to 216.1677327156067 for processing 15 audios
0.7373473199841913
Model improve: 0.7368 -> 0.7373
Epoch: 14/200
With concurrent ThreadPoolExecutor, time cost reduced to 262.4666826725006 for processing 15 audios
0.7365401395644755
Epoch: 15/200
Date :04/24/2023, 10:19:46
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
12999
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Date :04/24/2023, 10:21:03
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
12999
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Date :04/24/2023, 10:24:30
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
12999
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Date :04/24/2023, 10:28:00
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
12999
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
With concurrent ThreadPoolExecutor, time cost reduced to 567.3315653800964 for validation audios
0.40290733179796295
Model improve: 0.0000 -> 0.4029
Epoch: 2/200
Date :04/24/2023, 10:40:51
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
12999
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Date :04/24/2023, 10:41:35
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
12999
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
With concurrent ThreadPoolExecutor, time cost reduced to 234.63931608200073 for validation audios
0.3812577916208495
Model improve: 0.0000 -> 0.3813
Epoch: 2/200
With concurrent ThreadPoolExecutor, time cost reduced to 237.11930680274963 for validation audios
0.5307936187336049
Model improve: 0.3813 -> 0.5308
Epoch: 3/200
With concurrent ThreadPoolExecutor, time cost reduced to 234.24869966506958 for validation audios
0.5938634021934489
Model improve: 0.5308 -> 0.5939
Epoch: 4/200
With concurrent ThreadPoolExecutor, time cost reduced to 235.18277835845947 for validation audios
0.6266702045543887
Model improve: 0.5939 -> 0.6267
Epoch: 5/200
With concurrent ThreadPoolExecutor, time cost reduced to 235.35901761054993 for validation audios
0.6490343263899431
Model improve: 0.6267 -> 0.6490
Epoch: 6/200
With concurrent ThreadPoolExecutor, time cost reduced to 234.9203004837036 for validation audios
0.6677571547254685
Model improve: 0.6490 -> 0.6678
Epoch: 7/200
Date :04/24/2023, 11:25:02
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
12999
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
With concurrent ThreadPoolExecutor, time cost reduced to 290.0154824256897 for validation audios
0.33900292646213476
Model improve: 0.0000 -> 0.3390
Epoch: 2/200
Date :04/24/2023, 11:36:03
Duration: 5
Sample rate: 32000
nfft: 512
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 320
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
12999
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
With concurrent ThreadPoolExecutor, time cost reduced to 230.8901801109314 for validation audios
0.34794778989037667
Model improve: 0.0000 -> 0.3479
Epoch: 2/200
Date :04/24/2023, 11:45:31
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
18955
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
With concurrent ThreadPoolExecutor, time cost reduced to 238.40189003944397 for validation audios
0.49519208935955755
Model improve: 0.0000 -> 0.4952
Epoch: 2/200
With concurrent ThreadPoolExecutor, time cost reduced to 235.67901515960693 for validation audios
0.6075818741516369
Model improve: 0.4952 -> 0.6076
Epoch: 3/200
Date :04/24/2023, 12:02:59
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
18955
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
With concurrent ThreadPoolExecutor, time cost reduced to 350.9026029109955 for validation audios
0.6020217252893687
Model improve: 0.0000 -> 0.6020
Epoch: 2/200
With concurrent ThreadPoolExecutor, time cost reduced to 503.6935477256775 for validation audios
0.6754089230193773
Model improve: 0.6020 -> 0.6754
Epoch: 3/200
With concurrent ThreadPoolExecutor, time cost reduced to 447.3979141712189 for validation audios
0.6922379961582212
Model improve: 0.6754 -> 0.6922
Epoch: 4/200
With concurrent ThreadPoolExecutor, time cost reduced to 479.4987516403198 for validation audios
0.7037525654098353
Model improve: 0.6922 -> 0.7038
Epoch: 5/200
With concurrent ThreadPoolExecutor, time cost reduced to 596.2569785118103 for validation audios
0.7117917804957647
Model improve: 0.7038 -> 0.7118
Epoch: 6/200
Date :04/24/2023, 13:20:57
Duration: 5
Sample rate: 32000
nfft: 1024
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
12999
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
With concurrent ThreadPoolExecutor, time cost reduced to 631.1015086174011 for validation audios
0.28366356934232595
Model improve: 0.0000 -> 0.2837
Epoch: 2/200
Date :04/24/2023, 13:39:02
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13830
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Date :04/24/2023, 13:39:30
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13830
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
With concurrent ThreadPoolExecutor, time cost reduced to 1.18526291847229 for validation audios
0.998182670429806
Model improve: 0.0000 -> 0.9982
Epoch: 2/200
With concurrent ThreadPoolExecutor, time cost reduced to 0.23064064979553223 for validation audios
0.998182670429806
Epoch: 3/200
With concurrent ThreadPoolExecutor, time cost reduced to 0.22955322265625 for validation audios
0.998182670429806
Epoch: 4/200
With concurrent ThreadPoolExecutor, time cost reduced to 0.22904562950134277 for validation audios
0.998182670429806
Epoch: 5/200
With concurrent ThreadPoolExecutor, time cost reduced to 0.22794222831726074 for validation audios
0.998182670429806
Epoch: 6/200
With concurrent ThreadPoolExecutor, time cost reduced to 0.2282090187072754 for validation audios
0.998182670429806
Epoch: 7/200
With concurrent ThreadPoolExecutor, time cost reduced to 0.22783493995666504 for validation audios
0.998182670429806
Epoch: 8/200
With concurrent ThreadPoolExecutor, time cost reduced to 0.2272183895111084 for validation audios
0.998182670429806
Epoch: 9/200
With concurrent ThreadPoolExecutor, time cost reduced to 0.2331383228302002 for validation audios
0.998182670429806
Epoch: 10/200
With concurrent ThreadPoolExecutor, time cost reduced to 0.22733592987060547 for validation audios
0.998182670429806
Epoch: 11/200
With concurrent ThreadPoolExecutor, time cost reduced to 0.22714662551879883 for validation audios
0.998182670429806
Epoch: 12/200
With concurrent ThreadPoolExecutor, time cost reduced to 0.2274763584136963 for validation audios
0.998182670429806
Epoch: 13/200
With concurrent ThreadPoolExecutor, time cost reduced to 0.22744369506835938 for validation audios
0.998182670429806
Epoch: 14/200
With concurrent ThreadPoolExecutor, time cost reduced to 0.2273859977722168 for validation audios
0.998182670429806
Epoch: 15/200
With concurrent ThreadPoolExecutor, time cost reduced to 0.22756338119506836 for validation audios
0.998182670429806
Epoch: 16/200
With concurrent ThreadPoolExecutor, time cost reduced to 0.22797131538391113 for validation audios
0.998182670429806
Epoch: 17/200
With concurrent ThreadPoolExecutor, time cost reduced to 0.22781753540039062 for validation audios
0.998182670429806
Epoch: 18/200
With concurrent ThreadPoolExecutor, time cost reduced to 0.2314152717590332 for validation audios
0.998182670429806
Epoch: 19/200
With concurrent ThreadPoolExecutor, time cost reduced to 0.22817492485046387 for validation audios
0.998182670429806
Epoch: 20/200
With concurrent ThreadPoolExecutor, time cost reduced to 0.22805333137512207 for validation audios
0.998182670429806
Epoch: 21/200
Date :04/24/2023, 13:40:04
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13830
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
With concurrent ThreadPoolExecutor, time cost reduced to 1.0854761600494385 for validation audios
0.998182670429806
Model improve: 0.0000 -> 0.9982
Date :04/24/2023, 13:40:28
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13830
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
With concurrent ThreadPoolExecutor, time cost reduced to 215.73619866371155 for validation audios
0.2340754218010069
Model improve: 0.0000 -> 0.2341
Date :04/24/2023, 13:45:55
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13830
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Date :04/24/2023, 13:46:05
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13830
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
With concurrent ThreadPoolExecutor, time cost reduced to 0.0011239051818847656 for processing 15 audios
Date :04/24/2023, 13:46:33
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13830
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
With concurrent ThreadPoolExecutor, time cost reduced to 0.0010597705841064453 for processing 15 audios
Date :04/24/2023, 13:47:24
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13830
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Date :04/24/2023, 13:47:42
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13830
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Date :04/24/2023, 13:47:52
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13830
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Date :04/24/2023, 13:57:46
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13830
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
With concurrent ThreadPoolExecutor, time cost reduced to 188.30305075645447 for validation audios
0.33341574944433755
Model improve: 0.0000 -> 0.3334
Date :04/24/2023, 14:04:56
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13830
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
With concurrent ThreadPoolExecutor, time cost reduced to 180.5341112613678 for validation audios
0.3332831546094554
Model improve: 0.0000 -> 0.3333
Epoch: 2/200
With concurrent ThreadPoolExecutor, time cost reduced to 176.20338129997253 for validation audios
0.4540440684263417
Model improve: 0.3333 -> 0.4540
Epoch: 3/200
With concurrent ThreadPoolExecutor, time cost reduced to 173.163560628891 for validation audios
0.547872107431004
Model improve: 0.4540 -> 0.5479
Epoch: 4/200
With concurrent ThreadPoolExecutor, time cost reduced to 177.2663447856903 for validation audios
0.5983710208461803
Model improve: 0.5479 -> 0.5984
Epoch: 5/200
With concurrent ThreadPoolExecutor, time cost reduced to 174.60617756843567 for validation audios
0.628698364390851
Model improve: 0.5984 -> 0.6287
Epoch: 6/200
With concurrent ThreadPoolExecutor, time cost reduced to 170.70842456817627 for validation audios
0.6507458346933335
Model improve: 0.6287 -> 0.6507
Epoch: 7/200
With concurrent ThreadPoolExecutor, time cost reduced to 174.93807172775269 for validation audios
0.6666109011541664
Model improve: 0.6507 -> 0.6666
Epoch: 8/200
With concurrent ThreadPoolExecutor, time cost reduced to 175.76906728744507 for validation audios
0.6829450726971259
Model improve: 0.6666 -> 0.6829
Epoch: 9/200
With concurrent ThreadPoolExecutor, time cost reduced to 172.99993252754211 for validation audios
0.6925007273201151
Model improve: 0.6829 -> 0.6925
Epoch: 10/200
With concurrent ThreadPoolExecutor, time cost reduced to 173.47849702835083 for validation audios
0.7014417536389933
Model improve: 0.6925 -> 0.7014
Epoch: 11/200
With concurrent ThreadPoolExecutor, time cost reduced to 173.08480787277222 for validation audios
0.7107792029884372
Model improve: 0.7014 -> 0.7108
Epoch: 12/200
With concurrent ThreadPoolExecutor, time cost reduced to 174.34955978393555 for validation audios
0.7178220031229814
Model improve: 0.7108 -> 0.7178
Epoch: 13/200
With concurrent ThreadPoolExecutor, time cost reduced to 172.89217972755432 for validation audios
0.7219303998572864
Model improve: 0.7178 -> 0.7219
Epoch: 14/200
With concurrent ThreadPoolExecutor, time cost reduced to 172.62007761001587 for validation audios
0.7329589133565957
Model improve: 0.7219 -> 0.7330
Epoch: 15/200
With concurrent ThreadPoolExecutor, time cost reduced to 176.35171961784363 for validation audios
0.7345657656513489
Model improve: 0.7330 -> 0.7346
Epoch: 16/200
With concurrent ThreadPoolExecutor, time cost reduced to 172.35186052322388 for validation audios
0.7377623536102071
Model improve: 0.7346 -> 0.7378
Epoch: 17/200
Date :04/24/2023, 15:35:08
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13830
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Date :04/24/2023, 15:35:25
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13830
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
With concurrent ThreadPoolExecutor, time cost reduced to 167.83109331130981 for validation audios
0.44389641547011927
Model improve: 0.0000 -> 0.4439
Epoch: 2/200
With concurrent ThreadPoolExecutor, time cost reduced to 167.99926710128784 for validation audios
0.5632947500660479
Model improve: 0.4439 -> 0.5633
Epoch: 3/200
With concurrent ThreadPoolExecutor, time cost reduced to 167.71260690689087 for validation audios
0.6186232564105327
Model improve: 0.5633 -> 0.6186
Epoch: 4/200
With concurrent ThreadPoolExecutor, time cost reduced to 165.76846528053284 for validation audios
0.6508952717526125
Model improve: 0.6186 -> 0.6509
Epoch: 5/200
With concurrent ThreadPoolExecutor, time cost reduced to 167.10571098327637 for validation audios
0.6705546577577841
Model improve: 0.6509 -> 0.6706
Epoch: 6/200
With concurrent ThreadPoolExecutor, time cost reduced to 169.62504243850708 for validation audios
0.6839498238432248
Model improve: 0.6706 -> 0.6839
Epoch: 7/200
With concurrent ThreadPoolExecutor, time cost reduced to 168.10106563568115 for validation audios
0.6947335602992822
Model improve: 0.6839 -> 0.6947
Epoch: 8/200
With concurrent ThreadPoolExecutor, time cost reduced to 166.4443998336792 for validation audios
0.7061959733992469
Model improve: 0.6947 -> 0.7062
Epoch: 9/200
With concurrent ThreadPoolExecutor, time cost reduced to 166.37585377693176 for validation audios
0.7119851730244516
Model improve: 0.7062 -> 0.7120
Epoch: 10/200
With concurrent ThreadPoolExecutor, time cost reduced to 167.11000871658325 for validation audios
0.7191100371241808
Model improve: 0.7120 -> 0.7191
Epoch: 11/200
With concurrent ThreadPoolExecutor, time cost reduced to 169.0673632621765 for validation audios
0.7259450490818999
Model improve: 0.7191 -> 0.7259
Epoch: 12/200
With concurrent ThreadPoolExecutor, time cost reduced to 167.31995606422424 for validation audios
0.7365655761785366
Model improve: 0.7259 -> 0.7366
Epoch: 13/200
With concurrent ThreadPoolExecutor, time cost reduced to 166.72430086135864 for validation audios
0.7370475099206095
Model improve: 0.7366 -> 0.7370
Epoch: 14/200
With concurrent ThreadPoolExecutor, time cost reduced to 168.3969268798828 for validation audios
0.7352326748459184
Epoch: 15/200
With concurrent ThreadPoolExecutor, time cost reduced to 168.32274317741394 for validation audios
0.7431485755510915
Model improve: 0.7370 -> 0.7431
Epoch: 16/200
With concurrent ThreadPoolExecutor, time cost reduced to 166.81376934051514 for validation audios
0.7448712019273245
Model improve: 0.7431 -> 0.7449
Epoch: 17/200
With concurrent ThreadPoolExecutor, time cost reduced to 167.0948359966278 for validation audios
0.7457071521364226
Model improve: 0.7449 -> 0.7457
Epoch: 18/200
With concurrent ThreadPoolExecutor, time cost reduced to 166.34195256233215 for validation audios
0.7459162847735994
Model improve: 0.7457 -> 0.7459
Epoch: 19/200
With concurrent ThreadPoolExecutor, time cost reduced to 166.22336173057556 for validation audios
0.748391255659784
Model improve: 0.7459 -> 0.7484
Epoch: 20/200
With concurrent ThreadPoolExecutor, time cost reduced to 164.7299280166626 for validation audios
0.747868179125455
Epoch: 21/200
With concurrent ThreadPoolExecutor, time cost reduced to 168.64405226707458 for validation audios
0.755036175837436
Model improve: 0.7484 -> 0.7550
Epoch: 22/200
With concurrent ThreadPoolExecutor, time cost reduced to 168.71452641487122 for validation audios
0.7562567463449359
Model improve: 0.7550 -> 0.7563
Epoch: 23/200
With concurrent ThreadPoolExecutor, time cost reduced to 167.7845766544342 for validation audios
0.75307037230808
Epoch: 24/200
With concurrent ThreadPoolExecutor, time cost reduced to 168.95890378952026 for validation audios
0.7532600544661127
Epoch: 25/200
With concurrent ThreadPoolExecutor, time cost reduced to 167.79777884483337 for validation audios
0.757986161154597
Model improve: 0.7563 -> 0.7580
Epoch: 26/200
With concurrent ThreadPoolExecutor, time cost reduced to 166.37149477005005 for validation audios
0.7573113853147766
Epoch: 27/200
With concurrent ThreadPoolExecutor, time cost reduced to 165.81721186637878 for validation audios
0.7590552849655354
Model improve: 0.7580 -> 0.7591
Epoch: 28/200
With concurrent ThreadPoolExecutor, time cost reduced to 167.09379768371582 for validation audios
0.7584232803818286
Epoch: 29/200
With concurrent ThreadPoolExecutor, time cost reduced to 167.45646381378174 for validation audios
0.7599921634971413
Model improve: 0.7591 -> 0.7600
Epoch: 30/200
With concurrent ThreadPoolExecutor, time cost reduced to 167.41976976394653 for validation audios
0.7588419058455083
Epoch: 31/200
With concurrent ThreadPoolExecutor, time cost reduced to 166.90504384040833 for validation audios
0.7585437093053173
Epoch: 32/200
With concurrent ThreadPoolExecutor, time cost reduced to 167.2100112438202 for validation audios
0.7608633232611677
Model improve: 0.7600 -> 0.7609
Epoch: 33/200
With concurrent ThreadPoolExecutor, time cost reduced to 167.31758761405945 for validation audios
0.7632148723673724
Model improve: 0.7609 -> 0.7632
Epoch: 34/200
With concurrent ThreadPoolExecutor, time cost reduced to 168.43974232673645 for validation audios
0.7642978067924697
Model improve: 0.7632 -> 0.7643
Epoch: 35/200
With concurrent ThreadPoolExecutor, time cost reduced to 168.8910071849823 for validation audios
0.7631299138270126
Epoch: 36/200
With concurrent ThreadPoolExecutor, time cost reduced to 166.7192952632904 for validation audios
0.7672601962994429
Model improve: 0.7643 -> 0.7673
Epoch: 37/200
With concurrent ThreadPoolExecutor, time cost reduced to 167.43335008621216 for validation audios
0.7622790527109408
Epoch: 38/200
With concurrent ThreadPoolExecutor, time cost reduced to 167.24438095092773 for validation audios
0.7689312639376242
Model improve: 0.7673 -> 0.7689
Epoch: 39/200
With concurrent ThreadPoolExecutor, time cost reduced to 168.5819444656372 for validation audios
0.7626050611847556
Epoch: 40/200
With concurrent ThreadPoolExecutor, time cost reduced to 168.16731929779053 for validation audios
0.7652249942723346
Epoch: 41/200
With concurrent ThreadPoolExecutor, time cost reduced to 168.17419290542603 for validation audios
0.7644647652080696
Epoch: 42/200
With concurrent ThreadPoolExecutor, time cost reduced to 168.85240817070007 for validation audios
0.7651200424900294
Epoch: 43/200
With concurrent ThreadPoolExecutor, time cost reduced to 166.95416903495789 for validation audios
0.7668719338868996
Epoch: 44/200
With concurrent ThreadPoolExecutor, time cost reduced to 166.69335198402405 for validation audios
0.7701265485663671
Model improve: 0.7689 -> 0.7701
Epoch: 45/200
With concurrent ThreadPoolExecutor, time cost reduced to 167.96144843101501 for validation audios
0.7637760600887691
Epoch: 46/200
With concurrent ThreadPoolExecutor, time cost reduced to 167.88393783569336 for validation audios
0.7619303596791211
Epoch: 47/200
With concurrent ThreadPoolExecutor, time cost reduced to 165.83551931381226 for validation audios
0.7632559866553444
Epoch: 48/200
With concurrent ThreadPoolExecutor, time cost reduced to 164.59993052482605 for validation audios
0.7662973106474739
Epoch: 49/200
With concurrent ThreadPoolExecutor, time cost reduced to 167.5756118297577 for validation audios
0.7636893654147962
Epoch: 50/200
With concurrent ThreadPoolExecutor, time cost reduced to 167.4192717075348 for validation audios
0.7652464120998772
Epoch: 51/200
With concurrent ThreadPoolExecutor, time cost reduced to 168.32717180252075 for validation audios
0.7664499162905448
Epoch: 52/200
With concurrent ThreadPoolExecutor, time cost reduced to 167.9531831741333 for validation audios
0.76885090310349
Epoch: 53/200
With concurrent ThreadPoolExecutor, time cost reduced to 167.58006620407104 for validation audios
0.7694914212199102
Epoch: 54/200
With concurrent ThreadPoolExecutor, time cost reduced to 170.76768708229065 for validation audios
0.763012869241189
Epoch: 55/200
With concurrent ThreadPoolExecutor, time cost reduced to 169.59524607658386 for validation audios
0.7673915411557285
Epoch: 56/200
With concurrent ThreadPoolExecutor, time cost reduced to 169.270103931427 for validation audios
0.7655281984795888
Epoch: 57/200
With concurrent ThreadPoolExecutor, time cost reduced to 169.47960495948792 for validation audios
0.7678790780142823
Epoch: 58/200
With concurrent ThreadPoolExecutor, time cost reduced to 169.26483631134033 for validation audios
0.7648082189914668
Epoch: 59/200
With concurrent ThreadPoolExecutor, time cost reduced to 168.95941042900085 for validation audios
0.7639381599466882
Epoch: 60/200
With concurrent ThreadPoolExecutor, time cost reduced to 168.1941363811493 for validation audios
0.7657600095424169
Epoch: 61/200
With concurrent ThreadPoolExecutor, time cost reduced to 167.18428897857666 for validation audios
0.7643282877803778
Epoch: 62/200
With concurrent ThreadPoolExecutor, time cost reduced to 167.45233273506165 for validation audios
0.767803750502385
Epoch: 63/200
With concurrent ThreadPoolExecutor, time cost reduced to 168.47828459739685 for validation audios
0.7668210913427986
Epoch: 64/200
With concurrent ThreadPoolExecutor, time cost reduced to 171.1508228778839 for validation audios
0.7650659650949142
Epoch: 65/200
With concurrent ThreadPoolExecutor, time cost reduced to 168.97501015663147 for validation audios
0.7685963424050196
Epoch: 66/200
With concurrent ThreadPoolExecutor, time cost reduced to 169.4644615650177 for validation audios
0.767624690103268
Epoch: 67/200
With concurrent ThreadPoolExecutor, time cost reduced to 168.94871640205383 for validation audios
0.7696192464359121
Epoch: 68/200
With concurrent ThreadPoolExecutor, time cost reduced to 169.03020930290222 for validation audios
0.7677887485835161
Epoch: 69/200
With concurrent ThreadPoolExecutor, time cost reduced to 167.5331506729126 for validation audios
0.7640426191669268
Epoch: 70/200
With concurrent ThreadPoolExecutor, time cost reduced to 168.08715152740479 for validation audios
0.7659069137952912
Epoch: 71/200
With concurrent ThreadPoolExecutor, time cost reduced to 167.6939718723297 for validation audios
0.7687445132562787
Epoch: 72/200
With concurrent ThreadPoolExecutor, time cost reduced to 172.5269832611084 for validation audios
0.7670648547648105
Epoch: 73/200
With concurrent ThreadPoolExecutor, time cost reduced to 195.85589504241943 for validation audios
0.7636165481983348
Epoch: 74/200
With concurrent ThreadPoolExecutor, time cost reduced to 212.53833651542664 for validation audios
0.7673912115706885
Epoch: 75/200
With concurrent ThreadPoolExecutor, time cost reduced to 168.1494026184082 for validation audios
0.7661218065419382
Epoch: 76/200
With concurrent ThreadPoolExecutor, time cost reduced to 167.1426820755005 for validation audios
0.7654077215261531
Epoch: 77/200
With concurrent ThreadPoolExecutor, time cost reduced to 168.66447043418884 for validation audios
0.7657189865443579
Epoch: 78/200
With concurrent ThreadPoolExecutor, time cost reduced to 168.69924068450928 for validation audios
0.765688799079283
Epoch: 79/200
With concurrent ThreadPoolExecutor, time cost reduced to 167.9485318660736 for validation audios
0.7655010570693446
Epoch: 80/200
With concurrent ThreadPoolExecutor, time cost reduced to 167.64050674438477 for validation audios
0.7706502363138052
Model improve: 0.7701 -> 0.7707
Epoch: 81/200
With concurrent ThreadPoolExecutor, time cost reduced to 166.84781455993652 for validation audios
0.7683434321822894
Epoch: 82/200
With concurrent ThreadPoolExecutor, time cost reduced to 168.75381779670715 for validation audios
0.769005016025412
Epoch: 83/200
With concurrent ThreadPoolExecutor, time cost reduced to 167.14833402633667 for validation audios
0.7662080379284073
Epoch: 84/200
With concurrent ThreadPoolExecutor, time cost reduced to 169.27338361740112 for validation audios
0.7677451112045025
Epoch: 85/200
With concurrent ThreadPoolExecutor, time cost reduced to 170.82166504859924 for validation audios
0.7685185403960619
Epoch: 86/200
With concurrent ThreadPoolExecutor, time cost reduced to 166.89922738075256 for validation audios
0.7672438434632385
Epoch: 87/200
With concurrent ThreadPoolExecutor, time cost reduced to 168.80147528648376 for validation audios
0.7696602326511287
Epoch: 88/200
With concurrent ThreadPoolExecutor, time cost reduced to 169.0909824371338 for validation audios
0.7701767030735303
Epoch: 89/200
With concurrent ThreadPoolExecutor, time cost reduced to 168.83815240859985 for validation audios
0.770791355165979
Model improve: 0.7707 -> 0.7708
Epoch: 90/200
With concurrent ThreadPoolExecutor, time cost reduced to 166.967426776886 for validation audios
0.770011441665283
Epoch: 91/200
With concurrent ThreadPoolExecutor, time cost reduced to 168.56068062782288 for validation audios
0.773212360165614
Model improve: 0.7708 -> 0.7732
Epoch: 92/200
With concurrent ThreadPoolExecutor, time cost reduced to 168.0948932170868 for validation audios
0.7692582076060841
Epoch: 93/200
With concurrent ThreadPoolExecutor, time cost reduced to 168.8838107585907 for validation audios
0.767465554557548
Epoch: 94/200
With concurrent ThreadPoolExecutor, time cost reduced to 170.27552270889282 for validation audios
0.7665408566572419
Epoch: 95/200
With concurrent ThreadPoolExecutor, time cost reduced to 168.1696789264679 for validation audios
0.7728666343456158
Epoch: 96/200
With concurrent ThreadPoolExecutor, time cost reduced to 168.94902634620667 for validation audios
0.7691919183474663
Epoch: 97/200
With concurrent ThreadPoolExecutor, time cost reduced to 167.36902117729187 for validation audios
0.7724969441743328
Epoch: 98/200
With concurrent ThreadPoolExecutor, time cost reduced to 167.1022117137909 for validation audios
0.7692683545247031
Epoch: 99/200
With concurrent ThreadPoolExecutor, time cost reduced to 167.70748257637024 for validation audios
0.7680997376057674
Epoch: 100/200
With concurrent ThreadPoolExecutor, time cost reduced to 168.70638966560364 for validation audios
0.7702816459204439
Epoch: 101/200
With concurrent ThreadPoolExecutor, time cost reduced to 166.87296676635742 for validation audios
0.7711775333009405
Epoch: 102/200
With concurrent ThreadPoolExecutor, time cost reduced to 167.84700751304626 for validation audios
0.7698274688887573
Epoch: 103/200
With concurrent ThreadPoolExecutor, time cost reduced to 167.34071707725525 for validation audios
0.7678071194121033
Epoch: 104/200
With concurrent ThreadPoolExecutor, time cost reduced to 167.49339818954468 for validation audios
0.7690086091854536
Epoch: 105/200
With concurrent ThreadPoolExecutor, time cost reduced to 167.31297254562378 for validation audios
0.7694504789231523
Epoch: 106/200
With concurrent ThreadPoolExecutor, time cost reduced to 168.20736598968506 for validation audios
0.7677947014559853
Epoch: 107/200
Date :04/25/2023, 03:33:28
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13830
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Date :04/25/2023, 03:33:46
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13830
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
With concurrent ThreadPoolExecutor, time cost reduced to 160.12178897857666 for validation audios
0.7818932952422228
Date :04/25/2023, 03:37:52
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
With concurrent ThreadPoolExecutor, time cost reduced to 156.08869743347168 for validation audios
0.8001495184615237
Date :04/25/2023, 03:46:04
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 2
13807
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
With concurrent ThreadPoolExecutor, time cost reduced to 163.88827681541443 for validation audios
0.9950529713836472
Date :04/25/2023, 03:55:36
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 4
13830
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
With concurrent ThreadPoolExecutor, time cost reduced to 158.74534702301025 for validation audios
0.7967205055428028
Date :04/25/2023, 04:02:59
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.6
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :04/25/2023, 04:18:37
Duration: 5
Sample rate: 32000
nfft: 1024
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.6
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :04/25/2023, 04:19:16
Duration: 5
Sample rate: 32000
nfft: 1024
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 1024
epochwarmup: 0
totalepoch: 60
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.6
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Epoch: 2/60
Epoch: 3/60
Epoch: 4/60
Epoch: 5/60
Epoch: 6/60
Epoch: 7/60
Date :04/25/2023, 04:43:20
Duration: 5
Sample rate: 32000
nfft: 1024
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.6
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Epoch: 8/60
Epoch: 2/60
Epoch: 9/60
Epoch: 3/60
Epoch: 10/60
Epoch: 4/60
Epoch: 11/60
Epoch: 5/60
Epoch: 12/60
Epoch: 13/60
Epoch: 6/60
Epoch: 14/60
Epoch: 7/60
Epoch: 15/60
Epoch: 8/60
Epoch: 16/60
Epoch: 17/60
Epoch: 9/60
Epoch: 18/60
Epoch: 10/60
Epoch: 19/60
Epoch: 11/60
Epoch: 20/60
Epoch: 12/60
Epoch: 21/60
Epoch: 22/60
Epoch: 13/60
Epoch: 23/60
Epoch: 14/60
Epoch: 24/60
Epoch: 15/60
Epoch: 25/60
Epoch: 16/60
Epoch: 26/60
Epoch: 27/60
Epoch: 17/60
Epoch: 28/60
Epoch: 18/60
Epoch: 29/60
Date :04/25/2023, 06:48:11
Duration: 5
Sample rate: 32000
nfft: 1024
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.6
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Epoch: 2/60
Epoch: 3/60
Epoch: 4/60
Epoch: 5/60
Epoch: 6/60
Epoch: 7/60
Epoch: 8/60
Epoch: 9/60
Epoch: 10/60
Epoch: 11/60
Epoch: 12/60
Epoch: 13/60
Epoch: 14/60
Epoch: 15/60
Epoch: 16/60
Epoch: 17/60
Epoch: 18/60
Epoch: 19/60
Epoch: 20/60
Epoch: 21/60
Epoch: 22/60
Epoch: 23/60
Epoch: 24/60
Epoch: 25/60
Epoch: 26/60
Epoch: 27/60
Epoch: 28/60
Epoch: 29/60
Epoch: 30/60
Epoch: 31/60
Epoch: 32/60
Time needed: 164.48584151268005 for validation audios
0.787046169775013
Model improve: 0.0000 -> 0.7870
Epoch: 33/60
Time needed: 155.9181227684021 for validation audios
0.7896856148388776
Model improve: 0.7870 -> 0.7897
Epoch: 34/60
Time needed: 155.66691875457764 for validation audios
0.789707544636396
Model improve: 0.7897 -> 0.7897
Epoch: 35/60
Time needed: 156.55389714241028 for validation audios
0.7904054981195522
Model improve: 0.7897 -> 0.7904
Epoch: 36/60
Time needed: 154.53779339790344 for validation audios
0.7905714491623694
Model improve: 0.7904 -> 0.7906
Epoch: 37/60
Time needed: 156.05216097831726 for validation audios
0.7921270068168721
Model improve: 0.7906 -> 0.7921
Epoch: 38/60
Time needed: 153.89298176765442 for validation audios
0.7899528470971019
Epoch: 39/60
Time needed: 155.33774733543396 for validation audios
0.7922762649190337
Model improve: 0.7921 -> 0.7923
Epoch: 40/60
Time needed: 154.07508206367493 for validation audios
0.7919770273068475
Epoch: 41/60
Time needed: 155.84208416938782 for validation audios
0.7932964902927911
Model improve: 0.7923 -> 0.7933
Epoch: 42/60
Time needed: 154.90979838371277 for validation audios
0.7943416342270964
Model improve: 0.7933 -> 0.7943
Epoch: 43/60
Time needed: 154.18307876586914 for validation audios
0.7943352617354641
Epoch: 44/60
Time needed: 153.4044234752655 for validation audios
0.7936263454463531
Epoch: 45/60
Time needed: 153.62768745422363 for validation audios
0.7913260659806394
Epoch: 46/60
Time needed: 154.9124071598053 for validation audios
0.7931794453034406
Epoch: 47/60
Date :04/25/2023, 08:51:49
Duration: 5
Sample rate: 32000
nfft: 512
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.9
hop_length: 320
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Epoch: 2/60
Epoch: 3/60
Date :04/25/2023, 08:56:56
Duration: 5
Sample rate: 32000
nfft: 512
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.9
hop_length: 320
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :04/25/2023, 08:57:22
Duration: 5
Sample rate: 32000
nfft: 512
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.9
hop_length: 320
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :04/25/2023, 09:08:40
Duration: 5
Sample rate: 32000
nfft: 512
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 120
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.9
hop_length: 320
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/120
Train loss: 14.2344
Epoch: 2/120
Train loss: 4.6789
Epoch: 3/120
Train loss: 4.2617
Epoch: 4/120
Train loss: 4.0120
Epoch: 5/120
Train loss: 3.8405
Epoch: 6/120
Train loss: 3.5683
Epoch: 7/120
Train loss: 3.5318
Epoch: 8/120
Train loss: 3.4569
Epoch: 9/120
Train loss: 3.3242
Epoch: 10/120
Train loss: 3.2614
Epoch: 11/120
Train loss: 3.2431
Epoch: 12/120
Train loss: 3.1705
Epoch: 13/120
Train loss: 3.1075
Epoch: 14/120
Train loss: 3.1238
Epoch: 15/120
Train loss: 3.0745
Epoch: 16/120
Train loss: 3.0801
Epoch: 17/120
Train loss: 3.0895
Epoch: 18/120
Train loss: 3.0027
Epoch: 19/120
Train loss: 2.9714
Epoch: 20/120
Train loss: 2.9192
Epoch: 21/120
Train loss: 2.8600
Epoch: 22/120
Train loss: 2.8881
Epoch: 23/120
Train loss: 2.8597
Epoch: 24/120
Train loss: 2.8639
Epoch: 25/120
Train loss: 2.7850
Epoch: 26/120
Train loss: 2.8477
Epoch: 27/120
Train loss: 2.8175
Epoch: 28/120
Train loss: 2.8227
Epoch: 29/120
Train loss: 2.7318
Epoch: 30/120
Train loss: 2.7975
Epoch: 31/120
Train loss: 2.8053
Epoch: 32/120
Train loss: 2.7219
Epoch: 33/120
Train loss: 2.6840
Epoch: 34/120
Train loss: 2.6447
Epoch: 35/120
Train loss: 2.6754
Epoch: 36/120
Train loss: 2.6958
Epoch: 37/120
Train loss: 2.6471
Epoch: 38/120
Train loss: 2.6628
Epoch: 39/120
Train loss: 2.6456
Epoch: 40/120
Train loss: 2.6724
Epoch: 41/120
Train loss: 2.6654
Epoch: 42/120
Train loss: 2.5814
Epoch: 43/120
Train loss: 2.6566
Epoch: 44/120
Train loss: 2.6506
Epoch: 45/120
Train loss: 2.7022
Epoch: 46/120
Train loss: 2.5636
Epoch: 47/120
Train loss: 2.6218
Epoch: 48/120
Train loss: 2.5804
Epoch: 49/120
Train loss: 2.5758
Epoch: 50/120
Train loss: 2.6080
Epoch: 51/120
Train loss: 2.5260
Epoch: 52/120
Train loss: 2.5838
Epoch: 53/120
Train loss: 2.5785
Epoch: 54/120
Train loss: 2.5505
Epoch: 55/120
Train loss: 2.5013
Epoch: 56/120
Train loss: 2.4526
Epoch: 57/120
Train loss: 2.4596
Epoch: 58/120
Train loss: 2.5251
Epoch: 59/120
Train loss: 2.5326
Epoch: 60/120
Train loss: 2.5211
Epoch: 61/120
Train loss: 2.4203
Epoch: 62/120
Train loss: 2.4283
Epoch: 63/120
Train loss: 2.4090
Epoch: 64/120
Train loss: 2.5366
Epoch: 65/120
Train loss: 2.3995
Epoch: 66/120
Train loss: 2.4091
Epoch: 67/120
Train loss: 2.4747
Epoch: 68/120
Train loss: 2.4123
Epoch: 69/120
Train loss: 2.4212
Epoch: 70/120
Train loss: 2.4591
Epoch: 71/120
Train loss: 2.3669
Epoch: 72/120
Train loss: 2.3773
Epoch: 73/120
Train loss: 2.4260
Epoch: 74/120
Train loss: 2.4305
Epoch: 75/120
Train loss: 2.3448
Epoch: 76/120
Train loss: 2.3750
Epoch: 77/120
Train loss: 2.4093
Epoch: 78/120
Train loss: 2.4111
Epoch: 79/120
Train loss: 2.4140
Epoch: 80/120
Train loss: 2.3814
Epoch: 81/120
Train loss: 2.3431
Epoch: 82/120
Train loss: 2.3482
Epoch: 83/120
Train loss: 2.4039
Epoch: 84/120
Train loss: 2.3750
Epoch: 85/120
Train loss: 2.3750
Epoch: 86/120
Train loss: 2.3903
Epoch: 87/120
Train loss: 2.4220
Epoch: 88/120
Train loss: 2.3790
Epoch: 89/120
Train loss: 2.3604
Epoch: 90/120
Train loss: 2.3459
Epoch: 91/120
Train loss: 2.3573
Epoch: 92/120
Train loss: 2.4193
Epoch: 93/120
Train loss: 2.3535
Epoch: 94/120
Train loss: 2.3594
Epoch: 95/120
Train loss: 2.2677
Epoch: 96/120
Train loss: 2.2902
Epoch: 97/120
Train loss: 2.3338
Epoch: 98/120
Train loss: 2.3808
Epoch: 99/120
Train loss: 2.3331
Epoch: 100/120
Train loss: 2.3365
Epoch: 101/120
Train loss: 2.3943
Epoch: 102/120
Train loss: 2.3500
Time needed: 143.00466752052307 for validation audios
0.7887557099385728
Model improve: 0.0000 -> 0.7888
Epoch: 103/120
Train loss: 2.3927
Time needed: 144.23049569129944 for validation audios
0.7868889138887759
Epoch: 104/120
Date :04/25/2023, 11:44:56
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 25
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.9
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/25
Train loss: 13.6857
Epoch: 2/25
Train loss: 4.5992
Epoch: 3/25
Train loss: 4.1706
Epoch: 4/25
Train loss: 3.9023
Epoch: 5/25
Train loss: 3.7349
Epoch: 6/25
Train loss: 3.5745
Epoch: 7/25
Train loss: 3.4251
Epoch: 8/25
Train loss: 3.3880
Epoch: 9/25
Train loss: 3.3325
Epoch: 10/25
Train loss: 3.2378
Epoch: 11/25
Train loss: 3.2107
Epoch: 12/25
Train loss: 3.0376
Epoch: 13/25
Train loss: 3.0639
Epoch: 14/25
Train loss: 3.0874
Epoch: 15/25
Train loss: 3.0327
Epoch: 16/25
Train loss: 3.0739
Epoch: 17/25
Train loss: 2.9711
Epoch: 18/25
Train loss: 2.9747
Epoch: 19/25
Train loss: 2.9268
Epoch: 20/25
Train loss: 2.8758
Epoch: 21/25
Train loss: 2.8908
Epoch: 22/25
Train loss: 2.9608
Time needed: 150.10674858093262 for validation audios
0.765769455787277
Model improve: 0.0000 -> 0.7658
Epoch: 23/25
Train loss: 2.9209
Date :04/25/2023, 12:27:48
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 25
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.9
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/25
Train loss: 9.1782
Epoch: 2/25
Train loss: 5.2098
Epoch: 3/25
Train loss: 4.7495
Epoch: 4/25
Date :04/25/2023, 12:33:03
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.9
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 9.1724
Epoch: 2/200
Train loss: 5.2063
Epoch: 3/200
Train loss: 4.7457
Epoch: 4/200
Date :04/25/2023, 12:38:37
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.9
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 13.6833
Epoch: 2/200
Train loss: 4.5974
Epoch: 3/200
Train loss: 4.1673
Epoch: 4/200
Train loss: 3.8951
Epoch: 5/200
Train loss: 3.7269
Epoch: 6/200
Train loss: 3.5651
Epoch: 7/200
Train loss: 3.4165
Epoch: 8/200
Train loss: 3.3789
Epoch: 9/200
Train loss: 3.3222
Epoch: 10/200
Train loss: 3.2276
Epoch: 11/200
Train loss: 3.1987
Epoch: 12/200
Train loss: 3.0251
Epoch: 13/200
Train loss: 3.0500
Epoch: 14/200
Train loss: 3.0723
Epoch: 15/200
Train loss: 3.0142
Epoch: 16/200
Train loss: 3.0480
Epoch: 17/200
Train loss: 2.9408
Epoch: 18/200
Train loss: 2.9310
Epoch: 19/200
Train loss: 2.8765
Epoch: 20/200
Train loss: 2.8175
Epoch: 21/200
Train loss: 2.8189
Epoch: 22/200
Train loss: 2.8792
Epoch: 23/200
Train loss: 2.8300
Epoch: 24/200
Train loss: 2.7138
Epoch: 25/200
Train loss: 2.7473
Epoch: 26/200
Train loss: 2.7351
Epoch: 27/200
Train loss: 2.7510
Epoch: 28/200
Train loss: 2.6863
Epoch: 29/200
Train loss: 2.7217
Epoch: 30/200
Train loss: 2.7070
Epoch: 31/200
Train loss: 2.6703
Epoch: 32/200
Train loss: 2.6410
Epoch: 33/200
Train loss: 2.6091
Epoch: 34/200
Train loss: 2.6448
Epoch: 35/200
Train loss: 2.6669
Epoch: 36/200
Train loss: 2.5224
Epoch: 37/200
Train loss: 2.5950
Epoch: 38/200
Train loss: 2.6070
Epoch: 39/200
Train loss: 2.6283
Epoch: 40/200
Train loss: 2.6123
Epoch: 41/200
Train loss: 2.5808
Epoch: 42/200
Train loss: 2.5179
Epoch: 43/200
Train loss: 2.5986
Epoch: 44/200
Train loss: 2.5660
Epoch: 45/200
Train loss: 2.5620
Epoch: 46/200
Train loss: 2.5603
Epoch: 47/200
Train loss: 2.5379
Epoch: 48/200
Train loss: 2.6004
Epoch: 49/200
Train loss: 2.4544
Epoch: 50/200
Train loss: 2.5358
Epoch: 51/200
Train loss: 2.5224
Epoch: 52/200
Train loss: 2.5431
Epoch: 53/200
Train loss: 2.5211
Epoch: 54/200
Train loss: 2.4779
Epoch: 55/200
Train loss: 2.4641
Epoch: 56/200
Train loss: 2.4504
Epoch: 57/200
Train loss: 2.4964
Epoch: 58/200
Train loss: 2.5042
Epoch: 59/200
Train loss: 2.4328
Epoch: 60/200
Train loss: 2.4393
Epoch: 61/200
Train loss: 2.4474
Epoch: 62/200
Train loss: 2.4685
Epoch: 63/200
Train loss: 2.3856
Epoch: 64/200
Train loss: 2.4039
Epoch: 65/200
Train loss: 2.4261
Epoch: 66/200
Train loss: 2.4506
Epoch: 67/200
Train loss: 2.4537
Epoch: 68/200
Train loss: 2.4482
Epoch: 69/200
Train loss: 2.3682
Epoch: 70/200
Train loss: 2.3439
Epoch: 71/200
Train loss: 2.3395
Epoch: 72/200
Train loss: 2.2736
Epoch: 73/200
Train loss: 2.3697
Epoch: 74/200
Train loss: 2.3058
Epoch: 75/200
Train loss: 2.3606
Epoch: 76/200
Train loss: 2.3963
Epoch: 77/200
Train loss: 2.3587
Epoch: 78/200
Train loss: 2.3657
Epoch: 79/200
Train loss: 2.3821
Epoch: 80/200
Train loss: 2.3445
Epoch: 81/200
Train loss: 2.3301
Epoch: 82/200
Train loss: 2.3483
Epoch: 83/200
Train loss: 2.3163
Epoch: 84/200
Train loss: 2.2870
Epoch: 85/200
Train loss: 2.3159
Epoch: 86/200
Train loss: 2.2983
Epoch: 87/200
Train loss: 2.3680
Epoch: 88/200
Train loss: 2.2677
Epoch: 89/200
Train loss: 2.4102
Epoch: 90/200
Train loss: 2.2798
Epoch: 91/200
Train loss: 2.3008
Epoch: 92/200
Train loss: 2.2719
Epoch: 93/200
Train loss: 2.2696
Epoch: 94/200
Train loss: 2.3064
Epoch: 95/200
Date :04/25/2023, 15:21:12
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 350
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.9
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/350
Train loss: 13.6855
Epoch: 2/350
Train loss: 4.5971
Epoch: 3/350
Train loss: 4.1669
Epoch: 4/350
Train loss: 3.8953
Epoch: 5/350
Train loss: 3.7278
Epoch: 6/350
Train loss: 3.5644
Epoch: 7/350
Train loss: 3.4162
Epoch: 8/350
Train loss: 3.3782
Epoch: 9/350
Train loss: 3.3220
Epoch: 10/350
Train loss: 3.2284
Epoch: 11/350
Train loss: 3.1985
Epoch: 12/350
Train loss: 3.0243
Epoch: 13/350
Train loss: 3.0498
Epoch: 14/350
Train loss: 3.0725
Epoch: 15/350
Train loss: 3.0142
Epoch: 16/350
Train loss: 3.0473
Epoch: 17/350
Train loss: 2.9408
Epoch: 18/350
Train loss: 2.9312
Epoch: 19/350
Train loss: 2.8772
Epoch: 20/350
Train loss: 2.8175
Epoch: 21/350
Train loss: 2.8185
Epoch: 22/350
Train loss: 2.8797
Epoch: 23/350
Train loss: 2.8301
Epoch: 24/350
Train loss: 2.7138
Epoch: 25/350
Train loss: 2.7481
Epoch: 26/350
Train loss: 2.7368
Epoch: 27/350
Train loss: 2.7515
Epoch: 28/350
Train loss: 2.6867
Epoch: 29/350
Train loss: 2.7223
Epoch: 30/350
Train loss: 2.7085
Epoch: 31/350
Train loss: 2.6718
Epoch: 32/350
Train loss: 2.6424
Epoch: 33/350
Train loss: 2.6107
Epoch: 34/350
Train loss: 2.6466
Epoch: 35/350
Train loss: 2.6695
Epoch: 36/350
Train loss: 2.5235
Epoch: 37/350
Train loss: 2.5967
Epoch: 38/350
Train loss: 2.6098
Epoch: 39/350
Train loss: 2.6305
Epoch: 40/350
Train loss: 2.6153
Epoch: 41/350
Train loss: 2.5839
Epoch: 42/350
Train loss: 2.5213
Epoch: 43/350
Train loss: 2.6023
Epoch: 44/350
Train loss: 2.5695
Epoch: 45/350
Train loss: 2.5657
Epoch: 46/350
Train loss: 2.5662
Epoch: 47/350
Train loss: 2.5424
Epoch: 48/350
Train loss: 2.6044
Epoch: 49/350
Train loss: 2.4602
Epoch: 50/350
Train loss: 2.5414
Epoch: 51/350
Train loss: 2.5289
Epoch: 52/350
Train loss: 2.5506
Epoch: 53/350
Train loss: 2.5292
Epoch: 54/350
Train loss: 2.4856
Epoch: 55/350
Train loss: 2.4712
Epoch: 56/350
Train loss: 2.4590
Epoch: 57/350
Train loss: 2.5055
Epoch: 58/350
Train loss: 2.5135
Epoch: 59/350
Train loss: 2.4410
Epoch: 60/350
Train loss: 2.4480
Epoch: 61/350
Train loss: 2.4567
Epoch: 62/350
Train loss: 2.4788
Epoch: 63/350
Train loss: 2.3952
Epoch: 64/350
Train loss: 2.4153
Epoch: 65/350
Train loss: 2.4359
Epoch: 66/350
Train loss: 2.4609
Epoch: 67/350
Train loss: 2.4667
Epoch: 68/350
Train loss: 2.4595
Epoch: 69/350
Train loss: 2.3784
Epoch: 70/350
Train loss: 2.3560
Epoch: 71/350
Train loss: 2.3527
Epoch: 72/350
Train loss: 2.2859
Epoch: 73/350
Train loss: 2.3842
Epoch: 74/350
Train loss: 2.3200
Epoch: 75/350
Train loss: 2.3746
Epoch: 76/350
Train loss: 2.4105
Epoch: 77/350
Train loss: 2.3734
Epoch: 78/350
Train loss: 2.3819
Epoch: 79/350
Train loss: 2.3985
Epoch: 80/350
Train loss: 2.3624
Epoch: 81/350
Train loss: 2.3456
Epoch: 82/350
Train loss: 2.3661
Epoch: 83/350
Train loss: 2.3325
Epoch: 84/350
Train loss: 2.3027
Epoch: 85/350
Train loss: 2.3348
Epoch: 86/350
Train loss: 2.3155
Epoch: 87/350
Train loss: 2.3867
Epoch: 88/350
Train loss: 2.2871
Epoch: 89/350
Train loss: 2.4310
Epoch: 90/350
Train loss: 2.2992
Epoch: 91/350
Train loss: 2.3211
Epoch: 92/350
Train loss: 2.2901
Epoch: 93/350
Train loss: 2.2896
Epoch: 94/350
Train loss: 2.3280
Epoch: 95/350
Train loss: 2.3194
Epoch: 96/350
Train loss: 2.2585
Epoch: 97/350
Train loss: 2.2938
Epoch: 98/350
Train loss: 2.2713
Epoch: 99/350
Train loss: 2.3413
Epoch: 100/350
Train loss: 2.3317
Epoch: 101/350
Train loss: 2.3179
Epoch: 102/350
Train loss: 2.2316
Epoch: 103/350
Train loss: 2.2586
Epoch: 104/350
Train loss: 2.3081
Epoch: 105/350
Train loss: 2.2805
Epoch: 106/350
Train loss: 2.2884
Epoch: 107/350
Train loss: 2.2767
Epoch: 108/350
Train loss: 2.3232
Epoch: 109/350
Train loss: 2.2515
Epoch: 110/350
Train loss: 2.3305
Epoch: 111/350
Train loss: 2.2864
Epoch: 112/350
Train loss: 2.2845
Epoch: 113/350
Train loss: 2.2620
Epoch: 114/350
Train loss: 2.2250
Epoch: 115/350
Train loss: 2.2590
Epoch: 116/350
Train loss: 2.2746
Epoch: 117/350
Train loss: 2.2926
Epoch: 118/350
Train loss: 2.2034
Epoch: 119/350
Train loss: 2.2206
Epoch: 120/350
Train loss: 2.2054
Epoch: 121/350
Train loss: 2.1645
Epoch: 122/350
Train loss: 2.2912
Epoch: 123/350
Train loss: 2.2239
Epoch: 124/350
Train loss: 2.2432
Epoch: 125/350
Train loss: 2.2024
Epoch: 126/350
Train loss: 2.2906
Epoch: 127/350
Train loss: 2.2524
Epoch: 128/350
Train loss: 2.2162
Epoch: 129/350
Train loss: 2.2053
Epoch: 130/350
Train loss: 2.2015
Epoch: 131/350
Train loss: 2.2067
Epoch: 132/350
Train loss: 2.2586
Epoch: 133/350
Train loss: 2.2551
Epoch: 134/350
Train loss: 2.2346
Epoch: 135/350
Train loss: 2.2186
Epoch: 136/350
Train loss: 2.2101
Epoch: 137/350
Train loss: 2.2330
Epoch: 138/350
Train loss: 2.2572
Epoch: 139/350
Train loss: 2.1616
Epoch: 140/350
Train loss: 2.2243
Epoch: 141/350
Train loss: 2.2049
Epoch: 142/350
Train loss: 2.1397
Epoch: 143/350
Train loss: 2.2160
Epoch: 144/350
Train loss: 2.2100
Epoch: 145/350
Train loss: 2.1931
Epoch: 146/350
Train loss: 2.1965
Epoch: 147/350
Train loss: 2.1265
Epoch: 148/350
Train loss: 2.2011
Epoch: 149/350
Train loss: 2.2151
Epoch: 150/350
Train loss: 2.1683
Epoch: 151/350
Train loss: 2.1675
Epoch: 152/350
Train loss: 2.1891
Epoch: 153/350
Train loss: 2.2060
Epoch: 154/350
Train loss: 2.1881
Epoch: 155/350
Train loss: 2.1227
Epoch: 156/350
Train loss: 2.1662
Epoch: 157/350
Train loss: 2.2190
Epoch: 158/350
Train loss: 2.1342
Epoch: 159/350
Train loss: 2.1559
Epoch: 160/350
Train loss: 2.1097
Epoch: 161/350
Train loss: 2.1245
Epoch: 162/350
Train loss: 2.1618
Epoch: 163/350
Train loss: 2.1294
Epoch: 164/350
Train loss: 2.1639
Epoch: 165/350
Train loss: 2.1839
Epoch: 166/350
Train loss: 2.2016
Epoch: 167/350
Train loss: 2.1683
Epoch: 168/350
Train loss: 2.1808
Epoch: 169/350
Train loss: 2.1296
Epoch: 170/350
Train loss: 2.0889
Epoch: 171/350
Train loss: 2.0682
Epoch: 172/350
Train loss: 2.1237
Epoch: 173/350
Train loss: 2.1223
Epoch: 174/350
Train loss: 2.1175
Epoch: 175/350
Train loss: 2.1912
Epoch: 176/350
Train loss: 2.1604
Epoch: 177/350
Train loss: 2.1443
Epoch: 178/350
Train loss: 2.0895
Epoch: 179/350
Train loss: 2.1542
Epoch: 180/350
Train loss: 2.1479
Epoch: 181/350
Train loss: 2.1294
Epoch: 182/350
Train loss: 2.1458
Epoch: 183/350
Train loss: 2.1653
Epoch: 184/350
Train loss: 2.1182
Epoch: 185/350
Train loss: 2.0928
Epoch: 186/350
Train loss: 2.1401
Epoch: 187/350
Train loss: 2.0842
Epoch: 188/350
Train loss: 2.1186
Epoch: 189/350
Train loss: 2.1549
Epoch: 190/350
Train loss: 2.0822
Epoch: 191/350
Train loss: 2.1453
Epoch: 192/350
Train loss: 2.0915
Epoch: 193/350
Train loss: 2.1671
Epoch: 194/350
Train loss: 2.1474
Epoch: 195/350
Train loss: 2.0956
Epoch: 196/350
Train loss: 2.1309
Epoch: 197/350
Train loss: 2.0580
Epoch: 198/350
Train loss: 2.0886
Epoch: 199/350
Train loss: 2.1125
Epoch: 200/350
Train loss: 2.1793
Epoch: 201/350
Train loss: 2.1379
Epoch: 202/350
Train loss: 2.0978
Epoch: 203/350
Train loss: 2.1228
Epoch: 204/350
Train loss: 2.1529
Epoch: 205/350
Train loss: 2.1433
Epoch: 206/350
Train loss: 2.0831
Epoch: 207/350
Train loss: 2.1529
Epoch: 208/350
Train loss: 2.0432
Epoch: 209/350
Train loss: 2.1070
Epoch: 210/350
Train loss: 2.0345
Epoch: 211/350
Train loss: 2.0225
Epoch: 212/350
Train loss: 2.0113
Epoch: 213/350
Train loss: 2.1361
Epoch: 214/350
Train loss: 2.1980
Epoch: 215/350
Train loss: 2.0557
Epoch: 216/350
Train loss: 2.0593
Epoch: 217/350
Train loss: 2.0568
Epoch: 218/350
Train loss: 2.0637
Epoch: 219/350
Train loss: 2.0818
Epoch: 220/350
Train loss: 2.1456
Epoch: 221/350
Train loss: 2.0513
Epoch: 222/350
Train loss: 2.0424
Epoch: 223/350
Train loss: 2.0502
Epoch: 224/350
Train loss: 2.0437
Epoch: 225/350
Train loss: 2.0552
Epoch: 226/350
Train loss: 2.0990
Epoch: 227/350
Train loss: 2.0328
Epoch: 228/350
Train loss: 1.9982
Epoch: 229/350
Train loss: 2.0495
Epoch: 230/350
Train loss: 2.0305
Epoch: 231/350
Train loss: 2.0683
Epoch: 232/350
Train loss: 2.0309
Epoch: 233/350
Train loss: 2.0609
Epoch: 234/350
Train loss: 2.0238
Epoch: 235/350
Train loss: 2.0570
Epoch: 236/350
Train loss: 2.0513
Epoch: 237/350
Train loss: 2.0220
Epoch: 238/350
Train loss: 2.0563
Epoch: 239/350
Train loss: 2.0642
Epoch: 240/350
Train loss: 2.0795
Epoch: 241/350
Train loss: 2.0294
Epoch: 242/350
Train loss: 2.0147
Epoch: 243/350
Train loss: 2.0147
Epoch: 244/350
Train loss: 2.0932
Epoch: 245/350
Train loss: 2.0695
Epoch: 246/350
Train loss: 2.1010
Epoch: 247/350
Train loss: 2.0034
Epoch: 248/350
Train loss: 2.1044
Epoch: 249/350
Train loss: 2.1068
Epoch: 250/350
Train loss: 2.0228
Epoch: 251/350
Train loss: 2.1288
Epoch: 252/350
Train loss: 2.0862
Time needed: 150.036226272583 for validation audios
0.8002792252763081
Model improve: 0.0000 -> 0.8003
Epoch: 253/350
Train loss: 2.0349
Time needed: 149.66288042068481 for validation audios
0.8006475529834891
Model improve: 0.8003 -> 0.8006
Epoch: 254/350
Train loss: 2.0802
Time needed: 154.9563901424408 for validation audios
0.7992511305443215
Epoch: 255/350
Train loss: 1.9760
Time needed: 151.0146141052246 for validation audios
0.8013308767421045
Model improve: 0.8006 -> 0.8013
Epoch: 256/350
Train loss: 1.9898
Time needed: 151.53081059455872 for validation audios
0.7997323402096643
Epoch: 257/350
Train loss: 2.0214
Time needed: 149.8871841430664 for validation audios
0.8014160489759417
Model improve: 0.8013 -> 0.8014
Epoch: 258/350
Train loss: 2.0429
Time needed: 153.2027723789215 for validation audios
0.7992322150720838
Epoch: 259/350
Train loss: 2.0812
Time needed: 153.2072398662567 for validation audios
0.8008295212499288
Epoch: 260/350
Train loss: 2.0292
Time needed: 151.34841799736023 for validation audios
0.8009997416593867
Epoch: 261/350
Train loss: 2.0408
Time needed: 151.85698056221008 for validation audios
0.8000276043228833
Epoch: 262/350
Train loss: 2.0199
Time needed: 149.65106296539307 for validation audios
0.799935006857945
Epoch: 263/350
Train loss: 2.0206
Time needed: 157.32345747947693 for validation audios
0.8001754937612616
Epoch: 264/350
Train loss: 2.0465
Time needed: 150.4477663040161 for validation audios
0.7997743486955667
Epoch: 265/350
Train loss: 2.0278
Time needed: 148.6397569179535 for validation audios
0.800075708781994
Epoch: 266/350
Train loss: 2.0120
Time needed: 151.60705089569092 for validation audios
0.7994750526463389
Epoch: 267/350
Train loss: 2.0134
Time needed: 150.52599787712097 for validation audios
0.7999714598920259
Epoch: 268/350
Train loss: 2.0223
Time needed: 148.56144618988037 for validation audios
0.7992209968034177
Epoch: 269/350
Train loss: 2.0621
Time needed: 152.25098133087158 for validation audios
0.8000616991169309
Epoch: 270/350
Train loss: 2.0331
Time needed: 151.27654933929443 for validation audios
0.8007678227961306
Epoch: 271/350
Train loss: 2.0246
Time needed: 150.735102891922 for validation audios
0.7988034078192451
Epoch: 272/350
Train loss: 2.0729
Time needed: 150.58721470832825 for validation audios
0.8014813973060294
Model improve: 0.8014 -> 0.8015
Epoch: 273/350
Train loss: 1.9922
Time needed: 150.24410152435303 for validation audios
0.8009745361216685
Epoch: 274/350
Train loss: 2.0180
Time needed: 155.1418902873993 for validation audios
0.8008177397957842
Epoch: 275/350
Date :04/26/2023, 00:05:25
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 13.5176
Epoch: 2/200
Train loss: 4.5456
Epoch: 3/200
Train loss: 4.1105
Epoch: 4/200
Date :04/26/2023, 00:11:03
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 13.8307
Epoch: 2/200
Train loss: 4.6502
Epoch: 3/200
Train loss: 4.1442
Epoch: 4/200
Train loss: 3.8660
Epoch: 5/200
Train loss: 3.6630
Epoch: 6/200
Train loss: 3.4333
Epoch: 7/200
Train loss: 3.3918
Epoch: 8/200
Train loss: 3.2698
Epoch: 9/200
Train loss: 3.1936
Epoch: 10/200
Train loss: 3.1636
Epoch: 11/200
Train loss: 3.1131
Epoch: 12/200
Train loss: 2.9994
Epoch: 13/200
Train loss: 2.9199
Epoch: 14/200
Train loss: 2.9249
Epoch: 15/200
Train loss: 2.9315
Epoch: 16/200
Train loss: 2.8506
Epoch: 17/200
Train loss: 2.8599
Epoch: 18/200
Train loss: 2.8059
Epoch: 19/200
Train loss: 2.8269
Epoch: 20/200
Train loss: 2.6903
Epoch: 21/200
Train loss: 2.6923
Epoch: 22/200
Train loss: 2.7245
Epoch: 23/200
Train loss: 2.7634
Epoch: 24/200
Train loss: 2.7264
Epoch: 25/200
Train loss: 2.6698
Epoch: 26/200
Train loss: 2.6094
Epoch: 27/200
Train loss: 2.6620
Epoch: 28/200
Train loss: 2.6094
Epoch: 29/200
Train loss: 2.6292
Epoch: 30/200
Train loss: 2.5825
Epoch: 31/200
Date :04/26/2023, 01:04:33
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 300
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/300
Train loss: 13.8624
Epoch: 2/300
Train loss: 4.6516
Epoch: 3/300
Train loss: 4.1417
Epoch: 4/300
Train loss: 3.8611
Epoch: 5/300
Train loss: 3.6581
Epoch: 6/300
Train loss: 3.4291
Epoch: 7/300
Train loss: 3.3874
Epoch: 8/300
Train loss: 3.2664
Epoch: 9/300
Train loss: 3.1900
Epoch: 10/300
Train loss: 3.1599
Epoch: 11/300
Train loss: 3.1111
Epoch: 12/300
Train loss: 2.9979
Epoch: 13/300
Train loss: 2.9185
Epoch: 14/300
Train loss: 2.9219
Epoch: 15/300
Train loss: 2.9275
Epoch: 16/300
Train loss: 2.8471
Epoch: 17/300
Train loss: 2.8570
Epoch: 18/300
Train loss: 2.8042
Epoch: 19/300
Train loss: 2.8258
Epoch: 20/300
Train loss: 2.6885
Epoch: 21/300
Train loss: 2.6923
Epoch: 22/300
Train loss: 2.7230
Epoch: 23/300
Train loss: 2.7621
Epoch: 24/300
Train loss: 2.7252
Epoch: 25/300
Train loss: 2.6702
Epoch: 26/300
Train loss: 2.6074
Epoch: 27/300
Train loss: 2.6613
Epoch: 28/300
Train loss: 2.6070
Epoch: 29/300
Train loss: 2.6279
Epoch: 30/300
Train loss: 2.5822
Epoch: 31/300
Train loss: 2.6660
Epoch: 32/300
Train loss: 2.5804
Epoch: 33/300
Train loss: 2.5353
Epoch: 34/300
Train loss: 2.5072
Epoch: 35/300
Train loss: 2.5186
Epoch: 36/300
Train loss: 2.5296
Epoch: 37/300
Train loss: 2.5617
Epoch: 38/300
Train loss: 2.4322
Epoch: 39/300
Train loss: 2.4790
Epoch: 40/300
Train loss: 2.4321
Epoch: 41/300
Train loss: 2.5070
Epoch: 42/300
Train loss: 2.5574
Epoch: 43/300
Train loss: 2.4860
Epoch: 44/300
Train loss: 2.4380
Epoch: 45/300
Train loss: 2.4411
Epoch: 46/300
Train loss: 2.4225
Epoch: 47/300
Train loss: 2.4714
Epoch: 48/300
Train loss: 2.4859
Epoch: 49/300
Train loss: 2.4914
Epoch: 50/300
Train loss: 2.4467
Epoch: 51/300
Train loss: 2.4607
Epoch: 52/300
Train loss: 2.3259
Epoch: 53/300
Train loss: 2.4536
Epoch: 54/300
Train loss: 2.3771
Epoch: 55/300
Train loss: 2.3938
Epoch: 56/300
Train loss: 2.3657
Epoch: 57/300
Train loss: 2.4248
Epoch: 58/300
Train loss: 2.3127
Epoch: 59/300
Train loss: 2.3104
Epoch: 60/300
Train loss: 2.3388
Epoch: 61/300
Train loss: 2.3421
Epoch: 62/300
Train loss: 2.3876
Epoch: 63/300
Train loss: 2.3512
Epoch: 64/300
Train loss: 2.3254
Epoch: 65/300
Train loss: 2.3514
Epoch: 66/300
Train loss: 2.3449
Epoch: 67/300
Train loss: 2.2757
Epoch: 68/300
Train loss: 2.3290
Epoch: 69/300
Train loss: 2.3225
Epoch: 70/300
Train loss: 2.3333
Epoch: 71/300
Train loss: 2.3181
Epoch: 72/300
Train loss: 2.3412
Epoch: 73/300
Train loss: 2.3606
Epoch: 74/300
Train loss: 2.3304
Epoch: 75/300
Train loss: 2.2467
Epoch: 76/300
Train loss: 2.2516
Epoch: 77/300
Train loss: 2.2357
Epoch: 78/300
Train loss: 2.3127
Epoch: 79/300
Train loss: 2.2445
Epoch: 80/300
Train loss: 2.2864
Epoch: 81/300
Train loss: 2.3450
Epoch: 82/300
Train loss: 2.2551
Epoch: 83/300
Train loss: 2.2355
Epoch: 84/300
Train loss: 2.3132
Epoch: 85/300
Train loss: 2.2718
Epoch: 86/300
Train loss: 2.2627
Epoch: 87/300
Train loss: 2.2563
Epoch: 88/300
Train loss: 2.2071
Epoch: 89/300
Train loss: 2.2481
Epoch: 90/300
Train loss: 2.2367
Epoch: 91/300
Train loss: 2.2730
Epoch: 92/300
Train loss: 2.1756
Epoch: 93/300
Train loss: 2.2951
Epoch: 94/300
Train loss: 2.2754
Epoch: 95/300
Train loss: 2.3254
Epoch: 96/300
Train loss: 2.2418
Epoch: 97/300
Train loss: 2.2074
Epoch: 98/300
Train loss: 2.1676
Epoch: 99/300
Train loss: 2.1694
Epoch: 100/300
Train loss: 2.2062
Epoch: 101/300
Train loss: 2.2795
Epoch: 102/300
Train loss: 2.1630
Epoch: 103/300
Train loss: 2.1302
Epoch: 104/300
Train loss: 2.2079
Epoch: 105/300
Train loss: 2.1759
Epoch: 106/300
Train loss: 2.2433
Epoch: 107/300
Train loss: 2.2380
Epoch: 108/300
Train loss: 2.1704
Epoch: 109/300
Train loss: 2.2167
Epoch: 110/300
Train loss: 2.1725
Epoch: 111/300
Train loss: 2.1740
Epoch: 112/300
Train loss: 2.2040
Epoch: 113/300
Train loss: 2.2042
Epoch: 114/300
Train loss: 2.1657
Epoch: 115/300
Train loss: 2.1982
Epoch: 116/300
Train loss: 2.1403
Epoch: 117/300
Train loss: 2.2172
Epoch: 118/300
Train loss: 2.1956
Epoch: 119/300
Train loss: 2.1808
Epoch: 120/300
Train loss: 2.2068
Epoch: 121/300
Train loss: 2.0972
Epoch: 122/300
Train loss: 2.1236
Epoch: 123/300
Train loss: 2.1628
Epoch: 124/300
Train loss: 2.1983
Epoch: 125/300
Train loss: 2.1536
Epoch: 126/300
Train loss: 2.1449
Epoch: 127/300
Train loss: 2.1832
Epoch: 128/300
Train loss: 2.0916
Epoch: 129/300
Train loss: 2.1239
Epoch: 130/300
Train loss: 2.1504
Epoch: 131/300
Train loss: 2.1243
Epoch: 132/300
Train loss: 2.1664
Epoch: 133/300
Train loss: 2.1727
Epoch: 134/300
Train loss: 2.0848
Epoch: 135/300
Train loss: 2.1249
Epoch: 136/300
Train loss: 2.1436
Epoch: 137/300
Train loss: 2.1516
Epoch: 138/300
Train loss: 2.0768
Epoch: 139/300
Train loss: 2.0742
Epoch: 140/300
Train loss: 2.0889
Epoch: 141/300
Train loss: 2.1309
Epoch: 142/300
Train loss: 2.1228
Epoch: 143/300
Train loss: 2.0822
Epoch: 144/300
Train loss: 2.1445
Epoch: 145/300
Train loss: 2.1442
Epoch: 146/300
Train loss: 2.0672
Epoch: 147/300
Train loss: 2.1711
Epoch: 148/300
Train loss: 2.0905
Epoch: 149/300
Train loss: 2.0911
Epoch: 150/300
Train loss: 2.1186
Epoch: 151/300
Train loss: 2.1628
Epoch: 152/300
Train loss: 2.0717
Epoch: 153/300
Train loss: 2.1524
Epoch: 154/300
Train loss: 2.0957
Epoch: 155/300
Train loss: 2.1066
Epoch: 156/300
Train loss: 2.0579
Epoch: 157/300
Train loss: 2.0579
Epoch: 158/300
Train loss: 2.1271
Epoch: 159/300
Train loss: 2.0585
Epoch: 160/300
Train loss: 2.0617
Epoch: 161/300
Train loss: 2.1089
Epoch: 162/300
Train loss: 2.1116
Epoch: 163/300
Train loss: 2.1100
Epoch: 164/300
Train loss: 2.0883
Epoch: 165/300
Train loss: 2.1233
Epoch: 166/300
Train loss: 2.0531
Epoch: 167/300
Train loss: 2.0880
Epoch: 168/300
Train loss: 2.1554
Epoch: 169/300
Train loss: 2.0660
Epoch: 170/300
Train loss: 2.0283
Epoch: 171/300
Train loss: 2.0157
Epoch: 172/300
Train loss: 2.0808
Epoch: 173/300
Train loss: 2.0404
Epoch: 174/300
Train loss: 1.9973
Epoch: 175/300
Train loss: 2.0951
Epoch: 176/300
Train loss: 2.0417
Epoch: 177/300
Train loss: 2.1107
Epoch: 178/300
Train loss: 2.0102
Epoch: 179/300
Train loss: 1.9956
Epoch: 180/300
Train loss: 2.0762
Epoch: 181/300
Train loss: 2.0271
Epoch: 182/300
Train loss: 2.0326
Epoch: 183/300
Train loss: 2.0503
Epoch: 184/300
Train loss: 2.0610
Epoch: 185/300
Train loss: 1.9926
Epoch: 186/300
Train loss: 2.0457
Epoch: 187/300
Train loss: 2.0722
Epoch: 188/300
Train loss: 2.0712
Epoch: 189/300
Train loss: 2.0315
Epoch: 190/300
Train loss: 2.0044
Epoch: 191/300
Train loss: 2.0275
Epoch: 192/300
Train loss: 2.0020
Epoch: 193/300
Train loss: 1.9980
Epoch: 194/300
Train loss: 2.0669
Epoch: 195/300
Train loss: 2.0045
Epoch: 196/300
Train loss: 2.0063
Epoch: 197/300
Train loss: 1.9717
Epoch: 198/300
Train loss: 2.0478
Epoch: 199/300
Train loss: 1.9596
Epoch: 200/300
Train loss: 2.0346
Epoch: 201/300
Train loss: 1.9982
Epoch: 202/300
Train loss: 2.0280
Epoch: 203/300
Train loss: 2.0298
Epoch: 204/300
Train loss: 2.0869
Epoch: 205/300
Train loss: 2.0214
Epoch: 206/300
Train loss: 2.0087
Epoch: 207/300
Train loss: 2.0972
Epoch: 208/300
Train loss: 1.9741
Epoch: 209/300
Train loss: 2.0475
Epoch: 210/300
Train loss: 1.9400
Epoch: 211/300
Train loss: 2.0088
Epoch: 212/300
Train loss: 1.9263
Epoch: 213/300
Train loss: 2.0751
Epoch: 214/300
Train loss: 2.0638
Epoch: 215/300
Train loss: 1.9729
Epoch: 216/300
Train loss: 2.0228
Epoch: 217/300
Train loss: 2.0339
Epoch: 218/300
Train loss: 2.0223
Epoch: 219/300
Train loss: 2.0118
Epoch: 220/300
Train loss: 1.9809
Epoch: 221/300
Train loss: 2.0114
Epoch: 222/300
Train loss: 1.9868
Epoch: 223/300
Train loss: 2.0523
Epoch: 224/300
Train loss: 1.9691
Epoch: 225/300
Train loss: 2.0366
Epoch: 226/300
Train loss: 1.9533
Epoch: 227/300
Train loss: 2.0201
Epoch: 228/300
Train loss: 2.0547
Epoch: 229/300
Train loss: 1.9859
Epoch: 230/300
Train loss: 2.0526
Epoch: 231/300
Train loss: 2.0034
Epoch: 232/300
Train loss: 1.9922
Epoch: 233/300
Train loss: 1.9594
Epoch: 234/300
Train loss: 2.0014
Epoch: 235/300
Train loss: 2.0494
Epoch: 236/300
Train loss: 1.9662
Epoch: 237/300
Train loss: 1.9949
Epoch: 238/300
Train loss: 1.9733
Epoch: 239/300
Train loss: 1.9918
Epoch: 240/300
Train loss: 1.9366
Epoch: 241/300
Train loss: 2.0662
Epoch: 242/300
Train loss: 1.9795
Epoch: 243/300
Train loss: 1.9562
Epoch: 244/300
Train loss: 1.9572
Epoch: 245/300
Train loss: 1.9402
Epoch: 246/300
Train loss: 1.9488
Epoch: 247/300
Train loss: 1.9585
Epoch: 248/300
Train loss: 1.9529
Epoch: 249/300
Train loss: 1.9433
Epoch: 250/300
Train loss: 1.9287
Epoch: 251/300
Train loss: 1.9985
Epoch: 252/300
Train loss: 1.9819
Time needed: 159.70579171180725 for validation audios
0.8011874853895867
Model improve: 0.0000 -> 0.8012
Epoch: 253/300
Train loss: 1.9849
Time needed: 151.0625364780426 for validation audios
0.7998048842535511
Epoch: 254/300
Train loss: 2.0250
Time needed: 150.25970125198364 for validation audios
0.7999824548179407
Epoch: 255/300
Train loss: 1.9896
Time needed: 152.1280210018158 for validation audios
0.7998356956284252
Epoch: 256/300
Train loss: 2.0383
Time needed: 150.7843952178955 for validation audios
0.7995517037033971
Epoch: 257/300
Train loss: 1.9615
Time needed: 152.14503049850464 for validation audios
0.7994997152434837
Epoch: 258/300
Train loss: 1.9519
Time needed: 149.0991187095642 for validation audios
0.8020643179445857
Model improve: 0.8012 -> 0.8021
Epoch: 259/300
Train loss: 1.9117
Date :04/26/2023, 08:45:35
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 300
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/300
Train loss: 13.5161
Epoch: 2/300
Train loss: 4.5455
Epoch: 3/300
Train loss: 4.1096
Epoch: 4/300
Train loss: 3.8566
Epoch: 5/300
Train loss: 3.6768
Epoch: 6/300
Train loss: 3.4538
Epoch: 7/300
Train loss: 3.4259
Epoch: 8/300
Train loss: 3.3053
Epoch: 9/300
Train loss: 3.2302
Epoch: 10/300
Train loss: 3.2014
Epoch: 11/300
Train loss: 3.1480
Epoch: 12/300
Train loss: 3.0502
Epoch: 13/300
Train loss: 2.9567
Epoch: 14/300
Train loss: 2.9704
Epoch: 15/300
Train loss: 2.9785
Epoch: 16/300
Train loss: 2.8930
Epoch: 17/300
Train loss: 2.9070
Epoch: 18/300
Train loss: 2.8430
Epoch: 19/300
Train loss: 2.8747
Epoch: 20/300
Train loss: 2.7315
Epoch: 21/300
Train loss: 2.7401
Epoch: 22/300
Train loss: 2.7713
Epoch: 23/300
Train loss: 2.8018
Epoch: 24/300
Train loss: 2.7682
Epoch: 25/300
Train loss: 2.7166
Epoch: 26/300
Train loss: 2.6463
Epoch: 27/300
Train loss: 2.7117
Epoch: 28/300
Train loss: 2.6544
Epoch: 29/300
Train loss: 2.6697
Epoch: 30/300
Train loss: 2.6369
Epoch: 31/300
Train loss: 2.7213
Epoch: 32/300
Train loss: 2.6213
Epoch: 33/300
Train loss: 2.5808
Epoch: 34/300
Train loss: 2.5435
Epoch: 35/300
Train loss: 2.5583
Epoch: 36/300
Train loss: 2.5666
Epoch: 37/300
Train loss: 2.6059
Epoch: 38/300
Train loss: 2.4719
Epoch: 39/300
Train loss: 2.5127
Epoch: 40/300
Train loss: 2.4594
Epoch: 41/300
Train loss: 2.5479
Epoch: 42/300
Train loss: 2.6025
Epoch: 43/300
Train loss: 2.5253
Epoch: 44/300
Train loss: 2.4806
Epoch: 45/300
Train loss: 2.4809
Epoch: 46/300
Train loss: 2.4628
Epoch: 47/300
Train loss: 2.5047
Epoch: 48/300
Train loss: 2.5229
Epoch: 49/300
Train loss: 2.5307
Epoch: 50/300
Train loss: 2.4886
Epoch: 51/300
Train loss: 2.5009
Epoch: 52/300
Train loss: 2.3668
Epoch: 53/300
Train loss: 2.4968
Epoch: 54/300
Train loss: 2.4163
Epoch: 55/300
Train loss: 2.4312
Epoch: 56/300
Train loss: 2.4091
Epoch: 57/300
Train loss: 2.4626
Epoch: 58/300
Train loss: 2.3466
Epoch: 59/300
Train loss: 2.3383
Epoch: 60/300
Train loss: 2.3710
Epoch: 61/300
Train loss: 2.3719
Epoch: 62/300
Train loss: 2.4247
Epoch: 63/300
Train loss: 2.3815
Epoch: 64/300
Train loss: 2.3566
Epoch: 65/300
Train loss: 2.3799
Epoch: 66/300
Train loss: 2.3797
Epoch: 67/300
Train loss: 2.2995
Epoch: 68/300
Train loss: 2.3668
Epoch: 69/300
Train loss: 2.3568
Epoch: 70/300
Train loss: 2.3674
Epoch: 71/300
Train loss: 2.3592
Epoch: 72/300
Train loss: 2.3737
Epoch: 73/300
Train loss: 2.4017
Epoch: 74/300
Train loss: 2.3730
Epoch: 75/300
Train loss: 2.2690
Epoch: 76/300
Train loss: 2.2860
Epoch: 77/300
Train loss: 2.2687
Epoch: 78/300
Train loss: 2.3408
Epoch: 79/300
Train loss: 2.2721
Epoch: 80/300
Train loss: 2.3126
Epoch: 81/300
Train loss: 2.3756
Epoch: 82/300
Train loss: 2.2965
Epoch: 83/300
Train loss: 2.2642
Epoch: 84/300
Train loss: 2.3441
Epoch: 85/300
Train loss: 2.3049
Epoch: 86/300
Train loss: 2.2919
Epoch: 87/300
Train loss: 2.2870
Epoch: 88/300
Train loss: 2.2382
Epoch: 89/300
Train loss: 2.2781
Epoch: 90/300
Train loss: 2.2701
Epoch: 91/300
Train loss: 2.2993
Epoch: 92/300
Train loss: 2.2153
Epoch: 93/300
Train loss: 2.3261
Epoch: 94/300
Train loss: 2.3039
Epoch: 95/300
Train loss: 2.3531
Epoch: 96/300
Train loss: 2.2657
Epoch: 97/300
Train loss: 2.2258
Epoch: 98/300
Train loss: 2.1945
Epoch: 99/300
Train loss: 2.2015
Epoch: 100/300
Train loss: 2.2347
Epoch: 101/300
Train loss: 2.3078
Epoch: 102/300
Train loss: 2.1975
Time needed: 150.13721418380737 for validation audios
0.7945852601285275
Model improve: 0.0000 -> 0.7946
Epoch: 103/300
Train loss: 2.1608
Time needed: 153.52564096450806 for validation audios
0.7930720595739584
Epoch: 104/300
Train loss: 2.2232
Time needed: 150.80872988700867 for validation audios
0.7965013974985803
Model improve: 0.7946 -> 0.7965
Epoch: 105/300
Train loss: 2.2328
Time needed: 151.43267512321472 for validation audios
0.7977360803640772
Model improve: 0.7965 -> 0.7977
Epoch: 106/300
Train loss: 2.2828
Time needed: 149.74586272239685 for validation audios
0.7953402970359756
Epoch: 107/300
Train loss: 2.2535
Time needed: 150.47405123710632 for validation audios
0.795068097289369
Epoch: 108/300
Train loss: 2.2020
Time needed: 151.0492353439331 for validation audios
0.7956587513824235
Epoch: 109/300
Train loss: 2.2355
Time needed: 153.20240545272827 for validation audios
0.7962193022306382
Epoch: 110/300
Train loss: 2.1932
Time needed: 150.0197412967682 for validation audios
0.7960701590302032
Epoch: 111/300
Train loss: 2.2066
Time needed: 152.28676509857178 for validation audios
0.7956693929591179
Epoch: 112/300
Train loss: 2.2441
Time needed: 151.14300203323364 for validation audios
0.7945224823525467
Epoch: 113/300
Train loss: 2.2279
Time needed: 150.99406147003174 for validation audios
0.7939507400001549
Epoch: 114/300
Train loss: 2.2049
Time needed: 151.11838912963867 for validation audios
0.7944326688458152
Epoch: 115/300
Train loss: 2.2224
Time needed: 150.64832139015198 for validation audios
0.7924494844451657
Epoch: 116/300
Train loss: 2.1551
Time needed: 151.7189633846283 for validation audios
0.7974371706068101
Epoch: 117/300
Train loss: 2.2551
Time needed: 149.4177451133728 for validation audios
0.7957887468590014
Epoch: 118/300
Train loss: 2.2242
Time needed: 150.0541489124298 for validation audios
0.7945909238146415
Epoch: 119/300
Train loss: 2.2216
Time needed: 150.6287236213684 for validation audios
0.7966544266998525
Epoch: 120/300
Train loss: 2.2471
Time needed: 151.94152402877808 for validation audios
0.7942299742536566
Epoch: 121/300
Train loss: 2.1066
Time needed: 152.01147031784058 for validation audios
0.7943781927329863
Epoch: 122/300
Train loss: 2.1463
Time needed: 152.49058365821838 for validation audios
0.7941125723542689
Epoch: 123/300
Train loss: 2.1926
Time needed: 150.94812321662903 for validation audios
0.7929737912058082
Epoch: 124/300
Train loss: 2.2196
Time needed: 150.9261920452118 for validation audios
0.7922495868082797
Epoch: 125/300
Train loss: 2.2026
Date :04/26/2023, 13:17:31
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 15
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
14198
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 13.5314
Epoch: 2/200
Train loss: 4.5917
Epoch: 3/200
Train loss: 4.1553
Epoch: 4/200
Train loss: 3.8343
Epoch: 5/200
Train loss: 3.7046
Epoch: 6/200
Train loss: 3.5135
Epoch: 7/200
Train loss: 3.3694
Epoch: 8/200
Train loss: 3.2942
Epoch: 9/200
Train loss: 3.2425
Epoch: 10/200
Train loss: 3.2194
Epoch: 11/200
Train loss: 3.1858
Epoch: 12/200
Train loss: 2.9918
Epoch: 13/200
Train loss: 2.9971
Epoch: 14/200
Train loss: 2.9497
Epoch: 15/200
Train loss: 2.9436
Epoch: 16/200
Train loss: 2.9355
Epoch: 17/200
Train loss: 2.9235
Epoch: 18/200
Train loss: 2.8411
Epoch: 19/200
Train loss: 2.8467
Epoch: 20/200
Train loss: 2.7308
Epoch: 21/200
Train loss: 2.7620
Epoch: 22/200
Train loss: 2.7939
Epoch: 23/200
Train loss: 2.7631
Epoch: 24/200
Train loss: 2.7663
Epoch: 25/200
Train loss: 2.6940
Epoch: 26/200
Train loss: 2.6622
Epoch: 27/200
Train loss: 2.7184
Epoch: 28/200
Train loss: 2.6685
Epoch: 29/200
Train loss: 2.6531
Epoch: 30/200
Train loss: 2.6824
Epoch: 31/200
Train loss: 2.6725
Epoch: 32/200
Train loss: 2.5638
Epoch: 33/200
Train loss: 2.5813
Epoch: 34/200
Train loss: 2.5854
Epoch: 35/200
Train loss: 2.5861
Epoch: 36/200
Train loss: 2.5868
Epoch: 37/200
Train loss: 2.4778
Epoch: 38/200
Train loss: 2.5302
Epoch: 39/200
Train loss: 2.4680
Epoch: 40/200
Train loss: 2.5861
Epoch: 41/200
Train loss: 2.5873
Epoch: 42/200
Train loss: 2.5430
Epoch: 43/200
Train loss: 2.4576
Epoch: 44/200
Train loss: 2.4766
Epoch: 45/200
Train loss: 2.4706
Epoch: 46/200
Train loss: 2.5100
Epoch: 47/200
Train loss: 2.5345
Epoch: 48/200
Train loss: 2.5009
Epoch: 49/200
Train loss: 2.4640
Epoch: 50/200
Train loss: 2.4927
Epoch: 51/200
Train loss: 2.3909
Epoch: 52/200
Train loss: 2.4554
Time needed: 155.1457097530365 for validation audios
0.7889570164295054
Model improve: 0.0000 -> 0.7890
Epoch: 53/200
Train loss: 2.3885
Time needed: 151.98602175712585 for validation audios
0.789851830678346
Model improve: 0.7890 -> 0.7899
Epoch: 54/200
Train loss: 2.4508
Time needed: 151.43643808364868 for validation audios
0.7910326585281108
Model improve: 0.7899 -> 0.7910
Epoch: 55/200
Train loss: 2.4756
Time needed: 151.58066058158875 for validation audios
0.790157587004786
Epoch: 56/200
Train loss: 2.3860
Time needed: 150.778165102005 for validation audios
0.7918764134626565
Model improve: 0.7910 -> 0.7919
Epoch: 57/200
Train loss: 2.3545
Time needed: 151.67127919197083 for validation audios
0.792243485716699
Model improve: 0.7919 -> 0.7922
Epoch: 58/200
Train loss: 2.4046
Time needed: 150.82160305976868 for validation audios
0.7919802532365823
Epoch: 59/200
Train loss: 2.3451
Time needed: 150.0752248764038 for validation audios
0.7929933961974079
Model improve: 0.7922 -> 0.7930
Epoch: 60/200
Train loss: 2.4483
Time needed: 150.31548690795898 for validation audios
0.7940044461087727
Model improve: 0.7930 -> 0.7940
Epoch: 61/200
Train loss: 2.3654
Time needed: 151.4291079044342 for validation audios
0.7917550864369997
Epoch: 62/200
Train loss: 2.3540
Time needed: 150.5609540939331 for validation audios
0.7905425580204996
Epoch: 63/200
Train loss: 2.3820
Time needed: 152.09130668640137 for validation audios
0.7941108628332095
Model improve: 0.7940 -> 0.7941
Epoch: 64/200
Train loss: 2.4178
Time needed: 152.1502764225006 for validation audios
0.7922051804575837
Epoch: 65/200
Train loss: 2.3660
Time needed: 150.7493634223938 for validation audios
0.7920764674428791
Epoch: 66/200
Train loss: 2.3105
Time needed: 151.98465156555176 for validation audios
0.7919510022350765
Epoch: 67/200
Train loss: 2.3868
Time needed: 150.23989129066467 for validation audios
0.7951933671117097
Model improve: 0.7941 -> 0.7952
Epoch: 68/200
Train loss: 2.3874
Time needed: 152.41250729560852 for validation audios
0.7956948055050584
Model improve: 0.7952 -> 0.7957
Epoch: 69/200
Train loss: 2.3266
Time needed: 151.814927816391 for validation audios
0.794588377059035
Epoch: 70/200
Train loss: 2.3845
Time needed: 151.3981637954712 for validation audios
0.7934023095395525
Epoch: 71/200
Train loss: 2.3974
Time needed: 150.44183158874512 for validation audios
0.7927691694748069
Epoch: 72/200
Train loss: 2.3803
Time needed: 150.58317923545837 for validation audios
0.7956481202543702
Epoch: 73/200
Train loss: 2.3177
Time needed: 150.17266964912415 for validation audios
0.7948267862776226
Epoch: 74/200
Train loss: 2.2668
Time needed: 149.61943674087524 for validation audios
0.793359096972253
Epoch: 75/200
Train loss: 2.2484
Time needed: 150.4129548072815 for validation audios
0.7935421413941594
Epoch: 76/200
Train loss: 2.3247
Time needed: 149.84427690505981 for validation audios
0.7953166988098122
Epoch: 77/200
Train loss: 2.2619
Time needed: 149.2443995475769 for validation audios
0.7924022121191792
Epoch: 78/200
Train loss: 2.3297
Time needed: 150.0713632106781 for validation audios
0.793426071635358
Epoch: 79/200
Train loss: 2.3342
Time needed: 154.1309576034546 for validation audios
0.7936244024767818
Epoch: 80/200
Train loss: 2.3005
Time needed: 150.38394045829773 for validation audios
0.7905585341919282
Epoch: 81/200
Train loss: 2.2558
Time needed: 150.520441532135 for validation audios
0.7962481408643427
Model improve: 0.7957 -> 0.7962
Epoch: 82/200
Train loss: 2.3521
Time needed: 152.81099343299866 for validation audios
0.7913197446685281
Epoch: 83/200
Train loss: 2.2450
Time needed: 151.60765624046326 for validation audios
0.793865454518703
Epoch: 84/200
Train loss: 2.2943
Time needed: 150.39080381393433 for validation audios
0.7984694707858481
Model improve: 0.7962 -> 0.7985
Epoch: 85/200
Train loss: 2.2861
Time needed: 151.24039316177368 for validation audios
0.7930060600725218
Epoch: 86/200
Train loss: 2.2265
Time needed: 149.54809260368347 for validation audios
0.7957222665510966
Epoch: 87/200
Train loss: 2.2523
Time needed: 151.55388617515564 for validation audios
0.7938623435664245
Epoch: 88/200
Train loss: 2.3305
Time needed: 150.60804724693298 for validation audios
0.7974872714630981
Epoch: 89/200
Train loss: 2.2923
Time needed: 149.57077980041504 for validation audios
0.793229472281408
Epoch: 90/200
Train loss: 2.1959
Time needed: 150.15594744682312 for validation audios
0.7980051469418861
Epoch: 91/200
Train loss: 2.3026
Time needed: 150.37444281578064 for validation audios
0.7918148805508418
Epoch: 92/200
Train loss: 2.3374
Time needed: 149.95270586013794 for validation audios
0.7973671074129509
Epoch: 93/200
Train loss: 2.3199
Time needed: 151.412273645401 for validation audios
0.7957213088187531
Epoch: 94/200
Train loss: 2.2384
Time needed: 149.93563985824585 for validation audios
0.7977537778511543
Epoch: 95/200
Train loss: 2.2152
Time needed: 153.5784695148468 for validation audios
0.7960936434997817
Epoch: 96/200
Train loss: 2.1820
Time needed: 150.37634682655334 for validation audios
0.7963145822902641
Epoch: 97/200
Train loss: 2.1952
Time needed: 150.4181911945343 for validation audios
0.7947870429642984
Epoch: 98/200
Train loss: 2.2537
Time needed: 150.01675176620483 for validation audios
0.7952741464527381
Epoch: 99/200
Train loss: 2.2283
Time needed: 151.5575976371765 for validation audios
0.7957638548648428
Epoch: 100/200
Train loss: 2.1386
Time needed: 151.35917472839355 for validation audios
0.7959900124166644
Epoch: 101/200
Train loss: 2.2250
Time needed: 150.56749439239502 for validation audios
0.7966842157056562
Epoch: 102/200
Train loss: 2.1809
Time needed: 151.52848362922668 for validation audios
0.7957326398189577
Epoch: 103/200
Train loss: 2.2488
Time needed: 151.2177221775055 for validation audios
0.7978584476110383
Epoch: 104/200
Train loss: 2.2557
Time needed: 151.7196855545044 for validation audios
0.7985951099899301
Model improve: 0.7985 -> 0.7986
Epoch: 105/200
Train loss: 2.2431
Time needed: 151.65828490257263 for validation audios
0.7957084038195095
Epoch: 106/200
Train loss: 2.2096
Time needed: 151.51586055755615 for validation audios
0.7959641052876619
Epoch: 107/200
Train loss: 2.1778
Time needed: 152.0718801021576 for validation audios
0.7969618132438651
Epoch: 108/200
Train loss: 2.2039
Time needed: 150.16028475761414 for validation audios
0.7945254925857255
Epoch: 109/200
Train loss: 2.2452
Time needed: 156.3189733028412 for validation audios
0.7958085407818942
Epoch: 110/200
Train loss: 2.1960
Time needed: 150.8363173007965 for validation audios
0.7950507714554683
Epoch: 111/200
Train loss: 2.2096
Time needed: 150.2215895652771 for validation audios
0.7934399574404785
Epoch: 112/200
Train loss: 2.2002
Time needed: 149.50518894195557 for validation audios
0.7959365331476499
Epoch: 113/200
Train loss: 2.2055
Time needed: 150.17060804367065 for validation audios
0.7918175020965074
Epoch: 114/200
Train loss: 2.2030
Time needed: 150.34011435508728 for validation audios
0.7963397627875733
Epoch: 115/200
Train loss: 2.2278
Time needed: 151.40136408805847 for validation audios
0.7933106308506688
Epoch: 116/200
Train loss: 2.1976
Time needed: 152.065438747406 for validation audios
0.7985382096472626
Epoch: 117/200
Train loss: 2.2480
Time needed: 150.23768162727356 for validation audios
0.7967985446045797
Epoch: 118/200
Train loss: 2.1192
Time needed: 149.8474850654602 for validation audios
0.7981369377776178
Epoch: 119/200
Train loss: 2.1461
Time needed: 150.6755874156952 for validation audios
0.7978454435669347
Epoch: 120/200
Train loss: 2.2090
Time needed: 151.3536732196808 for validation audios
0.7979173929865129
Epoch: 121/200
Train loss: 2.1796
Time needed: 151.0803141593933 for validation audios
0.7973103872309384
Epoch: 122/200
Train loss: 2.1908
Time needed: 151.17236733436584 for validation audios
0.7991818875346692
Model improve: 0.7986 -> 0.7992
Epoch: 123/200
Train loss: 2.1687
Time needed: 150.82654690742493 for validation audios
0.7979749268536379
Epoch: 124/200
Train loss: 2.2053
Time needed: 151.8316159248352 for validation audios
0.798535089098463
Epoch: 125/200
Train loss: 2.1341
Time needed: 150.593323469162 for validation audios
0.7968017873043385
Epoch: 126/200
Train loss: 2.1428
Time needed: 149.34613060951233 for validation audios
0.7997760278138759
Model improve: 0.7992 -> 0.7998
Epoch: 127/200
Train loss: 2.1541
Time needed: 149.85206413269043 for validation audios
0.7989384593688418
Epoch: 128/200
Train loss: 2.1650
Time needed: 151.25461721420288 for validation audios
0.7973950161245961
Epoch: 129/200
Train loss: 2.1750
Time needed: 150.281747341156 for validation audios
0.8007484468951616
Model improve: 0.7998 -> 0.8007
Epoch: 130/200
Train loss: 2.1314
Time needed: 151.37128019332886 for validation audios
0.7986926238291522
Epoch: 131/200
Train loss: 2.1629
Time needed: 149.7678039073944 for validation audios
0.7945830588937671
Epoch: 132/200
Train loss: 2.1379
Time needed: 149.67935180664062 for validation audios
0.7970122506675359
Epoch: 133/200
Train loss: 2.1451
Time needed: 149.98049235343933 for validation audios
0.7968050155810532
Epoch: 134/200
Train loss: 2.1572
Time needed: 151.1065480709076 for validation audios
0.8003913841139901
Epoch: 135/200
Train loss: 2.0935
Time needed: 149.1567747592926 for validation audios
0.7956252001175343
Epoch: 136/200
Train loss: 2.1338
Time needed: 151.327570438385 for validation audios
0.7990823506483412
Epoch: 137/200
Train loss: 2.1335
Time needed: 149.64255452156067 for validation audios
0.799453266255216
Epoch: 138/200
Train loss: 2.1479
Time needed: 150.2570230960846 for validation audios
0.8004238077093037
Epoch: 139/200
Train loss: 2.1284
Time needed: 152.07886576652527 for validation audios
0.7981937327585028
Epoch: 140/200
Train loss: 2.1351
Time needed: 153.34277510643005 for validation audios
0.7982844396658282
Epoch: 141/200
Train loss: 2.1183
Time needed: 152.29770231246948 for validation audios
0.7988544342713708
Epoch: 142/200
Train loss: 2.1284
Time needed: 150.591703414917 for validation audios
0.8005494795773008
Epoch: 143/200
Train loss: 2.1465
Time needed: 149.58787083625793 for validation audios
0.7988801105784733
Epoch: 144/200
Train loss: 2.1429
Time needed: 152.96918153762817 for validation audios
0.7988980761313997
Epoch: 145/200
Train loss: 2.1451
Time needed: 150.17446112632751 for validation audios
0.7986035333133715
Epoch: 146/200
Train loss: 2.1542
Time needed: 150.6350281238556 for validation audios
0.7980430431924517
Epoch: 147/200
Train loss: 2.1563
Time needed: 150.73393440246582 for validation audios
0.7988057116003499
Epoch: 148/200
Train loss: 2.0966
Time needed: 152.66094875335693 for validation audios
0.7976125838622854
Epoch: 149/200
Train loss: 2.1448
Time needed: 149.5250642299652 for validation audios
0.79728727972627
Epoch: 150/200
Train loss: 2.1489
Time needed: 150.3124966621399 for validation audios
0.7996944897311391
Epoch: 151/200
Train loss: 2.1407
Time needed: 149.7519497871399 for validation audios
0.7974252755264756
Epoch: 152/200
Train loss: 2.1179
Time needed: 149.1892340183258 for validation audios
0.7994774299804908
Epoch: 153/200
Train loss: 2.1055
Time needed: 149.39598679542542 for validation audios
0.7999431985074784
Epoch: 154/200
Train loss: 2.1157
Time needed: 149.76234483718872 for validation audios
0.7989258734965576
Epoch: 155/200
Train loss: 2.1080
Time needed: 149.56593775749207 for validation audios
0.8007184589951525
Epoch: 156/200
Train loss: 2.0841
Time needed: 150.14700436592102 for validation audios
0.7987688600910099
Epoch: 157/200
Train loss: 2.1302
Time needed: 150.76598262786865 for validation audios
0.8006587836707862
Epoch: 158/200
Train loss: 2.1553
Time needed: 150.1542637348175 for validation audios
0.7981436087629263
Epoch: 159/200
Train loss: 2.1201
Time needed: 150.44499397277832 for validation audios
0.8003655269406458
Epoch: 160/200
Train loss: 2.1307
Time needed: 151.2365424633026 for validation audios
0.7991485367634956
Epoch: 161/200
Train loss: 2.1478
Time needed: 149.68865394592285 for validation audios
0.7964114358294067
Epoch: 162/200
Train loss: 2.0654
Time needed: 151.09062504768372 for validation audios
0.8023560618668558
Model improve: 0.8007 -> 0.8024
Epoch: 163/200
Train loss: 2.1150
Time needed: 149.46985340118408 for validation audios
0.8010791927485876
Epoch: 164/200
Train loss: 2.1906
Time needed: 153.44396114349365 for validation audios
0.7994322323438995
Epoch: 165/200
Train loss: 2.1071
Time needed: 150.54544615745544 for validation audios
0.8006516627447643
Epoch: 166/200
Train loss: 2.0891
Time needed: 151.2755355834961 for validation audios
0.8016205633124561
Epoch: 167/200
Train loss: 2.0416
Time needed: 152.16235041618347 for validation audios
0.8026909798130355
Model improve: 0.8024 -> 0.8027
Epoch: 168/200
Train loss: 2.1278
Time needed: 151.04008412361145 for validation audios
0.7991379811103455
Epoch: 169/200
Train loss: 2.0701
Time needed: 150.11895179748535 for validation audios
0.8022863520519812
Epoch: 170/200
Train loss: 2.0605
Time needed: 149.90934944152832 for validation audios
0.7995574009957186
Epoch: 171/200
Train loss: 2.1245
Time needed: 150.3281180858612 for validation audios
0.8001249314436785
Epoch: 172/200
Train loss: 2.1060
Time needed: 151.70577812194824 for validation audios
0.8006960644763618
Epoch: 173/200
Train loss: 2.1154
Time needed: 150.17531967163086 for validation audios
0.8025375534620895
Epoch: 174/200
Train loss: 2.0715
Time needed: 150.04416513442993 for validation audios
0.8012868454158212
Epoch: 175/200
Train loss: 2.0628
Time needed: 149.8299024105072 for validation audios
0.8038311237600667
Model improve: 0.8027 -> 0.8038
Epoch: 176/200
Train loss: 2.1124
Time needed: 150.3627736568451 for validation audios
0.801033827659375
Epoch: 177/200
Train loss: 2.0665
Time needed: 151.91570591926575 for validation audios
0.8014154366144592
Epoch: 178/200
Train loss: 2.0788
Time needed: 152.47680807113647 for validation audios
0.8013024100230828
Epoch: 179/200
Train loss: 2.1227
Time needed: 149.35197162628174 for validation audios
0.8023469218771403
Epoch: 180/200
Train loss: 2.0284
Time needed: 149.24700713157654 for validation audios
0.8032032255922135
Epoch: 181/200
Train loss: 2.0883
Time needed: 149.93927526474 for validation audios
0.8004046888826252
Epoch: 182/200
Train loss: 2.1399
Time needed: 150.42284417152405 for validation audios
0.7990290538655477
Epoch: 183/200
Train loss: 2.0950
Time needed: 157.9127655029297 for validation audios
0.8017667881711201
Epoch: 184/200
Train loss: 2.1549
Time needed: 149.98061633110046 for validation audios
0.8016636040815212
Epoch: 185/200
Train loss: 2.0303
Time needed: 150.26060938835144 for validation audios
0.8009817966420777
Epoch: 186/200
Date :04/27/2023, 00:19:45
Duration: 5
Sample rate: 32000
nfft: 1024
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 5
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13615
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 13.5266
Epoch: 2/200
Train loss: 4.4846
Epoch: 3/200
Train loss: 4.0692
Epoch: 4/200
Train loss: 3.8120
Epoch: 5/200
Train loss: 3.6288
Epoch: 6/200
Train loss: 3.4420
Epoch: 7/200
Train loss: 3.4175
Epoch: 8/200
Train loss: 3.2625
Epoch: 9/200
Train loss: 3.2182
Epoch: 10/200
Train loss: 3.1930
Epoch: 11/200
Train loss: 3.1363
Epoch: 12/200
Train loss: 3.0714
Epoch: 13/200
Train loss: 2.9314
Epoch: 14/200
Train loss: 2.9439
Epoch: 15/200
Train loss: 2.9629
Epoch: 16/200
Train loss: 2.8990
Epoch: 17/200
Train loss: 2.8989
Epoch: 18/200
Train loss: 2.8623
Epoch: 19/200
Train loss: 2.8427
Epoch: 20/200
Train loss: 2.7705
Epoch: 21/200
Train loss: 2.7043
Epoch: 22/200
Train loss: 2.7341
Epoch: 23/200
Train loss: 2.7834
Epoch: 24/200
Train loss: 2.7504
Epoch: 25/200
Train loss: 2.7470
Epoch: 26/200
Train loss: 2.6770
Epoch: 27/200
Train loss: 2.6348
Epoch: 28/200
Train loss: 2.6878
Epoch: 29/200
Train loss: 2.6483
Epoch: 30/200
Train loss: 2.6105
Epoch: 31/200
Train loss: 2.6299
Epoch: 32/200
Train loss: 2.7265
Epoch: 33/200
Train loss: 2.5266
Epoch: 34/200
Train loss: 2.5953
Epoch: 35/200
Train loss: 2.5498
Epoch: 36/200
Train loss: 2.5188
Epoch: 37/200
Train loss: 2.5817
Epoch: 38/200
Train loss: 2.5334
Epoch: 39/200
Train loss: 2.4861
Epoch: 40/200
Train loss: 2.5052
Epoch: 41/200
Train loss: 2.4989
Epoch: 42/200
Train loss: 2.5475
Epoch: 43/200
Train loss: 2.5639
Epoch: 44/200
Train loss: 2.5161
Epoch: 45/200
Train loss: 2.4210
Epoch: 46/200
Train loss: 2.4349
Epoch: 47/200
Train loss: 2.4564
Epoch: 48/200
Train loss: 2.4903
Epoch: 49/200
Train loss: 2.4782
Epoch: 50/200
Train loss: 2.4981
Epoch: 51/200
Train loss: 2.4568
Epoch: 52/200
Train loss: 2.4507
Time needed: 152.95089650154114 for validation audios
0.7908865030258745
Model improve: 0.0000 -> 0.7909
Epoch: 53/200
Train loss: 2.3514
Time needed: 159.75855898857117 for validation audios
0.7905000681334712
Epoch: 54/200
Train loss: 2.4518
Time needed: 153.6190047264099 for validation audios
0.7911305077496774
Model improve: 0.7909 -> 0.7911
Epoch: 55/200
Train loss: 2.3644
Time needed: 155.11171579360962 for validation audios
0.794355610096579
Model improve: 0.7911 -> 0.7944
Epoch: 56/200
Train loss: 2.4257
Time needed: 155.53399062156677 for validation audios
0.7906244601558589
Epoch: 57/200
Train loss: 2.4320
Time needed: 153.32849502563477 for validation audios
0.7900174795922439
Epoch: 58/200
Train loss: 2.3995
Time needed: 153.08778023719788 for validation audios
0.7922984476686672
Epoch: 59/200
Train loss: 2.3398
Time needed: 155.40790271759033 for validation audios
0.7923566209364405
Epoch: 60/200
Train loss: 2.3467
Time needed: 154.6814637184143 for validation audios
0.7923256099339239
Epoch: 61/200
Train loss: 2.3394
Time needed: 155.16407465934753 for validation audios
0.7900808624549694
Epoch: 62/200
Train loss: 2.3717
Time needed: 154.16853427886963 for validation audios
0.7894951325386657
Epoch: 63/200
Train loss: 2.3936
Time needed: 153.66044211387634 for validation audios
0.7905401089666347
Epoch: 64/200
Train loss: 2.3384
Time needed: 154.02133560180664 for validation audios
0.7935779594748535
Epoch: 65/200
Train loss: 2.3618
Time needed: 154.06599259376526 for validation audios
0.7956026059230816
Model improve: 0.7944 -> 0.7956
Epoch: 66/200
Train loss: 2.3821
Time needed: 153.5104043483734 for validation audios
0.7965750281304654
Model improve: 0.7956 -> 0.7966
Epoch: 67/200
Train loss: 2.3575
Time needed: 154.0559275150299 for validation audios
0.7905026318804351
Epoch: 68/200
Train loss: 2.3050
Time needed: 155.91658639907837 for validation audios
0.7938002815211357
Epoch: 69/200
Train loss: 2.3053
Time needed: 154.1189022064209 for validation audios
0.7911665429520448
Epoch: 70/200
Train loss: 2.3524
Time needed: 153.46530628204346 for validation audios
0.7907864813027174
Epoch: 71/200
Train loss: 2.3435
Time needed: 155.58647274971008 for validation audios
0.7950420068016961
Epoch: 72/200
Train loss: 2.3333
Time needed: 153.92649054527283 for validation audios
0.792569452861191
Epoch: 73/200
Train loss: 2.3682
Date :04/27/2023, 03:21:04
Duration: 5
Sample rate: 32000
nfft: 1024
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 15.7481
Epoch: 2/200
Train loss: 4.8025
Epoch: 3/200
Train loss: 4.2284
Epoch: 4/200
Train loss: 3.8866
Epoch: 5/200
Train loss: 3.6567
Epoch: 6/200
Train loss: 3.4194
Epoch: 7/200
Train loss: 3.3586
Epoch: 8/200
Train loss: 3.2370
Epoch: 9/200
Train loss: 3.1571
Epoch: 10/200
Train loss: 3.1044
Epoch: 11/200
Train loss: 3.0553
Epoch: 12/200
Train loss: 2.9435
Epoch: 13/200
Train loss: 2.8335
Epoch: 14/200
Train loss: 2.8422
Epoch: 15/200
Train loss: 2.8589
Epoch: 16/200
Train loss: 2.7770
Epoch: 17/200
Train loss: 2.7860
Epoch: 18/200
Train loss: 2.7373
Epoch: 19/200
Train loss: 2.7760
Epoch: 20/200
Train loss: 2.6057
Epoch: 21/200
Train loss: 2.6161
Epoch: 22/200
Train loss: 2.6281
Epoch: 23/200
Train loss: 2.6507
Epoch: 24/200
Train loss: 2.6592
Epoch: 25/200
Train loss: 2.6001
Epoch: 26/200
Train loss: 2.5061
Epoch: 27/200
Train loss: 2.5627
Epoch: 28/200
Train loss: 2.5029
Epoch: 29/200
Train loss: 2.5348
Epoch: 30/200
Train loss: 2.4839
Epoch: 31/200
Train loss: 2.5688
Epoch: 32/200
Train loss: 2.4776
Epoch: 33/200
Train loss: 2.4476
Epoch: 34/200
Train loss: 2.3883
Epoch: 35/200
Train loss: 2.4086
Epoch: 36/200
Train loss: 2.4490
Epoch: 37/200
Train loss: 2.4586
Epoch: 38/200
Train loss: 2.3165
Epoch: 39/200
Train loss: 2.3923
Epoch: 40/200
Train loss: 2.3376
Epoch: 41/200
Train loss: 2.3958
Epoch: 42/200
Train loss: 2.4281
Epoch: 43/200
Train loss: 2.3781
Epoch: 44/200
Train loss: 2.3214
Epoch: 45/200
Train loss: 2.3287
Epoch: 46/200
Train loss: 2.3337
Epoch: 47/200
Train loss: 2.3520
Epoch: 48/200
Train loss: 2.3646
Epoch: 49/200
Train loss: 2.3773
Epoch: 50/200
Train loss: 2.3316
Epoch: 51/200
Train loss: 2.3684
Epoch: 52/200
Train loss: 2.2189
Time needed: 168.5086407661438 for validation audios
0.800904673428552
Model improve: 0.0000 -> 0.8009
Epoch: 53/200
Train loss: 2.3483
Time needed: 172.23031091690063 for validation audios
0.8002697947542521
Epoch: 54/200
Train loss: 2.2723
Time needed: 168.68130469322205 for validation audios
0.7976542466040903
Epoch: 55/200
Train loss: 2.2724
Time needed: 170.2416751384735 for validation audios
0.7932332824195818
Epoch: 56/200
Train loss: 2.2668
Time needed: 168.8006989955902 for validation audios
0.7989588366859492
Epoch: 57/200
Train loss: 2.3097
Time needed: 168.3731586933136 for validation audios
0.7977805896163341
Epoch: 58/200
Train loss: 2.2277
Time needed: 170.8098123073578 for validation audios
0.79859750268938
Epoch: 59/200
Train loss: 2.2178
Time needed: 171.93783044815063 for validation audios
0.799923450325537
Epoch: 60/200
Train loss: 2.2216
Time needed: 169.40307307243347 for validation audios
0.7962587736762403
Epoch: 61/200
Train loss: 2.2491
Time needed: 167.18805480003357 for validation audios
0.7978855670894668
Epoch: 62/200
Train loss: 2.2821
Time needed: 169.21414637565613 for validation audios
0.7972328651383304
Epoch: 63/200
Train loss: 2.2323
Time needed: 165.42573070526123 for validation audios
0.7948206025644513
Epoch: 64/200
Train loss: 2.2444
Time needed: 168.3904230594635 for validation audios
0.7952748537112745
Epoch: 65/200
Train loss: 2.2226
Time needed: 169.18041467666626 for validation audios
0.7984128637800557
Epoch: 66/200
Train loss: 2.2319
Time needed: 170.1438548564911 for validation audios
0.7990483194028071
Epoch: 67/200
Train loss: 2.1612
Time needed: 168.62205028533936 for validation audios
0.8001626277962662
Epoch: 68/200
Train loss: 2.2329
Time needed: 167.49408841133118 for validation audios
0.7980334175665954
Epoch: 69/200
Train loss: 2.2173
Time needed: 171.55941104888916 for validation audios
0.797003785373325
Epoch: 70/200
Train loss: 2.2149
Time needed: 169.43192601203918 for validation audios
0.7943442245835003
Epoch: 71/200
Train loss: 2.2094
Time needed: 171.38386750221252 for validation audios
0.7958592371968815
Epoch: 72/200
Train loss: 2.2458
Time needed: 168.2571997642517 for validation audios
0.7956659947920958
Epoch: 73/200
Train loss: 2.2319
Time needed: 169.5474328994751 for validation audios
0.7955007844032577
Epoch: 74/200
Train loss: 2.2286
Time needed: 170.10697960853577 for validation audios
0.7969570979875112
Epoch: 75/200
Train loss: 2.1600
Time needed: 168.7129271030426 for validation audios
0.7941188561594733
Epoch: 76/200
Train loss: 2.1393
Time needed: 167.47357511520386 for validation audios
0.7918807482690712
Epoch: 77/200
Train loss: 2.1152
Date :04/27/2023, 07:08:12
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 15.1001
Time needed: 166.8563129901886 for validation audios
0.37742031956913635
Model improve: 0.0000 -> 0.3774
Epoch: 2/200
Train loss: 4.8541
Time needed: 166.44497203826904 for validation audios
0.5422524281081764
Model improve: 0.3774 -> 0.5423
Epoch: 3/200
Train loss: 4.2371
Time needed: 166.8882598876953 for validation audios
0.6179176543603234
Model improve: 0.5423 -> 0.6179
Epoch: 4/200
Date :04/27/2023, 07:24:31
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 15.1084
Time needed: 169.1879448890686 for validation audios
0.3715273606052958
Model improve: 0.0000 -> 0.3715
Epoch: 2/200
Train loss: 4.8817
Time needed: 172.96535301208496 for validation audios
0.5377984348169161
Model improve: 0.3715 -> 0.5378
Epoch: 3/200
Train loss: 4.2510
Time needed: 165.6422016620636 for validation audios
0.6166148698335531
Model improve: 0.5378 -> 0.6166
Epoch: 4/200
Train loss: 3.9011
Time needed: 165.71274590492249 for validation audios
0.6590514871841515
Model improve: 0.6166 -> 0.6591
Epoch: 5/200
Date :04/27/2023, 07:45:29
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 15.1531
Time needed: 202.93195724487305 for validation audios
0.3350949085055967
Model improve: 0.0000 -> 0.3351
Epoch: 2/200
Date :04/27/2023, 07:51:22
Duration: 5
Sample rate: 32000
nfft: 512
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 14.6554
Time needed: 164.72279572486877 for validation audios
0.3489742388154895
Model improve: 0.0000 -> 0.3490
Epoch: 2/200
Train loss: 5.0188
Time needed: 168.2839949131012 for validation audios
0.5070182037105969
Model improve: 0.3490 -> 0.5070
Epoch: 3/200
Date :04/27/2023, 08:01:05
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 256
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
Date :04/27/2023, 08:02:56
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 512
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 15.9972
Time needed: 152.353910446167 for validation audios
0.34183878989373645
Model improve: 0.0000 -> 0.3418
Epoch: 2/200
Date :04/27/2023, 08:07:26
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 128
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 14.2266
Time needed: 209.2534728050232 for validation audios
0.29666037175694604
Model improve: 0.0000 -> 0.2967
Epoch: 2/200
Date :04/27/2023, 08:14:11
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 1280
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 17.8015
Time needed: 150.42414903640747 for validation audios
0.30562318640229597
Model improve: 0.0000 -> 0.3056
Epoch: 2/200
Train loss: 5.3389
Time needed: 149.17422223091125 for validation audios
0.4086059977134418
Model improve: 0.3056 -> 0.4086
Epoch: 3/200
Train loss: 4.8879
Time needed: 152.67951250076294 for validation audios
0.4807987795845745
Model improve: 0.4086 -> 0.4808
Epoch: 4/200
Date :04/27/2023, 08:26:12
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 14.9216
Time needed: 163.9167447090149 for validation audios
0.38668972394430834
Model improve: 0.0000 -> 0.3867
Epoch: 2/200
Train loss: 4.5951
Time needed: 167.10243582725525 for validation audios
0.5500956553860181
Model improve: 0.3867 -> 0.5501
Epoch: 3/200
Train loss: 3.8452
Time needed: 167.53728652000427 for validation audios
0.6244756703929154
Model improve: 0.5501 -> 0.6245
Epoch: 4/200
Train loss: 3.4898
Time needed: 166.62847113609314 for validation audios
0.6639870323277464
Model improve: 0.6245 -> 0.6640
Epoch: 5/200
Train loss: 3.1318
Time needed: 165.01546621322632 for validation audios
0.6979895163213897
Model improve: 0.6640 -> 0.6980
Epoch: 6/200
Train loss: 2.9800
Date :04/27/2023, 08:54:32
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 14.9409
Time needed: 167.78968453407288 for validation audios
0.35261926307671576
Model improve: 0.0000 -> 0.3526
Epoch: 2/200
Train loss: 4.8762
Time needed: 169.60152316093445 for validation audios
0.5203192917468611
Model improve: 0.3526 -> 0.5203
Epoch: 3/200
Train loss: 4.1982
Time needed: 165.60557460784912 for validation audios
0.5965747921859277
Model improve: 0.5203 -> 0.5966
Epoch: 4/200
Train loss: 3.8259
Time needed: 168.44708037376404 for validation audios
0.6421159558942243
Model improve: 0.5966 -> 0.6421
Epoch: 5/200
Train loss: 3.4840
Time needed: 164.78962016105652 for validation audios
0.6771689379505663
Model improve: 0.6421 -> 0.6772
Epoch: 6/200
Train loss: 3.3123
Time needed: 167.4462125301361 for validation audios
0.6976341424525796
Model improve: 0.6772 -> 0.6976
Epoch: 7/200
Train loss: 3.1793
Time needed: 166.93441677093506 for validation audios
0.7134176464021239
Model improve: 0.6976 -> 0.7134
Epoch: 8/200
Train loss: 3.0304
Time needed: 165.86374044418335 for validation audios
0.7301512799500034
Model improve: 0.7134 -> 0.7302
Epoch: 9/200
Train loss: 2.9920
Time needed: 165.81789255142212 for validation audios
0.7375549609384697
Model improve: 0.7302 -> 0.7376
Epoch: 10/200
Train loss: 2.8303
Time needed: 167.2914535999298 for validation audios
0.7461461789577672
Model improve: 0.7376 -> 0.7461
Epoch: 11/200
Date :04/27/2023, 09:43:49
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 15.1075
Time needed: 167.8063509464264 for validation audios
0.37222461637327914
Model improve: 0.0000 -> 0.3722
Epoch: 2/200
Train loss: 4.8799
Time needed: 166.94075870513916 for validation audios
0.5382588873667072
Model improve: 0.3722 -> 0.5383
Epoch: 3/200
Train loss: 4.2501
Time needed: 167.42867946624756 for validation audios
0.6164956288405623
Model improve: 0.5383 -> 0.6165
Epoch: 4/200
Train loss: 3.9003
Time needed: 170.010648727417 for validation audios
0.6588810822737734
Model improve: 0.6165 -> 0.6589
Epoch: 5/200
Train loss: 3.6513
Time needed: 166.37334060668945 for validation audios
0.6897397505832201
Model improve: 0.6589 -> 0.6897
Epoch: 6/200
Train loss: 3.4198
Time needed: 165.7616412639618 for validation audios
0.7051499114987174
Model improve: 0.6897 -> 0.7051
Epoch: 7/200
Train loss: 3.3518
Time needed: 167.74848413467407 for validation audios
0.7218079246900294
Model improve: 0.7051 -> 0.7218
Epoch: 8/200
Train loss: 3.2049
Time needed: 166.8886594772339 for validation audios
0.7339189434822309
Model improve: 0.7218 -> 0.7339
Epoch: 9/200
Train loss: 3.1450
Time needed: 169.14556622505188 for validation audios
0.7429379961788607
Model improve: 0.7339 -> 0.7429
Epoch: 10/200
Train loss: 3.0762
Time needed: 166.23399996757507 for validation audios
0.7529556328165006
Model improve: 0.7429 -> 0.7530
Epoch: 11/200
Train loss: 3.0486
Time needed: 168.10075998306274 for validation audios
0.7529528648270769
Epoch: 12/200
Train loss: 2.9312
Time needed: 168.41754364967346 for validation audios
0.7625882081599141
Model improve: 0.7530 -> 0.7626
Epoch: 13/200
Date :04/27/2023, 10:43:52
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 14.7532
Epoch: 2/200
Train loss: 4.5951
Epoch: 3/200
Train loss: 3.8671
Epoch: 4/200
Train loss: 3.4775
Epoch: 5/200
Train loss: 3.1349
Epoch: 6/200
Train loss: 2.9594
Epoch: 7/200
Train loss: 2.8557
Epoch: 8/200
Train loss: 2.6942
Epoch: 9/200
Train loss: 2.6310
Epoch: 10/200
Train loss: 2.4722
Epoch: 11/200
Train loss: 2.4543
Epoch: 12/200
Train loss: 2.3303
Epoch: 13/200
Train loss: 2.3384
Epoch: 14/200
Train loss: 2.3548
Epoch: 15/200
Train loss: 2.3405
Epoch: 16/200
Train loss: 2.0904
Epoch: 17/200
Train loss: 2.2047
Epoch: 18/200
Train loss: 2.1391
Epoch: 19/200
Train loss: 2.1147
Epoch: 20/200
Train loss: 2.0826
Epoch: 21/200
Train loss: 2.0444
Epoch: 22/200
Train loss: 2.0045
Epoch: 23/200
Train loss: 1.9496
Epoch: 24/200
Train loss: 1.9047
Epoch: 25/200
Train loss: 1.9767
Epoch: 26/200
Train loss: 1.9476
Epoch: 27/200
Train loss: 1.9206
Epoch: 28/200
Train loss: 1.8838
Epoch: 29/200
Train loss: 1.8204
Epoch: 30/200
Train loss: 1.9248
Epoch: 31/200
Train loss: 1.8979
Epoch: 32/200
Train loss: 1.8937
Epoch: 33/200
Train loss: 1.8238
Epoch: 34/200
Train loss: 1.8801
Epoch: 35/200
Train loss: 1.8172
Epoch: 36/200
Train loss: 1.8023
Epoch: 37/200
Train loss: 1.8638
Epoch: 38/200
Train loss: 1.7328
Epoch: 39/200
Train loss: 1.7537
Epoch: 40/200
Train loss: 1.7015
Epoch: 41/200
Train loss: 1.8287
Epoch: 42/200
Train loss: 1.7595
Epoch: 43/200
Train loss: 1.7569
Epoch: 44/200
Train loss: 1.7233
Epoch: 45/200
Train loss: 1.7594
Epoch: 46/200
Train loss: 1.7608
Epoch: 47/200
Train loss: 1.7589
Epoch: 48/200
Train loss: 1.6673
Epoch: 49/200
Train loss: 1.7773
Epoch: 50/200
Train loss: 1.6529
Epoch: 51/200
Train loss: 1.6833
Epoch: 52/200
Train loss: 1.6367
Time needed: 164.39699244499207 for validation audios
0.7970370421990025
Model improve: 0.0000 -> 0.7970
Epoch: 53/200
Train loss: 1.6915
Time needed: 166.00622820854187 for validation audios
0.7979842235569846
Model improve: 0.7970 -> 0.7980
Epoch: 54/200
Train loss: 1.6548
Time needed: 167.34234070777893 for validation audios
0.7958682847560836
Epoch: 55/200
Train loss: 1.5705
Time needed: 166.241379737854 for validation audios
0.7989268161401744
Model improve: 0.7980 -> 0.7989
Epoch: 56/200
Train loss: 1.7020
Time needed: 167.49585151672363 for validation audios
0.79707071637963
Epoch: 57/200
Train loss: 1.6090
Time needed: 169.10929203033447 for validation audios
0.7960123230082612
Epoch: 58/200
Train loss: 1.7438
Time needed: 164.90864396095276 for validation audios
0.7936160832368918
Epoch: 59/200
Train loss: 1.6648
Time needed: 168.04815697669983 for validation audios
0.7984271323222235
Epoch: 60/200
Train loss: 1.6084
Time needed: 165.29784321784973 for validation audios
0.794088641663318
Epoch: 61/200
Train loss: 1.6361
Time needed: 164.46732926368713 for validation audios
0.7951915829809142
Epoch: 62/200
Train loss: 1.5911
Time needed: 165.77327847480774 for validation audios
0.7957141204086339
Epoch: 63/200
Train loss: 1.6220
Time needed: 164.2993860244751 for validation audios
0.7947626199837653
Epoch: 64/200
Train loss: 1.6328
Time needed: 165.45045638084412 for validation audios
0.7961171529856824
Epoch: 65/200
Train loss: 1.5533
Time needed: 164.3863024711609 for validation audios
0.7941184912344772
Epoch: 66/200
Train loss: 1.6506
Time needed: 165.30530285835266 for validation audios
0.7964578493504633
Epoch: 67/200
Train loss: 1.6204
Time needed: 166.86284041404724 for validation audios
0.7977054172290973
Epoch: 68/200
Train loss: 1.6451
Date :04/27/2023, 13:45:42
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Date :04/27/2023, 13:46:00
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 15.1381
Epoch: 2/200
Date :04/27/2023, 13:49:59
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 14.9302
Time needed: 208.33209991455078 for validation audios
0.3211281017662241
Model improve: 0.0000 -> 0.3211
Epoch: 2/200
Train loss: 4.7432
Time needed: 206.94393372535706 for validation audios
0.45041738682911775
Model improve: 0.3211 -> 0.4504
Epoch: 3/200
Date :04/27/2023, 14:02:31
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 14.9484
Time needed: 208.53913474082947 for validation audios
0.3406476159711082
Model improve: 0.0000 -> 0.3406
Epoch: 2/200
Date :04/27/2023, 14:09:03
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 14.9466
Time needed: 206.4532823562622 for validation audios
0.3384545963978518
Model improve: 0.0000 -> 0.3385
Epoch: 2/200
Date :04/27/2023, 14:16:28
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 14.6003
Time needed: 206.80598068237305 for validation audios
0.23914555481918728
Model improve: 0.0000 -> 0.2391
Epoch: 2/200
Date :04/27/2023, 14:23:30
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 14.8739
Time needed: 207.39951062202454 for validation audios
0.3530845354445172
Model improve: 0.0000 -> 0.3531
Epoch: 2/200
Train loss: 4.7198
Time needed: 206.35423851013184 for validation audios
0.5010442249501089
Model improve: 0.3531 -> 0.5010
Epoch: 3/200
Train loss: 4.0050
Time needed: 205.41861653327942 for validation audios
0.5719834241774061
Model improve: 0.5010 -> 0.5720
Epoch: 4/200
Train loss: 3.6479
Time needed: 208.80623149871826 for validation audios
0.6190256377081891
Model improve: 0.5720 -> 0.6190
Epoch: 5/200
Train loss: 3.3275
Time needed: 205.63414764404297 for validation audios
0.6446558969967932
Model improve: 0.6190 -> 0.6447
Epoch: 6/200
Train loss: 3.1608
Time needed: 206.75482940673828 for validation audios
0.6598269621899683
Model improve: 0.6447 -> 0.6598
Epoch: 7/200
Train loss: 3.0386
Date :04/27/2023, 15:03:42
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 150
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/150
Date :04/27/2023, 15:04:08
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 250
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/250
Train loss: 23.5501
Epoch: 2/250
Train loss: 5.4071
Epoch: 3/250
Train loss: 4.8725
Epoch: 4/250
Date :04/27/2023, 15:09:38
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 250
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/250
Train loss: 23.4615
Epoch: 2/250
Train loss: 5.2178
Epoch: 3/250
Train loss: 4.5719
Epoch: 4/250
Train loss: 4.0613
Epoch: 5/250
Train loss: 3.6989
Epoch: 6/250
Train loss: 3.3964
Epoch: 7/250
Train loss: 3.2665
Epoch: 8/250
Train loss: 3.0878
Epoch: 9/250
Train loss: 2.9979
Epoch: 10/250
Train loss: 2.6863
Epoch: 11/250
Train loss: 2.6906
Epoch: 12/250
Train loss: 2.7160
Epoch: 13/250
Train loss: 2.6194
Epoch: 14/250
Train loss: 2.5456
Epoch: 15/250
Train loss: 2.4608
Epoch: 16/250
Train loss: 2.4098
Epoch: 17/250
Train loss: 2.4226
Epoch: 18/250
Train loss: 2.3262
Epoch: 19/250
Train loss: 2.1181
Epoch: 20/250
Train loss: 2.3235
Epoch: 21/250
Train loss: 2.2874
Epoch: 22/250
Train loss: 2.1648
Epoch: 23/250
Train loss: 2.1225
Epoch: 24/250
Train loss: 2.0659
Epoch: 25/250
Train loss: 2.0998
Epoch: 26/250
Train loss: 2.1219
Epoch: 27/250
Train loss: 2.0810
Epoch: 28/250
Train loss: 2.1549
Epoch: 29/250
Train loss: 2.0846
Epoch: 30/250
Train loss: 2.1219
Epoch: 31/250
Train loss: 1.8689
Epoch: 32/250
Train loss: 1.8346
Epoch: 33/250
Train loss: 1.9712
Epoch: 34/250
Train loss: 1.9905
Epoch: 35/250
Train loss: 1.9266
Epoch: 36/250
Train loss: 1.9096
Epoch: 37/250
Train loss: 1.8351
Epoch: 38/250
Train loss: 1.9036
Epoch: 39/250
Train loss: 1.8104
Epoch: 40/250
Train loss: 1.8789
Epoch: 41/250
Train loss: 1.8283
Epoch: 42/250
Train loss: 1.8086
Epoch: 43/250
Train loss: 1.8500
Epoch: 44/250
Train loss: 1.7636
Epoch: 45/250
Train loss: 1.8221
Epoch: 46/250
Train loss: 1.6769
Epoch: 47/250
Train loss: 1.6225
Epoch: 48/250
Train loss: 1.7412
Epoch: 49/250
Train loss: 1.8003
Epoch: 50/250
Train loss: 1.7280
Epoch: 51/250
Train loss: 1.7162
Epoch: 52/250
Train loss: 1.8116
Epoch: 53/250
Train loss: 1.8406
Epoch: 54/250
Train loss: 1.5898
Epoch: 55/250
Train loss: 1.7396
Epoch: 56/250
Train loss: 1.6829
Epoch: 57/250
Train loss: 1.5913
Epoch: 58/250
Train loss: 1.6930
Epoch: 59/250
Train loss: 1.7435
Epoch: 60/250
Train loss: 1.7207
Epoch: 61/250
Train loss: 1.7268
Epoch: 62/250
Train loss: 1.7282
Epoch: 63/250
Train loss: 1.7188
Epoch: 64/250
Train loss: 1.6830
Epoch: 65/250
Train loss: 1.6140
Epoch: 66/250
Train loss: 1.6643
Epoch: 67/250
Train loss: 1.6724
Epoch: 68/250
Train loss: 1.7280
Epoch: 69/250
Train loss: 1.6454
Epoch: 70/250
Train loss: 1.6027
Epoch: 71/250
Train loss: 1.6360
Epoch: 72/250
Train loss: 1.5899
Epoch: 73/250
Train loss: 1.7073
Epoch: 74/250
Train loss: 1.6421
Epoch: 75/250
Train loss: 1.4762
Epoch: 76/250
Train loss: 1.6551
Epoch: 77/250
Train loss: 1.4589
Epoch: 78/250
Train loss: 1.6701
Epoch: 79/250
Train loss: 1.4754
Epoch: 80/250
Train loss: 1.5678
Epoch: 81/250
Train loss: 1.7068
Epoch: 82/250
Train loss: 1.6342
Epoch: 83/250
Train loss: 1.5280
Epoch: 84/250
Train loss: 1.6468
Epoch: 85/250
Train loss: 1.6140
Epoch: 86/250
Train loss: 1.5416
Epoch: 87/250
Train loss: 1.5381
Epoch: 88/250
Train loss: 1.5584
Epoch: 89/250
Train loss: 1.5828
Epoch: 90/250
Train loss: 1.5977
Epoch: 91/250
Train loss: 1.5432
Epoch: 92/250
Train loss: 1.6252
Epoch: 93/250
Train loss: 1.5578
Epoch: 94/250
Train loss: 1.6217
Epoch: 95/250
Train loss: 1.6119
Epoch: 96/250
Train loss: 1.3950
Epoch: 97/250
Train loss: 1.6334
Epoch: 98/250
Train loss: 1.5737
Epoch: 99/250
Train loss: 1.4130
Epoch: 100/250
Train loss: 1.5367
Epoch: 101/250
Train loss: 1.4105
Epoch: 102/250
Train loss: 1.5978
Epoch: 103/250
Train loss: 1.4101
Epoch: 104/250
Train loss: 1.5165
Epoch: 105/250
Train loss: 1.5127
Epoch: 106/250
Train loss: 1.4937
Epoch: 107/250
Train loss: 1.4848
Epoch: 108/250
Train loss: 1.4878
Epoch: 109/250
Train loss: 1.4609
Epoch: 110/250
Train loss: 1.3691
Epoch: 111/250
Train loss: 1.5319
Epoch: 112/250
Train loss: 1.5511
Epoch: 113/250
Train loss: 1.4192
Epoch: 114/250
Train loss: 1.4958
Epoch: 115/250
Train loss: 1.5674
Epoch: 116/250
Train loss: 1.5762
Epoch: 117/250
Train loss: 1.5014
Epoch: 118/250
Train loss: 1.4866
Epoch: 119/250
Train loss: 1.4549
Epoch: 120/250
Train loss: 1.4371
Epoch: 121/250
Train loss: 1.4989
Epoch: 122/250
Train loss: 1.4212
Epoch: 123/250
Train loss: 1.3918
Epoch: 124/250
Train loss: 1.4901
Epoch: 125/250
Train loss: 1.5786
Epoch: 126/250
Train loss: 1.3526
Epoch: 127/250
Train loss: 1.5409
Epoch: 128/250
Train loss: 1.3957
Epoch: 129/250
Train loss: 1.3678
Epoch: 130/250
Train loss: 1.4226
Epoch: 131/250
Train loss: 1.4495
Epoch: 132/250
Train loss: 1.5133
Epoch: 133/250
Train loss: 1.4539
Epoch: 134/250
Train loss: 1.4656
Epoch: 135/250
Train loss: 1.5057
Epoch: 136/250
Train loss: 1.4377
Epoch: 137/250
Train loss: 1.4087
Epoch: 138/250
Train loss: 1.4318
Epoch: 139/250
Train loss: 1.4657
Epoch: 140/250
Train loss: 1.6534
Epoch: 141/250
Train loss: 1.3733
Epoch: 142/250
Train loss: 1.4412
Epoch: 143/250
Train loss: 1.3310
Epoch: 144/250
Train loss: 1.3241
Epoch: 145/250
Train loss: 1.6192
Epoch: 146/250
Train loss: 1.5836
Epoch: 147/250
Train loss: 1.4032
Epoch: 148/250
Train loss: 1.3094
Epoch: 149/250
Train loss: 1.4365
Epoch: 150/250
Train loss: 1.4448
Epoch: 151/250
Train loss: 1.4358
Epoch: 152/250
Train loss: 1.4608
Time needed: 165.0692048072815 for validation audios
0.803784276468067
Model improve: 0.0000 -> 0.8038
Epoch: 153/250
Train loss: 1.3093
Time needed: 164.21220755577087 for validation audios
0.8031292670248147
Epoch: 154/250
Train loss: 1.4465
Time needed: 166.14385962486267 for validation audios
0.8030395447787715
Epoch: 155/250
Train loss: 1.3773
Time needed: 166.60362935066223 for validation audios
0.8027909787046533
Epoch: 156/250
Train loss: 1.4370
Time needed: 166.25847268104553 for validation audios
0.8035190469733495
Epoch: 157/250
Train loss: 1.5390
Time needed: 164.67930459976196 for validation audios
0.8039502245212505
Model improve: 0.8038 -> 0.8040
Epoch: 158/250
Train loss: 1.3591
Time needed: 164.90753364562988 for validation audios
0.8034233108183442
Epoch: 159/250
Train loss: 1.4919
Time needed: 164.95872139930725 for validation audios
0.8030857618606918
Epoch: 160/250
Train loss: 1.3820
Time needed: 165.56561756134033 for validation audios
0.8047147048193184
Model improve: 0.8040 -> 0.8047
Epoch: 161/250
Train loss: 1.4726
Time needed: 164.23778128623962 for validation audios
0.80477074954785
Model improve: 0.8047 -> 0.8048
Epoch: 162/250
Train loss: 1.2686
Time needed: 165.61770033836365 for validation audios
0.8053881578486916
Model improve: 0.8048 -> 0.8054
Epoch: 163/250
Train loss: 1.5543
Time needed: 161.87268114089966 for validation audios
0.8056873945964914
Model improve: 0.8054 -> 0.8057
Epoch: 164/250
Train loss: 1.4429
Time needed: 163.00493907928467 for validation audios
0.8042746909751072
Epoch: 165/250
Train loss: 1.4391
Time needed: 164.89073395729065 for validation audios
0.8050884439717441
Epoch: 166/250
Train loss: 1.3640
Time needed: 166.47340369224548 for validation audios
0.8067685068366106
Model improve: 0.8057 -> 0.8068
Epoch: 167/250
Train loss: 1.4971
Time needed: 163.47220468521118 for validation audios
0.8058544843249996
Epoch: 168/250
Train loss: 1.3358
Time needed: 167.52521967887878 for validation audios
0.8042011815339281
Epoch: 169/250
Train loss: 1.2878
Time needed: 162.6460645198822 for validation audios
0.8060816302231553
Epoch: 170/250
Train loss: 1.6100
Time needed: 166.15084147453308 for validation audios
0.8068746371299713
Model improve: 0.8068 -> 0.8069
Epoch: 171/250
Train loss: 1.3956
Time needed: 165.60592818260193 for validation audios
0.8057860941482344
Epoch: 172/250
Train loss: 1.3973
Time needed: 163.4025776386261 for validation audios
0.8051369642829128
Epoch: 173/250
Train loss: 1.5235
Time needed: 165.83441925048828 for validation audios
0.8047539395305859
Epoch: 174/250
Train loss: 1.4821
Time needed: 163.30344557762146 for validation audios
0.8049813362927809
Epoch: 175/250
Train loss: 1.3523
Time needed: 163.19308853149414 for validation audios
0.804671419427004
Epoch: 176/250
Train loss: 1.4190
Time needed: 164.77508401870728 for validation audios
0.8069311356536194
Model improve: 0.8069 -> 0.8069
Epoch: 177/250
Train loss: 1.3663
Time needed: 164.40697312355042 for validation audios
0.8045472649397726
Epoch: 178/250
Train loss: 1.4935
Time needed: 164.00543403625488 for validation audios
0.8040009259210138
Epoch: 179/250
Train loss: 1.3875
Time needed: 164.39503598213196 for validation audios
0.8034801690098321
Epoch: 180/250
Train loss: 1.4230
Time needed: 164.0064799785614 for validation audios
0.8047574333261858
Epoch: 181/250
Train loss: 1.4362
Time needed: 166.48929953575134 for validation audios
0.8066179672736596
Epoch: 182/250
Train loss: 1.3309
Time needed: 162.8813874721527 for validation audios
0.805080750138464
Epoch: 183/250
Train loss: 1.3008
Time needed: 164.647287607193 for validation audios
0.8056305441337545
Epoch: 184/250
Train loss: 1.3350
Time needed: 165.62806153297424 for validation audios
0.805542051363779
Epoch: 185/250
Train loss: 1.3960
Time needed: 163.64143872261047 for validation audios
0.8048267392613258
Epoch: 186/250
Train loss: 1.3679
Time needed: 163.536452293396 for validation audios
0.8049853680754239
Epoch: 187/250
Train loss: 1.4602
Time needed: 163.60772967338562 for validation audios
0.80440635048631
Epoch: 188/250
Train loss: 1.4092
Time needed: 165.57985520362854 for validation audios
0.8047474137176127
Epoch: 189/250
Train loss: 1.3325
Time needed: 165.46582055091858 for validation audios
0.8046094870670294
Epoch: 190/250
Train loss: 1.3404
Time needed: 163.58857369422913 for validation audios
0.8043453326963909
Epoch: 191/250
Train loss: 1.4778
Time needed: 164.32431173324585 for validation audios
0.8050006099843747
Epoch: 192/250
Train loss: 1.4114
Time needed: 165.47541904449463 for validation audios
0.8041910673099683
Epoch: 193/250
Train loss: 1.3618
Time needed: 169.29765129089355 for validation audios
0.802095847029773
Epoch: 194/250
Train loss: 1.3119
Time needed: 164.24896478652954 for validation audios
0.8032742801201813
Epoch: 195/250
Train loss: 1.3409
Time needed: 163.4584276676178 for validation audios
0.805126441955446
Epoch: 196/250
Train loss: 1.4824
Time needed: 163.5422739982605 for validation audios
0.8041583175739576
Epoch: 197/250
Train loss: 1.2403
Time needed: 164.4657745361328 for validation audios
0.8043990368044905
Epoch: 198/250
Train loss: 1.3496
Time needed: 165.78463864326477 for validation audios
0.8030530667934388
Epoch: 199/250
Train loss: 1.3020
Time needed: 163.9452669620514 for validation audios
0.8047279863154493
Epoch: 200/250
Train loss: 1.4175
Time needed: 164.6997573375702 for validation audios
0.8035349435504175
Epoch: 201/250
Train loss: 1.5474
Time needed: 164.35331916809082 for validation audios
0.8034520721784911
Epoch: 202/250
Train loss: 1.3082
Time needed: 165.966135263443 for validation audios
0.8048405472415594
Epoch: 203/250
Train loss: 1.3158
Time needed: 165.377112865448 for validation audios
0.8041010449956529
Epoch: 204/250
Train loss: 1.2840
Time needed: 164.23721432685852 for validation audios
0.8052975013051945
Epoch: 205/250
Train loss: 1.2472
Time needed: 166.56421828269958 for validation audios
0.8049435567755321
Epoch: 206/250
Train loss: 1.4030
Time needed: 163.82829976081848 for validation audios
0.8048524755036285
Epoch: 207/250
Train loss: 1.2569
Time needed: 163.9795799255371 for validation audios
0.805244860853368
Epoch: 208/250
Train loss: 1.2811
Time needed: 163.22499227523804 for validation audios
0.8047277333853317
Epoch: 209/250
Train loss: 1.3820
Time needed: 164.27040076255798 for validation audios
0.8048481709160134
Epoch: 210/250
Train loss: 1.3332
Time needed: 163.19893598556519 for validation audios
0.8057456211257895
Epoch: 211/250
Train loss: 1.3813
Time needed: 163.5461230278015 for validation audios
0.8052422602987791
Epoch: 212/250
Train loss: 1.4365
Time needed: 164.03623032569885 for validation audios
0.8009447288823774
Epoch: 213/250
Train loss: 1.4769
Time needed: 162.88898086547852 for validation audios
0.8033409782350196
Epoch: 214/250
Train loss: 1.4798
Time needed: 162.56723594665527 for validation audios
0.804646738007718
Epoch: 215/250
Train loss: 1.3513
Time needed: 162.2905023097992 for validation audios
0.804877274656311
Epoch: 216/250
Train loss: 1.2718
Time needed: 165.08874917030334 for validation audios
0.805554992397389
Epoch: 217/250
Train loss: 1.2011
Time needed: 163.93481492996216 for validation audios
0.8055296070808405
Epoch: 218/250
Train loss: 1.3179
Time needed: 167.810462474823 for validation audios
0.8056050948279243
Epoch: 219/250
Train loss: 1.3728
Time needed: 167.12858629226685 for validation audios
0.805661283865384
Epoch: 220/250
Train loss: 1.3731
Time needed: 166.94222569465637 for validation audios
0.8057087458063378
Epoch: 221/250
Train loss: 1.3155
Time needed: 165.0231578350067 for validation audios
0.8048147469857659
Epoch: 222/250
Train loss: 1.3304
Time needed: 165.50707721710205 for validation audios
0.8046206268154575
Epoch: 223/250
Train loss: 1.4117
Time needed: 166.84475541114807 for validation audios
0.8051600185322098
Epoch: 224/250
Train loss: 1.3514
Time needed: 164.63433694839478 for validation audios
0.8058295196276456
Epoch: 225/250
Train loss: 1.3537
Time needed: 163.54708814620972 for validation audios
0.8055901428077656
Epoch: 226/250
Train loss: 1.3138
Time needed: 165.870014667511 for validation audios
0.8056731517616141
Epoch: 227/250
Train loss: 1.3566
Time needed: 162.18150448799133 for validation audios
0.8058012298618782
Epoch: 228/250
Train loss: 1.5619
Time needed: 164.68908977508545 for validation audios
0.8056534848063822
Epoch: 229/250
Train loss: 1.4065
Time needed: 166.34467220306396 for validation audios
0.8055681998980768
Epoch: 230/250
Train loss: 1.1967
Time needed: 164.86190724372864 for validation audios
0.8050800474643295
Epoch: 231/250
Train loss: 1.2789
Time needed: 164.02905821800232 for validation audios
0.8062465204923973
Epoch: 232/250
Train loss: 1.5508
Time needed: 166.38318753242493 for validation audios
0.8046314476386587
Epoch: 233/250
Train loss: 1.2507
Time needed: 164.2601671218872 for validation audios
0.8050266192293041
Epoch: 234/250
Train loss: 1.2307
Time needed: 164.84214973449707 for validation audios
0.8050478477944668
Epoch: 235/250
Train loss: 1.4514
Time needed: 165.16569709777832 for validation audios
0.8024876447447166
Epoch: 236/250
Train loss: 1.4462
Time needed: 164.369154214859 for validation audios
0.8058869142306042
Epoch: 237/250
Train loss: 1.3924
Time needed: 164.68607640266418 for validation audios
0.8039052454443075
Epoch: 238/250
Train loss: 1.3308
Time needed: 164.4587037563324 for validation audios
0.8048460906149955
Epoch: 239/250
Train loss: 1.3788
Time needed: 168.74094200134277 for validation audios
0.8059734386807198
Epoch: 240/250
Train loss: 1.2753
Time needed: 165.39715790748596 for validation audios
0.8064251094351419
Epoch: 241/250
Train loss: 1.3594
Time needed: 162.52143931388855 for validation audios
0.8041745220777567
Epoch: 242/250
Train loss: 1.2805
Time needed: 166.7142848968506 for validation audios
0.8059238258608816
Epoch: 243/250
Train loss: 1.4184
Time needed: 163.43596291542053 for validation audios
0.8041130281774884
Epoch: 244/250
Train loss: 1.1995
Time needed: 163.36602878570557 for validation audios
0.8052030305676903
Epoch: 245/250
Train loss: 1.4671
Time needed: 163.9655783176422 for validation audios
0.8050371526028374
Epoch: 246/250
Train loss: 1.3522
Time needed: 164.87204003334045 for validation audios
0.8068046679154521
Epoch: 247/250
Train loss: 1.2934
Time needed: 164.19738364219666 for validation audios
0.8059704829685898
Epoch: 248/250
Train loss: 1.3338
Time needed: 163.32802748680115 for validation audios
0.8057871108524643
Epoch: 249/250
Train loss: 1.4647
Time needed: 169.34335947036743 for validation audios
0.8043959095521308
Epoch: 250/250
Train loss: 1.4052
Time needed: 164.92850041389465 for validation audios
0.8047572330617597
Date :04/28/2023, 07:04:11
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 23.4638
Epoch: 2/200
Train loss: 5.2187
Epoch: 3/200
Train loss: 4.5730
Epoch: 4/200
Train loss: 4.0624
Epoch: 5/200
Train loss: 3.6998
Epoch: 6/200
Train loss: 3.3971
Epoch: 7/200
Train loss: 3.2674
Epoch: 8/200
Train loss: 3.0881
Epoch: 9/200
Train loss: 2.9979
Epoch: 10/200
Train loss: 2.6872
Epoch: 11/200
Train loss: 2.6906
Epoch: 12/200
Train loss: 2.7167
Epoch: 13/200
Train loss: 2.6190
Epoch: 14/200
Train loss: 2.5464
Epoch: 15/200
Train loss: 2.4613
Epoch: 16/200
Train loss: 2.4099
Epoch: 17/200
Train loss: 2.4225
Epoch: 18/200
Train loss: 2.3269
Epoch: 19/200
Date :04/28/2023, 07:31:37
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: False
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 22.7091
Epoch: 2/200
Train loss: 4.8607
Epoch: 3/200
Train loss: 3.9108
Epoch: 4/200
Train loss: 3.2545
Epoch: 5/200
Train loss: 2.7904
Epoch: 6/200
Train loss: 2.4343
Epoch: 7/200
Train loss: 2.1884
Epoch: 8/200
Train loss: 2.0075
Epoch: 9/200
Train loss: 1.8436
Epoch: 10/200
Train loss: 1.7134
Epoch: 11/200
Train loss: 1.5898
Epoch: 12/200
Train loss: 1.4869
Epoch: 13/200
Train loss: 1.3957
Epoch: 14/200
Train loss: 1.3206
Epoch: 15/200
Train loss: 1.2427
Epoch: 16/200
Train loss: 1.2007
Epoch: 17/200
Train loss: 1.1394
Epoch: 18/200
Train loss: 1.0951
Epoch: 19/200
Train loss: 1.0528
Epoch: 20/200
Train loss: 1.0228
Epoch: 21/200
Train loss: 0.9853
Epoch: 22/200
Train loss: 0.9409
Epoch: 23/200
Train loss: 0.8978
Epoch: 24/200
Train loss: 0.9018
Epoch: 25/200
Train loss: 0.8624
Epoch: 26/200
Train loss: 0.8193
Epoch: 27/200
Train loss: 0.8071
Epoch: 28/200
Train loss: 0.7985
Epoch: 29/200
Train loss: 0.7767
Epoch: 30/200
Train loss: 0.7511
Epoch: 31/200
Train loss: 0.7131
Epoch: 32/200
Train loss: 0.6977
Epoch: 33/200
Train loss: 0.6709
Epoch: 34/200
Train loss: 0.6620
Epoch: 35/200
Train loss: 0.6410
Epoch: 36/200
Train loss: 0.6437
Epoch: 37/200
Train loss: 0.6260
Epoch: 38/200
Train loss: 0.6097
Epoch: 39/200
Train loss: 0.6019
Epoch: 40/200
Train loss: 0.6005
Epoch: 41/200
Train loss: 0.5910
Epoch: 42/200
Train loss: 0.5760
Epoch: 43/200
Train loss: 0.5551
Epoch: 44/200
Train loss: 0.5679
Epoch: 45/200
Train loss: 0.5297
Epoch: 46/200
Train loss: 0.5202
Epoch: 47/200
Train loss: 0.5491
Epoch: 48/200
Train loss: 0.5191
Epoch: 49/200
Train loss: 0.5024
Epoch: 50/200
Train loss: 0.4872
Epoch: 51/200
Train loss: 0.5038
Epoch: 52/200
Train loss: 0.4705
Epoch: 53/200
Train loss: 0.4656
Epoch: 54/200
Train loss: 0.4683
Epoch: 55/200
Train loss: 0.4478
Epoch: 56/200
Train loss: 0.4648
Epoch: 57/200
Train loss: 0.4417
Epoch: 58/200
Train loss: 0.4284
Epoch: 59/200
Train loss: 0.4331
Epoch: 60/200
Train loss: 0.4258
Epoch: 61/200
Train loss: 0.4141
Epoch: 62/200
Train loss: 0.4112
Epoch: 63/200
Train loss: 0.4136
Epoch: 64/200
Train loss: 0.4192
Epoch: 65/200
Train loss: 0.3941
Epoch: 66/200
Train loss: 0.3933
Epoch: 67/200
Train loss: 0.3799
Epoch: 68/200
Train loss: 0.3868
Epoch: 69/200
Train loss: 0.3687
Epoch: 70/200
Train loss: 0.3628
Epoch: 71/200
Train loss: 0.3694
Epoch: 72/200
Train loss: 0.3820
Epoch: 73/200
Train loss: 0.3612
Epoch: 74/200
Train loss: 0.3512
Epoch: 75/200
Train loss: 0.3599
Epoch: 76/200
Train loss: 0.3625
Epoch: 77/200
Train loss: 0.3335
Epoch: 78/200
Train loss: 0.3509
Epoch: 79/200
Train loss: 0.3509
Epoch: 80/200
Train loss: 0.3395
Epoch: 81/200
Train loss: 0.3214
Epoch: 82/200
Train loss: 0.3154
Epoch: 83/200
Train loss: 0.3161
Epoch: 84/200
Train loss: 0.3194
Epoch: 85/200
Train loss: 0.3212
Epoch: 86/200
Train loss: 0.2970
Epoch: 87/200
Train loss: 0.3072
Epoch: 88/200
Train loss: 0.3101
Epoch: 89/200
Train loss: 0.3083
Epoch: 90/200
Train loss: 0.3014
Epoch: 91/200
Train loss: 0.2893
Epoch: 92/200
Train loss: 0.2876
Epoch: 93/200
Train loss: 0.2935
Epoch: 94/200
Train loss: 0.2851
Epoch: 95/200
Train loss: 0.2883
Epoch: 96/200
Train loss: 0.2922
Epoch: 97/200
Train loss: 0.2738
Epoch: 98/200
Train loss: 0.2814
Epoch: 99/200
Train loss: 0.2553
Epoch: 100/200
Train loss: 0.2911
Epoch: 101/200
Train loss: 0.2708
Epoch: 102/200
Train loss: 0.2601
Time needed: 163.34855437278748 for validation audios
0.779719854590159
Model improve: 0.0000 -> 0.7797
Epoch: 103/200
Train loss: 0.2685
Time needed: 164.50745105743408 for validation audios
0.7783461290521131
Epoch: 104/200
Train loss: 0.2650
Time needed: 168.662987947464 for validation audios
0.779547060360411
Epoch: 105/200
Train loss: 0.2552
Time needed: 164.72036242485046 for validation audios
0.7804258655353312
Model improve: 0.7797 -> 0.7804
Epoch: 106/200
Train loss: 0.2613
Time needed: 164.86489534378052 for validation audios
0.7795798191100874
Epoch: 107/200
Train loss: 0.2375
Time needed: 163.36891794204712 for validation audios
0.7774322491374399
Epoch: 108/200
Train loss: 0.2502
Time needed: 164.7156116962433 for validation audios
0.7785721821770497
Epoch: 109/200
Train loss: 0.2497
Date :04/28/2023, 10:27:54
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 14.9297
Epoch: 2/200
Train loss: 4.8629
Epoch: 3/200
Train loss: 4.2540
Epoch: 4/200
Train loss: 3.8836
Epoch: 5/200
Train loss: 3.6398
Epoch: 6/200
Train loss: 3.4019
Epoch: 7/200
Train loss: 3.3412
Epoch: 8/200
Train loss: 3.2187
Epoch: 9/200
Train loss: 3.1370
Epoch: 10/200
Train loss: 3.0902
Epoch: 11/200
Train loss: 3.0396
Epoch: 12/200
Train loss: 2.9294
Epoch: 13/200
Train loss: 2.8199
Epoch: 14/200
Train loss: 2.8299
Epoch: 15/200
Train loss: 2.8478
Epoch: 16/200
Train loss: 2.7658
Epoch: 17/200
Train loss: 2.7723
Epoch: 18/200
Train loss: 2.7245
Epoch: 19/200
Train loss: 2.7650
Epoch: 20/200
Train loss: 2.5945
Epoch: 21/200
Train loss: 2.6096
Epoch: 22/200
Train loss: 2.6141
Epoch: 23/200
Train loss: 2.6423
Epoch: 24/200
Train loss: 2.6521
Epoch: 25/200
Train loss: 2.5888
Epoch: 26/200
Train loss: 2.4970
Epoch: 27/200
Train loss: 2.5520
Epoch: 28/200
Train loss: 2.4957
Epoch: 29/200
Train loss: 2.5268
Epoch: 30/200
Train loss: 2.4748
Epoch: 31/200
Train loss: 2.5587
Epoch: 32/200
Train loss: 2.4703
Epoch: 33/200
Train loss: 2.4380
Epoch: 34/200
Train loss: 2.3771
Epoch: 35/200
Train loss: 2.4010
Epoch: 36/200
Train loss: 2.4401
Epoch: 37/200
Train loss: 2.4483
Epoch: 38/200
Train loss: 2.3186
Epoch: 39/200
Train loss: 2.3865
Epoch: 40/200
Train loss: 2.3327
Epoch: 41/200
Train loss: 2.3917
Epoch: 42/200
Train loss: 2.4229
Epoch: 43/200
Train loss: 2.3659
Epoch: 44/200
Train loss: 2.3165
Epoch: 45/200
Train loss: 2.3193
Epoch: 46/200
Train loss: 2.3248
Epoch: 47/200
Train loss: 2.3418
Epoch: 48/200
Train loss: 2.3598
Epoch: 49/200
Train loss: 2.3703
Epoch: 50/200
Train loss: 2.3229
Epoch: 51/200
Train loss: 2.3561
Epoch: 52/200
Train loss: 2.2092
Epoch: 53/200
Train loss: 2.3171
Epoch: 54/200
Train loss: 2.2662
Epoch: 55/200
Train loss: 2.2703
Epoch: 56/200
Train loss: 2.2904
Epoch: 57/200
Train loss: 2.3016
Epoch: 58/200
Train loss: 2.2167
Epoch: 59/200
Train loss: 2.2092
Epoch: 60/200
Train loss: 2.1970
Epoch: 61/200
Train loss: 2.2405
Epoch: 62/200
Train loss: 2.2737
Epoch: 63/200
Train loss: 2.2393
Epoch: 64/200
Train loss: 2.2236
Epoch: 65/200
Train loss: 2.2187
Epoch: 66/200
Train loss: 2.2313
Epoch: 67/200
Train loss: 2.1527
Epoch: 68/200
Train loss: 2.2033
Epoch: 69/200
Train loss: 2.2052
Epoch: 70/200
Train loss: 2.1961
Epoch: 71/200
Train loss: 2.2023
Epoch: 72/200
Train loss: 2.2286
Epoch: 73/200
Train loss: 2.2296
Epoch: 74/200
Train loss: 2.2114
Epoch: 75/200
Train loss: 2.1569
Epoch: 76/200
Train loss: 2.1174
Epoch: 77/200
Train loss: 2.0872
Epoch: 78/200
Train loss: 2.1907
Epoch: 79/200
Train loss: 2.1328
Epoch: 80/200
Train loss: 2.1447
Epoch: 81/200
Train loss: 2.2060
Epoch: 82/200
Train loss: 2.1394
Epoch: 83/200
Train loss: 2.1302
Epoch: 84/200
Train loss: 2.1665
Epoch: 85/200
Train loss: 2.1499
Epoch: 86/200
Train loss: 2.1262
Epoch: 87/200
Train loss: 2.1232
Epoch: 88/200
Train loss: 2.0727
Epoch: 89/200
Train loss: 2.1148
Epoch: 90/200
Train loss: 2.1019
Epoch: 91/200
Train loss: 2.1527
Epoch: 92/200
Train loss: 2.0628
Epoch: 93/200
Train loss: 2.1443
Epoch: 94/200
Train loss: 2.1400
Epoch: 95/200
Train loss: 2.1684
Epoch: 96/200
Train loss: 2.1225
Epoch: 97/200
Train loss: 2.0749
Epoch: 98/200
Train loss: 2.0637
Epoch: 99/200
Train loss: 2.0367
Epoch: 100/200
Train loss: 2.0704
Epoch: 101/200
Train loss: 2.1284
Epoch: 102/200
Train loss: 2.0412
Time needed: 163.89856457710266 for validation audios
0.8029125685259412
Model improve: 0.0000 -> 0.8029
Epoch: 103/200
Train loss: 2.0101
Time needed: 167.26634573936462 for validation audios
0.7991607074916662
Epoch: 104/200
Train loss: 2.0678
Time needed: 166.68207573890686 for validation audios
0.8012237501954409
Epoch: 105/200
Train loss: 2.0515
Time needed: 167.36824560165405 for validation audios
0.7977541714597863
Epoch: 106/200
Train loss: 2.1003
Time needed: 167.41570496559143 for validation audios
0.8025573097711284
Epoch: 107/200
Train loss: 2.0814
Time needed: 167.3756618499756 for validation audios
0.7995905089896215
Epoch: 108/200
Train loss: 2.0317
Time needed: 167.5464129447937 for validation audios
0.8026252023103795
Epoch: 109/200
Train loss: 2.0788
Time needed: 165.40640544891357 for validation audios
0.7982903262209818
Epoch: 110/200
Train loss: 2.0243
Time needed: 167.75108647346497 for validation audios
0.8028234351650438
Epoch: 111/200
Train loss: 2.0311
Time needed: 167.49976801872253 for validation audios
0.8005568013677591
Epoch: 112/200
Train loss: 2.0659
Time needed: 167.23186564445496 for validation audios
0.7997113155781291
Epoch: 113/200
Train loss: 2.0686
Time needed: 168.18253564834595 for validation audios
0.7994653469075106
Epoch: 114/200
Train loss: 2.0278
Time needed: 165.9064371585846 for validation audios
0.8015306979394489
Epoch: 115/200
Train loss: 2.0594
Time needed: 170.38925170898438 for validation audios
0.7983783408455164
Epoch: 116/200
Train loss: 2.0119
Time needed: 168.48502373695374 for validation audios
0.8023816177142394
Epoch: 117/200
Train loss: 2.0850
Time needed: 163.59157586097717 for validation audios
0.8002269275726285
Epoch: 118/200
Train loss: 2.0456
Time needed: 166.07231640815735 for validation audios
0.8004741807530008
Epoch: 119/200
Train loss: 2.0619
Time needed: 170.0722155570984 for validation audios
0.7990342421170828
Epoch: 120/200
Train loss: 2.0826
Time needed: 165.80638194084167 for validation audios
0.7992245026963344
Epoch: 121/200
Train loss: 1.9441
Time needed: 166.9144196510315 for validation audios
0.8002473191433512
Epoch: 122/200
Date :04/28/2023, 15:25:15
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 40.3261
Epoch: 2/200
Train loss: 5.7817
Epoch: 3/200
Train loss: 5.4617
Epoch: 4/200
Train loss: 5.1868
Epoch: 5/200
Train loss: 4.8700
Epoch: 6/200
Train loss: 4.6085
Epoch: 7/200
Train loss: 4.2834
Epoch: 8/200
Train loss: 4.1781
Epoch: 9/200
Train loss: 3.9979
Epoch: 10/200
Train loss: 3.8845
Epoch: 11/200
Train loss: 3.7750
Epoch: 12/200
Train loss: 3.6422
Epoch: 13/200
Train loss: 3.5639
Epoch: 14/200
Train loss: 3.4259
Epoch: 15/200
Train loss: 3.4426
Epoch: 16/200
Train loss: 3.3400
Epoch: 17/200
Train loss: 3.2266
Epoch: 18/200
Train loss: 3.1282
Epoch: 19/200
Train loss: 3.3062
Epoch: 20/200
Train loss: 3.1787
Epoch: 21/200
Train loss: 3.0330
Epoch: 22/200
Train loss: 2.9205
Epoch: 23/200
Train loss: 2.9434
Epoch: 24/200
Train loss: 3.0189
Epoch: 25/200
Train loss: 2.9968
Epoch: 26/200
Train loss: 2.9723
Epoch: 27/200
Train loss: 2.9416
Epoch: 28/200
Train loss: 2.8038
Epoch: 29/200
Train loss: 2.8241
Epoch: 30/200
Train loss: 2.6700
Epoch: 31/200
Train loss: 2.8132
Epoch: 32/200
Train loss: 2.8490
Epoch: 33/200
Train loss: 2.6585
Epoch: 34/200
Train loss: 2.7387
Epoch: 35/200
Train loss: 2.7391
Epoch: 36/200
Train loss: 2.7732
Epoch: 37/200
Train loss: 2.6433
Epoch: 38/200
Train loss: 2.8829
Epoch: 39/200
Train loss: 2.5498
Epoch: 40/200
Train loss: 2.7035
Epoch: 41/200
Train loss: 2.6198
Epoch: 42/200
Train loss: 2.6218
Epoch: 43/200
Train loss: 2.6889
Epoch: 44/200
Train loss: 2.6673
Epoch: 45/200
Train loss: 2.6955
Epoch: 46/200
Train loss: 2.4381
Epoch: 47/200
Train loss: 2.6390
Epoch: 48/200
Train loss: 2.4014
Epoch: 49/200
Train loss: 2.3825
Epoch: 50/200
Train loss: 2.4478
Epoch: 51/200
Train loss: 2.4885
Epoch: 52/200
Train loss: 2.4974
Epoch: 53/200
Train loss: 2.4073
Epoch: 54/200
Train loss: 2.5041
Epoch: 55/200
Train loss: 2.4698
Epoch: 56/200
Train loss: 2.4603
Epoch: 57/200
Train loss: 2.4904
Epoch: 58/200
Train loss: 2.4428
Epoch: 59/200
Train loss: 2.4586
Epoch: 60/200
Train loss: 2.4833
Epoch: 61/200
Train loss: 2.3862
Epoch: 62/200
Train loss: 2.3820
Epoch: 63/200
Train loss: 2.4178
Epoch: 64/200
Train loss: 2.4168
Epoch: 65/200
Train loss: 2.4601
Epoch: 66/200
Train loss: 2.3291
Epoch: 67/200
Train loss: 2.4362
Epoch: 68/200
Train loss: 2.3922
Epoch: 69/200
Train loss: 2.5644
Epoch: 70/200
Train loss: 2.3781
Epoch: 71/200
Train loss: 2.2452
Epoch: 72/200
Train loss: 2.2707
Epoch: 73/200
Train loss: 2.3852
Epoch: 74/200
Train loss: 2.3820
Epoch: 75/200
Train loss: 2.4239
Epoch: 76/200
Train loss: 2.3898
Epoch: 77/200
Train loss: 2.3591
Epoch: 78/200
Train loss: 2.2574
Epoch: 79/200
Train loss: 2.1116
Epoch: 80/200
Train loss: 2.2899
Epoch: 81/200
Train loss: 2.2940
Epoch: 82/200
Train loss: 2.2860
Epoch: 83/200
Train loss: 2.2463
Epoch: 84/200
Train loss: 2.2280
Epoch: 85/200
Train loss: 2.3470
Epoch: 86/200
Train loss: 2.2720
Epoch: 87/200
Train loss: 2.1657
Epoch: 88/200
Train loss: 2.3405
Epoch: 89/200
Train loss: 2.3769
Epoch: 90/200
Train loss: 2.2312
Epoch: 91/200
Train loss: 2.3452
Epoch: 92/200
Train loss: 2.3348
Epoch: 93/200
Train loss: 2.3234
Epoch: 94/200
Train loss: 2.2622
Epoch: 95/200
Train loss: 2.2859
Epoch: 96/200
Train loss: 2.3461
Epoch: 97/200
Train loss: 2.2820
Epoch: 98/200
Train loss: 2.2672
Epoch: 99/200
Train loss: 2.2018
Epoch: 100/200
Train loss: 2.2579
Epoch: 101/200
Train loss: 2.2999
Epoch: 102/200
Train loss: 2.0861
Time needed: 167.11323404312134 for validation audios
0.8064166978232596
Model improve: 0.0000 -> 0.8064
Epoch: 103/200
Train loss: 2.1597
Time needed: 166.8992247581482 for validation audios
0.8046334667279647
Epoch: 104/200
Train loss: 2.1664
Time needed: 164.58749103546143 for validation audios
0.8057571722670462
Epoch: 105/200
Train loss: 2.2138
Time needed: 164.24599266052246 for validation audios
0.8053821457920729
Epoch: 106/200
Train loss: 2.2146
Time needed: 164.9528043270111 for validation audios
0.8054779911480717
Epoch: 107/200
Train loss: 2.2694
Time needed: 167.11472487449646 for validation audios
0.8030121325583761
Epoch: 108/200
Train loss: 2.2431
Time needed: 165.94714641571045 for validation audios
0.8048895073109681
Epoch: 109/200
Train loss: 2.2222
Time needed: 164.26874089241028 for validation audios
0.8058743449796777
Epoch: 110/200
Train loss: 2.2990
Time needed: 164.23956966400146 for validation audios
0.8032016076719313
Epoch: 111/200
Train loss: 2.0867
Time needed: 164.39530038833618 for validation audios
0.8072922119613603
Model improve: 0.8064 -> 0.8073
Epoch: 112/200
Train loss: 2.1816
Time needed: 165.13780236244202 for validation audios
0.8023125200277406
Epoch: 113/200
Train loss: 2.1878
Time needed: 166.2605481147766 for validation audios
0.8064077257721673
Epoch: 114/200
Train loss: 2.2921
Time needed: 166.29044556617737 for validation audios
0.8050551565347153
Epoch: 115/200
Train loss: 2.1734
Time needed: 164.74674487113953 for validation audios
0.8074393407033311
Model improve: 0.8073 -> 0.8074
Epoch: 116/200
Train loss: 2.2128
Time needed: 165.6060745716095 for validation audios
0.8066638397295848
Epoch: 117/200
Train loss: 2.1832
Time needed: 166.47759437561035 for validation audios
0.8065233354913972
Epoch: 118/200
Train loss: 2.1462
Time needed: 165.3037223815918 for validation audios
0.8038259944649713
Epoch: 119/200
Train loss: 2.2529
Time needed: 167.39326333999634 for validation audios
0.8056004682244503
Epoch: 120/200
Train loss: 2.1394
Time needed: 166.1403603553772 for validation audios
0.8072302846723682
Epoch: 121/200
Train loss: 2.1452
Time needed: 167.35318326950073 for validation audios
0.8061546787112535
Epoch: 122/200
Train loss: 2.2309
Time needed: 166.17698764801025 for validation audios
0.8074314387590765
Epoch: 123/200
Train loss: 2.3368
Time needed: 164.79123663902283 for validation audios
0.8057828693154547
Epoch: 124/200
Train loss: 2.3424
Time needed: 166.53782510757446 for validation audios
0.8050857421782442
Epoch: 125/200
Train loss: 2.2725
Time needed: 164.6634407043457 for validation audios
0.8084835174020184
Model improve: 0.8074 -> 0.8085
Epoch: 126/200
Train loss: 2.1287
Time needed: 164.90455031394958 for validation audios
0.8083641720118215
Epoch: 127/200
Train loss: 2.1931
Time needed: 164.51996970176697 for validation audios
0.8066623897229842
Epoch: 128/200
Train loss: 2.0988
Time needed: 164.20261883735657 for validation audios
0.8087179930002855
Model improve: 0.8085 -> 0.8087
Epoch: 129/200
Train loss: 2.1249
Time needed: 164.94307613372803 for validation audios
0.8084748776046633
Epoch: 130/200
Train loss: 2.0368
Time needed: 164.87338852882385 for validation audios
0.8062008127318486
Epoch: 131/200
Train loss: 2.1321
Time needed: 165.5686502456665 for validation audios
0.807273136284221
Epoch: 132/200
Train loss: 2.2664
Time needed: 164.5786168575287 for validation audios
0.8028428509717805
Epoch: 133/200
Train loss: 2.0684
Time needed: 166.0898642539978 for validation audios
0.8082115556171854
Epoch: 134/200
Train loss: 2.2003
Time needed: 165.46171259880066 for validation audios
0.8079936005675066
Epoch: 135/200
Train loss: 2.0520
Time needed: 166.18102049827576 for validation audios
0.8096315993460191
Model improve: 0.8087 -> 0.8096
Epoch: 136/200
Train loss: 2.1347
Time needed: 168.08418822288513 for validation audios
0.8090230702569627
Epoch: 137/200
Train loss: 2.1621
Time needed: 165.17688727378845 for validation audios
0.8069914840373775
Epoch: 138/200
Train loss: 2.0302
Time needed: 165.59763646125793 for validation audios
0.8085902227489804
Epoch: 139/200
Train loss: 2.1731
Time needed: 166.69397640228271 for validation audios
0.8081930359654962
Epoch: 140/200
Train loss: 2.1189
Time needed: 166.3744661808014 for validation audios
0.807161377458239
Epoch: 141/200
Train loss: 2.1332
Time needed: 168.33812046051025 for validation audios
0.8068121011886599
Epoch: 142/200
Train loss: 2.0729
Time needed: 168.71348428726196 for validation audios
0.8079828921791554
Epoch: 143/200
Train loss: 2.2055
Time needed: 164.8515110015869 for validation audios
0.8080139427929599
Epoch: 144/200
Train loss: 2.1759
Time needed: 166.65511345863342 for validation audios
0.8086029024333413
Epoch: 145/200
Train loss: 2.0348
Time needed: 165.07637739181519 for validation audios
0.808272526059512
Epoch: 146/200
Train loss: 2.2049
Time needed: 164.85451102256775 for validation audios
0.808218750398219
Epoch: 147/200
Train loss: 2.1848
Time needed: 169.0024058818817 for validation audios
0.8086518052796187
Epoch: 148/200
Train loss: 2.2456
Time needed: 168.01588487625122 for validation audios
0.8079550117133308
Epoch: 149/200
Train loss: 2.1040
Time needed: 165.252023935318 for validation audios
0.8081272954254096
Epoch: 150/200
Train loss: 1.9454
Time needed: 169.50539684295654 for validation audios
0.8083142999543913
Epoch: 151/200
Train loss: 2.1342
Time needed: 167.19182705879211 for validation audios
0.8081099703043819
Epoch: 152/200
Train loss: 2.0555
Time needed: 166.35318660736084 for validation audios
0.8089346849474119
Epoch: 153/200
Train loss: 2.0000
Time needed: 168.20282196998596 for validation audios
0.8104261859414894
Model improve: 0.8096 -> 0.8104
Epoch: 154/200
Train loss: 2.1301
Time needed: 165.75178599357605 for validation audios
0.8095359510196598
Epoch: 155/200
Train loss: 2.1872
Time needed: 165.57977032661438 for validation audios
0.8073107637860217
Epoch: 156/200
Train loss: 2.1414
Time needed: 164.5792155265808 for validation audios
0.8093735772579592
Epoch: 157/200
Train loss: 2.1033
Time needed: 164.89882159233093 for validation audios
0.8081451171637466
Epoch: 158/200
Train loss: 1.9761
Time needed: 166.12481427192688 for validation audios
0.8108343393603662
Model improve: 0.8104 -> 0.8108
Epoch: 159/200
Train loss: 2.1694
Time needed: 168.43505144119263 for validation audios
0.8076121007376407
Epoch: 160/200
Train loss: 1.9917
Time needed: 165.69087481498718 for validation audios
0.8103912053358219
Epoch: 161/200
Train loss: 2.2368
Time needed: 166.68805646896362 for validation audios
0.8097815831941767
Epoch: 162/200
Train loss: 2.0982
Time needed: 169.5436496734619 for validation audios
0.8071015563934453
Epoch: 163/200
Train loss: 2.0768
Time needed: 165.58685684204102 for validation audios
0.8090766166305565
Epoch: 164/200
Train loss: 2.2079
Time needed: 164.23992729187012 for validation audios
0.8073107678416386
Epoch: 165/200
Train loss: 2.2206
Time needed: 164.6905288696289 for validation audios
0.807435854105121
Epoch: 166/200
Train loss: 2.1585
Time needed: 165.47209000587463 for validation audios
0.8095951839915309
Epoch: 167/200
Train loss: 2.2223
Time needed: 167.0339961051941 for validation audios
0.808821245456048
Epoch: 168/200
Train loss: 2.1323
Time needed: 166.32397508621216 for validation audios
0.8104230198379874
Epoch: 169/200
Train loss: 2.1761
Time needed: 166.44086837768555 for validation audios
0.8099305391226603
Epoch: 170/200
Train loss: 2.0764
Time needed: 165.0372085571289 for validation audios
0.8090712222565225
Epoch: 171/200
Train loss: 2.1962
Time needed: 164.23506808280945 for validation audios
0.8030754412993697
Epoch: 172/200
Train loss: 2.0597
Time needed: 165.1855344772339 for validation audios
0.8090872348032663
Epoch: 173/200
Train loss: 2.1734
Time needed: 164.63844633102417 for validation audios
0.8087967781640986
Epoch: 174/200
Train loss: 2.0524
Time needed: 167.59544610977173 for validation audios
0.808341225634582
Epoch: 175/200
Train loss: 2.1672
Time needed: 164.510098695755 for validation audios
0.8063795136169309
Epoch: 176/200
Train loss: 1.9684
Time needed: 166.11945104599 for validation audios
0.8117087188249115
Model improve: 0.8108 -> 0.8117
Epoch: 177/200
Train loss: 2.0513
Time needed: 164.97196340560913 for validation audios
0.8081483982035373
Epoch: 178/200
Train loss: 2.0664
Time needed: 167.17825770378113 for validation audios
0.8108812723773573
Epoch: 179/200
Train loss: 2.0363
Time needed: 167.22168588638306 for validation audios
0.8103147873117169
Epoch: 180/200
Train loss: 2.1966
Time needed: 165.98401403427124 for validation audios
0.8068917128470533
Epoch: 181/200
Train loss: 2.0880
Time needed: 165.02406239509583 for validation audios
0.8102618656120032
Epoch: 182/200
Train loss: 2.0092
Time needed: 165.20937061309814 for validation audios
0.8080454634321671
Epoch: 183/200
Train loss: 2.0596
Time needed: 165.66103339195251 for validation audios
0.810954275846041
Epoch: 184/200
Train loss: 2.1781
Time needed: 167.26717329025269 for validation audios
0.8084335435393211
Epoch: 185/200
Train loss: 2.0675
Time needed: 164.3986656665802 for validation audios
0.8114029409150555
Epoch: 186/200
Train loss: 2.2256
Time needed: 167.17994475364685 for validation audios
0.8083139356071027
Epoch: 187/200
Train loss: 2.0306
Time needed: 164.8826093673706 for validation audios
0.8096033171182119
Epoch: 188/200
Train loss: 2.2177
Time needed: 166.74458122253418 for validation audios
0.8087790912187474
Epoch: 189/200
Train loss: 2.1863
Time needed: 165.1362419128418 for validation audios
0.8069169078106985
Epoch: 190/200
Train loss: 2.0756
Time needed: 167.21153712272644 for validation audios
0.8112875166906754
Epoch: 191/200
Train loss: 2.1794
Time needed: 164.54616022109985 for validation audios
0.8075756597957126
Epoch: 192/200
Train loss: 2.1231
Time needed: 166.12529349327087 for validation audios
0.8098580534014082
Epoch: 193/200
Train loss: 2.1670
Time needed: 166.2982587814331 for validation audios
0.8079542417454928
Epoch: 194/200
Train loss: 2.1854
Time needed: 167.16409516334534 for validation audios
0.809766951186786
Epoch: 195/200
Train loss: 2.1447
Time needed: 165.3308756351471 for validation audios
0.809746999159361
Epoch: 196/200
Train loss: 2.1765
Date :04/28/2023, 23:58:17
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 256
validbs: 1024
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Date :04/28/2023, 23:59:00
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 192
validbs: 576
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 57.0962
Epoch: 2/200
Train loss: 5.9364
Epoch: 3/200
Train loss: 5.7098
Epoch: 4/200
Train loss: 5.4774
Epoch: 5/200
Train loss: 5.2741
Epoch: 6/200
Train loss: 5.0720
Epoch: 7/200
Train loss: 4.8542
Epoch: 8/200
Train loss: 4.6266
Epoch: 9/200
Train loss: 4.5305
Epoch: 10/200
Train loss: 4.2607
Epoch: 11/200
Train loss: 4.1867
Epoch: 12/200
Train loss: 4.0188
Epoch: 13/200
Train loss: 3.9562
Epoch: 14/200
Train loss: 3.7962
Epoch: 15/200
Train loss: 3.7785
Epoch: 16/200
Train loss: 3.7145
Epoch: 17/200
Train loss: 3.5709
Epoch: 18/200
Train loss: 3.5447
Epoch: 19/200
Train loss: 3.5157
Epoch: 20/200
Train loss: 3.4595
Epoch: 21/200
Train loss: 3.2648
Epoch: 22/200
Train loss: 3.3428
Epoch: 23/200
Train loss: 3.4017
Epoch: 24/200
Train loss: 3.1961
Epoch: 25/200
Train loss: 3.1022
Epoch: 26/200
Train loss: 3.1104
Epoch: 27/200
Train loss: 3.0414
Epoch: 28/200
Train loss: 3.2981
Epoch: 29/200
Train loss: 3.0665
Epoch: 30/200
Train loss: 3.0907
Epoch: 31/200
Train loss: 3.0036
Epoch: 32/200
Train loss: 2.8472
Epoch: 33/200
Train loss: 2.8329
Epoch: 34/200
Train loss: 2.8700
Epoch: 35/200
Train loss: 2.9275
Epoch: 36/200
Train loss: 2.9222
Epoch: 37/200
Train loss: 2.9870
Epoch: 38/200
Train loss: 2.7942
Epoch: 39/200
Train loss: 2.8660
Epoch: 40/200
Train loss: 2.8606
Epoch: 41/200
Train loss: 2.7655
Epoch: 42/200
Train loss: 2.7424
Epoch: 43/200
Train loss: 2.7454
Epoch: 44/200
Train loss: 2.6495
Epoch: 45/200
Train loss: 2.6000
Epoch: 46/200
Train loss: 2.7238
Epoch: 47/200
Train loss: 2.7614
Epoch: 48/200
Train loss: 2.7679
Epoch: 49/200
Train loss: 2.5477
Epoch: 50/200
Train loss: 2.5636
Epoch: 51/200
Train loss: 2.7445
Epoch: 52/200
Train loss: 2.6370
Epoch: 53/200
Train loss: 2.7300
Epoch: 54/200
Train loss: 2.6287
Epoch: 55/200
Train loss: 2.5092
Epoch: 56/200
Train loss: 2.7152
Epoch: 57/200
Train loss: 2.8535
Epoch: 58/200
Train loss: 2.5022
Epoch: 59/200
Train loss: 2.4996
Epoch: 60/200
Train loss: 2.6760
Epoch: 61/200
Train loss: 2.6341
Epoch: 62/200
Train loss: 2.4734
Epoch: 63/200
Train loss: 2.5268
Epoch: 64/200
Train loss: 2.6407
Epoch: 65/200
Train loss: 2.5446
Epoch: 66/200
Train loss: 2.6518
Epoch: 67/200
Train loss: 2.5897
Epoch: 68/200
Train loss: 2.5629
Epoch: 69/200
Train loss: 2.3307
Epoch: 70/200
Train loss: 2.6068
Epoch: 71/200
Train loss: 2.4111
Epoch: 72/200
Train loss: 2.3164
Epoch: 73/200
Train loss: 2.2984
Epoch: 74/200
Train loss: 2.4469
Epoch: 75/200
Train loss: 2.2990
Epoch: 76/200
Train loss: 2.4387
Epoch: 77/200
Train loss: 2.4287
Epoch: 78/200
Train loss: 2.4077
Epoch: 79/200
Train loss: 2.2754
Epoch: 80/200
Train loss: 2.5395
Epoch: 81/200
Train loss: 2.3538
Epoch: 82/200
Train loss: 2.4862
Epoch: 83/200
Train loss: 2.1909
Epoch: 84/200
Train loss: 2.5402
Epoch: 85/200
Train loss: 2.3929
Epoch: 86/200
Train loss: 2.3590
Epoch: 87/200
Train loss: 2.4466
Epoch: 88/200
Train loss: 2.4125
Epoch: 89/200
Train loss: 2.4126
Epoch: 90/200
Train loss: 2.4135
Epoch: 91/200
Train loss: 2.3751
Epoch: 92/200
Train loss: 2.2838
Epoch: 93/200
Train loss: 2.3251
Epoch: 94/200
Train loss: 2.3787
Epoch: 95/200
Train loss: 2.3969
Epoch: 96/200
Train loss: 2.2975
Epoch: 97/200
Train loss: 2.4109
Epoch: 98/200
Train loss: 2.3912
Epoch: 99/200
Train loss: 2.2046
Epoch: 100/200
Train loss: 2.3140
Epoch: 101/200
Train loss: 2.5073
Epoch: 102/200
Train loss: 2.2232
Time needed: 172.32230043411255 for validation audios
0.8009044205132155
Model improve: 0.0000 -> 0.8009
Epoch: 103/200
Train loss: 2.4538
Time needed: 166.9201099872589 for validation audios
0.8008408105635434
Epoch: 104/200
Train loss: 2.4253
Time needed: 165.14015173912048 for validation audios
0.8003631928701713
Epoch: 105/200
Date :04/29/2023, 02:21:01
Duration: 5
Sample rate: 32000
nfft: 1024
fmin: 50
nmels: 128
fmax: 14000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 40.1130
Epoch: 2/200
Train loss: 5.7726
Epoch: 3/200
Train loss: 5.4444
Epoch: 4/200
Train loss: 5.1741
Epoch: 5/200
Train loss: 4.8638
Epoch: 6/200
Train loss: 4.6000
Epoch: 7/200
Train loss: 4.2728
Epoch: 8/200
Train loss: 4.1666
Epoch: 9/200
Train loss: 3.9926
Epoch: 10/200
Train loss: 3.8784
Epoch: 11/200
Train loss: 3.7707
Epoch: 12/200
Train loss: 3.6395
Epoch: 13/200
Train loss: 3.5638
Epoch: 14/200
Train loss: 3.4280
Epoch: 15/200
Train loss: 3.4449
Epoch: 16/200
Train loss: 3.3407
Epoch: 17/200
Train loss: 3.2276
Epoch: 18/200
Train loss: 3.1336
Epoch: 19/200
Train loss: 3.3131
Epoch: 20/200
Train loss: 3.1836
Epoch: 21/200
Train loss: 3.0362
Epoch: 22/200
Train loss: 2.9243
Epoch: 23/200
Train loss: 2.9461
Epoch: 24/200
Train loss: 3.0199
Epoch: 25/200
Train loss: 2.9947
Epoch: 26/200
Train loss: 2.9726
Epoch: 27/200
Train loss: 2.9403
Epoch: 28/200
Train loss: 2.8080
Epoch: 29/200
Train loss: 2.8293
Epoch: 30/200
Train loss: 2.6739
Epoch: 31/200
Train loss: 2.8114
Epoch: 32/200
Train loss: 2.8554
Epoch: 33/200
Train loss: 2.6685
Epoch: 34/200
Train loss: 2.7393
Epoch: 35/200
Train loss: 2.7431
Epoch: 36/200
Train loss: 2.7734
Epoch: 37/200
Train loss: 2.6498
Epoch: 38/200
Train loss: 2.8821
Epoch: 39/200
Train loss: 2.5544
Epoch: 40/200
Train loss: 2.7082
Epoch: 41/200
Train loss: 2.6270
Epoch: 42/200
Train loss: 2.6248
Epoch: 43/200
Train loss: 2.6894
Epoch: 44/200
Train loss: 2.6687
Epoch: 45/200
Train loss: 2.6975
Epoch: 46/200
Train loss: 2.4409
Epoch: 47/200
Train loss: 2.6451
Epoch: 48/200
Train loss: 2.4030
Epoch: 49/200
Train loss: 2.3877
Epoch: 50/200
Train loss: 2.4584
Epoch: 51/200
Train loss: 2.4935
Epoch: 52/200
Train loss: 2.5002
Epoch: 53/200
Train loss: 2.4129
Epoch: 54/200
Train loss: 2.5024
Epoch: 55/200
Train loss: 2.4742
Epoch: 56/200
Train loss: 2.4610
Epoch: 57/200
Train loss: 2.4915
Epoch: 58/200
Train loss: 2.4509
Epoch: 59/200
Train loss: 2.4578
Epoch: 60/200
Train loss: 2.4843
Epoch: 61/200
Train loss: 2.3890
Epoch: 62/200
Train loss: 2.3873
Epoch: 63/200
Train loss: 2.4249
Epoch: 64/200
Train loss: 2.4181
Epoch: 65/200
Train loss: 2.4633
Epoch: 66/200
Train loss: 2.3269
Epoch: 67/200
Train loss: 2.4352
Epoch: 68/200
Train loss: 2.3994
Epoch: 69/200
Train loss: 2.5678
Epoch: 70/200
Train loss: 2.3854
Epoch: 71/200
Train loss: 2.2439
Epoch: 72/200
Train loss: 2.2736
Epoch: 73/200
Train loss: 2.3856
Epoch: 74/200
Train loss: 2.3826
Epoch: 75/200
Train loss: 2.4281
Epoch: 76/200
Train loss: 2.3885
Epoch: 77/200
Train loss: 2.3615
Epoch: 78/200
Train loss: 2.2628
Epoch: 79/200
Train loss: 2.1127
Epoch: 80/200
Train loss: 2.2971
Epoch: 81/200
Train loss: 2.2888
Epoch: 82/200
Train loss: 2.2893
Epoch: 83/200
Train loss: 2.2504
Epoch: 84/200
Train loss: 2.2304
Epoch: 85/200
Train loss: 2.3497
Epoch: 86/200
Train loss: 2.2803
Epoch: 87/200
Train loss: 2.1698
Epoch: 88/200
Train loss: 2.3479
Epoch: 89/200
Train loss: 2.3771
Epoch: 90/200
Train loss: 2.2342
Epoch: 91/200
Train loss: 2.3512
Epoch: 92/200
Train loss: 2.3397
Epoch: 93/200
Train loss: 2.3311
Epoch: 94/200
Train loss: 2.2698
Epoch: 95/200
Train loss: 2.2883
Epoch: 96/200
Train loss: 2.3491
Epoch: 97/200
Train loss: 2.2843
Epoch: 98/200
Train loss: 2.2726
Epoch: 99/200
Train loss: 2.1997
Epoch: 100/200
Train loss: 2.2578
Epoch: 101/200
Train loss: 2.3042
Epoch: 102/200
Train loss: 2.0813
Time needed: 167.5208284854889 for validation audios
0.8069100821528146
Model improve: 0.0000 -> 0.8069
Epoch: 103/200
Train loss: 2.1661
Time needed: 169.94245028495789 for validation audios
0.8043261297421407
Epoch: 104/200
Train loss: 2.1681
Time needed: 167.05006194114685 for validation audios
0.8062021145638536
Epoch: 105/200
Train loss: 2.2164
Time needed: 172.5268530845642 for validation audios
0.806993641088466
Model improve: 0.8069 -> 0.8070
Epoch: 106/200
Train loss: 2.2155
Time needed: 167.55285596847534 for validation audios
0.8044036527195033
Epoch: 107/200
Train loss: 2.2752
Time needed: 169.070081949234 for validation audios
0.8036683858065454
Epoch: 108/200
Train loss: 2.2467
Time needed: 168.0179557800293 for validation audios
0.8046807678402876
Epoch: 109/200
Train loss: 2.2228
Time needed: 169.65667462348938 for validation audios
0.8060266027921188
Epoch: 110/200
Train loss: 2.3057
Time needed: 169.11088681221008 for validation audios
0.8054083259221451
Epoch: 111/200
Train loss: 2.0882
Time needed: 168.42642617225647 for validation audios
0.8061572217012953
Epoch: 112/200
Train loss: 2.1838
Time needed: 169.20821714401245 for validation audios
0.8009534732334156
Epoch: 113/200
Train loss: 2.1890
Time needed: 169.02642488479614 for validation audios
0.8058902054370695
Epoch: 114/200
Train loss: 2.2918
Time needed: 169.5021800994873 for validation audios
0.8048757441339284
Epoch: 115/200
Train loss: 2.1799
Time needed: 169.74956464767456 for validation audios
0.8069380891288842
Epoch: 116/200
Train loss: 2.2134
Time needed: 168.42151379585266 for validation audios
0.8076170927113963
Model improve: 0.8070 -> 0.8076
Epoch: 117/200
Train loss: 2.1825
Time needed: 170.4235875606537 for validation audios
0.807369666198952
Epoch: 118/200
Train loss: 2.1440
Time needed: 168.8336374759674 for validation audios
0.8049659695069139
Epoch: 119/200
Train loss: 2.2546
Time needed: 167.94383835792542 for validation audios
0.8066648296157856
Epoch: 120/200
Train loss: 2.1408
Time needed: 169.160386800766 for validation audios
0.8070274629250896
Epoch: 121/200
Train loss: 2.1482
Time needed: 170.37401628494263 for validation audios
0.8059799891785777
Epoch: 122/200
Train loss: 2.2283
Time needed: 169.74139976501465 for validation audios
0.8077965507709459
Model improve: 0.8076 -> 0.8078
Epoch: 123/200
Train loss: 2.3447
Time needed: 168.1726393699646 for validation audios
0.8066219308686609
Epoch: 124/200
Train loss: 2.3437
Time needed: 170.22329258918762 for validation audios
0.8053729141433358
Epoch: 125/200
Train loss: 2.2676
Time needed: 168.64493918418884 for validation audios
0.8096181538451485
Model improve: 0.8078 -> 0.8096
Epoch: 126/200
Train loss: 2.1282
Time needed: 168.87808513641357 for validation audios
0.8086901264783443
Epoch: 127/200
Train loss: 2.1925
Time needed: 172.54179048538208 for validation audios
0.807013832222978
Epoch: 128/200
Train loss: 2.0986
Time needed: 167.94906449317932 for validation audios
0.8096449647358115
Model improve: 0.8096 -> 0.8096
Epoch: 129/200
Train loss: 2.1297
Time needed: 170.8739993572235 for validation audios
0.8089156413434206
Epoch: 130/200
Train loss: 2.0393
Time needed: 168.57796216011047 for validation audios
0.806732266966696
Epoch: 131/200
Train loss: 2.1388
Time needed: 166.44175720214844 for validation audios
0.8078139161802185
Epoch: 132/200
Train loss: 2.2688
Time needed: 167.26813435554504 for validation audios
0.8040306645111666
Epoch: 133/200
Train loss: 2.0695
Time needed: 168.88946652412415 for validation audios
0.8094205084719084
Epoch: 134/200
Train loss: 2.2006
Time needed: 168.0568826198578 for validation audios
0.8089389268294381
Epoch: 135/200
Train loss: 2.0576
Time needed: 168.79945278167725 for validation audios
0.8102536460298058
Model improve: 0.8096 -> 0.8103
Epoch: 136/200
Train loss: 2.1329
Time needed: 168.34102630615234 for validation audios
0.8090298928353488
Epoch: 137/200
Train loss: 2.1644
Time needed: 171.09631037712097 for validation audios
0.8076484397514311
Epoch: 138/200
Train loss: 2.0302
Time needed: 166.9742193222046 for validation audios
0.8089378794222576
Epoch: 139/200
Train loss: 2.1796
Time needed: 167.3053638935089 for validation audios
0.8074139574603942
Epoch: 140/200
Train loss: 2.1209
Time needed: 168.53047227859497 for validation audios
0.8067755522815134
Epoch: 141/200
Train loss: 2.1374
Time needed: 166.5203559398651 for validation audios
0.8058099716171262
Epoch: 142/200
Train loss: 2.0778
Time needed: 167.05653047561646 for validation audios
0.8087066200973315
Epoch: 143/200
Train loss: 2.2090
Time needed: 167.97734117507935 for validation audios
0.8081512385887872
Epoch: 144/200
Train loss: 2.1817
Time needed: 167.16271018981934 for validation audios
0.8084497174117554
Epoch: 145/200
Train loss: 2.0400
Time needed: 168.6422564983368 for validation audios
0.8081660306719533
Epoch: 146/200
Train loss: 2.2052
Time needed: 167.96186327934265 for validation audios
0.8082343942993886
Epoch: 147/200
Train loss: 2.1910
Time needed: 168.28813219070435 for validation audios
0.8085382060098963
Epoch: 148/200
Train loss: 2.2425
Time needed: 171.6271209716797 for validation audios
0.8088514257979137
Epoch: 149/200
Train loss: 2.1022
Time needed: 169.9996497631073 for validation audios
0.8084343783784093
Epoch: 150/200
Train loss: 1.9476
Time needed: 168.23875737190247 for validation audios
0.809064738757738
Epoch: 151/200
Train loss: 2.1326
Time needed: 168.90934896469116 for validation audios
0.8088076335355995
Epoch: 152/200
Train loss: 2.0556
Time needed: 167.899822473526 for validation audios
0.8092716263265379
Epoch: 153/200
Train loss: 1.9982
Time needed: 167.9049837589264 for validation audios
0.809850742291323
Epoch: 154/200
Train loss: 2.1277
Time needed: 166.80760312080383 for validation audios
0.8101660458363159
Epoch: 155/200
Train loss: 2.1923
Time needed: 170.49283719062805 for validation audios
0.8078181173304939
Epoch: 156/200
Train loss: 2.1469
Time needed: 170.93472504615784 for validation audios
0.809589747216571
Epoch: 157/200
Train loss: 2.1071
Time needed: 168.96706438064575 for validation audios
0.8084478729372738
Epoch: 158/200
Train loss: 1.9825
Time needed: 168.39069199562073 for validation audios
0.8113505392441537
Model improve: 0.8103 -> 0.8114
Epoch: 159/200
Train loss: 2.1719
Time needed: 169.3341076374054 for validation audios
0.8085711971872269
Epoch: 160/200
Train loss: 1.9938
Time needed: 172.2245614528656 for validation audios
0.8107654798419005
Epoch: 161/200
Train loss: 2.2337
Time needed: 166.93885254859924 for validation audios
0.8105720311929607
Epoch: 162/200
Train loss: 2.1018
Time needed: 169.11147451400757 for validation audios
0.8088624809303884
Epoch: 163/200
Train loss: 2.0796
Time needed: 174.8699996471405 for validation audios
0.8098178437020216
Epoch: 164/200
Train loss: 2.2091
Time needed: 170.39437055587769 for validation audios
0.8083142048780757
Epoch: 165/200
Train loss: 2.2247
Time needed: 168.1222529411316 for validation audios
0.808092116846638
Epoch: 166/200
Train loss: 2.1610
Time needed: 170.9819300174713 for validation audios
0.8102679848406192
Epoch: 167/200
Train loss: 2.2174
Time needed: 170.01924920082092 for validation audios
0.8090800823444242
Epoch: 168/200
Train loss: 2.1384
Time needed: 167.5914740562439 for validation audios
0.8112634179970252
Epoch: 169/200
Train loss: 2.1816
Time needed: 169.56187462806702 for validation audios
0.8103573085191775
Epoch: 170/200
Train loss: 2.0788
Time needed: 169.40226650238037 for validation audios
0.8094826036318554
Epoch: 171/200
Train loss: 2.1997
Time needed: 169.6695683002472 for validation audios
0.803651047968939
Epoch: 172/200
Date :04/29/2023, 09:51:09
Duration: 5
Sample rate: 32000
nfft: 1024
fmin: 50
nmels: 128
fmax: 14000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Date :04/29/2023, 09:52:31
Duration: 5
Sample rate: 32000
nfft: 1024
fmin: 50
nmels: 128
fmax: 14000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Date :04/29/2023, 09:53:20
Duration: 5
Sample rate: 32000
nfft: 1024
fmin: 50
nmels: 128
fmax: 14000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Date :04/29/2023, 09:56:11
Duration: 5
Sample rate: 32000
nfft: 1024
fmin: 50
nmels: 128
fmax: 14000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 39.9128
Epoch: 2/200
Train loss: 5.8552
Epoch: 3/200
Date :04/29/2023, 09:59:55
Duration: 5
Sample rate: 32000
nfft: 1024
fmin: 50
nmels: 128
fmax: 14000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Date :04/29/2023, 10:00:26
Duration: 5
Sample rate: 32000
nfft: 1024
fmin: 50
nmels: 128
fmax: 14000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 0.1335
Time needed: 168.26920533180237 for validation audios
0.24974785307446948
Model improve: 0.0000 -> 0.2497
Epoch: 2/200
Train loss: 0.0221
Time needed: 169.49923968315125 for validation audios
0.2775849520714738
Model improve: 0.2497 -> 0.2776
Epoch: 3/200
Train loss: 0.0209
Time needed: 170.73832440376282 for validation audios
0.32586417294379083
Model improve: 0.2776 -> 0.3259
Epoch: 4/200
Train loss: 0.0197
Time needed: 169.08209538459778 for validation audios
0.39009831160059827
Model improve: 0.3259 -> 0.3901
Epoch: 5/200
Train loss: 0.0185
Time needed: 169.82652950286865 for validation audios
0.4519111994409461
Model improve: 0.3901 -> 0.4519
Epoch: 6/200
Train loss: 0.0175
Time needed: 168.8049988746643 for validation audios
0.5009067869160885
Model improve: 0.4519 -> 0.5009
Epoch: 7/200
Train loss: 0.0167
Time needed: 170.56181716918945 for validation audios
0.5427382711173199
Model improve: 0.5009 -> 0.5427
Epoch: 8/200
Train loss: 0.0160
Time needed: 168.73938465118408 for validation audios
0.5741900386386731
Model improve: 0.5427 -> 0.5742
Epoch: 9/200
Train loss: 0.0153
Time needed: 170.8767855167389 for validation audios
0.599224151976198
Model improve: 0.5742 -> 0.5992
Epoch: 10/200
Train loss: 0.0148
Time needed: 171.3731484413147 for validation audios
0.617671974303503
Model improve: 0.5992 -> 0.6177
Epoch: 11/200
Train loss: 0.0143
Time needed: 169.71890354156494 for validation audios
0.6385557647000125
Model improve: 0.6177 -> 0.6386
Epoch: 12/200
Train loss: 0.0138
Time needed: 168.4535882472992 for validation audios
0.6536014192573368
Model improve: 0.6386 -> 0.6536
Epoch: 13/200
Train loss: 0.0135
Time needed: 169.46838569641113 for validation audios
0.6708214414092185
Model improve: 0.6536 -> 0.6708
Epoch: 14/200
Train loss: 0.0131
Time needed: 172.55490040779114 for validation audios
0.6811160644099469
Model improve: 0.6708 -> 0.6811
Epoch: 15/200
Train loss: 0.0128
Time needed: 176.55415630340576 for validation audios
0.6915183750303611
Model improve: 0.6811 -> 0.6915
Epoch: 16/200
Train loss: 0.0125
Time needed: 168.17487788200378 for validation audios
0.7015011255017125
Model improve: 0.6915 -> 0.7015
Epoch: 17/200
Train loss: 0.0124
Time needed: 171.00662398338318 for validation audios
0.7087630820980528
Model improve: 0.7015 -> 0.7088
Epoch: 18/200
Train loss: 0.0122
Time needed: 168.21323013305664 for validation audios
0.7158375880972976
Model improve: 0.7088 -> 0.7158
Epoch: 19/200
Train loss: 0.0119
Time needed: 171.55229687690735 for validation audios
0.7222282212577601
Model improve: 0.7158 -> 0.7222
Epoch: 20/200
Train loss: 0.0118
Time needed: 167.14591574668884 for validation audios
0.7278536658834518
Model improve: 0.7222 -> 0.7279
Epoch: 21/200
Train loss: 0.0116
Time needed: 169.74860763549805 for validation audios
0.732235334402057
Model improve: 0.7279 -> 0.7322
Epoch: 22/200
Train loss: 0.0114
Time needed: 169.8134105205536 for validation audios
0.7371605140638733
Model improve: 0.7322 -> 0.7372
Epoch: 23/200
Train loss: 0.0114
Time needed: 178.7920002937317 for validation audios
0.7405326506630353
Model improve: 0.7372 -> 0.7405
Epoch: 24/200
Train loss: 0.0112
Time needed: 171.04205346107483 for validation audios
0.7416074313854819
Model improve: 0.7405 -> 0.7416
Epoch: 25/200
Train loss: 0.0111
Time needed: 168.3545436859131 for validation audios
0.7456413219295481
Model improve: 0.7416 -> 0.7456
Epoch: 26/200
Train loss: 0.0109
Time needed: 168.26419401168823 for validation audios
0.7496316054625102
Model improve: 0.7456 -> 0.7496
Epoch: 27/200
Train loss: 0.0109
Time needed: 169.71579384803772 for validation audios
0.7541562112508431
Model improve: 0.7496 -> 0.7542
Epoch: 28/200
Train loss: 0.0109
Time needed: 167.21623468399048 for validation audios
0.7574626373102011
Model improve: 0.7542 -> 0.7575
Epoch: 29/200
Train loss: 0.0106
Time needed: 170.4168815612793 for validation audios
0.759490351087373
Model improve: 0.7575 -> 0.7595
Epoch: 30/200
Train loss: 0.0106
Time needed: 168.24707889556885 for validation audios
0.7606685215638345
Model improve: 0.7595 -> 0.7607
Epoch: 31/200
Train loss: 0.0106
Time needed: 169.33325147628784 for validation audios
0.7639662328877019
Model improve: 0.7607 -> 0.7640
Epoch: 32/200
Train loss: 0.0104
Time needed: 169.12938690185547 for validation audios
0.7667101095934763
Model improve: 0.7640 -> 0.7667
Epoch: 33/200
Train loss: 0.0103
Time needed: 167.9769835472107 for validation audios
0.7679206098437011
Model improve: 0.7667 -> 0.7679
Epoch: 34/200
Train loss: 0.0103
Time needed: 172.13838815689087 for validation audios
0.7695901779290292
Model improve: 0.7679 -> 0.7696
Epoch: 35/200
Train loss: 0.0102
Time needed: 169.77681279182434 for validation audios
0.7725439752842608
Model improve: 0.7696 -> 0.7725
Epoch: 36/200
Train loss: 0.0101
Time needed: 170.27027463912964 for validation audios
0.7737291805410814
Model improve: 0.7725 -> 0.7737
Epoch: 37/200
Train loss: 0.0100
Time needed: 168.22801876068115 for validation audios
0.7731708286166824
Epoch: 38/200
Train loss: 0.0100
Time needed: 170.04787135124207 for validation audios
0.7758251738236092
Model improve: 0.7737 -> 0.7758
Epoch: 39/200
Train loss: 0.0100
Time needed: 168.2873659133911 for validation audios
0.7745242814271939
Epoch: 40/200
Train loss: 0.0100
Time needed: 168.89923334121704 for validation audios
0.774607077117798
Epoch: 41/200
Train loss: 0.0098
Date :04/29/2023, 12:51:39
Duration: 5
Sample rate: 32000
nfft: 1024
fmin: 50
nmels: 128
fmax: 14000
trainbs: 16
validbs: 48
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 0.0366
Time needed: 170.71466064453125 for validation audios
0.4673734012616361
Model improve: 0.0000 -> 0.4674
Epoch: 2/200
Train loss: 0.0168
Time needed: 167.83850932121277 for validation audios
0.612554981675367
Model improve: 0.4674 -> 0.6126
Epoch: 3/200
Train loss: 0.0146
Time needed: 168.54297494888306 for validation audios
0.6590704085705045
Model improve: 0.6126 -> 0.6591
Epoch: 4/200
Train loss: 0.0134
Time needed: 168.92120957374573 for validation audios
0.6894878569588829
Model improve: 0.6591 -> 0.6895
Epoch: 5/200
Train loss: 0.0126
Time needed: 169.13909578323364 for validation audios
0.709652686332004
Model improve: 0.6895 -> 0.7097
Epoch: 6/200
Train loss: 0.0121
Time needed: 168.84748578071594 for validation audios
0.7245722424781985
Model improve: 0.7097 -> 0.7246
Epoch: 7/200
Train loss: 0.0117
Time needed: 169.31067323684692 for validation audios
0.7329824131092744
Model improve: 0.7246 -> 0.7330
Epoch: 8/200
Train loss: 0.0114
Time needed: 167.72109484672546 for validation audios
0.7413350071164135
Model improve: 0.7330 -> 0.7413
Epoch: 9/200
Train loss: 0.0110
Time needed: 169.85972213745117 for validation audios
0.7482882461596293
Model improve: 0.7413 -> 0.7483
Epoch: 10/200
Train loss: 0.0108
Date :04/29/2023, 13:46:06
Duration: 5
Sample rate: 32000
nfft: 1024
fmin: 50
nmels: 128
fmax: 14000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 39.8823
Time needed: 168.4237082004547 for validation audios
0.24645117793028443
Model improve: 0.0000 -> 0.2465
Epoch: 2/200
Train loss: 5.8623
Time needed: 169.8989520072937 for validation audios
0.2726718317276306
Model improve: 0.2465 -> 0.2727
Epoch: 3/200
Train loss: 5.5927
Date :04/29/2023, 13:56:09
Duration: 5
Sample rate: 32000
nfft: 1024
fmin: 50
nmels: 128
fmax: 14000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Date :04/29/2023, 13:56:56
Duration: 5
Sample rate: 32000
nfft: 1024
fmin: 50
nmels: 128
fmax: 14000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 40.0430
Time needed: 171.39646553993225 for validation audios
0.2525109411231978
Model improve: 0.0000 -> 0.2525
Epoch: 2/200
Train loss: 5.7850
Time needed: 172.57046508789062 for validation audios
0.2911406027451422
Model improve: 0.2525 -> 0.2911
Epoch: 3/200
Train loss: 5.4412
Time needed: 170.8611204624176 for validation audios
0.35000713033108816
Model improve: 0.2911 -> 0.3500
Epoch: 4/200
Train loss: 5.1755
Date :04/29/2023, 14:12:51
Duration: 5
Sample rate: 32000
nfft: 1024
fmin: 50
nmels: 128
fmax: 14000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 40.2006
Time needed: 167.2103307247162 for validation audios
0.25254703451907623
Model improve: 0.0000 -> 0.2525
Epoch: 2/200
Train loss: 5.7923
Time needed: 168.75303268432617 for validation audios
0.288338870984166
Model improve: 0.2525 -> 0.2883
Epoch: 3/200
Date :04/29/2023, 14:21:56
Duration: 5
Sample rate: 32000
nfft: 1024
fmin: 50
nmels: 128
fmax: 14000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Date :04/29/2023, 14:22:23
Duration: 5
Sample rate: 32000
nfft: 1024
fmin: 50
nmels: 128
fmax: 14000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 29.3592
Time needed: 169.33368945121765 for validation audios
0.23949005209727087
Model improve: 0.0000 -> 0.2395
Epoch: 2/200
Train loss: 5.9522
Time needed: 170.47511768341064 for validation audios
0.24028842766854286
Model improve: 0.2395 -> 0.2403
Epoch: 3/200
Train loss: 5.9271
Date :04/29/2023, 14:34:16
Duration: 5
Sample rate: 32000
nfft: 1024
fmin: 50
nmels: 128
fmax: 14000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 29.3592
Date :04/29/2023, 14:36:23
Duration: 5
Sample rate: 32000
nfft: 1024
fmin: 50
nmels: 128
fmax: 14000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 39.9233
Time needed: 169.60563111305237 for validation audios
0.24715539469927458
Model improve: 0.0000 -> 0.2472
Epoch: 2/200
Train loss: 5.8615
Time needed: 168.98533177375793 for validation audios
0.2723056200385065
Model improve: 0.2472 -> 0.2723
Epoch: 3/200
Train loss: 5.5732
Date :04/29/2023, 14:47:00
Duration: 5
Sample rate: 32000
nfft: 1024
fmin: 50
nmels: 128
fmax: 14000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Date :04/29/2023, 14:49:47
Duration: 5
Sample rate: 32000
nfft: 1024
fmin: 50
nmels: 128
fmax: 14000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 40.1242
Time needed: 175.59180116653442 for validation audios
0.25393629225201986
Model improve: 0.0000 -> 0.2539
Epoch: 2/200
Train loss: 5.7748
Time needed: 175.6171977519989 for validation audios
0.29422675126931286
Model improve: 0.2539 -> 0.2942
Epoch: 3/200
Date :04/29/2023, 15:00:31
Duration: 5
Sample rate: 32000
nfft: 1024
fmin: 50
nmels: 128
fmax: 14000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Date :04/29/2023, 15:05:23
Duration: 5
Sample rate: 32000
nfft: 1024
fmin: 50
nmels: 128
fmax: 14000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 0.1259
Time needed: 169.98174476623535 for validation audios
0.24066706454672424
Model improve: 0.0000 -> 0.2407
Epoch: 2/200
Date :04/29/2023, 15:11:50
Duration: 5
Sample rate: 32000
nfft: 1024
fmin: 50
nmels: 128
fmax: 14000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 39.0775
Time needed: 172.28104543685913 for validation audios
0.24141859210971178
Model improve: 0.0000 -> 0.2414
Epoch: 2/200
Train loss: 5.9349
Time needed: 172.73862075805664 for validation audios
0.25306743570162066
Model improve: 0.2414 -> 0.2531
Epoch: 3/200
Train loss: 5.8523
Time needed: 170.61485934257507 for validation audios
0.27575265842196267
Model improve: 0.2531 -> 0.2758
Epoch: 4/200
Date :04/29/2023, 15:30:05
Duration: 5
Sample rate: 32000
nfft: 1024
fmin: 50
nmels: 128
fmax: 14000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 0.1335
Time needed: 165.43002319335938 for validation audios
0.24983317216320441
Model improve: 0.0000 -> 0.2498
Epoch: 2/200
Train loss: 0.0221
Time needed: 170.21463084220886 for validation audios
0.27783860643536795
Model improve: 0.2498 -> 0.2778
Epoch: 3/200
Date :04/29/2023, 15:39:09
Duration: 5
Sample rate: 32000
nfft: 1024
fmin: 50
nmels: 128
fmax: 14000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 0.1335
Time needed: 168.72345495224 for validation audios
0.24851155302517772
Model improve: 0.0000 -> 0.2485
Epoch: 2/200
Train loss: 0.0222
Time needed: 170.33300352096558 for validation audios
0.2746946723695742
Model improve: 0.2485 -> 0.2747
Epoch: 3/200
Train loss: 0.0211
Time needed: 172.6053352355957 for validation audios
0.32211675629884284
Model improve: 0.2747 -> 0.3221
Epoch: 4/200
Train loss: 0.0199
Time needed: 172.14874911308289 for validation audios
0.38542437645796845
Model improve: 0.3221 -> 0.3854
Epoch: 5/200
Train loss: 0.0187
Time needed: 170.93741464614868 for validation audios
0.4469200123717916
Model improve: 0.3854 -> 0.4469
Epoch: 6/200
Date :04/29/2023, 16:01:22
Duration: 5
Sample rate: 32000
nfft: 1024
fmin: 50
nmels: 128
fmax: 14000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 0.1336
Epoch: 2/200
Train loss: 0.0222
Epoch: 3/200
Train loss: 0.0213
Epoch: 4/200
Train loss: 0.0201
Epoch: 5/200
Train loss: 0.0190
Epoch: 6/200
Train loss: 0.0182
Epoch: 7/200
Train loss: 0.0173
Epoch: 8/200
Train loss: 0.0166
Epoch: 9/200
Train loss: 0.0159
Epoch: 10/200
Train loss: 0.0154
Epoch: 11/200
Train loss: 0.0150
Epoch: 12/200
Train loss: 0.0146
Epoch: 13/200
Train loss: 0.0142
Epoch: 14/200
Train loss: 0.0139
Epoch: 15/200
Train loss: 0.0136
Epoch: 16/200
Train loss: 0.0133
Epoch: 17/200
Train loss: 0.0131
Epoch: 18/200
Train loss: 0.0129
Epoch: 19/200
Train loss: 0.0128
Epoch: 20/200
Train loss: 0.0125
Epoch: 21/200
Train loss: 0.0124
Epoch: 22/200
Train loss: 0.0122
Epoch: 23/200
Train loss: 0.0120
Epoch: 24/200
Train loss: 0.0119
Epoch: 25/200
Train loss: 0.0119
Epoch: 26/200
Train loss: 0.0118
Epoch: 27/200
Train loss: 0.0117
Epoch: 28/200
Train loss: 0.0115
Epoch: 29/200
Train loss: 0.0114
Epoch: 30/200
Train loss: 0.0114
Epoch: 31/200
Train loss: 0.0112
Epoch: 32/200
Train loss: 0.0113
Epoch: 33/200
Train loss: 0.0112
Epoch: 34/200
Train loss: 0.0111
Epoch: 35/200
Train loss: 0.0110
Epoch: 36/200
Train loss: 0.0109
Epoch: 37/200
Train loss: 0.0109
Epoch: 38/200
Train loss: 0.0108
Epoch: 39/200
Train loss: 0.0108
Epoch: 40/200
Train loss: 0.0107
Epoch: 41/200
Train loss: 0.0106
Epoch: 42/200
Train loss: 0.0107
Epoch: 43/200
Train loss: 0.0106
Epoch: 44/200
Train loss: 0.0105
Epoch: 45/200
Train loss: 0.0105
Epoch: 46/200
Train loss: 0.0104
Epoch: 47/200
Train loss: 0.0104
Epoch: 48/200
Train loss: 0.0103
Epoch: 49/200
Train loss: 0.0103
Epoch: 50/200
Train loss: 0.0103
Epoch: 51/200
Train loss: 0.0102
Epoch: 52/200
Train loss: 0.0103
Epoch: 53/200
Train loss: 0.0102
Epoch: 54/200
Train loss: 0.0102
Epoch: 55/200
Train loss: 0.0101
Epoch: 56/200
Train loss: 0.0101
Epoch: 57/200
Train loss: 0.0101
Epoch: 58/200
Train loss: 0.0100
Epoch: 59/200
Train loss: 0.0101
Epoch: 60/200
Train loss: 0.0100
Epoch: 61/200
Train loss: 0.0099
Epoch: 62/200
Train loss: 0.0099
Epoch: 63/200
Train loss: 0.0099
Epoch: 64/200
Train loss: 0.0099
Epoch: 65/200
Train loss: 0.0098
Epoch: 66/200
Train loss: 0.0098
Epoch: 67/200
Train loss: 0.0098
Epoch: 68/200
Train loss: 0.0097
Epoch: 69/200
Train loss: 0.0097
Epoch: 70/200
Train loss: 0.0097
Epoch: 71/200
Train loss: 0.0097
Epoch: 72/200
Train loss: 0.0096
Epoch: 73/200
Train loss: 0.0096
Epoch: 74/200
Train loss: 0.0095
Epoch: 75/200
Train loss: 0.0096
Epoch: 76/200
Train loss: 0.0096
Epoch: 77/200
Train loss: 0.0095
Epoch: 78/200
Train loss: 0.0095
Epoch: 79/200
Train loss: 0.0096
Epoch: 80/200
Train loss: 0.0094
Epoch: 81/200
Train loss: 0.0094
Epoch: 82/200
Train loss: 0.0094
Epoch: 83/200
Train loss: 0.0094
Epoch: 84/200
Train loss: 0.0094
Epoch: 85/200
Train loss: 0.0094
Epoch: 86/200
Train loss: 0.0094
Epoch: 87/200
Train loss: 0.0094
Epoch: 88/200
Train loss: 0.0094
Epoch: 89/200
Train loss: 0.0093
Epoch: 90/200
Train loss: 0.0093
Epoch: 91/200
Train loss: 0.0093
Epoch: 92/200
Train loss: 0.0093
Epoch: 93/200
Train loss: 0.0092
Epoch: 94/200
Train loss: 0.0093
Epoch: 95/200
Train loss: 0.0093
Epoch: 96/200
Train loss: 0.0093
Epoch: 97/200
Train loss: 0.0092
Epoch: 98/200
Train loss: 0.0092
Epoch: 99/200
Train loss: 0.0091
Epoch: 100/200
Train loss: 0.0092
Epoch: 101/200
Train loss: 0.0092
Epoch: 102/200
Train loss: 0.0091
Epoch: 103/200
Train loss: 0.0092
Epoch: 104/200
Train loss: 0.0091
Epoch: 105/200
Train loss: 0.0091
Epoch: 106/200
Train loss: 0.0091
Epoch: 107/200
Train loss: 0.0091
Epoch: 108/200
Train loss: 0.0091
Epoch: 109/200
Train loss: 0.0090
Epoch: 110/200
Train loss: 0.0091
Epoch: 111/200
Train loss: 0.0090
Epoch: 112/200
Train loss: 0.0091
Epoch: 113/200
Train loss: 0.0090
Epoch: 114/200
Train loss: 0.0090
Epoch: 115/200
Train loss: 0.0091
Epoch: 116/200
Train loss: 0.0091
Epoch: 117/200
Train loss: 0.0090
Epoch: 118/200
Train loss: 0.0090
Epoch: 119/200
Train loss: 0.0090
Epoch: 120/200
Train loss: 0.0089
Epoch: 121/200
Train loss: 0.0089
Epoch: 122/200
Train loss: 0.0089
Epoch: 123/200
Train loss: 0.0090
Epoch: 124/200
Train loss: 0.0089
Epoch: 125/200
Train loss: 0.0089
Epoch: 126/200
Train loss: 0.0089
Epoch: 127/200
Train loss: 0.0089
Epoch: 128/200
Train loss: 0.0091
Epoch: 129/200
Train loss: 0.0089
Epoch: 130/200
Train loss: 0.0089
Epoch: 131/200
Train loss: 0.0088
Epoch: 132/200
Train loss: 0.0089
Time needed: 165.86880493164062 for validation audios
0.7907111101750772
Model improve: 0.0000 -> 0.7907
Epoch: 133/200
Train loss: 0.0089
Time needed: 168.07163405418396 for validation audios
0.7905823638154194
Epoch: 134/200
Train loss: 0.0088
Time needed: 166.78938627243042 for validation audios
0.7917813652632172
Model improve: 0.7907 -> 0.7918
Epoch: 135/200
Train loss: 0.0089
Time needed: 167.15966868400574 for validation audios
0.7921198807795701
Model improve: 0.7918 -> 0.7921
Epoch: 136/200
Train loss: 0.0088
Time needed: 168.5880105495453 for validation audios
0.7920702389096452
Epoch: 137/200
Train loss: 0.0090
Time needed: 167.7203872203827 for validation audios
0.7927634158641168
Model improve: 0.7921 -> 0.7928
Epoch: 138/200
Train loss: 0.0088
Time needed: 167.57784938812256 for validation audios
0.7918275939420386
Epoch: 139/200
Train loss: 0.0088
Time needed: 172.59632778167725 for validation audios
0.79231661781748
Epoch: 140/200
Train loss: 0.0088
Time needed: 169.74036407470703 for validation audios
0.791690768493548
Epoch: 141/200
Train loss: 0.0088
Time needed: 173.01629066467285 for validation audios
0.7921536313964743
Epoch: 142/200
Train loss: 0.0089
Time needed: 169.53996562957764 for validation audios
0.792987902034641
Model improve: 0.7928 -> 0.7930
Epoch: 143/200
Train loss: 0.0089
Time needed: 169.85287380218506 for validation audios
0.7919268027376996
Epoch: 144/200
Train loss: 0.0087
Time needed: 169.99823331832886 for validation audios
0.791198434605811
Epoch: 145/200
Train loss: 0.0088
Time needed: 167.69866704940796 for validation audios
0.7909465471260873
Epoch: 146/200
Train loss: 0.0088
Time needed: 168.36272835731506 for validation audios
0.7924013400645752
Epoch: 147/200
Train loss: 0.0088
Time needed: 169.63936281204224 for validation audios
0.7925996248043832
Epoch: 148/200
Train loss: 0.0088
Time needed: 167.5960340499878 for validation audios
0.7918904423354094
Epoch: 149/200
Train loss: 0.0089
Time needed: 167.5913372039795 for validation audios
0.7920297783442808
Epoch: 150/200
Date :04/29/2023, 23:45:29
Duration: 5
Sample rate: 32000
nfft: 1024
fmin: 50
nmels: 256
fmax: 14000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Date :04/29/2023, 23:45:53
Duration: 5
Sample rate: 32000
nfft: 1024
fmin: 50
nmels: 256
fmax: 14000
trainbs: 96
validbs: 288
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 42.3157
Epoch: 2/200
Train loss: 5.4714
Epoch: 3/200
Train loss: 4.9991
Epoch: 4/200
Train loss: 4.5837
Epoch: 5/200
Train loss: 4.2392
Epoch: 6/200
Train loss: 3.9877
Epoch: 7/200
Train loss: 3.8366
Epoch: 8/200
Train loss: 3.7105
Epoch: 9/200
Train loss: 3.5958
Epoch: 10/200
Train loss: 3.5253
Epoch: 11/200
Train loss: 3.3749
Epoch: 12/200
Train loss: 3.4002
Epoch: 13/200
Train loss: 3.1996
Epoch: 14/200
Train loss: 3.2466
Epoch: 15/200
Train loss: 3.1803
Epoch: 16/200
Train loss: 3.0561
Epoch: 17/200
Train loss: 2.9766
Epoch: 18/200
Train loss: 3.0649
Epoch: 19/200
Train loss: 3.0422
Epoch: 20/200
Train loss: 3.0505
Epoch: 21/200
Train loss: 2.9423
Epoch: 22/200
Train loss: 2.8748
Epoch: 23/200
Train loss: 2.8029
Epoch: 24/200
Train loss: 2.9290
Epoch: 25/200
Train loss: 2.7799
Epoch: 26/200
Train loss: 2.8704
Epoch: 27/200
Train loss: 2.8817
Epoch: 28/200
Train loss: 2.7587
Epoch: 29/200
Train loss: 2.8549
Epoch: 30/200
Train loss: 2.7563
Epoch: 31/200
Train loss: 2.7263
Epoch: 32/200
Train loss: 2.7687
Epoch: 33/200
Train loss: 2.7703
Epoch: 34/200
Train loss: 2.7516
Epoch: 35/200
Train loss: 2.6553
Epoch: 36/200
Train loss: 2.5223
Epoch: 37/200
Train loss: 2.5798
Epoch: 38/200
Train loss: 2.5365
Epoch: 39/200
Train loss: 2.6047
Epoch: 40/200
Train loss: 2.5678
Epoch: 41/200
Train loss: 2.5898
Epoch: 42/200
Train loss: 2.5176
Epoch: 43/200
Train loss: 2.5480
Epoch: 44/200
Train loss: 2.5899
Epoch: 45/200
Train loss: 2.5557
Epoch: 46/200
Train loss: 2.4876
Epoch: 47/200
Train loss: 2.5347
Epoch: 48/200
Train loss: 2.5192
Epoch: 49/200
Train loss: 2.5606
Epoch: 50/200
Train loss: 2.4485
Epoch: 51/200
Train loss: 2.5257
Epoch: 52/200
Train loss: 2.5954
Epoch: 53/200
Train loss: 2.3658
Epoch: 54/200
Train loss: 2.4057
Epoch: 55/200
Train loss: 2.4553
Epoch: 56/200
Train loss: 2.5117
Epoch: 57/200
Train loss: 2.4287
Epoch: 58/200
Train loss: 2.3856
Epoch: 59/200
Train loss: 2.3940
Epoch: 60/200
Train loss: 2.3018
Epoch: 61/200
Train loss: 2.4460
Epoch: 62/200
Train loss: 2.3212
Epoch: 63/200
Train loss: 2.3527
Epoch: 64/200
Train loss: 2.4482
Epoch: 65/200
Train loss: 2.2696
Epoch: 66/200
Train loss: 2.4666
Epoch: 67/200
Train loss: 2.4388
Epoch: 68/200
Train loss: 2.3621
Epoch: 69/200
Train loss: 2.3901
Epoch: 70/200
Train loss: 2.4259
Epoch: 71/200
Train loss: 2.4287
Epoch: 72/200
Train loss: 2.4179
Epoch: 73/200
Train loss: 2.3627
Epoch: 74/200
Train loss: 2.3345
Epoch: 75/200
Train loss: 2.3622
Epoch: 76/200
Train loss: 2.3947
Epoch: 77/200
Train loss: 2.1227
Epoch: 78/200
Train loss: 2.2773
Epoch: 79/200
Train loss: 2.2402
Epoch: 80/200
Train loss: 2.3586
Epoch: 81/200
Train loss: 2.3705
Epoch: 82/200
Train loss: 2.2998
Epoch: 83/200
Train loss: 2.3380
Epoch: 84/200
Train loss: 2.2895
Epoch: 85/200
Train loss: 2.3230
Epoch: 86/200
Train loss: 2.3142
Epoch: 87/200
Train loss: 2.2803
Epoch: 88/200
Train loss: 2.2842
Epoch: 89/200
Train loss: 2.2334
Epoch: 90/200
Train loss: 2.3215
Epoch: 91/200
Train loss: 2.2181
Epoch: 92/200
Train loss: 2.3784
Epoch: 93/200
Train loss: 2.4452
Epoch: 94/200
Train loss: 2.2659
Epoch: 95/200
Train loss: 2.3022
Epoch: 96/200
Train loss: 2.1873
Epoch: 97/200
Train loss: 2.1514
Epoch: 98/200
Train loss: 2.2317
Epoch: 99/200
Train loss: 2.3092
Epoch: 100/200
Train loss: 2.2255
Epoch: 101/200
Train loss: 2.2445
Epoch: 102/200
Train loss: 2.1479
Epoch: 103/200
Train loss: 2.2323
Epoch: 104/200
Train loss: 2.1741
Epoch: 105/200
Train loss: 2.2253
Epoch: 106/200
Train loss: 2.1821
Epoch: 107/200
Train loss: 2.2598
Epoch: 108/200
Train loss: 2.2834
Epoch: 109/200
Train loss: 2.2033
Epoch: 110/200
Train loss: 2.2450
Epoch: 111/200
Train loss: 2.2918
Epoch: 112/200
Train loss: 2.1379
Epoch: 113/200
Train loss: 2.1786
Epoch: 114/200
Train loss: 2.1465
Epoch: 115/200
Train loss: 2.0863
Epoch: 116/200
Train loss: 2.2220
Epoch: 117/200
Train loss: 2.2241
Epoch: 118/200
Train loss: 2.1501
Epoch: 119/200
Train loss: 2.1354
Epoch: 120/200
Train loss: 2.1214
Epoch: 121/200
Train loss: 2.2664
Epoch: 122/200
Train loss: 2.1392
Epoch: 123/200
Train loss: 2.2102
Epoch: 124/200
Train loss: 2.2525
Epoch: 125/200
Train loss: 2.2976
Epoch: 126/200
Train loss: 2.2080
Epoch: 127/200
Train loss: 2.1956
Epoch: 128/200
Train loss: 2.2517
Epoch: 129/200
Train loss: 2.1589
Epoch: 130/200
Train loss: 2.1848
Epoch: 131/200
Train loss: 2.1343
Epoch: 132/200
Train loss: 2.1355
Time needed: 199.35903000831604 for validation audios
0.8004453798694284
Model improve: 0.0000 -> 0.8004
Epoch: 133/200
Train loss: 2.1585
Time needed: 201.93962717056274 for validation audios
0.7965999095017715
Epoch: 134/200
Train loss: 2.0682
Time needed: 201.512859582901 for validation audios
0.799431910054271
Epoch: 135/200
Train loss: 2.2622
Time needed: 202.75337624549866 for validation audios
0.7973301807384017
Epoch: 136/200
Train loss: 2.1258
Time needed: 201.89034986495972 for validation audios
0.7974512740932026
Epoch: 137/200
Train loss: 2.0963
Time needed: 200.18749570846558 for validation audios
0.7980101593224679
Epoch: 138/200
Train loss: 2.2168
Time needed: 203.08665537834167 for validation audios
0.796481164399064
Epoch: 139/200
Train loss: 2.1657
Time needed: 202.5515434741974 for validation audios
0.7955467185219429
Epoch: 140/200
Train loss: 2.1462
Time needed: 198.93674206733704 for validation audios
0.7964397853621382
Epoch: 141/200
Train loss: 2.1945
Time needed: 199.04984140396118 for validation audios
0.7963140843779143
Epoch: 142/200
Train loss: 2.2305
Time needed: 200.9562246799469 for validation audios
0.7962593656975273
Epoch: 143/200
Train loss: 2.1927
Time needed: 200.15233945846558 for validation audios
0.7956746363907506
Epoch: 144/200
Train loss: 2.1974
Time needed: 201.6368112564087 for validation audios
0.7976887226860643
Epoch: 145/200
Train loss: 2.2331
Time needed: 200.92813682556152 for validation audios
0.7957541515433509
Epoch: 146/200
Train loss: 2.1458
Time needed: 200.26484513282776 for validation audios
0.7963718335989891
Epoch: 147/200
Train loss: 2.1942
Time needed: 200.36225414276123 for validation audios
0.7963556829348076
Epoch: 148/200
Train loss: 2.0810
Time needed: 199.9523570537567 for validation audios
0.7987014514537191
Epoch: 149/200
Train loss: 2.1913
Time needed: 202.06448435783386 for validation audios
0.7968658106323563
Epoch: 150/200
Train loss: 2.1792
Time needed: 202.3593921661377 for validation audios
0.7976974856036597
Epoch: 151/200
Train loss: 2.2526
Date :04/30/2023, 05:28:28
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 256
fmax: 16000
trainbs: 96
validbs: 288
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 42.2275
Epoch: 2/200
Train loss: 5.4466
Epoch: 3/200
Train loss: 4.9799
Epoch: 4/200
Train loss: 4.5656
Epoch: 5/200
Train loss: 4.2224
Epoch: 6/200
Train loss: 3.9673
Epoch: 7/200
Train loss: 3.8004
Epoch: 8/200
Train loss: 3.6731
Epoch: 9/200
Train loss: 3.5672
Epoch: 10/200
Train loss: 3.4980
Epoch: 11/200
Train loss: 3.3427
Epoch: 12/200
Train loss: 3.3715
Epoch: 13/200
Train loss: 3.1709
Epoch: 14/200
Train loss: 3.2219
Epoch: 15/200
Train loss: 3.1555
Epoch: 16/200
Train loss: 3.0328
Epoch: 17/200
Train loss: 2.9522
Epoch: 18/200
Train loss: 3.0460
Epoch: 19/200
Train loss: 3.0221
Epoch: 20/200
Train loss: 3.0301
Epoch: 21/200
Train loss: 2.9276
Epoch: 22/200
Train loss: 2.8566
Epoch: 23/200
Train loss: 2.7849
Epoch: 24/200
Train loss: 2.9081
Epoch: 25/200
Train loss: 2.7581
Epoch: 26/200
Train loss: 2.8489
Epoch: 27/200
Train loss: 2.8616
Epoch: 28/200
Train loss: 2.7404
Epoch: 29/200
Train loss: 2.8411
Epoch: 30/200
Train loss: 2.7393
Epoch: 31/200
Train loss: 2.7085
Epoch: 32/200
Train loss: 2.7482
Epoch: 33/200
Train loss: 2.7510
Epoch: 34/200
Train loss: 2.7322
Epoch: 35/200
Train loss: 2.6372
Epoch: 36/200
Train loss: 2.5047
Epoch: 37/200
Train loss: 2.5605
Epoch: 38/200
Train loss: 2.5232
Epoch: 39/200
Train loss: 2.5923
Epoch: 40/200
Train loss: 2.5521
Epoch: 41/200
Train loss: 2.5736
Epoch: 42/200
Train loss: 2.5037
Epoch: 43/200
Train loss: 2.5337
Epoch: 44/200
Train loss: 2.5729
Epoch: 45/200
Train loss: 2.5380
Epoch: 46/200
Train loss: 2.4680
Epoch: 47/200
Train loss: 2.5191
Epoch: 48/200
Train loss: 2.5098
Epoch: 49/200
Train loss: 2.5486
Epoch: 50/200
Train loss: 2.4361
Epoch: 51/200
Train loss: 2.5121
Epoch: 52/200
Train loss: 2.5816
Epoch: 53/200
Train loss: 2.3560
Epoch: 54/200
Train loss: 2.3950
Epoch: 55/200
Train loss: 2.4427
Epoch: 56/200
Train loss: 2.4961
Epoch: 57/200
Train loss: 2.4155
Epoch: 58/200
Train loss: 2.3742
Epoch: 59/200
Train loss: 2.3841
Epoch: 60/200
Train loss: 2.2892
Epoch: 61/200
Train loss: 2.4345
Epoch: 62/200
Train loss: 2.3092
Epoch: 63/200
Train loss: 2.3385
Epoch: 64/200
Train loss: 2.4338
Epoch: 65/200
Train loss: 2.2593
Epoch: 66/200
Train loss: 2.4525
Epoch: 67/200
Train loss: 2.4279
Epoch: 68/200
Train loss: 2.3494
Epoch: 69/200
Train loss: 2.3747
Epoch: 70/200
Train loss: 2.4142
Epoch: 71/200
Train loss: 2.4185
Epoch: 72/200
Train loss: 2.4047
Epoch: 73/200
Train loss: 2.3544
Epoch: 74/200
Train loss: 2.3282
Epoch: 75/200
Train loss: 2.3502
Epoch: 76/200
Train loss: 2.3826
Epoch: 77/200
Train loss: 2.1098
Epoch: 78/200
Train loss: 2.2665
Epoch: 79/200
Train loss: 2.2265
Epoch: 80/200
Train loss: 2.3450
Epoch: 81/200
Train loss: 2.3593
Epoch: 82/200
Train loss: 2.2873
Epoch: 83/200
Train loss: 2.3249
Epoch: 84/200
Train loss: 2.2778
Epoch: 85/200
Train loss: 2.3124
Epoch: 86/200
Train loss: 2.3039
Epoch: 87/200
Train loss: 2.2763
Epoch: 88/200
Train loss: 2.2748
Epoch: 89/200
Train loss: 2.2219
Epoch: 90/200
Train loss: 2.3091
Epoch: 91/200
Train loss: 2.2038
Epoch: 92/200
Train loss: 2.3693
Epoch: 93/200
Train loss: 2.4298
Epoch: 94/200
Train loss: 2.2493
Epoch: 95/200
Train loss: 2.2900
Epoch: 96/200
Train loss: 2.1763
Epoch: 97/200
Train loss: 2.1402
Epoch: 98/200
Train loss: 2.2199
Epoch: 99/200
Train loss: 2.2936
Epoch: 100/200
Train loss: 2.2130
Epoch: 101/200
Train loss: 2.2322
Epoch: 102/200
Train loss: 2.1396
Epoch: 103/200
Train loss: 2.2171
Epoch: 104/200
Train loss: 2.1625
Epoch: 105/200
Train loss: 2.2119
Epoch: 106/200
Train loss: 2.1732
Epoch: 107/200
Train loss: 2.2454
Epoch: 108/200
Train loss: 2.2678
Epoch: 109/200
Train loss: 2.1941
Epoch: 110/200
Train loss: 2.2339
Epoch: 111/200
Train loss: 2.2768
Epoch: 112/200
Train loss: 2.1248
Epoch: 113/200
Train loss: 2.1634
Epoch: 114/200
Train loss: 2.1356
Epoch: 115/200
Train loss: 2.0772
Epoch: 116/200
Train loss: 2.2069
Epoch: 117/200
Train loss: 2.2109
Epoch: 118/200
Train loss: 2.1393
Epoch: 119/200
Train loss: 2.1291
Epoch: 120/200
Train loss: 2.1152
Epoch: 121/200
Train loss: 2.2604
Epoch: 122/200
Train loss: 2.1277
Epoch: 123/200
Train loss: 2.1996
Epoch: 124/200
Train loss: 2.2463
Epoch: 125/200
Train loss: 2.2816
Epoch: 126/200
Train loss: 2.1939
Epoch: 127/200
Train loss: 2.1838
Epoch: 128/200
Train loss: 2.2424
Epoch: 129/200
Train loss: 2.1478
Epoch: 130/200
Train loss: 2.1795
Epoch: 131/200
Train loss: 2.1262
Epoch: 132/200
Train loss: 2.1298
Time needed: 193.67678022384644 for validation audios
0.8023979224973075
Model improve: 0.0000 -> 0.8024
Epoch: 133/200
Train loss: 2.1428
Time needed: 193.443608045578 for validation audios
0.7980415446747182
Epoch: 134/200
Train loss: 2.0614
Time needed: 194.0733199119568 for validation audios
0.8015930376478384
Epoch: 135/200
Train loss: 2.2514
Time needed: 193.78623604774475 for validation audios
0.7990165895990796
Epoch: 136/200
Train loss: 2.1185
Time needed: 193.80587577819824 for validation audios
0.7995604400674486
Epoch: 137/200
Train loss: 2.0859
Time needed: 196.40051984786987 for validation audios
0.7997833967577989
Epoch: 138/200
Train loss: 2.2020
Date :04/30/2023, 10:00:54
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 40.3427
Epoch: 2/200
Train loss: 5.7818
Epoch: 3/200
Train loss: 5.4618
Epoch: 4/200
Train loss: 5.1865
Epoch: 5/200
Date :04/30/2023, 10:06:44
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13830
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 40.2643
Epoch: 2/200
Train loss: 5.7723
Epoch: 3/200
Train loss: 5.4444
Epoch: 4/200
Train loss: 5.1813
Epoch: 5/200
Train loss: 4.8726
Epoch: 6/200
Train loss: 4.6079
Epoch: 7/200
Train loss: 4.2788
Epoch: 8/200
Train loss: 4.1863
Epoch: 9/200
Train loss: 3.9847
Epoch: 10/200
Train loss: 3.8790
Epoch: 11/200
Train loss: 3.7534
Epoch: 12/200
Train loss: 3.6307
Epoch: 13/200
Train loss: 3.5586
Epoch: 14/200
Train loss: 3.4347
Epoch: 15/200
Train loss: 3.4209
Epoch: 16/200
Train loss: 3.3572
Epoch: 17/200
Train loss: 3.2176
Epoch: 18/200
Train loss: 3.1267
Epoch: 19/200
Train loss: 3.3165
Epoch: 20/200
Train loss: 3.1836
Epoch: 21/200
Train loss: 3.0423
Epoch: 22/200
Train loss: 2.9204
Epoch: 23/200
Train loss: 2.9532
Epoch: 24/200
Train loss: 3.0120
Epoch: 25/200
Train loss: 2.9821
Epoch: 26/200
Train loss: 2.9618
Epoch: 27/200
Train loss: 2.9382
Epoch: 28/200
Train loss: 2.7994
Epoch: 29/200
Train loss: 2.8257
Epoch: 30/200
Train loss: 2.6776
Epoch: 31/200
Train loss: 2.8070
Epoch: 32/200
Train loss: 2.8525
Epoch: 33/200
Train loss: 2.6518
Epoch: 34/200
Train loss: 2.7268
Epoch: 35/200
Train loss: 2.7375
Epoch: 36/200
Train loss: 2.7506
Epoch: 37/200
Train loss: 2.6476
Epoch: 38/200
Train loss: 2.8509
Epoch: 39/200
Train loss: 2.5714
Epoch: 40/200
Train loss: 2.7002
Epoch: 41/200
Train loss: 2.6218
Epoch: 42/200
Train loss: 2.6241
Epoch: 43/200
Train loss: 2.6946
Epoch: 44/200
Train loss: 2.6584
Epoch: 45/200
Train loss: 2.6871
Epoch: 46/200
Train loss: 2.4425
Epoch: 47/200
Train loss: 2.6373
Epoch: 48/200
Train loss: 2.4015
Epoch: 49/200
Train loss: 2.3939
Epoch: 50/200
Train loss: 2.4505
Epoch: 51/200
Train loss: 2.4883
Epoch: 52/200
Train loss: 2.4885
Epoch: 53/200
Train loss: 2.3938
Epoch: 54/200
Train loss: 2.4847
Epoch: 55/200
Train loss: 2.4654
Epoch: 56/200
Train loss: 2.4547
Epoch: 57/200
Train loss: 2.4845
Epoch: 58/200
Train loss: 2.4468
Epoch: 59/200
Train loss: 2.4473
Epoch: 60/200
Train loss: 2.4724
Epoch: 61/200
Train loss: 2.3520
Epoch: 62/200
Train loss: 2.3983
Epoch: 63/200
Train loss: 2.4002
Epoch: 64/200
Train loss: 2.4258
Epoch: 65/200
Train loss: 2.4544
Epoch: 66/200
Train loss: 2.3364
Epoch: 67/200
Train loss: 2.4049
Epoch: 68/200
Train loss: 2.3894
Epoch: 69/200
Train loss: 2.5711
Epoch: 70/200
Train loss: 2.3625
Epoch: 71/200
Train loss: 2.2433
Epoch: 72/200
Train loss: 2.2752
Epoch: 73/200
Train loss: 2.3755
Epoch: 74/200
Train loss: 2.3644
Epoch: 75/200
Train loss: 2.4229
Epoch: 76/200
Train loss: 2.3955
Epoch: 77/200
Train loss: 2.3572
Epoch: 78/200
Train loss: 2.2677
Epoch: 79/200
Train loss: 2.1210
Epoch: 80/200
Train loss: 2.2906
Epoch: 81/200
Train loss: 2.2948
Epoch: 82/200
Train loss: 2.2869
Epoch: 83/200
Train loss: 2.2326
Epoch: 84/200
Train loss: 2.2315
Epoch: 85/200
Train loss: 2.3524
Epoch: 86/200
Train loss: 2.2857
Epoch: 87/200
Train loss: 2.1711
Epoch: 88/200
Train loss: 2.3426
Epoch: 89/200
Train loss: 2.3751
Epoch: 90/200
Train loss: 2.2308
Epoch: 91/200
Train loss: 2.3368
Epoch: 92/200
Train loss: 2.3270
Epoch: 93/200
Date :04/30/2023, 12:04:36
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13668
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 40.8513
Epoch: 2/200
Date :04/30/2023, 12:06:35
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13668
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 40.8526
Time needed: 165.2853181362152 for validation audios
0.23266619729105745
Model improve: 0.0000 -> 0.2327
Epoch: 2/200
Train loss: 5.7811
Time needed: 165.253812789917 for validation audios
0.26833182534481614
Model improve: 0.2327 -> 0.2683
Epoch: 3/200
Date :04/30/2023, 12:14:53
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13830
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 40.1302
Time needed: 173.46611618995667 for validation audios
0.2472735669706183
Model improve: 0.0000 -> 0.2473
Epoch: 2/200
Train loss: 5.8017
Time needed: 174.82448816299438 for validation audios
0.2832243901150145
Model improve: 0.2473 -> 0.2832
Epoch: 3/200
Train loss: 5.5180
Time needed: 174.87485790252686 for validation audios
0.3351603113867571
Model improve: 0.2832 -> 0.3352
Epoch: 4/200
Date :04/30/2023, 12:28:35
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 40.4137
Time needed: 164.26477098464966 for validation audios
0.25164604570410304
Model improve: 0.0000 -> 0.2516
Epoch: 2/200
Train loss: 5.8131
Time needed: 165.82024693489075 for validation audios
0.2846624209479092
Model improve: 0.2516 -> 0.2847
Epoch: 3/200
Train loss: 5.5345
Time needed: 165.95976066589355 for validation audios
0.33835249727839956
Model improve: 0.2847 -> 0.3384
Epoch: 4/200
Train loss: 5.2759
Time needed: 166.78462195396423 for validation audios
0.39638066491699053
Model improve: 0.3384 -> 0.3964
Epoch: 5/200
Train loss: 4.9692
Time needed: 165.69453740119934 for validation audios
0.4532933834272509
Model improve: 0.3964 -> 0.4533
Epoch: 6/200
Train loss: 4.6989
Time needed: 167.7904028892517 for validation audios
0.499126354540393
Model improve: 0.4533 -> 0.4991
Epoch: 7/200
Train loss: 4.4091
Time needed: 168.97319412231445 for validation audios
0.5378923024247955
Model improve: 0.4991 -> 0.5379
Epoch: 8/200
Train loss: 4.3026
Time needed: 168.6004946231842 for validation audios
0.5725152977536181
Model improve: 0.5379 -> 0.5725
Epoch: 9/200
Date :04/30/2023, 13:01:15
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 40.3323
Time needed: 167.7096869945526 for validation audios
0.25367897760243296
Model improve: 0.0000 -> 0.2537
Epoch: 2/200
Train loss: 5.7848
Time needed: 167.0543131828308 for validation audios
0.2933586906307561
Model improve: 0.2537 -> 0.2934
Epoch: 3/200
Train loss: 5.4565
Time needed: 167.8266499042511 for validation audios
0.35236534384449064
Model improve: 0.2934 -> 0.3524
Epoch: 4/200
Train loss: 5.1876
Time needed: 165.92129850387573 for validation audios
0.41318716457253174
Model improve: 0.3524 -> 0.4132
Epoch: 5/200
Train loss: 4.8786
Time needed: 168.32348728179932 for validation audios
0.47371934746122796
Model improve: 0.4132 -> 0.4737
Epoch: 6/200
Train loss: 4.6117
Time needed: 166.21302127838135 for validation audios
0.5223323269897495
Model improve: 0.4737 -> 0.5223
Epoch: 7/200
Train loss: 4.2936
Time needed: 168.2034924030304 for validation audios
0.5617926644137585
Model improve: 0.5223 -> 0.5618
Epoch: 8/200
Train loss: 4.1937
Time needed: 167.24392652511597 for validation audios
0.5944250263446758
Model improve: 0.5618 -> 0.5944
Epoch: 9/200
Train loss: 3.9943
Date :04/30/2023, 13:36:08
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 16
validbs: 48
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 10.3617
Time needed: 162.98026132583618 for validation audios
0.45391275783885515
Model improve: 0.0000 -> 0.4539
Epoch: 2/200
Train loss: 4.5315
Time needed: 164.4154577255249 for validation audios
0.5977517631357339
Model improve: 0.4539 -> 0.5978
Epoch: 3/200
Train loss: 3.9676
Time needed: 169.64636421203613 for validation audios
0.6504772513900645
Model improve: 0.5978 -> 0.6505
Epoch: 4/200
Date :04/30/2023, 13:53:51
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
Date :04/30/2023, 13:55:57
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
Date :04/30/2023, 13:56:51
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 43.6771
Time needed: 164.14089608192444 for validation audios
0.24511923323136742
Model improve: 0.0000 -> 0.2451
Epoch: 2/200
Train loss: 5.9331
Time needed: 164.59208822250366 for validation audios
0.2698619750406621
Model improve: 0.2451 -> 0.2699
Epoch: 3/200
Train loss: 5.7586
Date :04/30/2023, 14:07:03
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: False
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 39.3242
Time needed: 166.56835007667542 for validation audios
0.25410467886986443
Model improve: 0.0000 -> 0.2541
Epoch: 2/200
Train loss: 5.7299
Time needed: 165.84956455230713 for validation audios
0.29429484026586833
Model improve: 0.2541 -> 0.2943
Epoch: 3/200
Train loss: 5.3986
Time needed: 165.66780138015747 for validation audios
0.3548279352809879
Model improve: 0.2943 -> 0.3548
Epoch: 4/200
Date :04/30/2023, 14:19:43
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 39.3186
Time needed: 164.24688696861267 for validation audios
0.25414219308832386
Model improve: 0.0000 -> 0.2541
Epoch: 2/200
Train loss: 5.7298
Time needed: 164.06338930130005 for validation audios
0.2943696864652446
Model improve: 0.2541 -> 0.2944
Epoch: 3/200
Train loss: 5.3988
Time needed: 163.7811303138733 for validation audios
0.3548081813140247
Model improve: 0.2944 -> 0.3548
Epoch: 4/200
Train loss: 5.1356
Time needed: 164.1999650001526 for validation audios
0.4186203978546963
Model improve: 0.3548 -> 0.4186
Epoch: 5/200
Train loss: 4.8084
Date :04/30/2023, 14:37:53
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13670
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 39.8197
Time needed: 166.01228499412537 for validation audios
0.2252264860838278
Model improve: 0.0000 -> 0.2252
Epoch: 2/200
Train loss: 5.7038
Time needed: 165.76119875907898 for validation audios
0.2648617957928893
Model improve: 0.2252 -> 0.2649
Epoch: 3/200
Train loss: 5.3650
Time needed: 168.3749644756317 for validation audios
0.3261533248323203
Model improve: 0.2649 -> 0.3262
Epoch: 4/200
Train loss: 5.0974
Time needed: 168.12658047676086 for validation audios
0.38570261639263276
Model improve: 0.3262 -> 0.3857
Epoch: 5/200
Train loss: 4.7948
Time needed: 166.74456000328064 for validation audios
0.439254861419868
Model improve: 0.3857 -> 0.4393
Epoch: 6/200
Train loss: 4.5111
Time needed: 169.49635815620422 for validation audios
0.485381699315102
Model improve: 0.4393 -> 0.4854
Epoch: 7/200
Train loss: 4.2236
Date :04/30/2023, 15:05:11
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13670
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 39.8165
Time needed: 167.57055592536926 for validation audios
0.22526871324828718
Model improve: 0.0000 -> 0.2253
Epoch: 2/200
Date :04/30/2023, 15:10:36
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13772
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 39.6558
Time needed: 171.62465453147888 for validation audios
0.2424275129299622
Model improve: 0.0000 -> 0.2424
Epoch: 2/200
Train loss: 5.7110
Time needed: 167.14195561408997 for validation audios
0.28331033732929506
Model improve: 0.2424 -> 0.2833
Epoch: 3/200
Train loss: 5.3719
Time needed: 166.61666250228882 for validation audios
0.34463804167196765
Model improve: 0.2833 -> 0.3446
Epoch: 4/200
Train loss: 5.0966
Time needed: 166.83062052726746 for validation audios
0.4061890183496986
Model improve: 0.3446 -> 0.4062
Epoch: 5/200
Train loss: 4.8143
Time needed: 167.03340458869934 for validation audios
0.4593582937576198
Model improve: 0.4062 -> 0.4594
Epoch: 6/200
Train loss: 4.5268
Date :04/30/2023, 15:40:21
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 0
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
Date :04/30/2023, 15:40:40
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 0
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13553
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 40.0872
Time needed: 167.76339650154114 for validation audios
0.25438819530293266
Model improve: 0.0000 -> 0.2544
Epoch: 2/200
Train loss: 5.6838
Time needed: 168.78501915931702 for validation audios
0.2958698435988137
Model improve: 0.2544 -> 0.2959
Epoch: 3/200
Train loss: 5.3240
Time needed: 170.08111763000488 for validation audios
0.3549541676940198
Model improve: 0.2959 -> 0.3550
Epoch: 4/200
Train loss: 5.0643
Time needed: 166.3533387184143 for validation audios
0.41601813189807024
Model improve: 0.3550 -> 0.4160
Epoch: 5/200
Train loss: 4.7535
Time needed: 165.25246119499207 for validation audios
0.46972127713337586
Model improve: 0.4160 -> 0.4697
Epoch: 6/200
Train loss: 4.5137
Time needed: 168.35692644119263 for validation audios
0.5139027435160815
Model improve: 0.4697 -> 0.5139
Epoch: 7/200
Train loss: 4.2331
Time needed: 166.01113176345825 for validation audios
0.5529542352789252
Model improve: 0.5139 -> 0.5530
Epoch: 8/200
Train loss: 4.1374
Time needed: 167.106769323349 for validation audios
0.5756940059719535
Model improve: 0.5530 -> 0.5757
Epoch: 9/200
Train loss: 3.8977
Date :04/30/2023, 16:28:10
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 0
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13553
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 40.3575
Time needed: 168.01450037956238 for validation audios
0.2561821683624136
Model improve: 0.0000 -> 0.2562
Epoch: 2/200
Date :04/30/2023, 16:32:47
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 0
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13553
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 40.3609
Epoch: 2/200
Train loss: 5.6329
Epoch: 3/200
Train loss: 5.2429
Epoch: 4/200
Train loss: 4.9605
Epoch: 5/200
Train loss: 4.6511
Epoch: 6/200
Train loss: 4.3828
Epoch: 7/200
Train loss: 4.0993
Epoch: 8/200
Train loss: 4.0129
Epoch: 9/200
Train loss: 3.7800
Epoch: 10/200
Train loss: 3.6977
Epoch: 11/200
Train loss: 3.6048
Epoch: 12/200
Train loss: 3.4942
Epoch: 13/200
Train loss: 3.3954
Epoch: 14/200
Train loss: 3.3121
Epoch: 15/200
Train loss: 3.2807
Epoch: 16/200
Train loss: 3.2657
Epoch: 17/200
Train loss: 3.0942
Epoch: 18/200
Train loss: 3.0540
Epoch: 19/200
Train loss: 3.1672
Epoch: 20/200
Train loss: 3.0534
Epoch: 21/200
Train loss: 3.0124
Epoch: 22/200
Train loss: 2.8449
Epoch: 23/200
Train loss: 2.8293
Epoch: 24/200
Train loss: 2.8540
Epoch: 25/200
Train loss: 2.9750
Epoch: 26/200
Train loss: 2.7588
Epoch: 27/200
Train loss: 2.8546
Epoch: 28/200
Train loss: 2.7884
Epoch: 29/200
Train loss: 2.6859
Epoch: 30/200
Train loss: 2.6924
Epoch: 31/200
Train loss: 2.5740
Epoch: 32/200
Train loss: 2.7484
Epoch: 33/200
Train loss: 2.7109
Epoch: 34/200
Train loss: 2.4855
Epoch: 35/200
Train loss: 2.6838
Epoch: 36/200
Train loss: 2.6080
Epoch: 37/200
Train loss: 2.6708
Epoch: 38/200
Train loss: 2.5405
Epoch: 39/200
Train loss: 2.7632
Epoch: 40/200
Train loss: 2.4450
Epoch: 41/200
Train loss: 2.5945
Epoch: 42/200
Train loss: 2.5795
Epoch: 43/200
Train loss: 2.4543
Epoch: 44/200
Train loss: 2.6022
Epoch: 45/200
Train loss: 2.5444
Epoch: 46/200
Train loss: 2.5741
Epoch: 47/200
Train loss: 2.4304
Epoch: 48/200
Train loss: 2.4725
Epoch: 49/200
Train loss: 2.3859
Epoch: 50/200
Train loss: 2.2384
Epoch: 51/200
Train loss: 2.3903
Epoch: 52/200
Train loss: 2.3442
Epoch: 53/200
Train loss: 2.3790
Epoch: 54/200
Train loss: 2.2660
Epoch: 55/200
Train loss: 2.4644
Epoch: 56/200
Train loss: 2.3455
Epoch: 57/200
Train loss: 2.2095
Epoch: 58/200
Train loss: 2.4636
Epoch: 59/200
Train loss: 2.2720
Epoch: 60/200
Train loss: 2.4300
Epoch: 61/200
Train loss: 2.3260
Epoch: 62/200
Train loss: 2.3329
Epoch: 63/200
Train loss: 2.2685
Epoch: 64/200
Train loss: 2.2304
Epoch: 65/200
Train loss: 2.3172
Epoch: 66/200
Train loss: 2.2914
Epoch: 67/200
Train loss: 2.3843
Epoch: 68/200
Train loss: 2.1871
Epoch: 69/200
Train loss: 2.3595
Epoch: 70/200
Train loss: 2.3128
Epoch: 71/200
Train loss: 2.4163
Epoch: 72/200
Train loss: 2.2546
Epoch: 73/200
Train loss: 2.1174
Epoch: 74/200
Train loss: 2.1691
Epoch: 75/200
Train loss: 2.2553
Epoch: 76/200
Train loss: 2.2822
Epoch: 77/200
Train loss: 2.3786
Epoch: 78/200
Train loss: 2.1374
Epoch: 79/200
Train loss: 2.2472
Epoch: 80/200
Train loss: 2.2044
Epoch: 81/200
Train loss: 2.0843
Epoch: 82/200
Train loss: 2.0592
Epoch: 83/200
Train loss: 2.2084
Epoch: 84/200
Train loss: 2.1514
Epoch: 85/200
Train loss: 2.1085
Epoch: 86/200
Train loss: 2.0979
Epoch: 87/200
Train loss: 2.2043
Epoch: 88/200
Train loss: 2.2797
Epoch: 89/200
Train loss: 1.9844
Epoch: 90/200
Train loss: 2.2193
Epoch: 91/200
Train loss: 2.2141
Epoch: 92/200
Train loss: 2.2884
Epoch: 93/200
Train loss: 2.1749
Epoch: 94/200
Train loss: 2.1565
Epoch: 95/200
Train loss: 2.2930
Epoch: 96/200
Train loss: 2.1839
Epoch: 97/200
Train loss: 2.1283
Epoch: 98/200
Train loss: 2.2100
Epoch: 99/200
Train loss: 2.2839
Epoch: 100/200
Train loss: 2.1472
Epoch: 101/200
Train loss: 2.1290
Epoch: 102/200
Train loss: 2.0733
Epoch: 103/200
Train loss: 2.2045
Epoch: 104/200
Train loss: 2.1718
Epoch: 105/200
Train loss: 1.9525
Epoch: 106/200
Train loss: 2.0295
Epoch: 107/200
Train loss: 2.0561
Epoch: 108/200
Train loss: 2.0779
Epoch: 109/200
Train loss: 2.0962
Epoch: 110/200
Train loss: 2.1703
Epoch: 111/200
Train loss: 2.1333
Epoch: 112/200
Train loss: 2.1380
Epoch: 113/200
Train loss: 2.1783
Epoch: 114/200
Train loss: 2.0164
Epoch: 115/200
Train loss: 1.9677
Epoch: 116/200
Train loss: 2.1141
Epoch: 117/200
Train loss: 2.1499
Epoch: 118/200
Train loss: 2.0947
Epoch: 119/200
Train loss: 2.1066
Epoch: 120/200
Train loss: 2.1074
Epoch: 121/200
Train loss: 2.0137
Epoch: 122/200
Train loss: 2.0840
Epoch: 123/200
Train loss: 2.0836
Epoch: 124/200
Train loss: 2.0360
Epoch: 125/200
Train loss: 2.1022
Epoch: 126/200
Train loss: 2.1653
Epoch: 127/200
Train loss: 2.3110
Epoch: 128/200
Train loss: 2.1400
Epoch: 129/200
Train loss: 2.0514
Epoch: 130/200
Train loss: 2.1372
Epoch: 131/200
Train loss: 2.0153
Epoch: 132/200
Train loss: 2.0414
Time needed: 165.27029037475586 for validation audios
0.7982505991194282
Model improve: 0.0000 -> 0.7983
Epoch: 133/200
Train loss: 1.8848
Time needed: 167.22956776618958 for validation audios
0.8023079512038213
Model improve: 0.7983 -> 0.8023
Epoch: 134/200
Train loss: 2.0519
Time needed: 166.0987160205841 for validation audios
0.7974238342151526
Epoch: 135/200
Train loss: 2.0163
Time needed: 165.8399693965912 for validation audios
0.8004257195050334
Epoch: 136/200
Train loss: 2.1525
Time needed: 165.39784145355225 for validation audios
0.8002392781651556
Epoch: 137/200
Train loss: 1.9999
Time needed: 164.11531400680542 for validation audios
0.8002936807787879
Epoch: 138/200
Train loss: 2.0245
Time needed: 164.89775681495667 for validation audios
0.8021796231225344
Epoch: 139/200
Train loss: 1.9872
Time needed: 168.39996933937073 for validation audios
0.7993717796320122
Epoch: 140/200
Train loss: 1.9962
Time needed: 164.9575798511505 for validation audios
0.8019419071492347
Epoch: 141/200
Train loss: 2.0500
Time needed: 164.01908469200134 for validation audios
0.8017981955437519
Epoch: 142/200
Train loss: 1.9187
Time needed: 165.68796849250793 for validation audios
0.8038444389200601
Model improve: 0.8023 -> 0.8038
Epoch: 143/200
Train loss: 2.1184
Time needed: 165.6484842300415 for validation audios
0.7993498817245158
Epoch: 144/200
Train loss: 1.9842
Time needed: 165.0344476699829 for validation audios
0.799606913088992
Epoch: 145/200
Train loss: 2.0085
Time needed: 165.77128314971924 for validation audios
0.8003128448523513
Epoch: 146/200
Train loss: 1.9232
Time needed: 164.07018613815308 for validation audios
0.8025897381445942
Epoch: 147/200
Train loss: 2.1218
Time needed: 166.2535436153412 for validation audios
0.8006405510539704
Epoch: 148/200
Train loss: 2.0833
Time needed: 164.2407615184784 for validation audios
0.7992190498697229
Epoch: 149/200
Train loss: 1.9499
Time needed: 163.95360851287842 for validation audios
0.7992766520796739
Epoch: 150/200
Train loss: 2.0708
Time needed: 164.1403307914734 for validation audios
0.8000147424191061
Epoch: 151/200
Train loss: 2.0815
Time needed: 165.29195284843445 for validation audios
0.8005003254844585
Epoch: 152/200
Train loss: 2.1610
Time needed: 165.36047315597534 for validation audios
0.7992541665572975
Epoch: 153/200
Train loss: 1.9853
Time needed: 167.8192572593689 for validation audios
0.8023497927052617
Epoch: 154/200
Train loss: 1.8381
Time needed: 165.374169588089 for validation audios
0.8019336662652152
Epoch: 155/200
Train loss: 2.0347
Time needed: 167.43509197235107 for validation audios
0.8014287329139422
Epoch: 156/200
Train loss: 1.8859
Time needed: 164.26649355888367 for validation audios
0.802447520945633
Epoch: 157/200
Train loss: 1.9378
Time needed: 165.40134406089783 for validation audios
0.8021934609443225
Epoch: 158/200
Train loss: 2.0734
Time needed: 164.96816229820251 for validation audios
0.7957900998943147
Epoch: 159/200
Train loss: 1.9524
Time needed: 164.03350400924683 for validation audios
0.8006294373680871
Epoch: 160/200
Train loss: 2.0357
Time needed: 165.46374464035034 for validation audios
0.7995773166856602
Epoch: 161/200
Train loss: 2.0676
Time needed: 164.30207204818726 for validation audios
0.8017108100521558
Epoch: 162/200
Train loss: 1.8993
Time needed: 165.48133444786072 for validation audios
0.8014091884694584
Epoch: 163/200
Train loss: 1.9001
Time needed: 164.38882637023926 for validation audios
0.8042371064666906
Model improve: 0.8038 -> 0.8042
Epoch: 164/200
Train loss: 2.0698
Time needed: 165.219744682312 for validation audios
0.8028421092784782
Epoch: 165/200
Train loss: 2.0013
Time needed: 164.69489002227783 for validation audios
0.7986130079042388
Epoch: 166/200
Train loss: 2.0068
Time needed: 166.77218198776245 for validation audios
0.8036913002348135
Epoch: 167/200
Train loss: 2.0445
Time needed: 165.45664739608765 for validation audios
0.8009448695765861
Epoch: 168/200
Train loss: 1.9224
Time needed: 165.55149006843567 for validation audios
0.8016837914252801
Epoch: 169/200
Train loss: 2.1740
Time needed: 165.00138521194458 for validation audios
0.7999095742529975
Epoch: 170/200
Train loss: 2.0406
Time needed: 166.1129505634308 for validation audios
0.8024365252408524
Epoch: 171/200
Train loss: 2.0863
Time needed: 166.5658073425293 for validation audios
0.8006891211521547
Epoch: 172/200
Train loss: 2.1331
Time needed: 165.0078980922699 for validation audios
0.7996574143188027
Epoch: 173/200
Train loss: 2.0045
Time needed: 165.0686752796173 for validation audios
0.8019738944688016
Epoch: 174/200
Train loss: 2.0472
Time needed: 164.37092900276184 for validation audios
0.8019132363417754
Epoch: 175/200
Train loss: 1.9888
Time needed: 165.922869682312 for validation audios
0.8026704195954052
Epoch: 176/200
Train loss: 2.1000
Time needed: 164.32171416282654 for validation audios
0.8001210339460085
Epoch: 177/200
Train loss: 1.9533
Time needed: 165.40670609474182 for validation audios
0.8005681963788885
Epoch: 178/200
Train loss: 2.0372
Time needed: 165.79811644554138 for validation audios
0.802729355987428
Epoch: 179/200
Train loss: 1.9488
Time needed: 164.05518436431885 for validation audios
0.8006410794141847
Epoch: 180/200
Train loss: 2.0689
Time needed: 166.34530234336853 for validation audios
0.7987338846410481
Epoch: 181/200
Train loss: 1.8645
Time needed: 163.4391827583313 for validation audios
0.8057344120514554
Model improve: 0.8042 -> 0.8057
Epoch: 182/200
Train loss: 1.9342
Time needed: 165.23744869232178 for validation audios
0.8011843410231326
Epoch: 183/200
Train loss: 1.9892
Time needed: 163.4466073513031 for validation audios
0.8041497683033442
Epoch: 184/200
Train loss: 1.9122
Time needed: 165.45886754989624 for validation audios
0.8019367743075402
Epoch: 185/200
Train loss: 2.0649
Time needed: 167.59088730812073 for validation audios
0.8008291781003911
Epoch: 186/200
Train loss: 2.0270
Time needed: 165.5869369506836 for validation audios
0.7976941626264723
Epoch: 187/200
Train loss: 1.8534
Time needed: 165.7492220401764 for validation audios
0.8041076882919461
Epoch: 188/200
Train loss: 1.9589
Time needed: 168.55279088020325 for validation audios
0.8035143997316646
Epoch: 189/200
Train loss: 2.0226
Time needed: 163.7054305076599 for validation audios
0.8016287844549255
Epoch: 190/200
Train loss: 2.0844
Time needed: 167.14629745483398 for validation audios
0.8029146459647678
Epoch: 191/200
Train loss: 2.0800
Time needed: 166.37974977493286 for validation audios
0.8003625957850974
Epoch: 192/200
Train loss: 1.9714
Time needed: 164.27792716026306 for validation audios
0.8010205144606286
Epoch: 193/200
Train loss: 2.0256
Time needed: 166.80500841140747 for validation audios
0.8000428396990172
Epoch: 194/200
Train loss: 2.0329
Time needed: 164.68372511863708 for validation audios
0.7998887915623827
Epoch: 195/200
Train loss: 2.1287
Time needed: 164.09154963493347 for validation audios
0.8005839839842406
Epoch: 196/200
Train loss: 2.0047
Time needed: 164.62353324890137 for validation audios
0.8004998516198318
Epoch: 197/200
Train loss: 2.0399
Time needed: 168.17194724082947 for validation audios
0.8027198754079148
Epoch: 198/200
Train loss: 2.0886
Time needed: 164.4360010623932 for validation audios
0.8017189240916807
Epoch: 199/200
Train loss: 2.1011
Date :04/30/2023, 23:51:40
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 100
nmels: 128
fmax: 14000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 0
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13553
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 39.5609
Time needed: 165.40822982788086 for validation audios
0.25592682809505146
Model improve: 0.0000 -> 0.2559
Epoch: 2/200
Train loss: 5.6441
Time needed: 168.61472964286804 for validation audios
0.3016556624060883
Model improve: 0.2559 -> 0.3017
Epoch: 3/200
Train loss: 5.2541
Time needed: 168.77368307113647 for validation audios
0.3639329447269599
Model improve: 0.3017 -> 0.3639
Epoch: 4/200
Train loss: 4.9860
Time needed: 166.87000179290771 for validation audios
0.42983535934103595
Model improve: 0.3639 -> 0.4298
Epoch: 5/200
Train loss: 4.6878
Time needed: 170.86133241653442 for validation audios
0.48437688470166623
Model improve: 0.4298 -> 0.4844
Epoch: 6/200
Train loss: 4.4229
Time needed: 169.50215458869934 for validation audios
0.5350539612522335
Model improve: 0.4844 -> 0.5351
Epoch: 7/200
Train loss: 4.1357
Time needed: 165.16301035881042 for validation audios
0.5701438093046787
Model improve: 0.5351 -> 0.5701
Epoch: 8/200
Train loss: 4.0404
Time needed: 178.07763576507568 for validation audios
0.59737843224285
Model improve: 0.5701 -> 0.5974
Epoch: 9/200
Train loss: 3.8119
Time needed: 165.95348477363586 for validation audios
0.6174536822650533
Model improve: 0.5974 -> 0.6175
Epoch: 10/200
Train loss: 3.7289
Time needed: 165.65617871284485 for validation audios
0.6366517521577943
Model improve: 0.6175 -> 0.6367
Epoch: 11/200
Train loss: 3.6145
Time needed: 166.58075046539307 for validation audios
0.6526122754304495
Model improve: 0.6367 -> 0.6526
Epoch: 12/200
Train loss: 3.5283
Time needed: 164.76008582115173 for validation audios
0.6621178934996332
Model improve: 0.6526 -> 0.6621
Epoch: 13/200
Train loss: 3.4532
Time needed: 165.61128187179565 for validation audios
0.6749699760787482
Model improve: 0.6621 -> 0.6750
Epoch: 14/200
Train loss: 3.3475
Time needed: 165.76648020744324 for validation audios
0.6885143815117112
Model improve: 0.6750 -> 0.6885
Epoch: 15/200
Train loss: 3.3141
Time needed: 164.25103282928467 for validation audios
0.6955735064150043
Model improve: 0.6885 -> 0.6956
Epoch: 16/200
Train loss: 3.2877
Time needed: 168.52159643173218 for validation audios
0.704477069910689
Model improve: 0.6956 -> 0.7045
Epoch: 17/200
Train loss: 3.1067
Date :05/01/2023, 01:01:25
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 40.3428
Epoch: 2/200
Train loss: 5.7815
Epoch: 3/200
Train loss: 5.4609
Epoch: 4/200
Train loss: 5.1853
Epoch: 5/200
Train loss: 4.8685
Epoch: 6/200
Train loss: 4.6070
Epoch: 7/200
Train loss: 4.2822
Epoch: 8/200
Train loss: 4.1765
Epoch: 9/200
Train loss: 3.9970
Epoch: 10/200
Train loss: 3.8835
Epoch: 11/200
Train loss: 3.7739
Epoch: 12/200
Train loss: 3.6416
Epoch: 13/200
Train loss: 3.5636
Epoch: 14/200
Train loss: 3.4256
Epoch: 15/200
Train loss: 3.4427
Time needed: 167.31452798843384 for validation audios
0.7013220019019224
Model improve: 0.0000 -> 0.7013
Epoch: 16/200
Train loss: 3.3478
Time needed: 167.3657898902893 for validation audios
0.7100373575578027
Model improve: 0.7013 -> 0.7100
Epoch: 17/200
Date :05/01/2023, 01:28:27
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 100
nmels: 128
fmax: 14000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 39.8633
Epoch: 2/200
Train loss: 5.7752
Epoch: 3/200
Train loss: 5.4613
Epoch: 4/200
Train loss: 5.2044
Epoch: 5/200
Train loss: 4.8936
Epoch: 6/200
Train loss: 4.6250
Epoch: 7/200
Train loss: 4.3018
Epoch: 8/200
Train loss: 4.1918
Epoch: 9/200
Train loss: 4.0163
Epoch: 10/200
Train loss: 3.8969
Epoch: 11/200
Train loss: 3.7868
Epoch: 12/200
Train loss: 3.6555
Epoch: 13/200
Train loss: 3.5770
Epoch: 14/200
Train loss: 3.4397
Epoch: 15/200
Train loss: 3.4571
Time needed: 169.37936997413635 for validation audios
0.7015914386016836
Model improve: 0.0000 -> 0.7016
Epoch: 16/200
Train loss: 3.3630
Time needed: 168.79250144958496 for validation audios
0.7111057616291208
Model improve: 0.7016 -> 0.7111
Epoch: 17/200
Train loss: 3.2104
Time needed: 165.46093082427979 for validation audios
0.7176687844182678
Model improve: 0.7111 -> 0.7177
Epoch: 18/200
Date :05/01/2023, 02:01:02
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.5
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 40.3987
Epoch: 2/200
Train loss: 5.7836
Epoch: 3/200
Train loss: 5.4435
Epoch: 4/200
Train loss: 5.1497
Epoch: 5/200
Train loss: 4.7856
Epoch: 6/200
Train loss: 4.5553
Epoch: 7/200
Train loss: 4.2205
Epoch: 8/200
Train loss: 3.9867
Epoch: 9/200
Train loss: 3.9178
Epoch: 10/200
Train loss: 3.6850
Epoch: 11/200
Train loss: 3.6256
Epoch: 12/200
Train loss: 3.4289
Epoch: 13/200
Train loss: 3.3178
Epoch: 14/200
Train loss: 3.3954
Epoch: 15/200
Train loss: 3.1773
Time needed: 169.69302320480347 for validation audios
0.703488192346362
Model improve: 0.0000 -> 0.7035
Epoch: 16/200
Train loss: 3.2282
Time needed: 167.2461175918579 for validation audios
0.7128426828807515
Model improve: 0.7035 -> 0.7128
Epoch: 17/200
Train loss: 3.0118
Time needed: 166.62337112426758 for validation audios
0.7194625037393055
Model improve: 0.7128 -> 0.7195
Epoch: 18/200
Date :05/01/2023, 02:32:24
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 40.2847
Epoch: 2/200
Train loss: 5.7829
Epoch: 3/200
Train loss: 5.4638
Epoch: 4/200
Train loss: 5.1915
Epoch: 5/200
Train loss: 4.8737
Epoch: 6/200
Train loss: 4.6098
Epoch: 7/200
Train loss: 4.2846
Epoch: 8/200
Train loss: 4.1783
Epoch: 9/200
Train loss: 3.9968
Epoch: 10/200
Train loss: 3.8848
Epoch: 11/200
Train loss: 3.7734
Epoch: 12/200
Train loss: 3.6424
Epoch: 13/200
Train loss: 3.5634
Epoch: 14/200
Train loss: 3.4262
Epoch: 15/200
Train loss: 3.4432
Time needed: 168.62440848350525 for validation audios
0.7013787999167265
Model improve: 0.0000 -> 0.7014
Epoch: 16/200
Train loss: 3.3474
Time needed: 167.94058418273926 for validation audios
0.7102786290550105
Model improve: 0.7014 -> 0.7103
Epoch: 17/200
Date :05/01/2023, 02:59:44
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.5
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Date :05/01/2023, 03:00:24
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.5
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Date :05/01/2023, 03:00:48
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.5
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Date :05/01/2023, 03:01:23
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.5
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Date :05/01/2023, 03:01:51
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.5
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Date :05/01/2023, 03:02:12
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.5
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Date :05/01/2023, 03:02:24
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.5
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 37.9231
Epoch: 2/200
Train loss: 5.9478
Epoch: 3/200
Train loss: 5.8960
Epoch: 4/200
Date :05/01/2023, 03:07:08
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.5
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Date :05/01/2023, 03:07:22
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.5
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Date :05/01/2023, 03:08:35
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.5
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 37.4081
Time needed: 165.9584333896637 for validation audios
0.23847195082923278
Model improve: 0.0000 -> 0.2385
Epoch: 2/200
Train loss: 5.9447
Time needed: 166.4226565361023 for validation audios
0.23933491104349292
Model improve: 0.2385 -> 0.2393
Epoch: 3/200
Date :05/01/2023, 03:17:48
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.5
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 38.6314
Time needed: 167.12239623069763 for validation audios
0.23936898424676573
Model improve: 0.0000 -> 0.2394
Epoch: 2/200
Train loss: 5.9581
Time needed: 169.16307044029236 for validation audios
0.2443237227952462
Model improve: 0.2394 -> 0.2443
Epoch: 3/200
Date :05/01/2023, 03:26:15
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.5
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.95, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 44.5737
Time needed: 165.9501497745514 for validation audios
0.24649569850412661
Model improve: 0.0000 -> 0.2465
Epoch: 2/200
Train loss: 5.9479
Time needed: 167.44457530975342 for validation audios
0.2686611366216344
Model improve: 0.2465 -> 0.2687
Epoch: 3/200
Date :05/01/2023, 03:39:11
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.5
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.95, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 44.5327
Epoch: 2/200
Train loss: 5.9548
Epoch: 3/200
Train loss: 5.7769
Epoch: 4/200
Train loss: 5.5695
Time needed: 165.1821391582489 for validation audios
0.3425818168445329
Model improve: 0.0000 -> 0.3426
Epoch: 5/200
Date :05/01/2023, 03:55:18
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.5
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 42.1509
Epoch: 2/200
Date :05/01/2023, 04:01:26
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.5
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 42.3969
Epoch: 2/200
Train loss: 5.8727
Epoch: 3/200
Train loss: 5.5982
Epoch: 4/200
Train loss: 5.3130
Epoch: 5/200
Train loss: 4.9682
Epoch: 6/200
Train loss: 4.7245
Epoch: 7/200
Train loss: 4.4191
Epoch: 8/200
Train loss: 4.1857
Epoch: 9/200
Train loss: 4.0833
Epoch: 10/200
Train loss: 3.8497
Epoch: 11/200
Train loss: 3.7675
Epoch: 12/200
Train loss: 3.5774
Epoch: 13/200
Train loss: 3.4716
Epoch: 14/200
Train loss: 3.5398
Epoch: 15/200
Train loss: 3.3186
Time needed: 169.83649468421936 for validation audios
0.6943781429334273
Model improve: 0.0000 -> 0.6944
Epoch: 16/200
Train loss: 3.3525
Time needed: 169.21686458587646 for validation audios
0.7041637818216144
Model improve: 0.6944 -> 0.7042
Epoch: 17/200
Date :05/01/2023, 04:29:29
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.5
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Date :05/01/2023, 04:30:59
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.5
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 42.2988
Epoch: 2/200
Train loss: 5.8801
Epoch: 3/200
Train loss: 5.6186
Epoch: 4/200
Train loss: 5.3269
Epoch: 5/200
Train loss: 4.9921
Epoch: 6/200
Train loss: 4.7552
Epoch: 7/200
Train loss: 4.4348
Epoch: 8/200
Train loss: 4.2002
Epoch: 9/200
Train loss: 4.1183
Epoch: 10/200
Train loss: 3.8805
Epoch: 11/200
Train loss: 3.8065
Epoch: 12/200
Train loss: 3.6001
Epoch: 13/200
Train loss: 3.4801
Epoch: 14/200
Train loss: 3.5445
Epoch: 15/200
Train loss: 3.3193
Time needed: 165.78428030014038 for validation audios
0.6918510456789639
Model improve: 0.0000 -> 0.6919
Epoch: 16/200
Train loss: 3.3651
Time needed: 166.35335159301758 for validation audios
0.7011770395464357
Model improve: 0.6919 -> 0.7012
Epoch: 17/200
Train loss: 3.1510
Time needed: 166.09563422203064 for validation audios
0.7092699104336934
Model improve: 0.7012 -> 0.7093
Epoch: 18/200
Train loss: 3.1180
Date :05/01/2023, 05:05:45
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.5
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 39.2478
Epoch: 2/200
Train loss: 5.7152
Epoch: 3/200
Train loss: 5.3491
Epoch: 4/200
Train loss: 5.0558
Epoch: 5/200
Train loss: 4.7083
Epoch: 6/200
Train loss: 4.4747
Epoch: 7/200
Train loss: 4.1642
Epoch: 8/200
Train loss: 3.9614
Epoch: 9/200
Train loss: 3.8846
Epoch: 10/200
Train loss: 3.6863
Epoch: 11/200
Train loss: 3.6233
Epoch: 12/200
Train loss: 3.4383
Epoch: 13/200
Train loss: 3.3575
Epoch: 14/200
Train loss: 3.4335
Epoch: 15/200
Train loss: 3.2095
Time needed: 166.34938168525696 for validation audios
0.7056953508446328
Model improve: 0.0000 -> 0.7057
Epoch: 16/200
Train loss: 3.2645
Time needed: 173.64704060554504 for validation audios
0.717449145037705
Model improve: 0.7057 -> 0.7174
Epoch: 17/200
Train loss: 3.0829
Time needed: 167.09258460998535 for validation audios
0.7252130234243926
Model improve: 0.7174 -> 0.7252
Epoch: 18/200
Train loss: 3.0287
Time needed: 167.97435092926025 for validation audios
0.729457517338789
Model improve: 0.7252 -> 0.7295
Epoch: 19/200
Train loss: 3.0357
Time needed: 169.79617857933044 for validation audios
0.7387066529185164
Model improve: 0.7295 -> 0.7387
Epoch: 20/200
Train loss: 2.9006
Time needed: 166.71724200248718 for validation audios
0.7376001545842512
Epoch: 21/200
Train loss: 2.9541
Time needed: 166.80270409584045 for validation audios
0.7475558474203368
Model improve: 0.7387 -> 0.7476
Epoch: 22/200
Train loss: 2.9074
Time needed: 167.3507902622223 for validation audios
0.7488389511028178
Model improve: 0.7476 -> 0.7488
Epoch: 23/200
Train loss: 2.8889
Time needed: 173.14774179458618 for validation audios
0.7547072813849687
Model improve: 0.7488 -> 0.7547
Epoch: 24/200
Train loss: 2.8519
Time needed: 167.4638032913208 for validation audios
0.7581981649606394
Model improve: 0.7547 -> 0.7582
Epoch: 25/200
Train loss: 2.7068
Time needed: 166.6898090839386 for validation audios
0.7630957391479078
Model improve: 0.7582 -> 0.7631
Epoch: 26/200
Train loss: 2.7508
Time needed: 166.2311863899231 for validation audios
0.7608393749044737
Epoch: 27/200
Train loss: 2.7678
Time needed: 166.91759371757507 for validation audios
0.764668647611233
Model improve: 0.7631 -> 0.7647
Epoch: 28/200
Train loss: 2.6211
Time needed: 169.050443649292 for validation audios
0.7680447811778305
Model improve: 0.7647 -> 0.7680
Epoch: 29/200
Train loss: 2.8017
Time needed: 167.7265772819519 for validation audios
0.7703061912329082
Model improve: 0.7680 -> 0.7703
Epoch: 30/200
Train loss: 2.6954
Time needed: 168.14047765731812 for validation audios
0.7715745212328895
Model improve: 0.7703 -> 0.7716
Epoch: 31/200
Train loss: 2.7414
Time needed: 166.47396111488342 for validation audios
0.7729308602015855
Model improve: 0.7716 -> 0.7729
Epoch: 32/200
Date :05/01/2023, 05:16:14
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.5
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Date :05/01/2023, 05:17:05
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 96
validbs: 288
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.5
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 31.2552
Epoch: 2/200
Train loss: 5.5586
Epoch: 3/200
Train loss: 5.1339
Epoch: 4/200
Train loss: 4.6967
Epoch: 5/200
Train loss: 4.3550
Epoch: 6/200
Train loss: 4.0965
Epoch: 7/200
Train loss: 3.9234
Epoch: 8/200
Train loss: 3.7330
Epoch: 9/200
Train loss: 3.5285
Epoch: 10/200
Train loss: 3.4844
Epoch: 11/200
Train loss: 3.3724
Epoch: 12/200
Train loss: 3.3167
Epoch: 13/200
Train loss: 3.1577
Epoch: 14/200
Train loss: 3.1517
Epoch: 15/200
Train loss: 2.9810
Epoch: 16/200
Train loss: 3.0150
Epoch: 17/200
Train loss: 2.9802
Epoch: 18/200
Train loss: 2.9510
Epoch: 19/200
Train loss: 2.7822
Epoch: 20/200
Train loss: 2.7892
Epoch: 21/200
Train loss: 2.7766
Epoch: 22/200
Train loss: 2.8727
Epoch: 23/200
Train loss: 2.7568
Epoch: 24/200
Train loss: 2.8458
Epoch: 25/200
Train loss: 2.7303
Epoch: 26/200
Train loss: 2.5228
Epoch: 27/200
Train loss: 2.6419
Epoch: 28/200
Train loss: 2.6037
Epoch: 29/200
Train loss: 2.6160
Epoch: 30/200
Train loss: 2.6139
Epoch: 31/200
Train loss: 2.4923
Epoch: 32/200
Train loss: 2.5355
Epoch: 33/200
Train loss: 2.5491
Epoch: 34/200
Train loss: 2.4456
Epoch: 35/200
Train loss: 2.5231
Epoch: 36/200
Train loss: 2.4660
Epoch: 37/200
Train loss: 2.5891
Epoch: 38/200
Train loss: 2.5143
Epoch: 39/200
Train loss: 2.5561
Epoch: 40/200
Train loss: 2.5357
Epoch: 41/200
Train loss: 2.3889
Epoch: 42/200
Train loss: 2.3486
Epoch: 43/200
Train loss: 2.2767
Epoch: 44/200
Train loss: 2.3401
Epoch: 45/200
Train loss: 2.3667
Epoch: 46/200
Train loss: 2.3565
Epoch: 47/200
Train loss: 2.4369
Epoch: 48/200
Train loss: 2.2879
Epoch: 49/200
Train loss: 2.3820
Epoch: 50/200
Train loss: 2.3406
Epoch: 51/200
Train loss: 2.3131
Epoch: 52/200
Train loss: 2.4397
Epoch: 53/200
Train loss: 2.3594
Epoch: 54/200
Train loss: 2.2703
Epoch: 55/200
Train loss: 2.1808
Epoch: 56/200
Train loss: 2.1608
Epoch: 57/200
Train loss: 2.1588
Epoch: 58/200
Train loss: 2.3071
Epoch: 59/200
Train loss: 2.2285
Epoch: 60/200
Train loss: 2.1617
Epoch: 61/200
Train loss: 2.2505
Epoch: 62/200
Train loss: 2.1949
Epoch: 63/200
Train loss: 2.0228
Epoch: 64/200
Train loss: 2.2805
Epoch: 65/200
Train loss: 2.3382
Epoch: 66/200
Train loss: 2.2584
Epoch: 67/200
Train loss: 2.1008
Epoch: 68/200
Train loss: 2.1434
Epoch: 69/200
Train loss: 2.2672
Epoch: 70/200
Train loss: 2.1312
Epoch: 71/200
Train loss: 2.2058
Epoch: 72/200
Train loss: 2.1814
Epoch: 73/200
Train loss: 2.1788
Epoch: 74/200
Train loss: 2.0809
Epoch: 75/200
Train loss: 2.1374
Epoch: 76/200
Train loss: 2.1076
Epoch: 77/200
Train loss: 2.0213
Epoch: 78/200
Train loss: 2.1576
Epoch: 79/200
Train loss: 2.1863
Epoch: 80/200
Train loss: 2.2318
Epoch: 81/200
Train loss: 2.0899
Epoch: 82/200
Train loss: 2.1382
Epoch: 83/200
Train loss: 2.2196
Epoch: 84/200
Train loss: 2.1508
Epoch: 85/200
Train loss: 2.1485
Epoch: 86/200
Train loss: 2.1818
Epoch: 87/200
Train loss: 2.0583
Epoch: 88/200
Train loss: 2.1917
Epoch: 89/200
Train loss: 2.2165
Epoch: 90/200
Train loss: 2.2064
Epoch: 91/200
Train loss: 2.0874
Epoch: 92/200
Train loss: 1.9130
Epoch: 93/200
Train loss: 2.0770
Epoch: 94/200
Train loss: 1.9528
Epoch: 95/200
Train loss: 2.0933
Epoch: 96/200
Train loss: 2.1241
Epoch: 97/200
Train loss: 2.0909
Epoch: 98/200
Train loss: 2.0576
Epoch: 99/200
Train loss: 2.1105
Epoch: 100/200
Train loss: 2.1236
Epoch: 101/200
Train loss: 2.0975
Epoch: 102/200
Train loss: 2.0545
Epoch: 103/200
Train loss: 1.9951
Epoch: 104/200
Date :05/15/2023, 06:25:57
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 400
learningrate: 0.001
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/400
Date :05/15/2023, 06:26:49
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 96
validbs: 192
epochwarmup: 0
totalepoch: 400
learningrate: 0.001
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/400
Train loss: 16.2642
Epoch: 2/400
Train loss: 4.6404
Epoch: 3/400
Train loss: 4.1330
Epoch: 4/400
Train loss: 3.7215
Epoch: 5/400
Train loss: 3.4358
Epoch: 6/400
Train loss: 3.2268
Epoch: 7/400
Train loss: 3.1110
Epoch: 8/400
Train loss: 3.1097
Epoch: 9/400
Train loss: 2.9923
Epoch: 10/400
Train loss: 2.9879
Epoch: 11/400
Train loss: 2.8458
Epoch: 12/400
Train loss: 2.8947
Epoch: 13/400
Train loss: 2.7382
Epoch: 14/400
Train loss: 2.8467
Epoch: 15/400
Train loss: 2.7742
Epoch: 16/400
Train loss: 2.6215
Epoch: 17/400
Train loss: 2.5604
Epoch: 18/400
Train loss: 2.6707
Epoch: 19/400
Train loss: 2.6654
Epoch: 20/400
Train loss: 2.6664
Epoch: 21/400
Train loss: 2.5324
Epoch: 22/400
Train loss: 2.4985
Epoch: 23/400
Train loss: 2.4566
Epoch: 24/400
Train loss: 2.6010
Epoch: 25/400
Train loss: 2.3936
Epoch: 26/400
Train loss: 2.5191
Epoch: 27/400
Train loss: 2.5556
Epoch: 28/400
Train loss: 2.4665
Epoch: 29/400
Train loss: 2.5365
Epoch: 30/400
Train loss: 2.4682
Epoch: 31/400
Train loss: 2.4342
Epoch: 32/400
Train loss: 2.4689
Epoch: 33/400
Train loss: 2.4818
Epoch: 34/400
Train loss: 2.4650
Epoch: 35/400
Train loss: 2.3551
Epoch: 36/400
Train loss: 2.2695
Epoch: 37/400
Train loss: 2.2538
Epoch: 38/400
Train loss: 2.2716
Epoch: 39/400
Train loss: 2.3328
Epoch: 40/400
Train loss: 2.3233
Epoch: 41/400
Train loss: 2.3221
Epoch: 42/400
Train loss: 2.2623
Epoch: 43/400
Train loss: 2.2883
Epoch: 44/400
Train loss: 2.3536
Epoch: 45/400
Train loss: 2.3239
Epoch: 46/400
Train loss: 2.2581
Epoch: 47/400
Train loss: 2.2761
Epoch: 48/400
Train loss: 2.2614
Epoch: 49/400
Train loss: 2.3508
Epoch: 50/400
Train loss: 2.2113
Epoch: 51/400
Train loss: 2.3088
Epoch: 52/400
Train loss: 2.3831
Epoch: 53/400
Train loss: 2.1604
Epoch: 54/400
Train loss: 2.1751
Epoch: 55/400
Train loss: 2.2906
Epoch: 56/400
Train loss: 2.3390
Epoch: 57/400
Train loss: 2.2190
Epoch: 58/400
Train loss: 2.1760
Epoch: 59/400
Train loss: 2.1897
Epoch: 60/400
Train loss: 2.0685
Epoch: 61/400
Train loss: 2.1953
Epoch: 62/400
Train loss: 2.1227
Epoch: 63/400
Train loss: 2.1387
Epoch: 64/400
Train loss: 2.2495
Epoch: 65/400
Train loss: 2.0783
Epoch: 66/400
Train loss: 2.2392
Epoch: 67/400
Train loss: 2.2534
Epoch: 68/400
Train loss: 2.1856
Epoch: 69/400
Train loss: 2.2048
Epoch: 70/400
Train loss: 2.2259
Epoch: 71/400
Train loss: 2.1985
Epoch: 72/400
Train loss: 2.2144
Epoch: 73/400
Train loss: 2.1854
Epoch: 74/400
Train loss: 2.1117
Epoch: 75/400
Train loss: 2.1441
Epoch: 76/400
Train loss: 2.1869
Epoch: 77/400
Train loss: 1.9468
Epoch: 78/400
Train loss: 2.0914
Epoch: 79/400
Train loss: 2.0884
Epoch: 80/400
Train loss: 2.1653
Epoch: 81/400
Train loss: 2.1758
Epoch: 82/400
Train loss: 2.1104
Epoch: 83/400
Train loss: 2.1181
Epoch: 84/400
Train loss: 2.0728
Epoch: 85/400
Train loss: 2.1275
Epoch: 86/400
Train loss: 2.1272
Epoch: 87/400
Train loss: 2.0883
Epoch: 88/400
Train loss: 2.0790
Epoch: 89/400
Train loss: 2.0541
Epoch: 90/400
Train loss: 2.0944
Epoch: 91/400
Train loss: 2.0365
Epoch: 92/400
Train loss: 2.1865
Epoch: 93/400
Train loss: 2.2656
Epoch: 94/400
Train loss: 2.1093
Epoch: 95/400
Train loss: 2.0976
Epoch: 96/400
Train loss: 1.9947
Epoch: 97/400
Train loss: 1.9773
Epoch: 98/400
Train loss: 2.0368
Epoch: 99/400
Train loss: 2.0995
Epoch: 100/400
Train loss: 2.0245
Epoch: 101/400
Train loss: 2.0541
Epoch: 102/400
Train loss: 1.9614
Epoch: 103/400
Train loss: 2.0480
Epoch: 104/400
Train loss: 1.9625
Epoch: 105/400
Train loss: 2.0173
Epoch: 106/400
Train loss: 1.9778
Epoch: 107/400
Train loss: 2.0701
Epoch: 108/400
Train loss: 2.0816
Epoch: 109/400
Train loss: 1.9917
Epoch: 110/400
Train loss: 2.0471
Epoch: 111/400
Train loss: 2.1134
Epoch: 112/400
Train loss: 1.9113
Epoch: 113/400
Train loss: 1.9534
Epoch: 114/400
Train loss: 1.9495
Epoch: 115/400
Train loss: 1.9163
Epoch: 116/400
Train loss: 2.0058
Epoch: 117/400
Train loss: 2.0302
Epoch: 118/400
Train loss: 1.9325
Epoch: 119/400
Train loss: 1.9589
Epoch: 120/400
Train loss: 1.9331
Epoch: 121/400
Train loss: 2.0636
Epoch: 122/400
Train loss: 1.9486
Epoch: 123/400
Train loss: 2.0362
Epoch: 124/400
Train loss: 2.0604
Epoch: 125/400
Train loss: 2.0594
Epoch: 126/400
Train loss: 2.0309
Epoch: 127/400
Train loss: 2.0025
Epoch: 128/400
Train loss: 2.0346
Epoch: 129/400
Train loss: 1.9250
Epoch: 130/400
Train loss: 1.9968
Epoch: 131/400
Train loss: 1.9328
Epoch: 132/400
Train loss: 1.9084
Epoch: 133/400
Train loss: 1.9199
Epoch: 134/400
Train loss: 1.8663
Epoch: 135/400
Train loss: 2.0302
Epoch: 136/400
Train loss: 1.9263
Epoch: 137/400
Train loss: 1.8718
Epoch: 138/400
Train loss: 2.0133
Epoch: 139/400
Train loss: 1.9663
Epoch: 140/400
Train loss: 1.9722
Epoch: 141/400
Train loss: 2.0042
Epoch: 142/400
Train loss: 2.0241
Epoch: 143/400
Train loss: 1.9352
Epoch: 144/400
Train loss: 1.9901
Epoch: 145/400
Train loss: 2.0014
Epoch: 146/400
Train loss: 1.9571
Epoch: 147/400
Train loss: 1.9693
Epoch: 148/400
Train loss: 1.8823
Epoch: 149/400
Train loss: 1.9596
Epoch: 150/400
Train loss: 1.9664
Epoch: 151/400
Train loss: 2.0201
Epoch: 152/400
Train loss: 1.8899
Epoch: 153/400
Train loss: 1.9993
Epoch: 154/400
Train loss: 1.8636
Epoch: 155/400
Train loss: 1.9294
Epoch: 156/400
Train loss: 1.6973
Epoch: 157/400
Train loss: 2.0277
Epoch: 158/400
Train loss: 1.9330
Epoch: 159/400
Train loss: 1.8738
Epoch: 160/400
Train loss: 1.9079
Epoch: 161/400
Train loss: 1.8724
Epoch: 162/400
Train loss: 1.8766
Epoch: 163/400
Train loss: 1.8254
Epoch: 164/400
Train loss: 1.9472
Epoch: 165/400
Train loss: 1.9115
Epoch: 166/400
Train loss: 1.8623
Epoch: 167/400
Train loss: 1.8786
Epoch: 168/400
Train loss: 1.9133
Epoch: 169/400
Train loss: 1.9584
Epoch: 170/400
Train loss: 1.9072
Epoch: 171/400
Train loss: 1.9071
Epoch: 172/400
Train loss: 1.8150
Epoch: 173/400
Train loss: 1.8513
Epoch: 174/400
Train loss: 1.8336
Epoch: 175/400
Train loss: 1.7793
Epoch: 176/400
Train loss: 1.8210
Epoch: 177/400
Train loss: 1.8650
Epoch: 178/400
Train loss: 1.9445
Epoch: 179/400
Train loss: 1.7838
Epoch: 180/400
Train loss: 1.7688
Epoch: 181/400
Train loss: 1.7720
Epoch: 182/400
Train loss: 1.9571
Epoch: 183/400
Train loss: 1.8651
Epoch: 184/400
Train loss: 1.9329
Epoch: 185/400
Train loss: 1.8946
Epoch: 186/400
Train loss: 1.8507
Epoch: 187/400
Train loss: 1.9005
Epoch: 188/400
Train loss: 1.7824
Epoch: 189/400
Train loss: 1.8435
Epoch: 190/400
Train loss: 1.7829
Epoch: 191/400
Train loss: 1.8256
Epoch: 192/400
Train loss: 1.9093
Epoch: 193/400
Train loss: 1.8088
Epoch: 194/400
Train loss: 1.8714
Epoch: 195/400
Train loss: 1.8513
Epoch: 196/400
Train loss: 1.7502
Epoch: 197/400
Train loss: 1.9210
Epoch: 198/400
Train loss: 1.8339
Epoch: 199/400
Train loss: 1.8303
Epoch: 200/400
Train loss: 1.7950
Epoch: 201/400
Train loss: 1.7097
Epoch: 202/400
Train loss: 1.8100
Epoch: 203/400
Train loss: 1.8150
Epoch: 204/400
Train loss: 1.8548
Epoch: 205/400
Train loss: 1.9100
Epoch: 206/400
Train loss: 1.7765
Epoch: 207/400
Train loss: 1.7802
Epoch: 208/400
Date :05/15/2023, 11:42:28
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 96
validbs: 192
epochwarmup: 0
totalepoch: 400
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 0
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 1
13553
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/400
Date :05/15/2023, 11:42:43
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 96
validbs: 192
epochwarmup: 0
totalepoch: 400
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 0
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 1
13553
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/400
Train loss: 31.5284
Epoch: 2/400
Train loss: 5.2871
Epoch: 3/400
Train loss: 4.8349
Epoch: 4/400
Train loss: 4.4520
Epoch: 5/400
Train loss: 4.1262
Epoch: 6/400
Train loss: 3.9162
Epoch: 7/400
Train loss: 3.7200
Epoch: 8/400
Train loss: 3.6568
Epoch: 9/400
Train loss: 3.4969
Epoch: 10/400
Train loss: 3.3993
Epoch: 11/400
Train loss: 3.3784
Epoch: 12/400
Train loss: 3.2406
Epoch: 13/400
Train loss: 3.1646
Epoch: 14/400
Train loss: 3.1054
Epoch: 15/400
Train loss: 3.1343
Epoch: 16/400
Train loss: 3.0186
Epoch: 17/400
Train loss: 2.8187
Epoch: 18/400
Train loss: 2.9588
Epoch: 19/400
Train loss: 2.9700
Epoch: 20/400
Train loss: 2.8728
Epoch: 21/400
Train loss: 2.8321
Epoch: 22/400
Train loss: 2.7657
Epoch: 23/400
Train loss: 2.6371
Epoch: 24/400
Train loss: 2.7698
Epoch: 25/400
Train loss: 2.6573
Epoch: 26/400
Train loss: 2.6585
Epoch: 27/400
Train loss: 2.7176
Epoch: 28/400
Train loss: 2.6189
Epoch: 29/400
Train loss: 2.7607
Epoch: 30/400
Train loss: 2.5228
Epoch: 31/400
Train loss: 2.6184
Epoch: 32/400
Train loss: 2.5250
Epoch: 33/400
Train loss: 2.6182
Epoch: 34/400
Train loss: 2.6016
Epoch: 35/400
Train loss: 2.5144
Epoch: 36/400
Train loss: 2.5246
Epoch: 37/400
Train loss: 2.3155
Epoch: 38/400
Train loss: 2.3725
Epoch: 39/400
Train loss: 2.3754
Epoch: 40/400
Train loss: 2.3660
Epoch: 41/400
Train loss: 2.4652
Epoch: 42/400
Train loss: 2.4073
Epoch: 43/400
Train loss: 2.3038
Epoch: 44/400
Train loss: 2.3728
Epoch: 45/400
Train loss: 2.4579
Epoch: 46/400
Train loss: 2.3289
Epoch: 47/400
Train loss: 2.3065
Epoch: 48/400
Train loss: 2.3429
Epoch: 49/400
Train loss: 2.3328
Epoch: 50/400
Train loss: 2.3702
Epoch: 51/400
Train loss: 2.2556
Epoch: 52/400
Train loss: 2.3685
Epoch: 53/400
Train loss: 2.4664
Epoch: 54/400
Train loss: 2.2443
Epoch: 55/400
Train loss: 2.1629
Epoch: 56/400
Train loss: 2.2726
Epoch: 57/400
Train loss: 2.3788
Epoch: 58/400
Train loss: 2.1817
Epoch: 59/400
Train loss: 2.3433
Epoch: 60/400
Train loss: 2.1188
Epoch: 61/400
Train loss: 2.1437
Epoch: 62/400
Train loss: 2.2257
Epoch: 63/400
Train loss: 2.1806
Epoch: 64/400
Train loss: 2.0922
Epoch: 65/400
Train loss: 2.2983
Epoch: 66/400
Train loss: 2.2053
Epoch: 67/400
Train loss: 2.1027
Epoch: 68/400
Train loss: 2.2853
Epoch: 69/400
Train loss: 2.2413
Epoch: 70/400
Train loss: 2.2003
Epoch: 71/400
Train loss: 2.2737
Epoch: 72/400
Train loss: 2.2110
Epoch: 73/400
Train loss: 2.1943
Epoch: 74/400
Train loss: 2.2794
Epoch: 75/400
Train loss: 2.1847
Epoch: 76/400
Train loss: 2.1007
Epoch: 77/400
Train loss: 2.1937
Epoch: 78/400
Train loss: 2.0652
Epoch: 79/400
Train loss: 2.0506
Epoch: 80/400
Train loss: 2.0816
Epoch: 81/400
Train loss: 2.1053
Epoch: 82/400
Train loss: 2.1617
Epoch: 83/400
Date :05/16/2023, 10:51:42
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.3
drop_path_rate: 0.4
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Date :05/16/2023, 10:52:01
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 96
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.3
drop_path_rate: 0.4
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 33.2965
Epoch: 2/200
Train loss: 5.5000
Epoch: 3/200
Date :05/16/2023, 10:55:31
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 96
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.3
drop_path_rate: 0.4
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 33.3236
Epoch: 2/200
Train loss: 5.5113
Epoch: 3/200
Train loss: 5.0813
Epoch: 4/200
Train loss: 4.7081
Epoch: 5/200
Train loss: 4.4075
Epoch: 6/200
Train loss: 4.1610
Epoch: 7/200
Train loss: 3.9651
Epoch: 8/200
Train loss: 3.8898
Epoch: 9/200
Train loss: 3.7397
Epoch: 10/200
Train loss: 3.6763
Epoch: 11/200
Train loss: 3.5091
Epoch: 12/200
Train loss: 3.5192
Epoch: 13/200
Train loss: 3.3557
Epoch: 14/200
Train loss: 3.4143
Epoch: 15/200
Train loss: 3.3384
Epoch: 16/200
Train loss: 3.1808
Epoch: 17/200
Train loss: 3.0922
Epoch: 18/200
Train loss: 3.1656
Epoch: 19/200
Train loss: 3.1368
Epoch: 20/200
Train loss: 3.1267
Epoch: 21/200
Train loss: 2.9991
Epoch: 22/200
Train loss: 2.9415
Epoch: 23/200
Train loss: 2.9117
Epoch: 24/200
Train loss: 3.0149
Epoch: 25/200
Train loss: 2.8116
Epoch: 26/200
Train loss: 2.9304
Epoch: 27/200
Train loss: 2.9466
Epoch: 28/200
Train loss: 2.8338
Epoch: 29/200
Train loss: 2.9012
Epoch: 30/200
Train loss: 2.8299
Epoch: 31/200
Train loss: 2.7857
Epoch: 32/200
Train loss: 2.8004
Epoch: 33/200
Train loss: 2.8330
Epoch: 34/200
Train loss: 2.8071
Epoch: 35/200
Train loss: 2.6879
Epoch: 36/200
Train loss: 2.5906
Epoch: 37/200
Train loss: 2.5779
Epoch: 38/200
Train loss: 2.5813
Epoch: 39/200
Train loss: 2.6377
Epoch: 40/200
Train loss: 2.6313
Epoch: 41/200
Train loss: 2.6222
Epoch: 42/200
Train loss: 2.5612
Epoch: 43/200
Train loss: 2.5793
Epoch: 44/200
Train loss: 2.6179
Epoch: 45/200
Train loss: 2.6139
Epoch: 46/200
Train loss: 2.5304
Epoch: 47/200
Train loss: 2.5444
Epoch: 48/200
Train loss: 2.5394
Epoch: 49/200
Train loss: 2.6101
Epoch: 50/200
Train loss: 2.4890
Epoch: 51/200
Train loss: 2.5657
Epoch: 52/200
Train loss: 2.6217
Epoch: 53/200
Train loss: 2.4085
Epoch: 54/200
Train loss: 2.4290
Epoch: 55/200
Train loss: 2.5139
Epoch: 56/200
Train loss: 2.5794
Epoch: 57/200
Train loss: 2.4589
Epoch: 58/200
Train loss: 2.4123
Epoch: 59/200
Train loss: 2.4272
Epoch: 60/200
Train loss: 2.3066
Epoch: 61/200
Train loss: 2.4329
Epoch: 62/200
Train loss: 2.3435
Epoch: 63/200
Train loss: 2.3610
Epoch: 64/200
Train loss: 2.4664
Epoch: 65/200
Train loss: 2.3071
Epoch: 66/200
Train loss: 2.4626
Epoch: 67/200
Train loss: 2.4793
Epoch: 68/200
Train loss: 2.4108
Epoch: 69/200
Train loss: 2.4057
Epoch: 70/200
Train loss: 2.4570
Epoch: 71/200
Train loss: 2.4183
Epoch: 72/200
Train loss: 2.4124
Epoch: 73/200
Train loss: 2.4072
Epoch: 74/200
Train loss: 2.3284
Epoch: 75/200
Train loss: 2.3750
Epoch: 76/200
Train loss: 2.3986
Epoch: 77/200
Train loss: 2.1476
Epoch: 78/200
Train loss: 2.3068
Epoch: 79/200
Train loss: 2.2946
Epoch: 80/200
Train loss: 2.3705
Epoch: 81/200
Train loss: 2.3812
Epoch: 82/200
Train loss: 2.3135
Epoch: 83/200
Train loss: 2.3410
Epoch: 84/200
Train loss: 2.2678
Epoch: 85/200
Train loss: 2.3346
Epoch: 86/200
Train loss: 2.3394
Epoch: 87/200
Train loss: 2.2910
Epoch: 88/200
Train loss: 2.2726
Epoch: 89/200
Train loss: 2.2721
Epoch: 90/200
Train loss: 2.3112
Epoch: 91/200
Train loss: 2.2317
Epoch: 92/200
Train loss: 2.3612
Epoch: 93/200
Train loss: 2.4423
Epoch: 94/200
Train loss: 2.3209
Epoch: 95/200
Train loss: 2.2900
Epoch: 96/200
Train loss: 2.1751
Epoch: 97/200
Train loss: 2.1672
Epoch: 98/200
Train loss: 2.2457
Epoch: 99/200
Train loss: 2.3020
Epoch: 100/200
Train loss: 2.2242
Epoch: 101/200
Train loss: 2.2455
Epoch: 102/200
Train loss: 2.1429
0.8126684740511482
Model improve: 0.000000 -> 0.812668
Epoch: 103/200
Train loss: 2.2268
0.8116691941982753
Epoch: 104/200
Train loss: 2.1733
0.8121425467981789
Epoch: 105/200
Train loss: 2.2391
0.8124024188998034
Epoch: 106/200
Train loss: 2.1717
0.81402827251371
Model improve: 0.812668 -> 0.814028
Epoch: 107/200
Train loss: 2.2454
Date :05/16/2023, 14:02:26
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b0
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.2
drop_path_rate: 0.4
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 41.8517
Epoch: 2/200
Train loss: 5.7412
Epoch: 3/200
Train loss: 5.2788
Epoch: 4/200
Train loss: 5.0113
Epoch: 5/200
Train loss: 4.7128
Epoch: 6/200
Train loss: 4.4799
Epoch: 7/200
Train loss: 4.1722
Epoch: 8/200
Train loss: 4.1045
Epoch: 9/200
Train loss: 3.9345
Epoch: 10/200
Train loss: 3.8711
Epoch: 11/200
Train loss: 3.7879
Epoch: 12/200
Train loss: 3.6715
Epoch: 13/200
Train loss: 3.6253
Epoch: 14/200
Train loss: 3.4968
Epoch: 15/200
Train loss: 3.5059
Epoch: 16/200
Train loss: 3.4490
Epoch: 17/200
Train loss: 3.3373
Epoch: 18/200
Train loss: 3.2379
Epoch: 19/200
Train loss: 3.4050
Epoch: 20/200
Train loss: 3.2987
Epoch: 21/200
Train loss: 3.1397
Epoch: 22/200
Train loss: 3.0338
Epoch: 23/200
Train loss: 3.0735
Epoch: 24/200
Train loss: 3.1438
Epoch: 25/200
Train loss: 3.1092
Epoch: 26/200
Train loss: 3.0828
Epoch: 27/200
Train loss: 3.0663
Epoch: 28/200
Train loss: 2.9132
Epoch: 29/200
Train loss: 2.9499
Epoch: 30/200
Train loss: 2.7916
Epoch: 31/200
Train loss: 2.9406
Epoch: 32/200
Train loss: 2.9844
Epoch: 33/200
Train loss: 2.7608
Epoch: 34/200
Train loss: 2.8595
Epoch: 35/200
Train loss: 2.8677
Epoch: 36/200
Train loss: 2.8977
Epoch: 37/200
Train loss: 2.7576
Epoch: 38/200
Train loss: 3.0004
Epoch: 39/200
Train loss: 2.6814
Epoch: 40/200
Train loss: 2.8219
Epoch: 41/200
Train loss: 2.7449
Epoch: 42/200
Train loss: 2.7234
Epoch: 43/200
Train loss: 2.8224
Epoch: 44/200
Train loss: 2.7749
Epoch: 45/200
Train loss: 2.8077
Epoch: 46/200
Train loss: 2.5599
Epoch: 47/200
Train loss: 2.7686
Epoch: 48/200
Train loss: 2.4904
Epoch: 49/200
Train loss: 2.4992
Epoch: 50/200
Train loss: 2.5549
Epoch: 51/200
Train loss: 2.6046
Epoch: 52/200
Train loss: 2.5896
Epoch: 53/200
Train loss: 2.5053
Epoch: 54/200
Train loss: 2.6137
Epoch: 55/200
Train loss: 2.5861
Epoch: 56/200
Train loss: 2.5706
Epoch: 57/200
Train loss: 2.5879
Epoch: 58/200
Train loss: 2.5608
Epoch: 59/200
Train loss: 2.5692
Epoch: 60/200
Train loss: 2.5876
Epoch: 61/200
Train loss: 2.4756
Epoch: 62/200
Train loss: 2.5112
Epoch: 63/200
Train loss: 2.5136
Epoch: 64/200
Train loss: 2.5237
Epoch: 65/200
Train loss: 2.5788
Epoch: 66/200
Train loss: 2.4681
Epoch: 67/200
Train loss: 2.5379
Epoch: 68/200
Train loss: 2.4861
Epoch: 69/200
Train loss: 2.6785
Epoch: 70/200
Train loss: 2.4641
Epoch: 71/200
Train loss: 2.3501
Epoch: 72/200
Train loss: 2.3632
Epoch: 73/200
Train loss: 2.4887
Epoch: 74/200
Train loss: 2.4615
Epoch: 75/200
Train loss: 2.5062
Epoch: 76/200
Train loss: 2.4969
Epoch: 77/200
Train loss: 2.4717
Epoch: 78/200
Train loss: 2.3858
Epoch: 79/200
Train loss: 2.2000
Epoch: 80/200
Train loss: 2.3690
Epoch: 81/200
Train loss: 2.3826
Epoch: 82/200
Train loss: 2.3788
Epoch: 83/200
Train loss: 2.3439
Epoch: 84/200
Train loss: 2.3154
Epoch: 85/200
Train loss: 2.4461
Epoch: 86/200
Train loss: 2.3811
Epoch: 87/200
Train loss: 2.2430
Epoch: 88/200
Train loss: 2.4420
Epoch: 89/200
Train loss: 2.4755
Epoch: 90/200
Train loss: 2.3069
Epoch: 91/200
Train loss: 2.4476
Epoch: 92/200
Train loss: 2.4062
Epoch: 93/200
Train loss: 2.4535
Epoch: 94/200
Train loss: 2.3537
Epoch: 95/200
Train loss: 2.3905
Epoch: 96/200
Train loss: 2.4368
Epoch: 97/200
Train loss: 2.4011
Epoch: 98/200
Train loss: 2.3604
Epoch: 99/200
Train loss: 2.3111
Epoch: 100/200
Train loss: 2.3494
Epoch: 101/200
Train loss: 2.4016
Epoch: 102/200
Train loss: 2.1854
0.8065200281383653
Model improve: 0.000000 -> 0.806520
Epoch: 103/200
Train loss: 2.2524
Date :05/16/2023, 16:11:32
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b0
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.15
drop_path_rate: 0.35
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 41.8493
Epoch: 2/200
Train loss: 5.7396
Epoch: 3/200
Train loss: 5.2763
Epoch: 4/200
Train loss: 5.0101
Epoch: 5/200
Train loss: 4.7113
Epoch: 6/200
Train loss: 4.4792
Epoch: 7/200
Train loss: 4.1713
Epoch: 8/200
Train loss: 4.1034
Epoch: 9/200
Train loss: 3.9329
Epoch: 10/200
Train loss: 3.8701
Epoch: 11/200
Train loss: 3.7872
Epoch: 12/200
Train loss: 3.6716
Epoch: 13/200
Train loss: 3.6251
Epoch: 14/200
Train loss: 3.4977
Epoch: 15/200
Train loss: 3.5060
Epoch: 16/200
Train loss: 3.4497
Epoch: 17/200
Train loss: 3.3400
Epoch: 18/200
Train loss: 3.2390
Epoch: 19/200
Train loss: 3.4053
Epoch: 20/200
Train loss: 3.2990
Epoch: 21/200
Train loss: 3.1398
Epoch: 22/200
Train loss: 3.0336
Epoch: 23/200
Train loss: 3.0739
Epoch: 24/200
Train loss: 3.1442
Epoch: 25/200
Train loss: 3.1095
Epoch: 26/200
Train loss: 3.0829
Epoch: 27/200
Train loss: 3.0666
Epoch: 28/200
Train loss: 2.9129
Epoch: 29/200
Train loss: 2.9502
Epoch: 30/200
Train loss: 2.7907
Epoch: 31/200
Train loss: 2.9416
Epoch: 32/200
Train loss: 2.9846
Epoch: 33/200
Train loss: 2.7612
Epoch: 34/200
Train loss: 2.8598
Epoch: 35/200
Train loss: 2.8676
Epoch: 36/200
Train loss: 2.8982
Epoch: 37/200
Train loss: 2.7579
Epoch: 38/200
Train loss: 3.0007
Epoch: 39/200
Train loss: 2.6819
Epoch: 40/200
Train loss: 2.8229
Epoch: 41/200
Train loss: 2.7455
Epoch: 42/200
Train loss: 2.7241
Epoch: 43/200
Train loss: 2.8227
Epoch: 44/200
Train loss: 2.7747
Epoch: 45/200
Train loss: 2.8084
Epoch: 46/200
Train loss: 2.5599
Epoch: 47/200
Train loss: 2.7697
Epoch: 48/200
Train loss: 2.4901
Epoch: 49/200
Train loss: 2.5001
Epoch: 50/200
Train loss: 2.5551
Epoch: 51/200
Train loss: 2.6051
Epoch: 52/200
Train loss: 2.5904
Epoch: 53/200
Train loss: 2.5055
Epoch: 54/200
Train loss: 2.6139
Epoch: 55/200
Train loss: 2.5871
Epoch: 56/200
Train loss: 2.5701
Epoch: 57/200
Train loss: 2.5890
Epoch: 58/200
Train loss: 2.5615
Epoch: 59/200
Train loss: 2.5692
Epoch: 60/200
Train loss: 2.5880
Epoch: 61/200
Train loss: 2.4767
Epoch: 62/200
Train loss: 2.5115
Epoch: 63/200
Train loss: 2.5141
Epoch: 64/200
Train loss: 2.5248
Epoch: 65/200
Train loss: 2.5784
Epoch: 66/200
Train loss: 2.4686
Epoch: 67/200
Train loss: 2.5386
Epoch: 68/200
Train loss: 2.4863
Epoch: 69/200
Train loss: 2.6786
Epoch: 70/200
Train loss: 2.4635
Epoch: 71/200
Train loss: 2.3500
Epoch: 72/200
Train loss: 2.3635
Epoch: 73/200
Train loss: 2.4892
Epoch: 74/200
Train loss: 2.4611
Epoch: 75/200
Train loss: 2.5066
Epoch: 76/200
Train loss: 2.4970
Epoch: 77/200
Train loss: 2.4717
Epoch: 78/200
Train loss: 2.3861
Epoch: 79/200
Train loss: 2.2002
Epoch: 80/200
Train loss: 2.3691
Epoch: 81/200
Train loss: 2.3821
Epoch: 82/200
Train loss: 2.3788
Epoch: 83/200
Train loss: 2.3433
Epoch: 84/200
Train loss: 2.3160
Epoch: 85/200
Train loss: 2.4472
Epoch: 86/200
Train loss: 2.3810
Epoch: 87/200
Train loss: 2.2429
Epoch: 88/200
Train loss: 2.4420
Epoch: 89/200
Train loss: 2.4757
Epoch: 90/200
Train loss: 2.3074
Epoch: 91/200
Train loss: 2.4479
Epoch: 92/200
Train loss: 2.4067
Epoch: 93/200
Train loss: 2.4550
Epoch: 94/200
Train loss: 2.3540
Epoch: 95/200
Train loss: 2.3900
Epoch: 96/200
Train loss: 2.4366
Epoch: 97/200
Train loss: 2.4005
Epoch: 98/200
Train loss: 2.3596
Epoch: 99/200
Train loss: 2.3111
Epoch: 100/200
Train loss: 2.3496
Epoch: 101/200
Train loss: 2.4020
Epoch: 102/200
Train loss: 2.1847
0.806579080595453
Model improve: 0.000000 -> 0.806579
Epoch: 103/200
Train loss: 2.2535
Date :05/16/2023, 18:19:43
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b0
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.1
drop_path_rate: 0.4
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 41.8460
Epoch: 2/200
Train loss: 5.7405
Epoch: 3/200
Train loss: 5.2776
Epoch: 4/200
Train loss: 5.0104
Epoch: 5/200
Train loss: 4.7121
Epoch: 6/200
Train loss: 4.4799
Epoch: 7/200
Train loss: 4.1716
Epoch: 8/200
Train loss: 4.1043
Epoch: 9/200
Train loss: 3.9331
Epoch: 10/200
Train loss: 3.8702
Epoch: 11/200
Train loss: 3.7880
Epoch: 12/200
Train loss: 3.6714
Epoch: 13/200
Train loss: 3.6258
Epoch: 14/200
Train loss: 3.4978
Epoch: 15/200
Train loss: 3.5057
Epoch: 16/200
Train loss: 3.4493
Epoch: 17/200
Train loss: 3.3382
Epoch: 18/200
Train loss: 3.2387
Epoch: 19/200
Train loss: 3.4054
Epoch: 20/200
Train loss: 3.2994
Epoch: 21/200
Train loss: 3.1402
Epoch: 22/200
Train loss: 3.0347
Epoch: 23/200
Train loss: 3.0736
Epoch: 24/200
Train loss: 3.1443
Epoch: 25/200
Train loss: 3.1094
Epoch: 26/200
Train loss: 3.0835
Epoch: 27/200
Train loss: 3.0664
Epoch: 28/200
Train loss: 2.9134
Epoch: 29/200
Train loss: 2.9495
Epoch: 30/200
Train loss: 2.7916
Epoch: 31/200
Train loss: 2.9413
Epoch: 32/200
Train loss: 2.9845
Epoch: 33/200
Train loss: 2.7612
Epoch: 34/200
Train loss: 2.8601
Epoch: 35/200
Train loss: 2.8675
Epoch: 36/200
Train loss: 2.8982
Epoch: 37/200
Train loss: 2.7575
Epoch: 38/200
Train loss: 3.0017
Epoch: 39/200
Train loss: 2.6824
Epoch: 40/200
Train loss: 2.8225
Epoch: 41/200
Train loss: 2.7462
Epoch: 42/200
Train loss: 2.7239
Epoch: 43/200
Train loss: 2.8229
Epoch: 44/200
Train loss: 2.7755
Epoch: 45/200
Train loss: 2.8085
Epoch: 46/200
Train loss: 2.5598
Epoch: 47/200
Train loss: 2.7695
Epoch: 48/200
Train loss: 2.4895
Epoch: 49/200
Train loss: 2.4995
Epoch: 50/200
Train loss: 2.5555
Epoch: 51/200
Train loss: 2.6043
Epoch: 52/200
Train loss: 2.5900
Epoch: 53/200
Train loss: 2.5052
Epoch: 54/200
Train loss: 2.6140
Epoch: 55/200
Train loss: 2.5870
Epoch: 56/200
Train loss: 2.5701
Epoch: 57/200
Train loss: 2.5886
Epoch: 58/200
Train loss: 2.5616
Epoch: 59/200
Train loss: 2.5696
Epoch: 60/200
Train loss: 2.5888
Epoch: 61/200
Train loss: 2.4758
Epoch: 62/200
Train loss: 2.5113
Epoch: 63/200
Train loss: 2.5144
Epoch: 64/200
Train loss: 2.5249
Epoch: 65/200
Train loss: 2.5793
Epoch: 66/200
Train loss: 2.4692
Epoch: 67/200
Train loss: 2.5384
Epoch: 68/200
Train loss: 2.4863
Epoch: 69/200
Train loss: 2.6781
Epoch: 70/200
Train loss: 2.4637
Epoch: 71/200
Train loss: 2.3501
Epoch: 72/200
Train loss: 2.3631
Epoch: 73/200
Train loss: 2.4888
Epoch: 74/200
Train loss: 2.4609
Epoch: 75/200
Train loss: 2.5065
Epoch: 76/200
Train loss: 2.4969
Epoch: 77/200
Train loss: 2.4705
Epoch: 78/200
Train loss: 2.3858
Epoch: 79/200
Train loss: 2.1998
Epoch: 80/200
Train loss: 2.3699
Epoch: 81/200
Train loss: 2.3825
Epoch: 82/200
Train loss: 2.3788
Epoch: 83/200
Train loss: 2.3437
Epoch: 84/200
Train loss: 2.3156
Epoch: 85/200
Train loss: 2.4462
Epoch: 86/200
Train loss: 2.3810
Epoch: 87/200
Train loss: 2.2426
Epoch: 88/200
Train loss: 2.4422
Epoch: 89/200
Train loss: 2.4757
Epoch: 90/200
Train loss: 2.3072
Epoch: 91/200
Train loss: 2.4482
Epoch: 92/200
Train loss: 2.4062
Epoch: 93/200
Train loss: 2.4537
Epoch: 94/200
Train loss: 2.3543
Epoch: 95/200
Train loss: 2.3907
Epoch: 96/200
Train loss: 2.4361
Epoch: 97/200
Train loss: 2.4011
Epoch: 98/200
Train loss: 2.3600
Epoch: 99/200
Train loss: 2.3118
Epoch: 100/200
Train loss: 2.3489
Epoch: 101/200
Train loss: 2.4019
Epoch: 102/200
Train loss: 2.1855
0.8069317485201123
Model improve: 0.000000 -> 0.806932
Epoch: 103/200
Train loss: 2.2538
Date :05/16/2023, 20:27:26
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b0
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.1
drop_path_rate: 0.4
norandomvolume
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 41.8949
Epoch: 2/200
Train loss: 5.7318
Epoch: 3/200
Train loss: 5.2729
Epoch: 4/200
Train loss: 5.0123
Epoch: 5/200
Train loss: 4.7115
Epoch: 6/200
Train loss: 4.4707
Epoch: 7/200
Train loss: 4.1774
Epoch: 8/200
Train loss: 4.0880
Epoch: 9/200
Train loss: 3.9348
Epoch: 10/200
Train loss: 3.8718
Epoch: 11/200
Train loss: 3.7743
Epoch: 12/200
Train loss: 3.6791
Epoch: 13/200
Train loss: 3.6207
Epoch: 14/200
Train loss: 3.5015
Epoch: 15/200
Train loss: 3.5337
Epoch: 16/200
Train loss: 3.4469
Epoch: 17/200
Train loss: 3.3104
Epoch: 18/200
Train loss: 3.2434
Epoch: 19/200
Train loss: 3.4149
Epoch: 20/200
Train loss: 3.2771
Epoch: 21/200
Train loss: 3.1506
Epoch: 22/200
Train loss: 3.0408
Epoch: 23/200
Train loss: 3.0743
Epoch: 24/200
Train loss: 3.1360
Epoch: 25/200
Train loss: 3.0919
Epoch: 26/200
Train loss: 3.0828
Epoch: 27/200
Train loss: 3.0720
Epoch: 28/200
Train loss: 2.9100
Epoch: 29/200
Train loss: 2.9421
Epoch: 30/200
Train loss: 2.7915
Epoch: 31/200
Train loss: 2.9279
Epoch: 32/200
Train loss: 2.9819
Epoch: 33/200
Train loss: 2.7644
Epoch: 34/200
Train loss: 2.8585
Epoch: 35/200
Train loss: 2.8703
Epoch: 36/200
Train loss: 2.8939
Epoch: 37/200
Train loss: 2.7631
Epoch: 38/200
Train loss: 3.0060
Epoch: 39/200
Train loss: 2.6864
Epoch: 40/200
Train loss: 2.8287
Epoch: 41/200
Train loss: 2.7543
Epoch: 42/200
Train loss: 2.7311
Epoch: 43/200
Train loss: 2.8012
Epoch: 44/200
Train loss: 2.7645
Epoch: 45/200
Train loss: 2.8054
Epoch: 46/200
Train loss: 2.5538
Epoch: 47/200
Train loss: 2.7670
Epoch: 48/200
Train loss: 2.5063
Epoch: 49/200
Train loss: 2.4869
Epoch: 50/200
Train loss: 2.5593
Epoch: 51/200
Train loss: 2.5986
Epoch: 52/200
Train loss: 2.6046
Epoch: 53/200
Train loss: 2.5192
Epoch: 54/200
Train loss: 2.6031
Epoch: 55/200
Train loss: 2.5970
Epoch: 56/200
Train loss: 2.5733
Epoch: 57/200
Train loss: 2.5836
Epoch: 58/200
Train loss: 2.5605
Epoch: 59/200
Train loss: 2.5725
Epoch: 60/200
Train loss: 2.5761
Epoch: 61/200
Train loss: 2.4621
Epoch: 62/200
Train loss: 2.4842
Epoch: 63/200
Train loss: 2.5027
Epoch: 64/200
Train loss: 2.5117
Epoch: 65/200
Train loss: 2.5845
Epoch: 66/200
Train loss: 2.4538
Epoch: 67/200
Train loss: 2.5529
Epoch: 68/200
Train loss: 2.4903
Epoch: 69/200
Train loss: 2.6759
Epoch: 70/200
Train loss: 2.4628
Epoch: 71/200
Train loss: 2.3464
Epoch: 72/200
Train loss: 2.3605
Epoch: 73/200
Train loss: 2.4763
Epoch: 74/200
Train loss: 2.4636
Epoch: 75/200
Train loss: 2.5213
Epoch: 76/200
Train loss: 2.4946
Epoch: 77/200
Train loss: 2.4692
Epoch: 78/200
Train loss: 2.3708
Epoch: 79/200
Train loss: 2.1995
Epoch: 80/200
Train loss: 2.3785
Epoch: 81/200
Train loss: 2.3815
Epoch: 82/200
Train loss: 2.3807
Epoch: 83/200
Train loss: 2.3565
Epoch: 84/200
Train loss: 2.3251
Epoch: 85/200
Train loss: 2.4546
Epoch: 86/200
Train loss: 2.3871
Epoch: 87/200
Train loss: 2.2477
Epoch: 88/200
Train loss: 2.4626
Epoch: 89/200
Train loss: 2.4867
Epoch: 90/200
Train loss: 2.3149
Epoch: 91/200
Train loss: 2.4339
Epoch: 92/200
Train loss: 2.4164
Epoch: 93/200
Train loss: 2.4320
Epoch: 94/200
Train loss: 2.3793
Epoch: 95/200
Train loss: 2.4080
Epoch: 96/200
Train loss: 2.4398
Epoch: 97/200
Train loss: 2.3944
Epoch: 98/200
Train loss: 2.3626
Epoch: 99/200
Train loss: 2.3014
Epoch: 100/200
Train loss: 2.3475
Epoch: 101/200
Train loss: 2.3970
Epoch: 102/200
Train loss: 2.1959
0.8079165099531647
Model improve: 0.000000 -> 0.807917
Epoch: 103/200
Train loss: 2.2420
Date :05/16/2023, 23:25:03
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b0
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.1
drop_path_rate: 0.4
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 41.8469
Epoch: 2/200
Train loss: 5.7402
Epoch: 3/200
Train loss: 5.2768
Epoch: 4/200
Train loss: 5.0107
Epoch: 5/200
Train loss: 4.7120
Epoch: 6/200
Train loss: 4.4800
Epoch: 7/200
Train loss: 4.1719
Epoch: 8/200
Train loss: 4.1043
Epoch: 9/200
Train loss: 3.9336
Epoch: 10/200
Train loss: 3.8709
Epoch: 11/200
Train loss: 3.7883
Epoch: 12/200
Train loss: 3.6717
Epoch: 13/200
Train loss: 3.6256
Epoch: 14/200
Train loss: 3.4968
Epoch: 15/200
Train loss: 3.5058
Epoch: 16/200
Train loss: 3.4489
Epoch: 17/200
Train loss: 3.3368
Epoch: 18/200
Train loss: 3.2388
Epoch: 19/200
Train loss: 3.4055
Epoch: 20/200
Train loss: 3.2986
Epoch: 21/200
Train loss: 3.1396
Epoch: 22/200
Train loss: 3.0339
Epoch: 23/200
Train loss: 3.0738
Epoch: 24/200
Train loss: 3.1436
Epoch: 25/200
Train loss: 3.1091
Epoch: 26/200
Train loss: 3.0831
Epoch: 27/200
Train loss: 3.0660
Epoch: 28/200
Train loss: 2.9141
Epoch: 29/200
Train loss: 2.9501
Epoch: 30/200
Train loss: 2.7904
Epoch: 31/200
Train loss: 2.9409
Epoch: 32/200
Train loss: 2.9844
Epoch: 33/200
Train loss: 2.7609
Epoch: 34/200
Train loss: 2.8597
Epoch: 35/200
Train loss: 2.8676
Epoch: 36/200
Train loss: 2.8979
Epoch: 37/200
Train loss: 2.7575
Epoch: 38/200
Train loss: 3.0004
Epoch: 39/200
Train loss: 2.6813
Epoch: 40/200
Train loss: 2.8216
Epoch: 41/200
Train loss: 2.7451
Epoch: 42/200
Train loss: 2.7226
Epoch: 43/200
Train loss: 2.8223
Epoch: 44/200
Train loss: 2.7753
Epoch: 45/200
Train loss: 2.8084
Epoch: 46/200
Train loss: 2.5592
Epoch: 47/200
Train loss: 2.7695
Epoch: 48/200
Train loss: 2.4894
Epoch: 49/200
Train loss: 2.5001
Epoch: 50/200
Train loss: 2.5546
Epoch: 51/200
Train loss: 2.6041
Epoch: 52/200
Train loss: 2.5901
Epoch: 53/200
Train loss: 2.5049
Epoch: 54/200
Train loss: 2.6139
Epoch: 55/200
Train loss: 2.5866
Epoch: 56/200
Train loss: 2.5697
Epoch: 57/200
Train loss: 2.5883
Epoch: 58/200
Train loss: 2.5612
Epoch: 59/200
Train loss: 2.5698
Epoch: 60/200
Train loss: 2.5875
Epoch: 61/200
Train loss: 2.4760
Epoch: 62/200
Train loss: 2.5115
Epoch: 63/200
Train loss: 2.5142
Epoch: 64/200
Train loss: 2.5235
Epoch: 65/200
Train loss: 2.5785
Epoch: 66/200
Train loss: 2.4680
Epoch: 67/200
Train loss: 2.5375
Epoch: 68/200
Train loss: 2.4856
Epoch: 69/200
Train loss: 2.6791
Epoch: 70/200
Train loss: 2.4637
Epoch: 71/200
Train loss: 2.3501
Epoch: 72/200
Train loss: 2.3626
Epoch: 73/200
Train loss: 2.4889
Epoch: 74/200
Train loss: 2.4609
Epoch: 75/200
Train loss: 2.5068
Epoch: 76/200
Train loss: 2.4964
Epoch: 77/200
Train loss: 2.4711
Epoch: 78/200
Train loss: 2.3856
Epoch: 79/200
Train loss: 2.2000
Epoch: 80/200
Train loss: 2.3690
Epoch: 81/200
Train loss: 2.3825
Epoch: 82/200
Train loss: 2.3783
Epoch: 83/200
Train loss: 2.3432
Epoch: 84/200
Train loss: 2.3162
Epoch: 85/200
Train loss: 2.4460
Epoch: 86/200
Train loss: 2.3809
Epoch: 87/200
Train loss: 2.2423
Epoch: 88/200
Train loss: 2.4416
Epoch: 89/200
Train loss: 2.4757
Epoch: 90/200
Train loss: 2.3077
Epoch: 91/200
Train loss: 2.4479
Epoch: 92/200
Train loss: 2.4071
Epoch: 93/200
Train loss: 2.4541
Epoch: 94/200
Train loss: 2.3538
Epoch: 95/200
Train loss: 2.3907
Epoch: 96/200
Train loss: 2.4361
Epoch: 97/200
Train loss: 2.4005
Epoch: 98/200
Train loss: 2.3597
Epoch: 99/200
Train loss: 2.3106
Epoch: 100/200
Train loss: 2.3493
Epoch: 101/200
Train loss: 2.4024
Epoch: 102/200
Train loss: 2.1856
0.8066939308201195
Model improve: 0.000000 -> 0.806694
Epoch: 103/200
Date :05/17/2023, 01:29:14
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b0
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.15
drop_path_rate: 0.4
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 41.8550
Epoch: 2/200
Train loss: 5.7413
Epoch: 3/200
Train loss: 5.2774
Epoch: 4/200
Train loss: 5.0107
Epoch: 5/200
Train loss: 4.7125
Epoch: 6/200
Train loss: 4.4799
Epoch: 7/200
Train loss: 4.1718
Epoch: 8/200
Train loss: 4.1051
Epoch: 9/200
Train loss: 3.9335
Epoch: 10/200
Train loss: 3.8706
Epoch: 11/200
Train loss: 3.7879
Epoch: 12/200
Train loss: 3.6714
Epoch: 13/200
Train loss: 3.6257
Epoch: 14/200
Train loss: 3.4975
Epoch: 15/200
Train loss: 3.5060
Epoch: 16/200
Train loss: 3.4493
Epoch: 17/200
Train loss: 3.3378
Epoch: 18/200
Train loss: 3.2383
Epoch: 19/200
Train loss: 3.4049
Epoch: 20/200
Train loss: 3.2986
Epoch: 21/200
Train loss: 3.1394
Epoch: 22/200
Train loss: 3.0338
Epoch: 23/200
Train loss: 3.0736
Epoch: 24/200
Train loss: 3.1437
Epoch: 25/200
Train loss: 3.1088
Epoch: 26/200
Train loss: 3.0829
Epoch: 27/200
Train loss: 3.0660
Epoch: 28/200
Train loss: 2.9126
Epoch: 29/200
Train loss: 2.9492
Epoch: 30/200
Train loss: 2.7909
Epoch: 31/200
Train loss: 2.9406
Epoch: 32/200
Train loss: 2.9840
Epoch: 33/200
Train loss: 2.7604
Epoch: 34/200
Train loss: 2.8595
Epoch: 35/200
Train loss: 2.8669
Epoch: 36/200
Train loss: 2.8973
Epoch: 37/200
Train loss: 2.7576
Epoch: 38/200
Train loss: 3.0009
Epoch: 39/200
Train loss: 2.6815
Epoch: 40/200
Train loss: 2.8215
Epoch: 41/200
Train loss: 2.7456
Epoch: 42/200
Train loss: 2.7234
Epoch: 43/200
Train loss: 2.8231
Epoch: 44/200
Train loss: 2.7752
Epoch: 45/200
Train loss: 2.8079
Epoch: 46/200
Train loss: 2.5593
Epoch: 47/200
Train loss: 2.7686
Epoch: 48/200
Train loss: 2.4902
Epoch: 49/200
Train loss: 2.4995
Epoch: 50/200
Train loss: 2.5543
Epoch: 51/200
Train loss: 2.6039
Epoch: 52/200
Train loss: 2.5893
Epoch: 53/200
Train loss: 2.5052
Epoch: 54/200
Train loss: 2.6137
Epoch: 55/200
Train loss: 2.5864
Epoch: 56/200
Train loss: 2.5693
Epoch: 57/200
Train loss: 2.5885
Epoch: 58/200
Train loss: 2.5609
Epoch: 59/200
Train loss: 2.5697
Epoch: 60/200
Train loss: 2.5880
Epoch: 61/200
Train loss: 2.4757
Epoch: 62/200
Train loss: 2.5106
Epoch: 63/200
Train loss: 2.5142
Epoch: 64/200
Train loss: 2.5242
Epoch: 65/200
Train loss: 2.5785
Epoch: 66/200
Train loss: 2.4686
Epoch: 67/200
Train loss: 2.5374
Epoch: 68/200
Train loss: 2.4852
Epoch: 69/200
Train loss: 2.6779
Epoch: 70/200
Train loss: 2.4632
Epoch: 71/200
Train loss: 2.3498
Epoch: 72/200
Train loss: 2.3635
Epoch: 73/200
Train loss: 2.4890
Epoch: 74/200
Train loss: 2.4616
Epoch: 75/200
Train loss: 2.5064
Epoch: 76/200
Train loss: 2.4969
Epoch: 77/200
Train loss: 2.4711
Epoch: 78/200
Train loss: 2.3858
Epoch: 79/200
Train loss: 2.1997
Epoch: 80/200
Train loss: 2.3682
Epoch: 81/200
Train loss: 2.3823
Epoch: 82/200
Train loss: 2.3783
Epoch: 83/200
Train loss: 2.3431
Epoch: 84/200
Train loss: 2.3154
Epoch: 85/200
Train loss: 2.4462
Epoch: 86/200
Train loss: 2.3805
Epoch: 87/200
Train loss: 2.2425
Epoch: 88/200
Train loss: 2.4411
Epoch: 89/200
Train loss: 2.4751
Epoch: 90/200
Train loss: 2.3073
Epoch: 91/200
Train loss: 2.4477
Epoch: 92/200
Train loss: 2.4061
Epoch: 93/200
Train loss: 2.4535
Epoch: 94/200
Train loss: 2.3539
Epoch: 95/200
Train loss: 2.3901
Epoch: 96/200
Train loss: 2.4360
Epoch: 97/200
Train loss: 2.4008
Epoch: 98/200
Train loss: 2.3598
Epoch: 99/200
Train loss: 2.3112
Epoch: 100/200
Train loss: 2.3494
Epoch: 101/200
Train loss: 2.4014
Epoch: 102/200
Train loss: 2.1851
0.8067961620206995
Model improve: 0.000000 -> 0.806796
Epoch: 103/200
Train loss: 2.2526
0.8049478308628116
Epoch: 104/200
Train loss: 2.2462
0.8074292539894614
Model improve: 0.806796 -> 0.807429
Epoch: 105/200
Train loss: 2.3105
0.8071040517711427
Epoch: 106/200
Train loss: 2.3211
0.8055440156757872
Epoch: 107/200
Train loss: 2.3746
0.8048642510973429
Epoch: 108/200
Train loss: 2.3457
0.8065089211056926
Epoch: 109/200
Train loss: 2.3069
0.8065020664638022
Epoch: 110/200
Train loss: 2.3826
0.8054040967663423
Epoch: 111/200
Train loss: 2.1846
0.8071686767439946
Epoch: 112/200
Train loss: 2.2776
0.8063379278473203
Epoch: 113/200
Train loss: 2.2739
0.807943107412646
Model improve: 0.807429 -> 0.807943
Epoch: 114/200
Train loss: 2.3804
0.8068401746398783
Epoch: 115/200
Train loss: 2.2652
0.8076729506756453
Epoch: 116/200
Train loss: 2.3277
0.8063985917283648
Epoch: 117/200
Train loss: 2.2640
0.8072190768959282
Epoch: 118/200
Train loss: 2.2390
0.8075047527479615
Epoch: 119/200
Train loss: 2.3371
0.8058893216925545
Epoch: 120/200
Train loss: 2.2271
0.8084432107825302
Model improve: 0.807943 -> 0.808443
Epoch: 121/200
Train loss: 2.2368
0.8082178144853537
Epoch: 122/200
Train loss: 2.3252
0.8080374006808442
Epoch: 123/200
Train loss: 2.4428
0.8064948471108535
Epoch: 124/200
Train loss: 2.4371
0.8097710786392851
Model improve: 0.808443 -> 0.809771
Epoch: 125/200
Train loss: 2.3713
0.8099267669489381
Model improve: 0.809771 -> 0.809927
Epoch: 126/200
Train loss: 2.2206
0.8092330160726627
Epoch: 127/200
Train loss: 2.2886
0.8086079799831014
Epoch: 128/200
Train loss: 2.1603
0.8098141161220661
Epoch: 129/200
Train loss: 2.2245
0.8096902467184125
Epoch: 130/200
Train loss: 2.1316
0.8079936260587052
Epoch: 131/200
Train loss: 2.2152
Date :05/17/2023, 06:11:15
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b0
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.15
drop_path_rate: 0.4
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 41.8498
Epoch: 2/200
Train loss: 5.7391
Epoch: 3/200
Train loss: 5.2761
Epoch: 4/200
Train loss: 5.0096
Epoch: 5/200
Train loss: 4.7113
Epoch: 6/200
Train loss: 4.4791
Epoch: 7/200
Train loss: 4.1712
Epoch: 8/200
Train loss: 4.1040
Epoch: 9/200
Train loss: 3.9333
Epoch: 10/200
Train loss: 3.8702
Epoch: 11/200
Train loss: 3.7878
Epoch: 12/200
Train loss: 3.6715
Epoch: 13/200
Train loss: 3.6250
Epoch: 14/200
Train loss: 3.4963
Epoch: 15/200
Train loss: 3.5047
Epoch: 16/200
Train loss: 3.4485
Epoch: 17/200
Train loss: 3.3367
Epoch: 18/200
Train loss: 3.2378
Epoch: 19/200
Train loss: 3.4050
Epoch: 20/200
Train loss: 3.2988
Epoch: 21/200
Train loss: 3.1397
Epoch: 22/200
Train loss: 3.0339
Epoch: 23/200
Train loss: 3.0740
Epoch: 24/200
Train loss: 3.1432
Epoch: 25/200
Train loss: 3.1088
Epoch: 26/200
Train loss: 3.0826
Epoch: 27/200
Train loss: 3.0662
Epoch: 28/200
Train loss: 2.9133
Epoch: 29/200
Train loss: 2.9494
Epoch: 30/200
Train loss: 2.7913
Epoch: 31/200
Train loss: 2.9411
Epoch: 32/200
Train loss: 2.9843
Epoch: 33/200
Train loss: 2.7603
Epoch: 34/200
Train loss: 2.8596
Epoch: 35/200
Train loss: 2.8674
Epoch: 36/200
Train loss: 2.8980
Epoch: 37/200
Train loss: 2.7578
Epoch: 38/200
Train loss: 3.0012
Epoch: 39/200
Train loss: 2.6815
Epoch: 40/200
Train loss: 2.8222
Epoch: 41/200
Train loss: 2.7458
Epoch: 42/200
Train loss: 2.7233
Epoch: 43/200
Train loss: 2.8227
Epoch: 44/200
Train loss: 2.7757
Epoch: 45/200
Train loss: 2.8076
Epoch: 46/200
Train loss: 2.5599
Epoch: 47/200
Train loss: 2.7692
Epoch: 48/200
Train loss: 2.4902
Epoch: 49/200
Train loss: 2.5006
Epoch: 50/200
Train loss: 2.5551
Epoch: 51/200
Train loss: 2.6043
Epoch: 52/200
Train loss: 2.5898
Epoch: 53/200
Train loss: 2.5056
Epoch: 54/200
Train loss: 2.6138
Epoch: 55/200
Train loss: 2.5864
Epoch: 56/200
Train loss: 2.5706
Epoch: 57/200
Train loss: 2.5881
Epoch: 58/200
Train loss: 2.5614
Epoch: 59/200
Train loss: 2.5691
Epoch: 60/200
Train loss: 2.5875
Epoch: 61/200
Train loss: 2.4757
Epoch: 62/200
Train loss: 2.5118
Epoch: 63/200
Train loss: 2.5146
Epoch: 64/200
Train loss: 2.5240
Epoch: 65/200
Train loss: 2.5785
Epoch: 66/200
Train loss: 2.4690
Epoch: 67/200
Train loss: 2.5384
Epoch: 68/200
Train loss: 2.4866
Epoch: 69/200
Train loss: 2.6786
Epoch: 70/200
Train loss: 2.4639
Epoch: 71/200
Train loss: 2.3505
Epoch: 72/200
Train loss: 2.3636
Epoch: 73/200
Train loss: 2.4896
Epoch: 74/200
Train loss: 2.4617
Epoch: 75/200
Train loss: 2.5068
Epoch: 76/200
Train loss: 2.4974
Epoch: 77/200
Train loss: 2.4712
Epoch: 78/200
Train loss: 2.3861
Epoch: 79/200
Train loss: 2.2011
Epoch: 80/200
Train loss: 2.3684
Epoch: 81/200
Train loss: 2.3822
Epoch: 82/200
Train loss: 2.3788
Epoch: 83/200
Train loss: 2.3435
Epoch: 84/200
Train loss: 2.3160
Epoch: 85/200
Train loss: 2.4462
Epoch: 86/200
Train loss: 2.3806
Epoch: 87/200
Train loss: 2.2426
Epoch: 88/200
Train loss: 2.4416
Epoch: 89/200
Train loss: 2.4760
Epoch: 90/200
Train loss: 2.3074
Epoch: 91/200
Train loss: 2.4485
Epoch: 92/200
Train loss: 2.4070
Epoch: 93/200
Train loss: 2.4532
Epoch: 94/200
Train loss: 2.3542
Epoch: 95/200
Train loss: 2.3908
Epoch: 96/200
Train loss: 2.4363
Epoch: 97/200
Train loss: 2.4011
Epoch: 98/200
Train loss: 2.3610
Epoch: 99/200
Train loss: 2.3109
Epoch: 100/200
Train loss: 2.3500
Epoch: 101/200
Train loss: 2.4023
Epoch: 102/200
Train loss: 2.1860
0.8067844508030336
Model improve: 0.000000 -> 0.806784
Epoch: 103/200
Train loss: 2.2531
0.8048245690544806
Epoch: 104/200
Date :05/17/2023, 08:21:30
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b0
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.25
drop_path_rate: 0.4
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 41.8953
Epoch: 2/200
Train loss: 5.7328
Epoch: 3/200
Train loss: 5.2739
Epoch: 4/200
Train loss: 5.0133
Epoch: 5/200
Train loss: 4.7115
Epoch: 6/200
Train loss: 4.4709
Epoch: 7/200
Train loss: 4.1780
Epoch: 8/200
Train loss: 4.0886
Epoch: 9/200
Train loss: 3.9349
Epoch: 10/200
Train loss: 3.8719
Epoch: 11/200
Train loss: 3.7739
Epoch: 12/200
Train loss: 3.6794
Epoch: 13/200
Train loss: 3.6205
Epoch: 14/200
Train loss: 3.5013
Epoch: 15/200
Train loss: 3.5337
Epoch: 16/200
Train loss: 3.4467
Epoch: 17/200
Train loss: 3.3100
Epoch: 18/200
Train loss: 3.2440
Epoch: 19/200
Train loss: 3.4144
Epoch: 20/200
Train loss: 3.2773
Epoch: 21/200
Train loss: 3.1511
Epoch: 22/200
Train loss: 3.0411
Epoch: 23/200
Train loss: 3.0742
Epoch: 24/200
Train loss: 3.1361
Epoch: 25/200
Train loss: 3.0917
Epoch: 26/200
Train loss: 3.0823
Epoch: 27/200
Train loss: 3.0724
Epoch: 28/200
Train loss: 2.9101
Epoch: 29/200
Train loss: 2.9414
Epoch: 30/200
Train loss: 2.7918
Epoch: 31/200
Train loss: 2.9275
Epoch: 32/200
Train loss: 2.9817
Epoch: 33/200
Train loss: 2.7648
Epoch: 34/200
Train loss: 2.8587
Epoch: 35/200
Train loss: 2.8697
Epoch: 36/200
Train loss: 2.8944
Epoch: 37/200
Train loss: 2.7626
Epoch: 38/200
Train loss: 3.0065
Epoch: 39/200
Train loss: 2.6860
Epoch: 40/200
Train loss: 2.8286
Epoch: 41/200
Train loss: 2.7544
Epoch: 42/200
Train loss: 2.7315
Epoch: 43/200
Train loss: 2.8003
Epoch: 44/200
Train loss: 2.7648
Epoch: 45/200
Train loss: 2.8061
Epoch: 46/200
Train loss: 2.5543
Epoch: 47/200
Train loss: 2.7668
Epoch: 48/200
Train loss: 2.5073
Epoch: 49/200
Train loss: 2.4870
Epoch: 50/200
Train loss: 2.5593
Epoch: 51/200
Train loss: 2.5986
Epoch: 52/200
Train loss: 2.6054
Epoch: 53/200
Train loss: 2.5193
Epoch: 54/200
Train loss: 2.6037
Epoch: 55/200
Train loss: 2.5965
Epoch: 56/200
Train loss: 2.5739
Epoch: 57/200
Train loss: 2.5837
Epoch: 58/200
Train loss: 2.5612
Epoch: 59/200
Train loss: 2.5728
Epoch: 60/200
Train loss: 2.5762
Epoch: 61/200
Train loss: 2.4625
Epoch: 62/200
Train loss: 2.4840
Epoch: 63/200
Train loss: 2.5033
Epoch: 64/200
Train loss: 2.5126
Epoch: 65/200
Train loss: 2.5837
Epoch: 66/200
Train loss: 2.4532
Epoch: 67/200
Train loss: 2.5530
Epoch: 68/200
Train loss: 2.4914
Epoch: 69/200
Train loss: 2.6750
Epoch: 70/200
Train loss: 2.4627
Epoch: 71/200
Train loss: 2.3468
Epoch: 72/200
Train loss: 2.3604
Epoch: 73/200
Train loss: 2.4762
Epoch: 74/200
Train loss: 2.4632
Epoch: 75/200
Train loss: 2.5205
Epoch: 76/200
Train loss: 2.4951
Epoch: 77/200
Train loss: 2.4689
Epoch: 78/200
Train loss: 2.3712
Epoch: 79/200
Train loss: 2.1997
Epoch: 80/200
Train loss: 2.3781
Epoch: 81/200
Train loss: 2.3813
Epoch: 82/200
Train loss: 2.3808
Epoch: 83/200
Train loss: 2.3566
Epoch: 84/200
Train loss: 2.3256
Epoch: 85/200
Train loss: 2.4548
Epoch: 86/200
Train loss: 2.3875
Epoch: 87/200
Train loss: 2.2480
Epoch: 88/200
Train loss: 2.4630
Epoch: 89/200
Train loss: 2.4860
Epoch: 90/200
Train loss: 2.3159
Epoch: 91/200
Train loss: 2.4349
Epoch: 92/200
Train loss: 2.4169
Epoch: 93/200
Train loss: 2.4326
Epoch: 94/200
Train loss: 2.3799
Epoch: 95/200
Train loss: 2.4073
Epoch: 96/200
Train loss: 2.4398
Epoch: 97/200
Train loss: 2.3942
Epoch: 98/200
Train loss: 2.3621
Epoch: 99/200
Train loss: 2.3018
Epoch: 100/200
Train loss: 2.3483
Epoch: 101/200
Train loss: 2.3961
Epoch: 102/200
Train loss: 2.1957
0.8075817700547308
Model improve: 0.000000 -> 0.807582
Epoch: 103/200
Train loss: 2.2420
0.8052124987301221
Epoch: 104/200
Train loss: 2.2496
0.8080550419587302
Model improve: 0.807582 -> 0.808055
Epoch: 105/200
Train loss: 2.2907
0.8071261162790064
Epoch: 106/200
Train loss: 2.3155
0.8060992363642667
Epoch: 107/200
Train loss: 2.3796
0.8059087411234609
Epoch: 108/200
Train loss: 2.3547
0.8053236863707028
Epoch: 109/200
Train loss: 2.3161
0.8067995927279561
Epoch: 110/200
Train loss: 2.3879
0.8054403904883618
Epoch: 111/200
Train loss: 2.1784
0.8071888034105668
Epoch: 112/200
Train loss: 2.2864
0.8062216212456453
Epoch: 113/200
Train loss: 2.2850
0.8067115461825342
Epoch: 114/200
Date :05/17/2023, 11:27:38
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b0
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.3
drop_path_rate: 0.4
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 41.8412
Epoch: 2/200
Train loss: 5.7401
Epoch: 3/200
Train loss: 5.2770
Epoch: 4/200
Train loss: 5.0104
Epoch: 5/200
Train loss: 4.7120
Epoch: 6/200
Train loss: 4.4793
Epoch: 7/200
Train loss: 4.1715
Epoch: 8/200
Train loss: 4.1043
Epoch: 9/200
Train loss: 3.9335
Epoch: 10/200
Train loss: 3.8706
Epoch: 11/200
Train loss: 3.7876
Epoch: 12/200
Train loss: 3.6713
Epoch: 13/200
Train loss: 3.6254
Epoch: 14/200
Train loss: 3.4968
Epoch: 15/200
Train loss: 3.5051
Epoch: 16/200
Train loss: 3.4488
Epoch: 17/200
Train loss: 3.3367
Epoch: 18/200
Train loss: 3.2382
Epoch: 19/200
Train loss: 3.4047
Epoch: 20/200
Train loss: 3.2988
Epoch: 21/200
Train loss: 3.1397
Epoch: 22/200
Train loss: 3.0339
Epoch: 23/200
Train loss: 3.0731
Epoch: 24/200
Train loss: 3.1438
Epoch: 25/200
Train loss: 3.1091
Epoch: 26/200
Train loss: 3.0829
Epoch: 27/200
Train loss: 3.0667
Epoch: 28/200
Train loss: 2.9133
Epoch: 29/200
Train loss: 2.9500
Epoch: 30/200
Train loss: 2.7905
Epoch: 31/200
Train loss: 2.9409
Epoch: 32/200
Train loss: 2.9844
Epoch: 33/200
Train loss: 2.7612
Epoch: 34/200
Train loss: 2.8597
Epoch: 35/200
Train loss: 2.8672
Epoch: 36/200
Train loss: 2.8977
Epoch: 37/200
Train loss: 2.7570
Epoch: 38/200
Train loss: 3.0007
Epoch: 39/200
Train loss: 2.6820
Epoch: 40/200
Train loss: 2.8223
Epoch: 41/200
Train loss: 2.7460
Epoch: 42/200
Train loss: 2.7237
Epoch: 43/200
Train loss: 2.8234
Epoch: 44/200
Train loss: 2.7743
Epoch: 45/200
Train loss: 2.8089
Epoch: 46/200
Train loss: 2.5595
Epoch: 47/200
Train loss: 2.7695
Epoch: 48/200
Train loss: 2.4899
Epoch: 49/200
Train loss: 2.5002
Epoch: 50/200
Train loss: 2.5551
Epoch: 51/200
Train loss: 2.6048
Epoch: 52/200
Train loss: 2.5894
Epoch: 53/200
Train loss: 2.5056
Epoch: 54/200
Train loss: 2.6141
Epoch: 55/200
Train loss: 2.5865
Epoch: 56/200
Train loss: 2.5701
Epoch: 57/200
Train loss: 2.5888
Epoch: 58/200
Train loss: 2.5614
Epoch: 59/200
Train loss: 2.5697
Epoch: 60/200
Train loss: 2.5881
Epoch: 61/200
Train loss: 2.4764
Epoch: 62/200
Train loss: 2.5120
Epoch: 63/200
Train loss: 2.5143
Epoch: 64/200
Train loss: 2.5246
Epoch: 65/200
Train loss: 2.5783
Epoch: 66/200
Train loss: 2.4687
Epoch: 67/200
Train loss: 2.5392
Epoch: 68/200
Train loss: 2.4864
Epoch: 69/200
Train loss: 2.6785
Epoch: 70/200
Train loss: 2.4636
Epoch: 71/200
Train loss: 2.3511
Epoch: 72/200
Train loss: 2.3627
Epoch: 73/200
Train loss: 2.4896
Epoch: 74/200
Train loss: 2.4612
Epoch: 75/200
Train loss: 2.5063
Epoch: 76/200
Train loss: 2.4974
Epoch: 77/200
Train loss: 2.4713
Epoch: 78/200
Train loss: 2.3860
Epoch: 79/200
Train loss: 2.2008
Epoch: 80/200
Train loss: 2.3693
Epoch: 81/200
Train loss: 2.3828
Epoch: 82/200
Train loss: 2.3794
Epoch: 83/200
Train loss: 2.3440
Epoch: 84/200
Train loss: 2.3157
Epoch: 85/200
Train loss: 2.4473
Epoch: 86/200
Train loss: 2.3815
Epoch: 87/200
Train loss: 2.2435
Epoch: 88/200
Train loss: 2.4417
Epoch: 89/200
Train loss: 2.4761
Epoch: 90/200
Train loss: 2.3078
Epoch: 91/200
Train loss: 2.4487
Epoch: 92/200
Train loss: 2.4067
Epoch: 93/200
Train loss: 2.4537
Epoch: 94/200
Train loss: 2.3539
Epoch: 95/200
Train loss: 2.3906
Epoch: 96/200
Train loss: 2.4372
Epoch: 97/200
Train loss: 2.4014
Epoch: 98/200
Train loss: 2.3601
Epoch: 99/200
Train loss: 2.3111
Epoch: 100/200
Train loss: 2.3499
Epoch: 101/200
Train loss: 2.4018
Epoch: 102/200
Train loss: 2.1856
0.8068015619478482
Model improve: 0.000000 -> 0.806802
Epoch: 103/200
Train loss: 2.2535
0.8051356905630171
Epoch: 104/200
Train loss: 2.2467
Date :05/17/2023, 14:12:44
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b0
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.25
drop_path_rate: 0.4
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 41.8499
Epoch: 2/200
Train loss: 5.7403
Epoch: 3/200
Train loss: 5.2773
Epoch: 4/200
Train loss: 5.0108
Epoch: 5/200
Train loss: 4.7125
Epoch: 6/200
Train loss: 4.4798
Epoch: 7/200
Train loss: 4.1719
Epoch: 8/200
Train loss: 4.1047
Epoch: 9/200
Train loss: 3.9336
Epoch: 10/200
Date :05/17/2023, 18:05:01
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b0
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.25
drop_path_rate: 0.4
76407
Fold: 4
13830
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Date :05/17/2023, 18:05:28
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b0
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.25
drop_path_rate: 0.4
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 41.8517
Epoch: 2/200
Train loss: 5.7486
Epoch: 3/200
Date :05/17/2023, 19:23:18
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b0
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.25
drop_path_rate: 0.4
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 41.8585
Epoch: 2/200
Train loss: 5.7503
Epoch: 3/200
Train loss: 5.2860
Epoch: 4/200
Train loss: 5.0286
Epoch: 5/200
Train loss: 4.7090
Epoch: 6/200
Train loss: 4.4847
Epoch: 7/200
Train loss: 4.1788
Epoch: 8/200
Train loss: 4.1114
Epoch: 9/200
Train loss: 3.9373
Epoch: 10/200
Train loss: 3.8789
Epoch: 11/200
Train loss: 3.7839
Epoch: 12/200
Train loss: 3.6706
Epoch: 13/200
Train loss: 3.6287
Epoch: 14/200
Train loss: 3.5022
Epoch: 15/200
Train loss: 3.5219
Epoch: 16/200
Train loss: 3.4442
Epoch: 17/200
Train loss: 3.3307
Epoch: 18/200
Train loss: 3.2277
Epoch: 19/200
Train loss: 3.4259
Epoch: 20/200
Train loss: 3.3076
Epoch: 21/200
Train loss: 3.1574
Epoch: 22/200
Train loss: 3.0371
Epoch: 23/200
Train loss: 3.0665
Epoch: 24/200
Train loss: 3.1525
Epoch: 25/200
Train loss: 3.0893
Epoch: 26/200
Train loss: 3.0863
Epoch: 27/200
Train loss: 3.0669
Epoch: 28/200
Train loss: 2.9219
Epoch: 29/200
Train loss: 2.9604
Epoch: 30/200
Train loss: 2.7934
Epoch: 31/200
Train loss: 2.9317
Epoch: 32/200
Train loss: 2.9837
Epoch: 33/200
Train loss: 2.7688
Epoch: 34/200
Train loss: 2.8650
Epoch: 35/200
Train loss: 2.8667
Epoch: 36/200
Train loss: 2.8942
Epoch: 37/200
Train loss: 2.7633
Epoch: 38/200
Train loss: 2.9945
Epoch: 39/200
Train loss: 2.6945
Epoch: 40/200
Train loss: 2.8151
Epoch: 41/200
Train loss: 2.7508
Epoch: 42/200
Train loss: 2.7157
Epoch: 43/200
Train loss: 2.8229
Epoch: 44/200
Train loss: 2.7815
Epoch: 45/200
Train loss: 2.7989
Epoch: 46/200
Train loss: 2.5548
Epoch: 47/200
Train loss: 2.7566
Epoch: 48/200
Train loss: 2.5019
Epoch: 49/200
Train loss: 2.4954
Epoch: 50/200
Train loss: 2.5791
Epoch: 51/200
Train loss: 2.5875
Epoch: 52/200
Train loss: 2.5976
Epoch: 53/200
Train loss: 2.5101
Epoch: 54/200
Train loss: 2.5999
Epoch: 55/200
Train loss: 2.5877
Epoch: 56/200
Train loss: 2.5507
Epoch: 57/200
Train loss: 2.5923
Epoch: 58/200
Train loss: 2.5742
Epoch: 59/200
Train loss: 2.5765
Epoch: 60/200
Train loss: 2.5611
Epoch: 61/200
Train loss: 2.4673
Epoch: 62/200
Train loss: 2.5042
Epoch: 63/200
Train loss: 2.5106
Epoch: 64/200
Train loss: 2.5193
Epoch: 65/200
Train loss: 2.5848
Epoch: 66/200
Train loss: 2.4475
Epoch: 67/200
Train loss: 2.5469
Epoch: 68/200
Train loss: 2.5091
Epoch: 69/200
Train loss: 2.6596
Epoch: 70/200
Train loss: 2.4544
Epoch: 71/200
Train loss: 2.3518
Epoch: 72/200
Train loss: 2.3496
Epoch: 73/200
Train loss: 2.4979
Epoch: 74/200
Train loss: 2.4844
Epoch: 75/200
Train loss: 2.5207
Epoch: 76/200
Train loss: 2.4876
Epoch: 77/200
Train loss: 2.4581
Epoch: 78/200
Train loss: 2.3558
Epoch: 79/200
Train loss: 2.2183
Epoch: 80/200
Train loss: 2.3900
Epoch: 81/200
Train loss: 2.3665
Epoch: 82/200
Train loss: 2.3830
Epoch: 83/200
Train loss: 2.3418
Epoch: 84/200
Train loss: 2.3194
Epoch: 85/200
Train loss: 2.4572
Epoch: 86/200
Train loss: 2.3838
Epoch: 87/200
Train loss: 2.2597
Epoch: 88/200
Train loss: 2.4405
Epoch: 89/200
Train loss: 2.4679
Epoch: 90/200
Train loss: 2.3190
Epoch: 91/200
Train loss: 2.4401
Epoch: 92/200
Train loss: 2.4078
Epoch: 93/200
Train loss: 2.4451
Epoch: 94/200
Train loss: 2.3681
Epoch: 95/200
Train loss: 2.3936
Epoch: 96/200
Train loss: 2.4279
Epoch: 97/200
Train loss: 2.3982
Epoch: 98/200
Train loss: 2.3625
Epoch: 99/200
Train loss: 2.2907
Epoch: 100/200
Train loss: 2.3463
Epoch: 101/200
Train loss: 2.4100
Epoch: 102/200
Train loss: 2.1842
0.8088769954723128
Model improve: 0.000000 -> 0.808877
Epoch: 103/200
Train loss: 2.2266
0.8084358140463392
Epoch: 104/200
Train loss: 2.2400
0.8082506968704432
Epoch: 105/200
Train loss: 2.2997
0.8088228386566697
Epoch: 106/200
Train loss: 2.3123
0.8076943920937141
Epoch: 107/200
Train loss: 2.3738
0.8073161035350953
Epoch: 108/200
Train loss: 2.3449
0.8069092438591183
Epoch: 109/200
Train loss: 2.3276
0.80790984666491
Epoch: 110/200
Train loss: 2.3803
0.8070496337392931
Epoch: 111/200
Train loss: 2.1754
0.8084190975730363
Epoch: 112/200
Train loss: 2.2933
0.8081744873534416
Epoch: 113/200
Train loss: 2.2825
0.8091799390109662
Model improve: 0.808877 -> 0.809180
Epoch: 114/200
Train loss: 2.3832
0.8070498035817832
Epoch: 115/200
Train loss: 2.2789
0.8092775852933723
Model improve: 0.809180 -> 0.809278
Epoch: 116/200
Train loss: 2.3141
0.8087979859978088
Epoch: 117/200
Train loss: 2.2643
0.8095114033930221
Model improve: 0.809278 -> 0.809511
Epoch: 118/200
Train loss: 2.2424
0.8076883998289462
Epoch: 119/200
Train loss: 2.3438
0.8077146132017189
Epoch: 120/200
Train loss: 2.2315
0.8097655831607703
Model improve: 0.809511 -> 0.809766
Epoch: 121/200
Train loss: 2.2429
0.809216836552097
Epoch: 122/200
Train loss: 2.3499
0.8093704428839724
Epoch: 123/200
Train loss: 2.4347
0.8071235315616567
Epoch: 124/200
Train loss: 2.4406
0.8094059740254804
Epoch: 125/200
Train loss: 2.3737
0.8096221071588598
Epoch: 126/200
Train loss: 2.2195
0.8093697377558926
Epoch: 127/200
Train loss: 2.2941
0.8090165271206744
Epoch: 128/200
Train loss: 2.1811
0.8106806313599475
Model improve: 0.809766 -> 0.810681
Epoch: 129/200
Train loss: 2.2078
0.8109467887007956
Model improve: 0.810681 -> 0.810947
Epoch: 130/200
Train loss: 2.1351
0.8087321343819985
Epoch: 131/200
Date :05/18/2023, 05:59:25
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b0
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.25
drop_path_rate: 0.4
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 41.8482
Epoch: 2/200
Date :05/18/2023, 19:21:54
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b2
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.3
drop_path_rate: 0.45
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Date :05/18/2023, 19:22:39
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b0
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.3
drop_path_rate: 0.45
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 42.2660
Epoch: 2/200
Train loss: 5.7563
Epoch: 3/200
Train loss: 5.3024
Epoch: 4/200
Train loss: 4.9424
Epoch: 5/200
Train loss: 4.7102
Epoch: 6/200
Train loss: 4.5200
Epoch: 7/200
Train loss: 4.2644
Epoch: 8/200
Train loss: 4.1748
Epoch: 9/200
Train loss: 4.0485
Epoch: 10/200
Train loss: 3.9487
Epoch: 11/200
Train loss: 3.8044
Epoch: 12/200
Train loss: 3.7076
Epoch: 13/200
Train loss: 3.6458
Epoch: 14/200
Train loss: 3.5409
Epoch: 15/200
Train loss: 3.6099
Epoch: 16/200
Train loss: 3.3502
Epoch: 17/200
Train loss: 3.3862
Epoch: 18/200
Train loss: 3.2604
Epoch: 19/200
Train loss: 3.2956
Epoch: 20/200
Train loss: 3.3809
Epoch: 21/200
Train loss: 3.2187
Epoch: 22/200
Train loss: 3.2491
Epoch: 23/200
Train loss: 3.2153
Epoch: 24/200
Train loss: 3.0883
Epoch: 25/200
Train loss: 3.1332
Epoch: 26/200
Train loss: 3.0423
Epoch: 27/200
Train loss: 3.1596
Epoch: 28/200
Train loss: 3.0030
Epoch: 29/200
Train loss: 2.9430
Epoch: 30/200
Train loss: 2.9721
Epoch: 31/200
Train loss: 2.9039
Epoch: 32/200
Train loss: 2.9005
Epoch: 33/200
Train loss: 2.9863
Epoch: 34/200
Train loss: 2.9509
Epoch: 35/200
Train loss: 2.8092
Epoch: 36/200
Train loss: 2.8559
Epoch: 37/200
Train loss: 2.7745
Epoch: 38/200
Train loss: 2.8221
Epoch: 39/200
Train loss: 2.8304
Epoch: 40/200
Train loss: 2.6643
Epoch: 41/200
Train loss: 2.9446
Epoch: 42/200
Train loss: 2.7518
Epoch: 43/200
Train loss: 2.7965
Epoch: 44/200
Train loss: 2.8088
Epoch: 45/200
Train loss: 2.8605
Epoch: 46/200
Train loss: 2.7677
Epoch: 47/200
Train loss: 2.6108
Epoch: 48/200
Train loss: 2.5898
Epoch: 49/200
Train loss: 2.6513
Epoch: 50/200
Train loss: 2.6648
Epoch: 51/200
Train loss: 2.5834
Epoch: 52/200
Train loss: 2.5940
Epoch: 53/200
Train loss: 2.6603
Epoch: 54/200
Train loss: 2.6144
Epoch: 55/200
Train loss: 2.7006
Epoch: 56/200
Train loss: 2.6290
Epoch: 57/200
Train loss: 2.5583
Epoch: 58/200
Train loss: 2.4925
Epoch: 59/200
Train loss: 2.5131
Epoch: 60/200
Train loss: 2.5828
Epoch: 61/200
Train loss: 2.5594
Epoch: 62/200
Train loss: 2.4641
Epoch: 63/200
Train loss: 2.5287
Epoch: 64/200
Train loss: 2.6423
Epoch: 65/200
Train loss: 2.5581
Epoch: 66/200
Train loss: 2.5066
Epoch: 67/200
Train loss: 2.5466
Epoch: 68/200
Train loss: 2.5393
Epoch: 69/200
Train loss: 2.4832
Epoch: 70/200
Train loss: 2.5369
Epoch: 71/200
Train loss: 2.4792
Epoch: 72/200
Train loss: 2.3892
Epoch: 73/200
Train loss: 2.4002
Epoch: 74/200
Train loss: 2.4813
Epoch: 75/200
Train loss: 2.7103
Epoch: 76/200
Train loss: 2.5189
Epoch: 77/200
Train loss: 2.4662
Epoch: 78/200
Train loss: 2.5372
Epoch: 79/200
Train loss: 2.3692
Epoch: 80/200
Train loss: 2.3439
Epoch: 81/200
Train loss: 2.4074
Epoch: 82/200
Train loss: 2.3573
Epoch: 83/200
Train loss: 2.3978
Epoch: 84/200
Train loss: 2.4206
Epoch: 85/200
Train loss: 2.4351
Epoch: 86/200
Train loss: 2.5259
Epoch: 87/200
Train loss: 2.4466
Epoch: 88/200
Train loss: 2.3618
Epoch: 89/200
Train loss: 2.3051
Epoch: 90/200
Train loss: 2.4331
Epoch: 91/200
Train loss: 2.3291
Epoch: 92/200
Train loss: 2.5254
Epoch: 93/200
Train loss: 2.3952
Epoch: 94/200
Train loss: 2.4080
Epoch: 95/200
Train loss: 2.3082
Epoch: 96/200
Train loss: 2.3788
Epoch: 97/200
Train loss: 2.4507
Epoch: 98/200
Train loss: 2.4097
Epoch: 99/200
Train loss: 2.3558
Epoch: 100/200
Train loss: 2.3651
Epoch: 101/200
Train loss: 2.2085
Epoch: 102/200
Train loss: 2.3729
0.8077554352990495
Model improve: 0.000000 -> 0.807755
Epoch: 103/200
Train loss: 2.4320
0.8091326847065442
Model improve: 0.807755 -> 0.809133
Epoch: 104/200
Train loss: 2.3090
0.8109964726779326
Model improve: 0.809133 -> 0.810996
Epoch: 105/200
Train loss: 2.3898
0.8093400133866254
Epoch: 106/200
Train loss: 2.4279
0.8094350315201434
Epoch: 107/200
Train loss: 2.2753
0.8108833756983518
Epoch: 108/200
Train loss: 2.1605
0.8113485658316308
Model improve: 0.810996 -> 0.811349
Epoch: 109/200
Train loss: 2.2477
0.8108791843499622
Epoch: 110/200
Train loss: 2.3680
0.8107072054952675
Epoch: 111/200
Train loss: 2.3644
0.8100132121717164
Epoch: 112/200
Train loss: 2.2661
0.8093568183569734
Epoch: 113/200
Train loss: 2.2705
0.811000548874592
Epoch: 114/200
Train loss: 2.2986
0.8112215318682203
Epoch: 115/200
Train loss: 2.3853
0.8101640112129361
Epoch: 116/200
Train loss: 2.2399
0.8116835193477561
Model improve: 0.811349 -> 0.811684
Epoch: 117/200
Train loss: 2.2331
0.8116940179838248
Model improve: 0.811684 -> 0.811694
Epoch: 118/200
Train loss: 2.4520
0.80921673266886
Epoch: 119/200
Train loss: 2.3810
0.8106402357768174
Epoch: 120/200
Train loss: 2.3010
0.8093950967574298
Epoch: 121/200
Train loss: 2.2555
0.8110978282981094
Epoch: 122/200
Train loss: 2.2892
0.8104829559402992
Epoch: 123/200
Train loss: 2.3243
0.8107655232289539
Epoch: 124/200
Train loss: 2.3493
0.8073380608152636
Epoch: 125/200
Train loss: 2.2252
0.8115714832089238
Epoch: 126/200
Train loss: 2.2583
0.8098345166998181
Epoch: 127/200
Train loss: 2.1703
0.8114902718854585
Epoch: 128/200
Train loss: 2.2704
0.8104045206768654
Epoch: 129/200
Train loss: 2.4858
0.809610404974816
Epoch: 130/200
Train loss: 2.2218
0.8105999999874643
Epoch: 131/200
Train loss: 2.1519
0.8097022392037222
Epoch: 132/200
Train loss: 2.2226
0.8096853322060806
Epoch: 133/200
Train loss: 2.3283
0.8118419243940449
Model improve: 0.811694 -> 0.811842
Epoch: 134/200
Train loss: 2.3407
0.8109467397509034
Epoch: 135/200
Train loss: 2.3345
0.8083393122957808
Epoch: 136/200
Train loss: 2.3170
0.8107134691520441
Epoch: 137/200
Train loss: 2.2359
0.8112799229925385
Epoch: 138/200
Train loss: 2.2124
0.8116473454849177
Epoch: 139/200
Train loss: 2.2153
0.8114420299679327
Epoch: 140/200
Train loss: 2.2071
0.8116652434859233
Epoch: 141/200
Train loss: 2.2638
0.812038003881759
Model improve: 0.811842 -> 0.812038
Epoch: 142/200
Train loss: 2.3295
0.8114267180371226
Epoch: 143/200
Train loss: 2.2673
0.8111659355918752
Epoch: 144/200
Train loss: 2.3733
0.8119586251623879
Epoch: 145/200
Train loss: 2.2279
0.811067393379728
Epoch: 146/200
Train loss: 2.2163
0.8128310503793368
Model improve: 0.812038 -> 0.812831
Epoch: 147/200
Train loss: 2.3141
0.8107609070293536
Epoch: 148/200
Train loss: 2.2222
0.8125552198518342
Epoch: 149/200
Train loss: 2.2788
0.8103817478437615
Epoch: 150/200
Train loss: 2.2525
0.8112755553594733
Epoch: 151/200
Train loss: 2.3229
0.8120729613694336
Epoch: 152/200
Train loss: 2.3507
0.8117485114119242
Epoch: 153/200
Train loss: 2.2114
0.8123427050057614
Epoch: 154/200
Train loss: 2.2572
0.8122953605341305
Epoch: 155/200
Train loss: 2.2586
0.8127700714531049
Epoch: 156/200
Train loss: 2.2466
0.8103329727330567
Epoch: 157/200
Train loss: 2.2643
0.8110255048014456
Epoch: 158/200
Train loss: 2.3403
0.8107212987295465
Epoch: 159/200
Train loss: 2.2362
0.8099904132604033
Epoch: 160/200
Train loss: 2.1525
0.8124251801793609
Epoch: 161/200
Train loss: 2.2847
0.8130130659010552
Model improve: 0.812831 -> 0.813013
Epoch: 162/200
Train loss: 2.3638
0.8134850744662714
Model improve: 0.813013 -> 0.813485
Epoch: 163/200
Train loss: 2.1447
0.8131366140572691
Epoch: 164/200
Train loss: 2.4237
0.8102497654240274
Epoch: 165/200
Train loss: 2.2335
0.8123527343096055
Epoch: 166/200
Train loss: 2.2402
0.8108821376755901
Epoch: 167/200
Train loss: 2.2605
0.8119415262433272
Epoch: 168/200
Train loss: 2.1921
0.8122636101829712
Epoch: 169/200
Train loss: 2.3088
0.8134692882576022
Epoch: 170/200
Train loss: 2.1792
0.8114273612731088
Epoch: 171/200
Train loss: 2.2906
0.8135414955896616
Model improve: 0.813485 -> 0.813541
Epoch: 172/200
Train loss: 2.2845
0.8117258562305344
Epoch: 173/200
Train loss: 2.1634
0.8137635870431316
Model improve: 0.813541 -> 0.813764
Epoch: 174/200
Train loss: 2.2920
0.8103497012148195
Epoch: 175/200
Train loss: 2.3118
0.8110290965911137
Epoch: 176/200
Train loss: 2.2773
0.8132394146690557
Epoch: 177/200
Train loss: 2.1709
0.8129021011015439
Epoch: 178/200
Train loss: 2.1644
0.8131621070619843
Epoch: 179/200
Train loss: 2.3659
0.8127363720431047
Epoch: 180/200
Train loss: 2.2395
0.8115997374668898
Epoch: 181/200
Train loss: 2.2016
0.8125763652852667
Epoch: 182/200
Train loss: 2.3726
0.81002098402236
Epoch: 183/200
Train loss: 2.3618
0.8117337030467033
Epoch: 184/200
Train loss: 2.3137
0.8121020265720112
Epoch: 185/200
Train loss: 2.1183
0.8120371081071633
Epoch: 186/200
Train loss: 2.2101
0.8124971864088634
Epoch: 187/200
Train loss: 2.1252
0.813588872289973
Epoch: 188/200
Train loss: 2.1633
0.8126872159449248
Epoch: 189/200
Train loss: 2.3285
0.8131389299761448
Epoch: 190/200
Train loss: 2.2733
0.8121898456494309
Epoch: 191/200
Train loss: 2.2185
0.8116750251463544
Epoch: 192/200
Train loss: 2.2782
0.8111839375576393
Epoch: 193/200
Train loss: 2.2139
0.8110133975229721
Epoch: 194/200
Train loss: 2.1934
0.8125872257390204
Epoch: 195/200
Train loss: 2.1906
0.8124136907962064
Epoch: 196/200
Train loss: 2.2229
0.8109530651544589
Epoch: 197/200
Train loss: 2.4679
0.8091364914115042
Epoch: 198/200
Train loss: 2.2906
0.8116720941444111
Epoch: 199/200
Train loss: 2.1828
0.8131100752093534
Epoch: 200/200
Train loss: 2.2364
0.8138596941908613
Model improve: 0.813764 -> 0.813860
Date :05/19/2023, 06:57:31
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b2
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.2
drop_path_rate: 0.5
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
