Date :04/24/2023, 00:52:05
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
Epoch: 1/200
Date :04/24/2023, 00:54:21
Date :04/24/2023, 00:54:21
Duration: 5
Duration: 5
Sample rate: 32000
Sample rate: 32000
nfft: 768
nfft: 768
fmin: 20
fmin: 20
nmels: 128
nmels: 128
fmax: 16000
fmax: 16000
trainbs: 32
trainbs: 32
validbs: 128
validbs: 128
epochwarmup: 0
epochwarmup: 0
totalepoch: 200
totalepoch: 200
learningrate: 0.0003
learningrate: 0.0003
weightdecay: 0.01
weightdecay: 0.01
thrupsample: 10
thrupsample: 10
model_name: tf_efficientnet_b0_ns
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
mix_up: 0.8
hop_length: 256
hop_length: 256
train_with_mixup: True
train_with_mixup: True
num_channels: 1
num_channels: 1
use_spec_augmenter: False
use_spec_augmenter: False
use_drop_path: True
use_drop_path: True
Fold: 0
Fold: 0
Date :04/24/2023, 00:54:41
Date :04/24/2023, 00:54:41
Date :04/24/2023, 00:54:41
Duration: 5
Duration: 5
Duration: 5
Sample rate: 32000
Sample rate: 32000
Sample rate: 32000
nfft: 768
nfft: 768
nfft: 768
fmin: 20
fmin: 20
fmin: 20
nmels: 128
nmels: 128
nmels: 128
fmax: 16000
fmax: 16000
fmax: 16000
trainbs: 32
trainbs: 32
trainbs: 32
validbs: 128
validbs: 128
validbs: 128
epochwarmup: 0
epochwarmup: 0
epochwarmup: 0
totalepoch: 200
totalepoch: 200
totalepoch: 200
learningrate: 0.0003
learningrate: 0.0003
learningrate: 0.0003
weightdecay: 0.01
weightdecay: 0.01
weightdecay: 0.01
thrupsample: 10
thrupsample: 10
thrupsample: 10
model_name: tf_efficientnet_b0_ns
model_name: tf_efficientnet_b0_ns
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
mix_up: 0.8
mix_up: 0.8
hop_length: 256
hop_length: 256
hop_length: 256
train_with_mixup: True
train_with_mixup: True
train_with_mixup: True
num_channels: 1
num_channels: 1
num_channels: 1
use_spec_augmenter: False
use_spec_augmenter: False
use_spec_augmenter: False
use_drop_path: True
use_drop_path: True
use_drop_path: True
Fold: 0
Fold: 0
Fold: 0
Date :04/24/2023, 00:57:13
Date :04/24/2023, 00:57:13
Date :04/24/2023, 00:57:13
Date :04/24/2023, 00:57:13
Duration: 5
Duration: 5
Duration: 5
Duration: 5
Sample rate: 32000
Sample rate: 32000
Sample rate: 32000
Sample rate: 32000
nfft: 768
nfft: 768
nfft: 768
nfft: 768
fmin: 20
fmin: 20
fmin: 20
fmin: 20
nmels: 128
nmels: 128
nmels: 128
nmels: 128
fmax: 16000
fmax: 16000
fmax: 16000
fmax: 16000
trainbs: 32
trainbs: 32
trainbs: 32
trainbs: 32
validbs: 128
validbs: 128
validbs: 128
validbs: 128
epochwarmup: 0
epochwarmup: 0
epochwarmup: 0
epochwarmup: 0
totalepoch: 200
totalepoch: 200
totalepoch: 200
totalepoch: 200
learningrate: 0.0003
learningrate: 0.0003
learningrate: 0.0003
learningrate: 0.0003
weightdecay: 0.01
weightdecay: 0.01
weightdecay: 0.01
weightdecay: 0.01
thrupsample: 10
thrupsample: 10
thrupsample: 10
thrupsample: 10
model_name: tf_efficientnet_b0_ns
model_name: tf_efficientnet_b0_ns
model_name: tf_efficientnet_b0_ns
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
mix_up: 0.8
mix_up: 0.8
mix_up: 0.8
hop_length: 256
hop_length: 256
hop_length: 256
hop_length: 256
train_with_mixup: True
train_with_mixup: True
train_with_mixup: True
train_with_mixup: True
num_channels: 1
num_channels: 1
num_channels: 1
num_channels: 1
use_spec_augmenter: False
use_spec_augmenter: False
use_spec_augmenter: False
use_spec_augmenter: False
use_drop_path: True
use_drop_path: True
use_drop_path: True
use_drop_path: True
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Date :04/24/2023, 00:57:47
Date :04/24/2023, 00:57:47
Date :04/24/2023, 00:57:47
Date :04/24/2023, 00:57:47
Date :04/24/2023, 00:57:47
Duration: 5
Duration: 5
Duration: 5
Duration: 5
Duration: 5
Sample rate: 32000
Sample rate: 32000
Sample rate: 32000
Sample rate: 32000
Sample rate: 32000
nfft: 768
nfft: 768
nfft: 768
nfft: 768
nfft: 768
fmin: 20
fmin: 20
fmin: 20
fmin: 20
fmin: 20
nmels: 128
nmels: 128
nmels: 128
nmels: 128
nmels: 128
fmax: 16000
fmax: 16000
fmax: 16000
fmax: 16000
fmax: 16000
trainbs: 32
trainbs: 32
trainbs: 32
trainbs: 32
trainbs: 32
validbs: 128
validbs: 128
validbs: 128
validbs: 128
validbs: 128
epochwarmup: 0
epochwarmup: 0
epochwarmup: 0
epochwarmup: 0
epochwarmup: 0
totalepoch: 200
totalepoch: 200
totalepoch: 200
totalepoch: 200
totalepoch: 200
learningrate: 0.0003
learningrate: 0.0003
learningrate: 0.0003
learningrate: 0.0003
learningrate: 0.0003
weightdecay: 0.01
weightdecay: 0.01
weightdecay: 0.01
weightdecay: 0.01
weightdecay: 0.01
thrupsample: 10
thrupsample: 10
thrupsample: 10
thrupsample: 10
thrupsample: 10
model_name: tf_efficientnet_b0_ns
model_name: tf_efficientnet_b0_ns
model_name: tf_efficientnet_b0_ns
model_name: tf_efficientnet_b0_ns
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
mix_up: 0.8
mix_up: 0.8
mix_up: 0.8
mix_up: 0.8
hop_length: 256
hop_length: 256
hop_length: 256
hop_length: 256
hop_length: 256
train_with_mixup: True
train_with_mixup: True
train_with_mixup: True
train_with_mixup: True
train_with_mixup: True
num_channels: 1
num_channels: 1
num_channels: 1
num_channels: 1
num_channels: 1
use_spec_augmenter: False
use_spec_augmenter: False
use_spec_augmenter: False
use_spec_augmenter: False
use_spec_augmenter: False
use_drop_path: True
use_drop_path: True
use_drop_path: True
use_drop_path: True
use_drop_path: True
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Date :04/24/2023, 00:57:54
Date :04/24/2023, 00:57:54
Date :04/24/2023, 00:57:54
Date :04/24/2023, 00:57:54
Date :04/24/2023, 00:57:54
Date :04/24/2023, 00:57:54
Duration: 5
Duration: 5
Duration: 5
Duration: 5
Duration: 5
Duration: 5
Sample rate: 32000
Sample rate: 32000
Sample rate: 32000
Sample rate: 32000
Sample rate: 32000
Sample rate: 32000
nfft: 768
nfft: 768
nfft: 768
nfft: 768
nfft: 768
nfft: 768
fmin: 20
fmin: 20
fmin: 20
fmin: 20
fmin: 20
fmin: 20
nmels: 128
nmels: 128
nmels: 128
nmels: 128
nmels: 128
nmels: 128
fmax: 16000
fmax: 16000
fmax: 16000
fmax: 16000
fmax: 16000
fmax: 16000
trainbs: 32
trainbs: 32
trainbs: 32
trainbs: 32
trainbs: 32
trainbs: 32
validbs: 128
validbs: 128
validbs: 128
validbs: 128
validbs: 128
validbs: 128
epochwarmup: 0
epochwarmup: 0
epochwarmup: 0
epochwarmup: 0
epochwarmup: 0
epochwarmup: 0
totalepoch: 200
totalepoch: 200
totalepoch: 200
totalepoch: 200
totalepoch: 200
totalepoch: 200
learningrate: 0.0003
learningrate: 0.0003
learningrate: 0.0003
learningrate: 0.0003
learningrate: 0.0003
learningrate: 0.0003
weightdecay: 0.01
weightdecay: 0.01
weightdecay: 0.01
weightdecay: 0.01
weightdecay: 0.01
weightdecay: 0.01
thrupsample: 10
thrupsample: 10
thrupsample: 10
thrupsample: 10
thrupsample: 10
thrupsample: 10
model_name: tf_efficientnet_b0_ns
model_name: tf_efficientnet_b0_ns
model_name: tf_efficientnet_b0_ns
model_name: tf_efficientnet_b0_ns
model_name: tf_efficientnet_b0_ns
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
mix_up: 0.8
mix_up: 0.8
mix_up: 0.8
mix_up: 0.8
mix_up: 0.8
hop_length: 256
hop_length: 256
hop_length: 256
hop_length: 256
hop_length: 256
hop_length: 256
train_with_mixup: True
train_with_mixup: True
train_with_mixup: True
train_with_mixup: True
train_with_mixup: True
train_with_mixup: True
num_channels: 1
num_channels: 1
num_channels: 1
num_channels: 1
num_channels: 1
num_channels: 1
use_spec_augmenter: False
use_spec_augmenter: False
use_spec_augmenter: False
use_spec_augmenter: False
use_spec_augmenter: False
use_spec_augmenter: False
use_drop_path: True
use_drop_path: True
use_drop_path: True
use_drop_path: True
use_drop_path: True
use_drop_path: True
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Epoch: 1/200
Epoch: 1/200
Epoch: 1/200
Epoch: 1/200
Epoch: 1/200
Epoch: 1/200
Date :04/24/2023, 00:59:58
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 1
Fold: 1
Fold: 1
Fold: 1
Fold: 1
Fold: 1
Fold: 1
Fold: 1
Fold: 1
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 1
Fold: 1
Fold: 1
Fold: 1
Fold: 1
Fold: 1
Fold: 1
Fold: 1
Fold: 1
Fold: 1
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Fold: 0
Date :04/24/2023, 18:14:09
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
12999
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
With concurrent ThreadPoolExecutor, time cost reduced to 550.77574467659 for validation audios
0.33931288339359483
Model improve: 0.0000 -> 0.3393
Epoch: 2/200
Date :04/24/2023, 18:29:41
Duration: 5
Sample rate: 32000
nfft: 512
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
12999
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
With concurrent ThreadPoolExecutor, time cost reduced to 519.5118589401245 for validation audios
0.35235021726815074
Model improve: 0.0000 -> 0.3524
Epoch: 2/200
With concurrent ThreadPoolExecutor, time cost reduced to 399.1316578388214 for validation audios
0.5140775659394846
Model improve: 0.3524 -> 0.5141
Epoch: 3/200
Date :04/24/2023, 18:51:18
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.001
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
12999
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
With concurrent ThreadPoolExecutor, time cost reduced to 482.01633191108704 for validation audios
0.5203545439109448
Model improve: 0.0000 -> 0.5204
Epoch: 2/200
With concurrent ThreadPoolExecutor, time cost reduced to 510.49826312065125 for validation audios
0.6197075255355907
Model improve: 0.5204 -> 0.6197
Epoch: 3/200
With concurrent ThreadPoolExecutor, time cost reduced to 448.8842558860779 for validation audios
0.6529387221550882
Model improve: 0.6197 -> 0.6529
Epoch: 4/200
With concurrent ThreadPoolExecutor, time cost reduced to 442.3721830844879 for validation audios
0.667983735536266
Model improve: 0.6529 -> 0.6680
Epoch: 5/200
With concurrent ThreadPoolExecutor, time cost reduced to 406.43504667282104 for validation audios
0.6889860353337082
Model improve: 0.6680 -> 0.6890
Epoch: 6/200
With concurrent ThreadPoolExecutor, time cost reduced to 520.114565372467 for validation audios
0.7024881402000979
Model improve: 0.6890 -> 0.7025
Epoch: 7/200
With concurrent ThreadPoolExecutor, time cost reduced to 450.21506524086 for validation audios
0.7020921974672499
Epoch: 8/200
With concurrent ThreadPoolExecutor, time cost reduced to 414.13560485839844 for validation audios
0.7094419992023858
Model improve: 0.7025 -> 0.7094
Epoch: 9/200
With concurrent ThreadPoolExecutor, time cost reduced to 407.5659921169281 for validation audios
0.7100192857645684
Model improve: 0.7094 -> 0.7100
Epoch: 10/200
Date :04/24/2023, 20:33:04
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
12999
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
With concurrent ThreadPoolExecutor, time cost reduced to 484.45902943611145 for validation audios
0.43032449508443543
Model improve: 0.0000 -> 0.4303
Epoch: 2/200
Date :04/24/2023, 20:49:50
Duration: 5
Sample rate: 32000
nfft: 4096
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
12999
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
With concurrent ThreadPoolExecutor, time cost reduced to 467.7631869316101 for validation audios
0.3631878883493674
Model improve: 0.0000 -> 0.3632
Epoch: 2/200
With concurrent ThreadPoolExecutor, time cost reduced to 432.6145598888397 for validation audios
0.5154012274145732
Model improve: 0.3632 -> 0.5154
Epoch: 3/200
With concurrent ThreadPoolExecutor, time cost reduced to 433.0861382484436 for validation audios
0.5798128257994517
Model improve: 0.5154 -> 0.5798
Epoch: 4/200
With concurrent ThreadPoolExecutor, time cost reduced to 439.43051385879517 for validation audios
0.614378832762198
Model improve: 0.5798 -> 0.6144
Epoch: 5/200
With concurrent ThreadPoolExecutor, time cost reduced to 427.76302337646484 for validation audios
0.6363846530514508
Model improve: 0.6144 -> 0.6364
Epoch: 6/200
With concurrent ThreadPoolExecutor, time cost reduced to 452.6878561973572 for validation audios
0.6519669730960791
Model improve: 0.6364 -> 0.6520
Epoch: 7/200
With concurrent ThreadPoolExecutor, time cost reduced to 494.18364930152893 for validation audios
0.6674818545801251
Model improve: 0.6520 -> 0.6675
Epoch: 8/200
With concurrent ThreadPoolExecutor, time cost reduced to 435.82892751693726 for validation audios
0.6773984655249573
Model improve: 0.6675 -> 0.6774
Epoch: 9/200
With concurrent ThreadPoolExecutor, time cost reduced to 454.6678478717804 for validation audios
0.6854991486647877
Model improve: 0.6774 -> 0.6855
Epoch: 10/200
With concurrent ThreadPoolExecutor, time cost reduced to 428.1309971809387 for validation audios
0.6931226201466884
Model improve: 0.6855 -> 0.6931
Epoch: 11/200
With concurrent ThreadPoolExecutor, time cost reduced to 523.4106464385986 for validation audios
0.6975875930777915
Model improve: 0.6931 -> 0.6976
Epoch: 12/200
Date :04/24/2023, 23:00:25
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
With concurrent ThreadPoolExecutor, time cost reduced to 296.26364183425903 for validation audios
0.4455733328849541
Model improve: 0.0000 -> 0.4456
Epoch: 2/200
With concurrent ThreadPoolExecutor, time cost reduced to 284.536838054657 for validation audios
0.5770728988091401
Model improve: 0.4456 -> 0.5771
Epoch: 3/200
With concurrent ThreadPoolExecutor, time cost reduced to 279.9293396472931 for validation audios
0.6294838185216723
Model improve: 0.5771 -> 0.6295
Epoch: 4/200
With concurrent ThreadPoolExecutor, time cost reduced to 276.6606795787811 for validation audios
0.6603222620171814
Model improve: 0.6295 -> 0.6603
Epoch: 5/200
With concurrent ThreadPoolExecutor, time cost reduced to 281.63286328315735 for validation audios
0.686331019871064
Model improve: 0.6603 -> 0.6863
Epoch: 6/200
With concurrent ThreadPoolExecutor, time cost reduced to 285.41738390922546 for validation audios
0.7034778431587118
Model improve: 0.6863 -> 0.7035
Epoch: 7/200
With concurrent ThreadPoolExecutor, time cost reduced to 281.916321516037 for validation audios
0.7195414854395271
Model improve: 0.7035 -> 0.7195
Epoch: 8/200
With concurrent ThreadPoolExecutor, time cost reduced to 305.0903069972992 for validation audios
0.7272527811985275
Model improve: 0.7195 -> 0.7273
Epoch: 9/200
With concurrent ThreadPoolExecutor, time cost reduced to 319.9373791217804 for validation audios
0.7337570679175167
Model improve: 0.7273 -> 0.7338
Epoch: 10/200
With concurrent ThreadPoolExecutor, time cost reduced to 291.4755983352661 for validation audios
0.7379150565763978
Model improve: 0.7338 -> 0.7379
Epoch: 11/200
With concurrent ThreadPoolExecutor, time cost reduced to 300.3956742286682 for validation audios
0.7421924146312514
Model improve: 0.7379 -> 0.7422
Epoch: 12/200
With concurrent ThreadPoolExecutor, time cost reduced to 296.2365355491638 for validation audios
0.7513237506964049
Model improve: 0.7422 -> 0.7513
Epoch: 13/200
With concurrent ThreadPoolExecutor, time cost reduced to 288.5391335487366 for validation audios
0.7551829496276102
Model improve: 0.7513 -> 0.7552
Epoch: 14/200
With concurrent ThreadPoolExecutor, time cost reduced to 293.8567228317261 for validation audios
0.7573239011918513
Model improve: 0.7552 -> 0.7573
Epoch: 15/200
With concurrent ThreadPoolExecutor, time cost reduced to 295.77424597740173 for validation audios
0.7604327805009877
Model improve: 0.7573 -> 0.7604
Epoch: 16/200
With concurrent ThreadPoolExecutor, time cost reduced to 361.64636850357056 for validation audios
0.7638539191190111
Model improve: 0.7604 -> 0.7639
Epoch: 17/200
With concurrent ThreadPoolExecutor, time cost reduced to 303.0250768661499 for validation audios
0.768778603879395
Model improve: 0.7639 -> 0.7688
Epoch: 18/200
With concurrent ThreadPoolExecutor, time cost reduced to 294.736820936203 for validation audios
0.7695889718686387
Model improve: 0.7688 -> 0.7696
Epoch: 19/200
With concurrent ThreadPoolExecutor, time cost reduced to 283.48367261886597 for validation audios
0.7691434322916672
Epoch: 20/200
With concurrent ThreadPoolExecutor, time cost reduced to 308.8134503364563 for validation audios
0.7718674620987586
Model improve: 0.7696 -> 0.7719
Epoch: 21/200
With concurrent ThreadPoolExecutor, time cost reduced to 278.36736583709717 for validation audios
0.7744078891580929
Model improve: 0.7719 -> 0.7744
Epoch: 22/200
With concurrent ThreadPoolExecutor, time cost reduced to 284.38267731666565 for validation audios
0.7741185983557383
Epoch: 23/200
With concurrent ThreadPoolExecutor, time cost reduced to 279.0054740905762 for validation audios
0.7774336568348215
Model improve: 0.7744 -> 0.7774
Epoch: 24/200
With concurrent ThreadPoolExecutor, time cost reduced to 282.14508080482483 for validation audios
0.777062865387694
Epoch: 25/200
With concurrent ThreadPoolExecutor, time cost reduced to 283.19443368911743 for validation audios
0.7776610165979202
Model improve: 0.7774 -> 0.7777
Epoch: 26/200
With concurrent ThreadPoolExecutor, time cost reduced to 387.5428566932678 for validation audios
0.7807033267241398
Model improve: 0.7777 -> 0.7807
Epoch: 27/200
With concurrent ThreadPoolExecutor, time cost reduced to 348.7094781398773 for validation audios
0.7770915254352592
Epoch: 28/200
With concurrent ThreadPoolExecutor, time cost reduced to 284.89917755126953 for validation audios
0.7769438396753184
Epoch: 29/200
With concurrent ThreadPoolExecutor, time cost reduced to 341.3889191150665 for validation audios
0.781853198217843
Model improve: 0.7807 -> 0.7819
Epoch: 30/200
With concurrent ThreadPoolExecutor, time cost reduced to 353.95341753959656 for validation audios
0.7826498973561545
Model improve: 0.7819 -> 0.7826
Epoch: 31/200
With concurrent ThreadPoolExecutor, time cost reduced to 346.91075253486633 for validation audios
0.7822000361589683
Epoch: 32/200
With concurrent ThreadPoolExecutor, time cost reduced to 311.53359293937683 for validation audios
0.7826288631948419
Epoch: 33/200
With concurrent ThreadPoolExecutor, time cost reduced to 277.90289783477783 for validation audios
0.7846373679940724
Model improve: 0.7826 -> 0.7846
Epoch: 34/200
With concurrent ThreadPoolExecutor, time cost reduced to 286.00986886024475 for validation audios
0.7864514888135449
Model improve: 0.7846 -> 0.7865
Epoch: 35/200
With concurrent ThreadPoolExecutor, time cost reduced to 394.10004448890686 for validation audios
0.786478479246751
Model improve: 0.7865 -> 0.7865
Epoch: 36/200
With concurrent ThreadPoolExecutor, time cost reduced to 347.8624224662781 for validation audios
0.7821794991152334
Epoch: 37/200
With concurrent ThreadPoolExecutor, time cost reduced to 308.77023220062256 for validation audios
0.784817316400113
Epoch: 38/200
With concurrent ThreadPoolExecutor, time cost reduced to 287.35012102127075 for validation audios
0.7851963932785406
Epoch: 39/200
With concurrent ThreadPoolExecutor, time cost reduced to 280.9472110271454 for validation audios
0.7864005607235315
Epoch: 40/200
With concurrent ThreadPoolExecutor, time cost reduced to 336.2623460292816 for validation audios
0.7867775494530196
Model improve: 0.7865 -> 0.7868
Epoch: 41/200
With concurrent ThreadPoolExecutor, time cost reduced to 355.15309476852417 for validation audios
0.785071938950286
Epoch: 42/200
With concurrent ThreadPoolExecutor, time cost reduced to 327.65455055236816 for validation audios
0.7874017125821247
Model improve: 0.7868 -> 0.7874
Epoch: 43/200
With concurrent ThreadPoolExecutor, time cost reduced to 284.80777382850647 for validation audios
0.7872952961333187
Epoch: 44/200
With concurrent ThreadPoolExecutor, time cost reduced to 349.3183419704437 for validation audios
0.7914399556291074
Model improve: 0.7874 -> 0.7914
Epoch: 45/200
With concurrent ThreadPoolExecutor, time cost reduced to 341.4184260368347 for validation audios
0.7901300114909199
Epoch: 46/200
With concurrent ThreadPoolExecutor, time cost reduced to 306.996887922287 for validation audios
0.7901971005755195
Epoch: 47/200
With concurrent ThreadPoolExecutor, time cost reduced to 304.57388830184937 for validation audios
0.7885983581845862
Epoch: 48/200
With concurrent ThreadPoolExecutor, time cost reduced to 316.1483597755432 for validation audios
0.792241813199027
Model improve: 0.7914 -> 0.7922
Epoch: 49/200
With concurrent ThreadPoolExecutor, time cost reduced to 319.2783763408661 for validation audios
0.7902466417429871
Epoch: 50/200
With concurrent ThreadPoolExecutor, time cost reduced to 283.3467071056366 for validation audios
0.7905690757720302
Epoch: 51/200
With concurrent ThreadPoolExecutor, time cost reduced to 319.33368396759033 for validation audios
0.7898319071078354
Epoch: 52/200
With concurrent ThreadPoolExecutor, time cost reduced to 320.07851815223694 for validation audios
0.7913526028364594
Epoch: 53/200
With concurrent ThreadPoolExecutor, time cost reduced to 284.3066563606262 for validation audios
0.7889084177481277
Epoch: 54/200
With concurrent ThreadPoolExecutor, time cost reduced to 325.5836946964264 for validation audios
0.7882789310468603
Epoch: 55/200
With concurrent ThreadPoolExecutor, time cost reduced to 286.0630738735199 for validation audios
0.7883381346127998
Epoch: 56/200
With concurrent ThreadPoolExecutor, time cost reduced to 285.3621361255646 for validation audios
0.7893672760961722
Epoch: 57/200
With concurrent ThreadPoolExecutor, time cost reduced to 311.0409996509552 for validation audios
0.7902716810617134
Epoch: 58/200
With concurrent ThreadPoolExecutor, time cost reduced to 283.5563554763794 for validation audios
0.7941987518389033
Model improve: 0.7922 -> 0.7942
Epoch: 59/200
With concurrent ThreadPoolExecutor, time cost reduced to 286.3782870769501 for validation audios
0.7889737276610634
Epoch: 60/200
With concurrent ThreadPoolExecutor, time cost reduced to 294.3738386631012 for validation audios
0.7911654792393551
Epoch: 61/200
With concurrent ThreadPoolExecutor, time cost reduced to 283.5421884059906 for validation audios
0.7891482248426104
Epoch: 62/200
With concurrent ThreadPoolExecutor, time cost reduced to 283.3553454875946 for validation audios
0.7894995295791285
Epoch: 63/200
With concurrent ThreadPoolExecutor, time cost reduced to 387.02298855781555 for validation audios
0.7896422953876034
Epoch: 64/200
With concurrent ThreadPoolExecutor, time cost reduced to 281.00497937202454 for validation audios
0.791357589503437
Epoch: 65/200
With concurrent ThreadPoolExecutor, time cost reduced to 282.59306383132935 for validation audios
0.7916584102942003
Epoch: 66/200
With concurrent ThreadPoolExecutor, time cost reduced to 278.40666913986206 for validation audios
0.7946703207012547
Model improve: 0.7942 -> 0.7947
Epoch: 67/200
With concurrent ThreadPoolExecutor, time cost reduced to 278.75069880485535 for validation audios
0.7915961167742425
Epoch: 68/200
With concurrent ThreadPoolExecutor, time cost reduced to 345.2445185184479 for validation audios
0.7931523296341919
Epoch: 69/200
With concurrent ThreadPoolExecutor, time cost reduced to 354.7273349761963 for validation audios
0.794921620477461
Model improve: 0.7947 -> 0.7949
Epoch: 70/200
With concurrent ThreadPoolExecutor, time cost reduced to 349.29401111602783 for validation audios
0.7937029286352798
Epoch: 71/200
With concurrent ThreadPoolExecutor, time cost reduced to 358.2139892578125 for validation audios
0.7932480466380678
Epoch: 72/200
With concurrent ThreadPoolExecutor, time cost reduced to 345.349974155426 for validation audios
0.7918597067590425
Epoch: 73/200
With concurrent ThreadPoolExecutor, time cost reduced to 345.2817723751068 for validation audios
0.7920640171863469
Epoch: 74/200
With concurrent ThreadPoolExecutor, time cost reduced to 282.1148147583008 for validation audios
0.7942525653603303
Epoch: 75/200
With concurrent ThreadPoolExecutor, time cost reduced to 276.66278409957886 for validation audios
0.7969279209933217
Model improve: 0.7949 -> 0.7969
Epoch: 76/200
With concurrent ThreadPoolExecutor, time cost reduced to 282.7059829235077 for validation audios
0.7960069863027872
Epoch: 77/200
With concurrent ThreadPoolExecutor, time cost reduced to 280.1570861339569 for validation audios
0.8001354300532993
Model improve: 0.7969 -> 0.8001
Epoch: 78/200
With concurrent ThreadPoolExecutor, time cost reduced to 281.1439781188965 for validation audios
0.7980008495330912
Epoch: 79/200
With concurrent ThreadPoolExecutor, time cost reduced to 276.28374791145325 for validation audios
0.7980563144990269
Epoch: 80/200
With concurrent ThreadPoolExecutor, time cost reduced to 355.3315591812134 for validation audios
0.7989032508729663
Epoch: 81/200
With concurrent ThreadPoolExecutor, time cost reduced to 284.17015075683594 for validation audios
0.7980838744127523
Epoch: 82/200
With concurrent ThreadPoolExecutor, time cost reduced to 282.14287090301514 for validation audios
0.7976449549586099
Epoch: 83/200
With concurrent ThreadPoolExecutor, time cost reduced to 283.4520902633667 for validation audios
0.7947757496240165
Epoch: 84/200
With concurrent ThreadPoolExecutor, time cost reduced to 281.5012352466583 for validation audios
0.793940591788891
Epoch: 85/200
Date :04/25/2023, 15:19:31
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13830
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Date :04/25/2023, 15:21:01
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13830
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Date :04/25/2023, 15:21:32
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13830
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Date :04/25/2023, 15:21:49
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13830
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
With concurrent ThreadPoolExecutor, time cost reduced to 310.08880400657654 for validation audios
0.23401866895714296
Model improve: 0.0000 -> 0.2340
Epoch: 2/200
With concurrent ThreadPoolExecutor, time cost reduced to 311.3755741119385 for validation audios
0.23412934201440658
Model improve: 0.2340 -> 0.2341
Epoch: 3/200
Date :04/25/2023, 15:34:50
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13830
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Date :04/25/2023, 15:38:22
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13830
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Date :04/25/2023, 04:45:50
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13830
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Date :04/25/2023, 04:46:40
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13830
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Date :04/25/2023, 04:47:52
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13830
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Date :04/25/2023, 04:50:20
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13830
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Date :04/25/2023, 04:57:33
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13830
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Date :05/01/2023, 12:07:37
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 0
model_name: tf_efficientnet_b1_ns
mix_up: 0.5
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13553
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 40.1790
Epoch: 2/200
Train loss: 5.7893
Epoch: 3/200
Train loss: 5.5455
Epoch: 4/200
Train loss: 5.2999
Epoch: 5/200
Train loss: 5.0017
Epoch: 6/200
Train loss: 4.7597
Epoch: 7/200
Train loss: 4.4930
Epoch: 8/200
Train loss: 4.3240
Epoch: 9/200
Train loss: 4.2304
Epoch: 10/200
Train loss: 4.0588
Epoch: 11/200
Train loss: 3.9622
Epoch: 12/200
Train loss: 3.8146
Epoch: 13/200
Train loss: 3.7709
Epoch: 14/200
Train loss: 3.6919
Epoch: 15/200
Train loss: 3.6764
Time needed: 165.68309879302979 for validation audios
0.6766168401733885
Model improve: 0.0000 -> 0.6766
Epoch: 16/200
Date :05/01/2023, 12:57:18
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 0
model_name: tf_efficientnet_b1_ns
mix_up: 0.5
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13553
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 39.1701
Epoch: 2/200
Train loss: 5.6632
Epoch: 3/200
Train loss: 5.2816
Epoch: 4/200
Train loss: 5.0156
Epoch: 5/200
Train loss: 4.6702
Epoch: 6/200
Train loss: 4.4135
Epoch: 7/200
Train loss: 4.1274
Epoch: 8/200
Train loss: 3.9817
Epoch: 9/200
Train loss: 3.8635
Epoch: 10/200
Train loss: 3.7101
Epoch: 11/200
Train loss: 3.6044
Epoch: 12/200
Train loss: 3.4590
Epoch: 13/200
Train loss: 3.3995
Epoch: 14/200
Train loss: 3.3125
Epoch: 15/200
Train loss: 3.3118
Time needed: 168.59381890296936 for validation audios
0.6979630052148655
Model improve: 0.0000 -> 0.6980
Epoch: 16/200
Train loss: 3.2998
Time needed: 165.2917881011963 for validation audios
0.706446857072915
Model improve: 0.6980 -> 0.7064
Epoch: 17/200
Train loss: 3.1262
Time needed: 163.9013228416443 for validation audios
0.7102039680252941
Model improve: 0.7064 -> 0.7102
Epoch: 18/200
Date :05/01/2023, 13:57:28
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 0
model_name: tf_efficientnet_b1_ns
mix_up: 0.5
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13553
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 39.3315
Epoch: 2/200
Train loss: 5.6669
Epoch: 3/200
Train loss: 5.2862
Epoch: 4/200
Train loss: 5.0138
Epoch: 5/200
Train loss: 4.6723
Epoch: 6/200
Train loss: 4.4182
Epoch: 7/200
Train loss: 4.1296
Epoch: 8/200
Train loss: 3.9693
Epoch: 9/200
Train loss: 3.8615
Epoch: 10/200
Train loss: 3.7051
Epoch: 11/200
Train loss: 3.6051
Epoch: 12/200
Train loss: 3.4547
Epoch: 13/200
Train loss: 3.4212
Epoch: 14/200
Train loss: 3.3362
Epoch: 15/200
Train loss: 3.3081
Time needed: 164.51020288467407 for validation audios
0.6943972615600693
Model improve: 0.0000 -> 0.6944
Epoch: 16/200
Date :05/01/2023, 14:46:34
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 0
model_name: tf_efficientnet_b1_ns
mix_up: 0.5
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13553
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 40.2146
Epoch: 2/200
Train loss: 5.6694
Epoch: 3/200
Train loss: 5.2925
Epoch: 4/200
Train loss: 5.0208
Epoch: 5/200
Train loss: 4.6796
Epoch: 6/200
Train loss: 4.4258
Epoch: 7/200
Train loss: 4.1349
Epoch: 8/200
Train loss: 3.9727
Epoch: 9/200
Train loss: 3.8637
Epoch: 10/200
Train loss: 3.7064
Epoch: 11/200
Train loss: 3.6070
Epoch: 12/200
Train loss: 3.4575
Epoch: 13/200
Train loss: 3.4220
Epoch: 14/200
Train loss: 3.3385
Epoch: 15/200
Train loss: 3.3096
Time needed: 164.15886783599854 for validation audios
0.6931138976069133
Model improve: 0.0000 -> 0.6931
Epoch: 16/200
Date :05/01/2023, 15:35:17
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 0
model_name: tf_efficientnet_b1_ns
mix_up: 0.5
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13553
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 40.0996
Epoch: 2/200
Train loss: 5.6588
Epoch: 3/200
Train loss: 5.2860
Epoch: 4/200
Train loss: 5.0080
Epoch: 5/200
Train loss: 4.6546
Epoch: 6/200
Train loss: 4.4135
Epoch: 7/200
Train loss: 4.1209
Epoch: 8/200
Train loss: 3.9533
Epoch: 9/200
Train loss: 3.8445
Epoch: 10/200
Train loss: 3.6727
Epoch: 11/200
Train loss: 3.5795
Epoch: 12/200
Train loss: 3.4350
Epoch: 13/200
Train loss: 3.4033
Epoch: 14/200
Train loss: 3.3060
Epoch: 15/200
Train loss: 3.2984
Time needed: 171.74725723266602 for validation audios
0.6969057468726745
Model improve: 0.0000 -> 0.6969
Epoch: 16/200
Date :05/01/2023, 16:25:15
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 0
model_name: tf_efficientnet_b1_ns
mix_up: 0.5
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13553
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Date :05/01/2023, 16:27:11
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 0
model_name: tf_efficientnetv2_b1
mix_up: 0.5
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13553
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 38.0831
Epoch: 2/200
Train loss: 5.5690
Epoch: 3/200
Train loss: 5.0446
Epoch: 4/200
Train loss: 4.7259
Epoch: 5/200
Train loss: 4.2959
Epoch: 6/200
Train loss: 4.0549
Epoch: 7/200
Train loss: 3.7561
Epoch: 8/200
Train loss: 3.6241
Epoch: 9/200
Train loss: 3.5654
Epoch: 10/200
Train loss: 3.4270
Epoch: 11/200
Train loss: 3.3746
Epoch: 12/200
Train loss: 3.2370
Epoch: 13/200
Train loss: 3.2214
Epoch: 14/200
Train loss: 3.1067
Epoch: 15/200
Train loss: 3.1560
Epoch: 16/200
Train loss: 3.1288
Epoch: 17/200
Train loss: 2.9590
Epoch: 18/200
Train loss: 2.9091
Epoch: 19/200
Train loss: 2.9392
Epoch: 20/200
Train loss: 2.7445
Epoch: 21/200
Train loss: 2.8472
Epoch: 22/200
Train loss: 2.8144
Epoch: 23/200
Train loss: 2.7637
Epoch: 24/200
Train loss: 2.8429
Epoch: 25/200
Train loss: 2.7337
Epoch: 26/200
Train loss: 2.5717
Epoch: 27/200
Train loss: 2.5769
Epoch: 28/200
Train loss: 2.7066
Epoch: 29/200
Train loss: 2.5567
Epoch: 30/200
Train loss: 2.8150
Epoch: 31/200
Train loss: 2.5272
Epoch: 32/200
Train loss: 2.6442
Epoch: 33/200
Train loss: 2.7360
Epoch: 34/200
Train loss: 2.4983
Epoch: 35/200
Train loss: 2.4745
Epoch: 36/200
Train loss: 2.4609
Epoch: 37/200
Train loss: 2.5152
Epoch: 38/200
Train loss: 2.5085
Epoch: 39/200
Train loss: 2.3648
Epoch: 40/200
Train loss: 2.4408
Epoch: 41/200
Train loss: 2.4994
Epoch: 42/200
Train loss: 2.2853
Epoch: 43/200
Train loss: 2.5341
Epoch: 44/200
Train loss: 2.3932
Epoch: 45/200
Train loss: 2.3228
Epoch: 46/200
Train loss: 2.3882
Epoch: 47/200
Train loss: 2.3363
Epoch: 48/200
Train loss: 2.4544
Epoch: 49/200
Train loss: 2.3702
Epoch: 50/200
Train loss: 2.4768
Epoch: 51/200
Train loss: 2.4094
Epoch: 52/200
Train loss: 2.3394
Epoch: 53/200
Train loss: 2.4637
Epoch: 54/200
Train loss: 2.4001
Epoch: 55/200
Train loss: 2.4087
Epoch: 56/200
Train loss: 2.2745
Epoch: 57/200
Train loss: 2.2399
Epoch: 58/200
Train loss: 2.2943
Epoch: 59/200
Train loss: 2.0926
Epoch: 60/200
Train loss: 2.2992
Epoch: 61/200
Train loss: 2.2400
Epoch: 62/200
Train loss: 2.1932
Epoch: 63/200
Train loss: 2.2404
Epoch: 64/200
Train loss: 2.3181
Epoch: 65/200
Train loss: 2.3442
Epoch: 66/200
Train loss: 2.1595
Epoch: 67/200
Train loss: 2.2350
Epoch: 68/200
Train loss: 2.3186
Epoch: 69/200
Train loss: 2.1342
Epoch: 70/200
Train loss: 2.1939
Epoch: 71/200
Train loss: 2.3178
Epoch: 72/200
Train loss: 2.3572
Epoch: 73/200
Train loss: 2.2150
Epoch: 74/200
Train loss: 2.1763
Epoch: 75/200
Train loss: 2.0555
Epoch: 76/200
Train loss: 2.0746
Epoch: 77/200
Train loss: 2.1073
Epoch: 78/200
Train loss: 2.0652
Epoch: 79/200
Train loss: 2.1569
Epoch: 80/200
Train loss: 2.0284
Epoch: 81/200
Train loss: 2.2907
Epoch: 82/200
Train loss: 2.0251
Epoch: 83/200
Train loss: 2.1146
Epoch: 84/200
Train loss: 2.1577
Epoch: 85/200
Train loss: 2.0988
Epoch: 86/200
Train loss: 1.8495
Epoch: 87/200
Train loss: 2.1100
Epoch: 88/200
Train loss: 2.2203
Epoch: 89/200
Train loss: 2.1904
Epoch: 90/200
Train loss: 2.2527
Epoch: 91/200
Train loss: 2.0503
Epoch: 92/200
Train loss: 2.0730
Epoch: 93/200
Train loss: 2.0446
Epoch: 94/200
Train loss: 2.2208
Epoch: 95/200
Train loss: 2.0648
Epoch: 96/200
Train loss: 2.0935
Epoch: 97/200
Train loss: 2.1616
Epoch: 98/200
Train loss: 2.2403
Epoch: 99/200
Train loss: 1.9408
Epoch: 100/200
Train loss: 2.0718
Epoch: 101/200
Train loss: 2.0714
Epoch: 102/200
Train loss: 2.0727
Epoch: 103/200
Train loss: 2.1145
Epoch: 104/200
Train loss: 2.0298
Epoch: 105/200
Train loss: 1.9312
Epoch: 106/200
Train loss: 2.0235
Epoch: 107/200
Train loss: 2.0598
Epoch: 108/200
Train loss: 2.1176
Epoch: 109/200
Train loss: 2.1617
Epoch: 110/200
Train loss: 2.1228
Epoch: 111/200
Train loss: 1.9990
Epoch: 112/200
Train loss: 2.0571
Epoch: 113/200
Train loss: 2.2222
Epoch: 114/200
Train loss: 2.1166
Epoch: 115/200
Train loss: 2.0772
Epoch: 116/200
Train loss: 2.1001
Epoch: 117/200
Train loss: 2.1622
Epoch: 118/200
Train loss: 2.0557
Epoch: 119/200
Train loss: 2.0221
Epoch: 120/200
Train loss: 1.9861
Epoch: 121/200
Train loss: 2.2112
Epoch: 122/200
Train loss: 2.1431
Epoch: 123/200
Train loss: 2.1985
Epoch: 124/200
Train loss: 1.9865
Epoch: 125/200
Train loss: 2.0932
Epoch: 126/200
Train loss: 1.8533
Epoch: 127/200
Train loss: 2.0707
Epoch: 128/200
Train loss: 1.8103
Epoch: 129/200
Train loss: 1.9832
Epoch: 130/200
Train loss: 1.9878
Epoch: 131/200
Train loss: 2.0665
Epoch: 132/200
Train loss: 2.0977
Time needed: 165.13269901275635 for validation audios
0.7934375018056492
Model improve: 0.0000 -> 0.7934
Epoch: 133/200
Train loss: 1.9946
Time needed: 165.47519874572754 for validation audios
0.7963690552965248
Model improve: 0.7934 -> 0.7964
Epoch: 134/200
Train loss: 1.9722
Time needed: 165.8183388710022 for validation audios
0.7963486889844158
Epoch: 135/200
Train loss: 2.0586
Time needed: 166.86413097381592 for validation audios
0.7974313443288238
Model improve: 0.7964 -> 0.7974
Epoch: 136/200
Date :05/01/2023, 23:28:03
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 0
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13553
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 38.2344
Epoch: 2/200
Train loss: 5.6040
Epoch: 3/200
Train loss: 5.1215
Epoch: 4/200
Train loss: 4.7620
Epoch: 5/200
Train loss: 4.3847
Epoch: 6/200
Train loss: 4.1011
Epoch: 7/200
Train loss: 3.8256
Epoch: 8/200
Train loss: 3.7297
Epoch: 9/200
Train loss: 3.5321
Epoch: 10/200
Date :05/01/2023, 23:39:28
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 25
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 0
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13553
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Train loss: 38.2429
Epoch: 2/25
Train loss: 5.6046
Epoch: 3/25
Train loss: 5.1273
Epoch: 4/25
Train loss: 4.7778
Epoch: 5/25
Train loss: 4.4125
Epoch: 6/25
Train loss: 4.1417
Epoch: 7/25
Train loss: 3.8778
Epoch: 8/25
Train loss: 3.7970
Epoch: 9/25
Train loss: 3.6142
Epoch: 10/25
Train loss: 3.5769
Epoch: 11/25
Train loss: 3.5157
Epoch: 12/25
Train loss: 3.4373
Epoch: 13/25
Train loss: 3.3842
Epoch: 14/25
Train loss: 3.3015
Epoch: 15/25
Train loss: 3.2989
Epoch: 16/25
Train loss: 3.3163
Epoch: 17/25
Train loss: 3.1682
Epoch: 18/25
Train loss: 3.1568
Epoch: 19/25
Train loss: 3.3011
Epoch: 20/25
Train loss: 3.2133
Epoch: 21/25
Train loss: 3.1838
Epoch: 22/25
Train loss: 3.0702
Time needed: 166.55665731430054 for validation audios
0.717173247030881
Model improve: 0.0000 -> 0.7172
Epoch: 23/25
Date :05/02/2023, 00:09:43
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 16
validbs: 48
epochwarmup: 0
totalepoch: 25
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 0
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13553
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Train loss: 9.7659
Epoch: 2/25
Train loss: 4.0171
Epoch: 3/25
Train loss: 3.5600
Epoch: 4/25
Train loss: 3.3433
Epoch: 5/25
Train loss: 3.1968
Epoch: 6/25
Train loss: 3.1040
Epoch: 7/25
Train loss: 2.9103
Epoch: 8/25
Train loss: 2.8654
Epoch: 9/25
Train loss: 2.8375
Epoch: 10/25
Train loss: 2.7527
Epoch: 11/25
Train loss: 2.6591
Epoch: 12/25
Train loss: 2.7132
Epoch: 13/25
Train loss: 2.6423
Epoch: 14/25
Train loss: 2.5831
Epoch: 15/25
Train loss: 2.5627
Epoch: 16/25
Train loss: 2.5737
Epoch: 17/25
Train loss: 2.4821
Epoch: 18/25
Train loss: 2.4523
Epoch: 19/25
Train loss: 2.4931
Epoch: 20/25
Train loss: 2.4113
Epoch: 21/25
Train loss: 2.4617
Epoch: 22/25
Train loss: 2.4925
Time needed: 170.81542992591858 for validation audios
0.778498568626984
Model improve: 0.0000 -> 0.7785
Epoch: 23/25
Date :05/02/2023, 01:09:37
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 4
validbs: 12
epochwarmup: 0
totalepoch: 20
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 0
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13553
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/20
Date :05/02/2023, 01:14:34
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 8
validbs: 24
epochwarmup: 0
totalepoch: 25
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 0
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13553
Date :05/02/2023, 01:15:02
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 8
validbs: 24
epochwarmup: 0
totalepoch: 25
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 0
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13553
Lookahead (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    counter: 0
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Date :05/02/2023, 01:16:14
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 8
validbs: 24
epochwarmup: 0
totalepoch: 25
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 0
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Date :05/02/2023, 01:16:33
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 8
validbs: 24
epochwarmup: 0
totalepoch: 25
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 0
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13553
Lookahead (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lookahead_alpha: 0.5
    lookahead_k: 5
    lookahead_step: 0
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Train loss: 9.0906
Epoch: 2/25
Train loss: 4.1836
Epoch: 3/25
Train loss: 3.7272
Epoch: 4/25
Train loss: 3.4294
Epoch: 5/25
Train loss: 3.2763
Epoch: 6/25
Train loss: 3.1510
Epoch: 7/25
Train loss: 3.0529
Epoch: 8/25
Train loss: 2.9965
Epoch: 9/25
Train loss: 2.8901
Epoch: 10/25
Train loss: 2.8406
Epoch: 11/25
Train loss: 2.8245
Epoch: 12/25
Train loss: 2.7491
Epoch: 13/25
Train loss: 2.7387
Epoch: 14/25
Date :05/02/2023, 02:16:50
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 16
validbs: 48
epochwarmup: 0
totalepoch: 25
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 0
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13553
Lookahead (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lookahead_alpha: 0.5
    lookahead_k: 5
    lookahead_step: 0
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Date :05/02/2023, 02:18:59
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 64
validbs: 192
epochwarmup: 0
totalepoch: 50
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 0
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13553
Lookahead (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lookahead_alpha: 0.5
    lookahead_k: 5
    lookahead_step: 0
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/50
Date :05/02/2023, 02:20:01
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 64
validbs: 192
epochwarmup: 0
totalepoch: 50
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 0
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13553
Lookahead (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lookahead_alpha: 0.5
    lookahead_k: 5
    lookahead_step: 0
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/50
Date :05/02/2023, 02:20:31
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 64
validbs: 192
epochwarmup: 0
totalepoch: 50
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 0
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13553
Lookahead (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lookahead_alpha: 0.5
    lookahead_k: 5
    lookahead_step: 0
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/50
Date :05/02/2023, 02:20:52
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 64
validbs: 192
epochwarmup: 0
totalepoch: 50
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 0
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13553
Lookahead (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lookahead_alpha: 0.5
    lookahead_k: 5
    lookahead_step: 0
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/50
Date :05/02/2023, 02:22:23
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 64
validbs: 192
epochwarmup: 0
totalepoch: 50
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 0
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13553
Lookahead (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lookahead_alpha: 0.5
    lookahead_k: 5
    lookahead_step: 0
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/50
Date :05/02/2023, 02:23:10
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 64
validbs: 192
epochwarmup: 0
totalepoch: 50
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 0
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13553
Lookahead (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lookahead_alpha: 0.5
    lookahead_k: 5
    lookahead_step: 0
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/50
Train loss: 0.1113
Epoch: 2/50
Train loss: 0.0210
Epoch: 3/50
Train loss: 0.0193
Epoch: 4/50
Train loss: 0.0178
Epoch: 5/50
Train loss: 0.0163
Epoch: 6/50
Train loss: 0.0155
Epoch: 7/50
Train loss: 0.0149
Epoch: 8/50
Train loss: 0.0143
Epoch: 9/50
Train loss: 0.0137
Epoch: 10/50
Train loss: 0.0134
Epoch: 11/50
Train loss: 0.0130
Epoch: 12/50
Train loss: 0.0128
Epoch: 13/50
Train loss: 0.0126
Epoch: 14/50
Train loss: 0.0123
Epoch: 15/50
Train loss: 0.0120
Epoch: 16/50
Train loss: 0.0118
Epoch: 17/50
Train loss: 0.0118
Time needed: 168.30964612960815 for validation audios
0.7143822745215215
Model improve: 0.0000 -> 0.7144
Epoch: 18/50
Train loss: 0.0116
Time needed: 169.223961353302 for validation audios
0.7202983856772516
Model improve: 0.7144 -> 0.7203
Epoch: 19/50
Train loss: 0.0114
Time needed: 167.40130949020386 for validation audios
0.7225662229040761
Model improve: 0.7203 -> 0.7226
Epoch: 20/50
Train loss: 0.0113
Time needed: 167.47168397903442 for validation audios
0.72642388043165
Model improve: 0.7226 -> 0.7264
Epoch: 21/50
Train loss: 0.0112
Time needed: 167.97815704345703 for validation audios
0.7302683179216304
Model improve: 0.7264 -> 0.7303
Epoch: 22/50
Train loss: 0.0111
Time needed: 169.52659797668457 for validation audios
0.7348162948353004
Model improve: 0.7303 -> 0.7348
Epoch: 23/50
Date :05/02/2023, 03:12:03
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 100
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 0
model_name: tf_efficientnet_b1_ns
mix_up: 0.5
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13553
Lookahead (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lookahead_alpha: 0.5
    lookahead_k: 5
    lookahead_step: 0
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Date :05/02/2023, 03:12:33
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 100
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 0
model_name: tf_efficientnet_b1_ns
mix_up: 0.5
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13553
Lookahead (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lookahead_alpha: 0.5
    lookahead_k: 5
    lookahead_step: 0
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Date :05/02/2023, 03:13:00
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 100
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 0
model_name: tf_efficientnet_b1_ns
mix_up: 0.5
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13553
Lookahead (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lookahead_alpha: 0.5
    lookahead_k: 5
    lookahead_step: 0
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Train loss: 54.6541
Epoch: 2/100
Train loss: 5.9024
Epoch: 3/100
Train loss: 5.7034
Epoch: 4/100
Train loss: 5.5511
Epoch: 5/100
Train loss: 5.3254
Epoch: 6/100
Train loss: 5.1303
Epoch: 7/100
Train loss: 4.8829
Epoch: 8/100
Train loss: 4.7226
Epoch: 9/100
Train loss: 4.5900
Epoch: 10/100
Train loss: 4.4194
Epoch: 11/100
Train loss: 4.2981
Epoch: 12/100
Train loss: 4.1290
Epoch: 13/100
Train loss: 4.0501
Epoch: 14/100
Train loss: 3.9266
Epoch: 15/100
Train loss: 3.8850
Epoch: 16/100
Train loss: 3.8199
Epoch: 17/100
Train loss: 3.6424
Time needed: 166.92760515213013 for validation audios
0.613163454515254
Model improve: 0.0000 -> 0.6132
Epoch: 18/100
Date :05/02/2023, 03:37:51
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 100
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
Lookahead (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lookahead_alpha: 0.5
    lookahead_k: 5
    lookahead_step: 0
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Train loss: 53.6038
Epoch: 2/100
Date :05/02/2023, 03:39:46
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 100
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Train loss: 39.3154
Epoch: 2/100
Date :05/02/2023, 03:41:40
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 100
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
RAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    weight_decay: 0.0
)
Epoch: 1/100
Train loss: 150.0370
Epoch: 2/100
Date :05/02/2023, 03:43:09
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 100
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
Lookahead (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lookahead_alpha: 0.5
    lookahead_k: 1
    lookahead_step: 0
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Train loss: 61.3098
Epoch: 2/100
Date :05/02/2023, 03:44:36
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 100
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
Lookahead (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lookahead_alpha: 0.5
    lookahead_k: 10
    lookahead_step: 0
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Train loss: 45.8121
Epoch: 2/100
Date :05/02/2023, 03:46:05
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 100
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
Lookahead (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lookahead_alpha: 0.5
    lookahead_k: 30
    lookahead_step: 0
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Train loss: 39.4003
Epoch: 2/100
Train loss: 5.8365
Epoch: 3/100
Date :05/02/2023, 03:49:13
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 100
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
Lookahead (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lookahead_alpha: 0.5
    lookahead_k: 60
    lookahead_step: 0
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Train loss: 39.3145
Epoch: 2/100
Train loss: 5.8010
Epoch: 3/100
Train loss: 5.5647
Epoch: 4/100
Train loss: 5.3611
Epoch: 5/100
Train loss: 5.1132
Epoch: 6/100
Date :05/02/2023, 03:58:44
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 100
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
Lookahead (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lookahead_alpha: 0.5
    lookahead_k: 100
    lookahead_step: 0
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Train loss: 39.3158
Epoch: 2/100
Train loss: 5.7143
Epoch: 3/100
Train loss: 5.4872
Epoch: 4/100
Train loss: 5.2922
Epoch: 5/100
Date :05/02/2023, 04:06:10
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 100
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
Ranger (
Parameter Group 0
    N_sma_threshhold: 5
    alpha: 0.5
    betas: (0.95, 0.999)
    eps: 1e-05
    initial_lr: 0.0003
    k: 5
    lr: 0.0003
    step_counter: 0
    weight_decay: 0
)
Epoch: 1/100
Date :05/02/2023, 04:06:58
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 100
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
Ranger (
Parameter Group 0
    N_sma_threshhold: 5
    alpha: 0.5
    betas: (0.95, 0.999)
    eps: 1e-05
    initial_lr: 0.0003
    k: 30
    lr: 0.0003
    step_counter: 0
    weight_decay: 0
)
Epoch: 1/100
Date :05/02/2023, 04:07:57
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 100
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
Ranger (
Parameter Group 0
    N_sma_threshhold: 5
    alpha: 0.5
    betas: (0.95, 0.999)
    eps: 1e-05
    initial_lr: 0.001
    k: 30
    lr: 0.001
    step_counter: 0
    weight_decay: 0
)
Epoch: 1/100
Train loss: 139.2757
Epoch: 2/100
Date :05/02/2023, 04:10:24
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 100
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
Ranger (
Parameter Group 0
    N_sma_threshhold: 5
    alpha: 0.5
    betas: (0.95, 0.999)
    eps: 1e-05
    initial_lr: 0.001
    k: 200
    lr: 0.001
    step_counter: 0
    weight_decay: 0
)
Epoch: 1/100
Date :05/02/2023, 04:11:57
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 100
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
Lookahead (
Parameter Group 0
    betas: (0.9, 0.999)
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lookahead_alpha: 0.5
    lookahead_k: 100
    lookahead_step: 0
    lr: 0.0003
    weight_decay: 0.0
)
Epoch: 1/100
Date :05/02/2023, 04:13:14
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 100
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
Lookahead (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: False
    initial_lr: 0.0003
    lookahead_alpha: 0.2
    lookahead_k: 100
    lookahead_step: 0
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Train loss: 39.3084
Epoch: 2/100
Train loss: 5.7283
Epoch: 3/100
Train loss: 5.6133
Epoch: 4/100
Date :05/02/2023, 04:17:44
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 100
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
Ranger (
Parameter Group 0
    N_sma_threshhold: 5
    alpha: 0.5
    betas: (0.95, 0.999)
    eps: 1e-05
    initial_lr: 0.1
    k: 10
    lr: 0.1
    step_counter: 0
    weight_decay: 0
)
Epoch: 1/100
Train loss: 12.9685
Epoch: 2/100
Train loss: 4.6964
Epoch: 3/100
Train loss: 4.3302
Epoch: 4/100
Train loss: 4.3470
Epoch: 5/100
Train loss: 4.1359
Epoch: 6/100
Train loss: 4.0913
Epoch: 7/100
Date :05/02/2023, 04:26:35
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 100
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.01
    lr: 0.01
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Train loss: 11.7844
Epoch: 2/100
Train loss: 4.8843
Epoch: 3/100
Train loss: 4.4850
Epoch: 4/100
Train loss: 4.3799
Epoch: 5/100
Train loss: 4.1394
Epoch: 6/100
Train loss: 3.9812
Epoch: 7/100
Train loss: 3.7400
Epoch: 8/100
Train loss: 3.7608
Epoch: 9/100
Train loss: 3.6543
Epoch: 10/100
Train loss: 3.6558
Epoch: 11/100
Train loss: 3.6103
Epoch: 12/100
Train loss: 3.5117
Epoch: 13/100
Train loss: 3.4847
Epoch: 14/100
Train loss: 3.3638
Epoch: 15/100
Train loss: 3.4398
Epoch: 16/100
Train loss: 3.3573
Epoch: 17/100
Train loss: 3.2608
Time needed: 166.11794710159302 for validation audios
0.6703433967847443
Model improve: 0.0000 -> 0.6703
Epoch: 18/100
Train loss: 3.1802
Date :05/02/2023, 04:55:01
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 100
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Train loss: 39.3332
Epoch: 2/100
Train loss: 5.6936
Epoch: 3/100
Train loss: 5.3166
Epoch: 4/100
Train loss: 5.0350
Epoch: 5/100
Train loss: 4.7074
Epoch: 6/100
Train loss: 4.4437
Epoch: 7/100
Train loss: 4.1161
Epoch: 8/100
Date :05/02/2023, 05:04:25
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 39.3341
Epoch: 2/200
Train loss: 5.6938
Epoch: 3/200
Train loss: 5.3172
Epoch: 4/200
Train loss: 5.0350
Epoch: 5/200
Train loss: 4.7071
Epoch: 6/200
Train loss: 4.4423
Epoch: 7/200
Train loss: 4.1133
Epoch: 8/200
Train loss: 4.0241
Epoch: 9/200
Train loss: 3.8522
Epoch: 10/200
Train loss: 3.7425
Epoch: 11/200
Train loss: 3.6424
Epoch: 12/200
Train loss: 3.5158
Epoch: 13/200
Train loss: 3.4457
Epoch: 14/200
Train loss: 3.3074
Epoch: 15/200
Train loss: 3.3314
Epoch: 16/200
Train loss: 3.2279
Epoch: 17/200
Train loss: 3.1169
Epoch: 18/200
Train loss: 3.0192
Epoch: 19/200
Train loss: 3.2052
Epoch: 20/200
Train loss: 3.0749
Epoch: 21/200
Train loss: 2.9237
Epoch: 22/200
Train loss: 2.8158
Epoch: 23/200
Train loss: 2.8403
Epoch: 24/200
Train loss: 2.9160
Epoch: 25/200
Train loss: 2.8939
Epoch: 26/200
Train loss: 2.8734
Epoch: 27/200
Train loss: 2.8366
Epoch: 28/200
Train loss: 2.6966
Epoch: 29/200
Train loss: 2.7162
Epoch: 30/200
Train loss: 2.5602
Epoch: 31/200
Train loss: 2.7083
Epoch: 32/200
Train loss: 2.7468
Epoch: 33/200
Train loss: 2.5566
Epoch: 34/200
Train loss: 2.6350
Epoch: 35/200
Train loss: 2.6283
Epoch: 36/200
Train loss: 2.6679
Epoch: 37/200
Train loss: 2.5356
Epoch: 38/200
Train loss: 2.7799
Epoch: 39/200
Train loss: 2.4448
Epoch: 40/200
Train loss: 2.5949
Epoch: 41/200
Train loss: 2.5160
Epoch: 42/200
Train loss: 2.5199
Epoch: 43/200
Train loss: 2.5858
Epoch: 44/200
Train loss: 2.5608
Epoch: 45/200
Train loss: 2.5948
Epoch: 46/200
Train loss: 2.3291
Epoch: 47/200
Train loss: 2.5334
Epoch: 48/200
Train loss: 2.2949
Epoch: 49/200
Train loss: 2.2737
Epoch: 50/200
Train loss: 2.3467
Epoch: 51/200
Date :05/02/2023, 07:18:35
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 39.4807
Epoch: 2/200
Train loss: 5.6920
Epoch: 3/200
Train loss: 5.3142
Epoch: 4/200
Train loss: 5.0342
Epoch: 5/200
Train loss: 4.7074
Epoch: 6/200
Train loss: 4.4434
Epoch: 7/200
Train loss: 4.1144
Epoch: 8/200
Train loss: 4.0247
Epoch: 9/200
Train loss: 3.8528
Epoch: 10/200
Train loss: 3.7436
Epoch: 11/200
Train loss: 3.6442
Epoch: 12/200
Train loss: 3.5184
Epoch: 13/200
Train loss: 3.4481
Epoch: 14/200
Train loss: 3.3098
Epoch: 15/200
Train loss: 3.3334
Epoch: 16/200
Train loss: 3.2294
Epoch: 17/200
Train loss: 3.1190
Epoch: 18/200
Train loss: 3.0213
Epoch: 19/200
Train loss: 3.2072
Epoch: 20/200
Train loss: 3.0778
Epoch: 21/200
Train loss: 2.9270
Epoch: 22/200
Train loss: 2.8179
Epoch: 23/200
Train loss: 2.8423
Epoch: 24/200
Train loss: 2.9175
Epoch: 25/200
Train loss: 2.8955
Epoch: 26/200
Train loss: 2.8760
Epoch: 27/200
Train loss: 2.8380
Epoch: 28/200
Train loss: 2.6986
Epoch: 29/200
Train loss: 2.7181
Epoch: 30/200
Train loss: 2.5616
Epoch: 31/200
Train loss: 2.7097
Epoch: 32/200
Train loss: 2.7485
Epoch: 33/200
Train loss: 2.5591
Epoch: 34/200
Train loss: 2.6362
Epoch: 35/200
Train loss: 2.6297
Epoch: 36/200
Train loss: 2.6697
Epoch: 37/200
Train loss: 2.5369
Epoch: 38/200
Train loss: 2.7811
Epoch: 39/200
Train loss: 2.4460
Epoch: 40/200
Train loss: 2.5971
Epoch: 41/200
Train loss: 2.5185
Epoch: 42/200
Train loss: 2.5223
Epoch: 43/200
Train loss: 2.5868
Epoch: 44/200
Train loss: 2.5621
Epoch: 45/200
Train loss: 2.5964
Epoch: 46/200
Train loss: 2.3318
Epoch: 47/200
Train loss: 2.5355
Epoch: 48/200
Train loss: 2.2959
Epoch: 49/200
Train loss: 2.2754
Epoch: 50/200
Train loss: 2.3482
Epoch: 51/200
Train loss: 0.7310
Epoch: 52/200
Train loss: 0.6883
Epoch: 53/200
Train loss: 0.6509
Epoch: 54/200
Train loss: 0.6297
Epoch: 55/200
Train loss: 0.6252
Epoch: 56/200
Train loss: 0.5965
Epoch: 57/200
Train loss: 0.5641
Epoch: 58/200
Train loss: 0.5560
Epoch: 59/200
Train loss: 0.5318
Epoch: 60/200
Train loss: 0.5179
Epoch: 61/200
Train loss: 0.4999
Epoch: 62/200
Train loss: 0.4955
Epoch: 63/200
Train loss: 0.4916
Epoch: 64/200
Train loss: 0.4749
Epoch: 65/200
Train loss: 0.4579
Epoch: 66/200
Train loss: 0.4451
Epoch: 67/200
Train loss: 0.4308
Epoch: 68/200
Train loss: 0.4226
Epoch: 69/200
Train loss: 0.4150
Epoch: 70/200
Train loss: 0.4140
Epoch: 71/200
Train loss: 0.4094
Epoch: 72/200
Train loss: 0.4022
Epoch: 73/200
Train loss: 0.3985
Epoch: 74/200
Train loss: 0.3916
Epoch: 75/200
Train loss: 0.3677
Epoch: 76/200
Train loss: 0.3749
Epoch: 77/200
Train loss: 0.3723
Epoch: 78/200
Train loss: 0.3615
Epoch: 79/200
Train loss: 0.3478
Epoch: 80/200
Train loss: 0.3508
Epoch: 81/200
Train loss: 0.3452
Epoch: 82/200
Train loss: 0.3365
Epoch: 83/200
Train loss: 0.3314
Epoch: 84/200
Train loss: 0.3181
Epoch: 85/200
Train loss: 0.3235
Epoch: 86/200
Train loss: 0.3287
Epoch: 87/200
Train loss: 0.2972
Epoch: 88/200
Train loss: 0.3271
Epoch: 89/200
Train loss: 0.3070
Epoch: 90/200
Train loss: 0.3011
Epoch: 91/200
Train loss: 0.2874
Epoch: 92/200
Train loss: 0.3069
Epoch: 93/200
Train loss: 0.2934
Epoch: 94/200
Train loss: 0.2851
Epoch: 95/200
Train loss: 0.2879
Epoch: 96/200
Train loss: 0.2730
Epoch: 97/200
Train loss: 0.2631
Epoch: 98/200
Train loss: 0.2682
Epoch: 99/200
Train loss: 0.2633
Epoch: 100/200
Train loss: 0.2655
Epoch: 101/200
Train loss: 0.2671
Epoch: 102/200
Train loss: 0.2441
Time needed: 168.99261164665222 for validation audios
0.7727312996659501
Model improve: 0.0000 -> 0.7727
Epoch: 103/200
Train loss: 0.2437
Date :05/02/2023, 09:35:10
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 100
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Train loss: 39.3597
Epoch: 2/100
Train loss: 5.6972
Epoch: 3/100
Train loss: 5.3210
Epoch: 4/100
Train loss: 4.9590
Epoch: 5/100
Train loss: 4.6522
Epoch: 6/100
Train loss: 4.3724
Epoch: 7/100
Train loss: 4.1178
Epoch: 8/100
Train loss: 3.8864
Epoch: 9/100
Train loss: 3.6716
Epoch: 10/100
Train loss: 3.4693
Epoch: 11/100
Train loss: 3.2761
Epoch: 12/100
Train loss: 3.0893
Epoch: 13/100
Train loss: 2.9056
Epoch: 14/100
Train loss: 2.7255
Epoch: 15/100
Train loss: 2.5484
Epoch: 16/100
Train loss: 2.3783
Epoch: 17/100
Train loss: 2.2173
Epoch: 18/100
Train loss: 2.0684
Epoch: 19/100
Train loss: 1.9414
Epoch: 20/100
Train loss: 1.8483
Epoch: 21/100
Train loss: 1.7694
Epoch: 22/100
Train loss: 1.6927
Epoch: 23/100
Train loss: 1.6480
Epoch: 24/100
Train loss: 1.5934
Epoch: 25/100
Train loss: 1.5272
Epoch: 26/100
Train loss: 1.4744
Epoch: 27/100
Train loss: 1.4309
Epoch: 28/100
Train loss: 1.4031
Epoch: 29/100
Train loss: 1.3847
Epoch: 30/100
Train loss: 1.3592
Epoch: 31/100
Train loss: 1.3225
Epoch: 32/100
Train loss: 1.2784
Epoch: 33/100
Train loss: 1.2533
Epoch: 34/100
Train loss: 1.2375
Epoch: 35/100
Train loss: 1.2331
Epoch: 36/100
Train loss: 1.2338
Epoch: 37/100
Train loss: 1.2297
Epoch: 38/100
Train loss: 1.2188
Epoch: 39/100
Train loss: 1.1997
Epoch: 40/100
Train loss: 1.1686
Epoch: 41/100
Train loss: 1.1473
Epoch: 42/100
Train loss: 1.1363
Epoch: 43/100
Train loss: 1.1275
Epoch: 44/100
Train loss: 1.1199
Epoch: 45/100
Train loss: 1.1188
Epoch: 46/100
Train loss: 1.1198
Epoch: 47/100
Train loss: 1.1172
Epoch: 48/100
Train loss: 1.1111
Epoch: 49/100
Train loss: 1.1051
Epoch: 50/100
Train loss: 1.1039
Epoch: 51/100
Train loss: 1.0992
Epoch: 52/100
Train loss: 1.1022
Epoch: 53/100
Train loss: 1.1025
Epoch: 54/100
Train loss: 1.0987
Epoch: 55/100
Train loss: 1.0845
Epoch: 56/100
Train loss: 1.0686
Epoch: 57/100
Train loss: 1.0554
Epoch: 58/100
Train loss: 1.0516
Epoch: 59/100
Train loss: 1.0469
Epoch: 60/100
Train loss: 1.0408
Epoch: 61/100
Train loss: 1.0349
Epoch: 62/100
Train loss: 1.0289
Epoch: 63/100
Train loss: 1.0244
Epoch: 64/100
Train loss: 1.0220
Epoch: 65/100
Train loss: 1.0205
Epoch: 66/100
Train loss: 1.0188
Epoch: 67/100
Train loss: 1.0172
Epoch: 68/100
Train loss: 1.0157
Epoch: 69/100
Train loss: 1.0139
Epoch: 70/100
Train loss: 1.0117
Epoch: 71/100
Train loss: 1.0091
Epoch: 72/100
Train loss: 1.0073
Time needed: 166.36468505859375 for validation audios
0.6143516843424002
Model improve: 0.0000 -> 0.6144
Epoch: 73/100
Train loss: 1.0063
Date :05/02/2023, 11:12:28
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 100
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
Date :05/02/2023, 11:12:46
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 100
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Train loss: 39.3217
Time needed: 172.77496337890625 for validation audios
0.25719601160250555
Model improve: 0.0000 -> 0.2572
Epoch: 2/100
Train loss: 5.7002
Time needed: 166.0774791240692 for validation audios
0.3028870226396849
Model improve: 0.2572 -> 0.3029
Epoch: 3/100
Train loss: 5.3243
Time needed: 168.33831357955933 for validation audios
0.3686902820435041
Model improve: 0.3029 -> 0.3687
Epoch: 4/100
Train loss: 4.9599
Time needed: 166.75357937812805 for validation audios
0.43378861680937536
Model improve: 0.3687 -> 0.4338
Epoch: 5/100
Train loss: 4.6516
Time needed: 167.2868070602417 for validation audios
0.49242356252878805
Model improve: 0.4338 -> 0.4924
Epoch: 6/100
Train loss: 4.3710
Time needed: 165.21123385429382 for validation audios
0.5412097649178088
Model improve: 0.4924 -> 0.5412
Epoch: 7/100
Train loss: 4.1157
Time needed: 167.03078651428223 for validation audios
0.5749551309975356
Model improve: 0.5412 -> 0.5750
Epoch: 8/100
Train loss: 3.8837
Time needed: 168.12431287765503 for validation audios
0.5993507186089809
Model improve: 0.5750 -> 0.5994
Epoch: 9/100
Train loss: 3.6682
Time needed: 167.89195036888123 for validation audios
0.6180571644958439
Model improve: 0.5994 -> 0.6181
Epoch: 10/100
Train loss: 3.4650
Time needed: 168.17774534225464 for validation audios
0.6333116798571888
Model improve: 0.6181 -> 0.6333
Epoch: 11/100
Train loss: 3.2709
Time needed: 165.80216121673584 for validation audios
0.6436201283842697
Model improve: 0.6333 -> 0.6436
Epoch: 12/100
Train loss: 3.0833
Time needed: 166.1590871810913 for validation audios
0.6501523431057131
Model improve: 0.6436 -> 0.6502
Epoch: 13/100
Date :05/02/2023, 12:03:27
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 100
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 0
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13553
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Train loss: 40.1186
Epoch: 2/100
Train loss: 5.6787
Epoch: 3/100
Train loss: 5.3120
Epoch: 4/100
Train loss: 4.9470
Epoch: 5/100
Train loss: 4.6291
Epoch: 6/100
Train loss: 4.3473
Epoch: 7/100
Train loss: 4.0967
Epoch: 8/100
Train loss: 3.8732
Epoch: 9/100
Train loss: 3.6651
Epoch: 10/100
Train loss: 3.4674
Epoch: 11/100
Train loss: 3.2776
Epoch: 12/100
Train loss: 3.0951
Epoch: 13/100
Train loss: 2.9154
Epoch: 14/100
Train loss: 2.7399
Time needed: 168.01742720603943 for validation audios
0.6487436042154372
Model improve: 0.0000 -> 0.6487
Epoch: 15/100
Date :05/02/2023, 12:25:15
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 10
nmels: 128
fmax: None
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 100
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.5
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Train loss: 39.5382
Epoch: 2/100
Date :05/02/2023, 12:27:54
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 100
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.5
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Train loss: 39.4813
Epoch: 2/100
Train loss: 5.6846
Epoch: 3/100
Train loss: 5.2935
Epoch: 4/100
Train loss: 4.9134
Epoch: 5/100
Train loss: 4.5860
Epoch: 6/100
Train loss: 4.2893
Epoch: 7/100
Train loss: 4.0233
Epoch: 8/100
Train loss: 3.7822
Epoch: 9/100
Train loss: 3.5600
Epoch: 10/100
Train loss: 3.3512
Epoch: 11/100
Train loss: 3.1527
Epoch: 12/100
Train loss: 2.9617
Epoch: 13/100
Train loss: 2.7733
Epoch: 14/100
Train loss: 2.5893
Time needed: 166.24997568130493 for validation audios
0.6592033329870725
Model improve: 0.0000 -> 0.6592
Epoch: 15/100
Train loss: 2.4087
Time needed: 168.32218050956726 for validation audios
0.6577381205945885
Epoch: 16/100
Date :05/02/2023, 12:54:10
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.5
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 39.4863
Epoch: 2/200
Train loss: 5.6842
Epoch: 3/200
Train loss: 5.2927
Epoch: 4/200
Train loss: 4.9120
Epoch: 5/200
Train loss: 4.5839
Epoch: 6/200
Train loss: 4.2863
Epoch: 7/200
Train loss: 4.0193
Epoch: 8/200
Train loss: 3.7776
Epoch: 9/200
Train loss: 3.5534
Epoch: 10/200
Train loss: 3.3432
Epoch: 11/200
Train loss: 3.1426
Epoch: 12/200
Train loss: 2.9478
Epoch: 13/200
Train loss: 2.7568
Epoch: 14/200
Train loss: 2.5682
Time needed: 163.7467815876007 for validation audios
0.6593184729934822
Model improve: 0.0000 -> 0.6593
Epoch: 15/200
Train loss: 2.3833
Date :05/02/2023, 13:18:03
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.5
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 39.4898
Epoch: 2/200
Train loss: 5.6857
Epoch: 3/200
Date :05/02/2023, 13:20:59
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.5
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 40.2447
Epoch: 2/200
Date :05/02/2023, 13:22:49
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.5
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 40.4284
Epoch: 2/200
Train loss: 5.7697
Epoch: 3/200
Date :05/02/2023, 13:26:52
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 384
epochwarmup: 0
totalepoch: 100
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 0
model_name: tf_efficientnet_b1_ns
mix_up: 0.5
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13553
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Train loss: 41.2708
Epoch: 2/100
Train loss: 5.7523
Epoch: 3/100
Train loss: 5.4133
Epoch: 4/100
Train loss: 5.0554
Epoch: 5/100
Train loss: 4.7249
Epoch: 6/100
Train loss: 4.4281
Epoch: 7/100
Train loss: 4.1637
Epoch: 8/100
Train loss: 3.9248
Epoch: 9/100
Train loss: 3.7046
Epoch: 10/100
Train loss: 3.4970
Epoch: 11/100
Train loss: 3.2980
Epoch: 12/100
Train loss: 3.1057
Epoch: 13/100
Train loss: 2.9201
Epoch: 14/100
Train loss: 2.7396
Time needed: 167.64712119102478 for validation audios
0.6477121224019233
Model improve: 0.0000 -> 0.6477
Epoch: 15/100
Train loss: 2.5648
Date :05/02/2023, 13:52:00
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13830
Date :05/02/2023, 13:52:12
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13830
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 15.0003
Epoch: 2/200
Date :05/02/2023, 13:55:55
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13830
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 40.2646
Epoch: 2/200
Train loss: 5.7726
Epoch: 3/200
Date :05/02/2023, 13:58:36
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 40.3327
Epoch: 2/200
Train loss: 5.7821
Epoch: 3/200
Train loss: 5.4623
Epoch: 4/200
Train loss: 5.1872
Epoch: 5/200
Train loss: 4.8709
Epoch: 6/200
Train loss: 4.6103
Epoch: 7/200
Date :05/02/2023, 14:06:59
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 100
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.5
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 40.4052
Epoch: 2/100
Train loss: 5.7839
Epoch: 3/100
Train loss: 5.4436
Epoch: 4/100
Train loss: 5.1502
Epoch: 5/100
Train loss: 4.7876
Epoch: 6/100
Train loss: 4.5590
Epoch: 7/100
Train loss: 4.2258
Epoch: 8/100
Train loss: 3.9938
Epoch: 9/100
Train loss: 3.9260
Epoch: 10/100
Train loss: 3.6947
Epoch: 11/100
Train loss: 3.6361
Epoch: 12/100
Train loss: 3.4405
Epoch: 13/100
Train loss: 3.3301
Epoch: 14/100
Train loss: 3.4081
Time needed: 167.48353147506714 for validation audios
0.6932842150127039
Model improve: 0.0000 -> 0.6933
Epoch: 15/100
Train loss: 3.1936
Time needed: 168.9017734527588 for validation audios
0.7021248651085678
Model improve: 0.6933 -> 0.7021
Epoch: 16/100
Train loss: 3.2220
Time needed: 168.11213660240173 for validation audios
0.7111832672549981
Model improve: 0.7021 -> 0.7112
Epoch: 17/100
Train loss: 3.0464
Date :05/02/2023, 14:38:10
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 100
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.5
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 40.0714
Epoch: 2/100
Train loss: 5.8096
Epoch: 3/100
Train loss: 5.5038
Epoch: 4/100
Train loss: 5.2261
Epoch: 5/100
Train loss: 4.8798
Epoch: 6/100
Train loss: 4.6488
Epoch: 7/100
Train loss: 4.3529
Epoch: 8/100
Train loss: 4.1274
Epoch: 9/100
Train loss: 4.0460
Epoch: 10/100
Train loss: 3.8230
Epoch: 11/100
Train loss: 3.7514
Epoch: 12/100
Train loss: 3.5944
Epoch: 13/100
Train loss: 3.4692
Epoch: 14/100
Train loss: 3.5548
Time needed: 165.58072066307068 for validation audios
0.6863615186881402
Model improve: 0.0000 -> 0.6864
Epoch: 15/100
Date :05/02/2023, 15:26:10
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 100
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.5
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 40.8236
Epoch: 2/100
Train loss: 5.8383
Epoch: 3/100
Train loss: 5.5451
Epoch: 4/100
Date :05/02/2023, 15:30:41
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 100
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.5
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 40.8451
Epoch: 2/100
Train loss: 5.8346
Epoch: 3/100
Train loss: 5.5327
Epoch: 4/100
Train loss: 5.2488
Epoch: 5/100
Train loss: 4.8998
Epoch: 6/100
Train loss: 4.6640
Epoch: 7/100
Train loss: 4.3367
Epoch: 8/100
Train loss: 4.1041
Epoch: 9/100
Train loss: 4.0283
Epoch: 10/100
Train loss: 3.7923
Epoch: 11/100
Train loss: 3.7231
Epoch: 12/100
Train loss: 3.5255
Epoch: 13/100
Train loss: 3.4122
Epoch: 14/100
Train loss: 3.4849
Time needed: 169.0078992843628 for validation audios
0.6870281707122743
Model improve: 0.0000 -> 0.6870
Epoch: 15/100
Date :05/02/2023, 15:53:06
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 100
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.5
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 38.5397
Epoch: 2/100
Train loss: 5.6960
Epoch: 3/100
Train loss: 5.3020
Epoch: 4/100
Train loss: 4.9870
Epoch: 5/100
Train loss: 4.6061
Epoch: 6/100
Train loss: 4.3830
Epoch: 7/100
Train loss: 4.0476
Epoch: 8/100
Train loss: 3.8212
Epoch: 9/100
Train loss: 3.7689
Epoch: 10/100
Train loss: 3.5424
Epoch: 11/100
Train loss: 3.4955
Epoch: 12/100
Train loss: 3.3026
Epoch: 13/100
Train loss: 3.2009
Epoch: 14/100
Train loss: 3.2866
Time needed: 167.04744291305542 for validation audios
0.7011232710983198
Model improve: 0.0000 -> 0.7011
Epoch: 15/100
Train loss: 3.0699
Time needed: 168.68638730049133 for validation audios
0.7082519162648607
Model improve: 0.7011 -> 0.7083
Epoch: 16/100
Date :05/02/2023, 16:19:06
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.5
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 37.0681
Epoch: 2/200
Train loss: 5.6364
Epoch: 3/200
Train loss: 5.1473
Epoch: 4/200
Train loss: 4.7593
Epoch: 5/200
Train loss: 4.3133
Epoch: 6/200
Train loss: 4.0471
Epoch: 7/200
Train loss: 3.7336
Epoch: 8/200
Train loss: 3.5163
Epoch: 9/200
Train loss: 3.4947
Epoch: 10/200
Train loss: 3.2909
Epoch: 11/200
Train loss: 3.2455
Epoch: 12/200
Train loss: 3.0922
Epoch: 13/200
Train loss: 3.0149
Epoch: 14/200
Train loss: 3.1347
Epoch: 15/200
Train loss: 2.8879
Epoch: 16/200
Train loss: 2.9684
Epoch: 17/200
Train loss: 2.7617
Epoch: 18/200
Train loss: 2.7215
Epoch: 19/200
Train loss: 2.7175
Epoch: 20/200
Train loss: 2.6196
Epoch: 21/200
Train loss: 2.6539
Epoch: 22/200
Train loss: 2.6167
Epoch: 23/200
Train loss: 2.6123
Epoch: 24/200
Train loss: 2.5698
Epoch: 25/200
Train loss: 2.4434
Epoch: 26/200
Train loss: 2.4732
Epoch: 27/200
Train loss: 2.4627
Epoch: 28/200
Train loss: 2.3448
Epoch: 29/200
Train loss: 2.5495
Epoch: 30/200
Train loss: 2.4166
Epoch: 31/200
Train loss: 2.4871
Epoch: 32/200
Train loss: 2.5191
Epoch: 33/200
Train loss: 2.3558
Epoch: 34/200
Train loss: 2.3351
Epoch: 35/200
Train loss: 2.2427
Epoch: 36/200
Train loss: 2.2848
Epoch: 37/200
Train loss: 2.3370
Epoch: 38/200
Train loss: 2.2036
Epoch: 39/200
Train loss: 2.2943
Epoch: 40/200
Train loss: 2.2661
Epoch: 41/200
Train loss: 2.1679
Epoch: 42/200
Train loss: 2.2468
Epoch: 43/200
Train loss: 2.2127
Epoch: 44/200
Train loss: 2.2314
Epoch: 45/200
Train loss: 2.1387
Epoch: 46/200
Train loss: 2.1499
Epoch: 47/200
Train loss: 2.2255
Epoch: 48/200
Train loss: 2.1496
Epoch: 49/200
Train loss: 2.3008
Epoch: 50/200
Train loss: 2.1559
Epoch: 51/200
Train loss: 2.1795
Epoch: 52/200
Train loss: 2.2868
Epoch: 53/200
Train loss: 2.2181
Epoch: 54/200
Train loss: 2.1705
Epoch: 55/200
Train loss: 2.0483
Epoch: 56/200
Train loss: 2.0438
Epoch: 57/200
Train loss: 1.9931
Epoch: 58/200
Train loss: 1.9636
Epoch: 59/200
Train loss: 2.1625
Epoch: 60/200
Train loss: 2.0235
Epoch: 61/200
Train loss: 2.0202
Epoch: 62/200
Train loss: 2.0756
Epoch: 63/200
Train loss: 2.1445
Epoch: 64/200
Train loss: 1.9865
Epoch: 65/200
Train loss: 2.0678
Epoch: 66/200
Train loss: 2.1374
Epoch: 67/200
Train loss: 1.9682
Epoch: 68/200
Train loss: 2.0641
Epoch: 69/200
Train loss: 2.1121
Epoch: 70/200
Train loss: 2.2033
Epoch: 71/200
Train loss: 2.0022
Epoch: 72/200
Train loss: 2.0091
Epoch: 73/200
Train loss: 1.8560
Epoch: 74/200
Train loss: 1.9593
Epoch: 75/200
Train loss: 1.8491
Epoch: 76/200
Train loss: 1.8563
Epoch: 77/200
Train loss: 2.0135
Epoch: 78/200
Train loss: 1.8387
Epoch: 79/200
Train loss: 2.0699
Epoch: 80/200
Train loss: 1.8750
Epoch: 81/200
Train loss: 1.9620
Epoch: 82/200
Train loss: 2.0105
Epoch: 83/200
Train loss: 1.7819
Epoch: 84/200
Train loss: 1.7503
Epoch: 85/200
Train loss: 1.9854
Epoch: 86/200
Train loss: 2.0979
Epoch: 87/200
Train loss: 1.9608
Epoch: 88/200
Train loss: 2.0353
Epoch: 89/200
Train loss: 1.7815
Epoch: 90/200
Train loss: 1.8503
Epoch: 91/200
Train loss: 1.9906
Epoch: 92/200
Train loss: 1.9647
Epoch: 93/200
Train loss: 1.8505
Epoch: 94/200
Train loss: 1.9269
Epoch: 95/200
Train loss: 1.9894
Epoch: 96/200
Train loss: 1.8648
Epoch: 97/200
Train loss: 1.8568
Epoch: 98/200
Train loss: 1.9409
Epoch: 99/200
Train loss: 1.8276
Epoch: 100/200
Train loss: 1.8875
Epoch: 101/200
Train loss: 1.8799
Epoch: 102/200
Train loss: 1.7841
Epoch: 103/200
Train loss: 1.7949
Epoch: 104/200
Train loss: 1.8781
Epoch: 105/200
Train loss: 1.9300
Epoch: 106/200
Train loss: 1.9959
Epoch: 107/200
Train loss: 1.9307
Epoch: 108/200
Train loss: 1.8059
Epoch: 109/200
Train loss: 1.8337
Epoch: 110/200
Train loss: 2.0350
Epoch: 111/200
Train loss: 1.9775
Epoch: 112/200
Train loss: 1.8633
Epoch: 113/200
Train loss: 1.8873
Epoch: 114/200
Train loss: 2.0684
Epoch: 115/200
Train loss: 1.7970
Epoch: 116/200
Train loss: 1.8266
Epoch: 117/200
Train loss: 1.9090
Epoch: 118/200
Train loss: 1.8624
Epoch: 119/200
Train loss: 2.0843
Epoch: 120/200
Train loss: 1.9708
Epoch: 121/200
Train loss: 1.8704
Epoch: 122/200
Train loss: 1.7726
Epoch: 123/200
Train loss: 1.7469
Epoch: 124/200
Train loss: 1.7945
Epoch: 125/200
Train loss: 1.6881
Epoch: 126/200
Train loss: 1.8314
Epoch: 127/200
Train loss: 1.8986
Epoch: 128/200
Train loss: 1.8781
Epoch: 129/200
Train loss: 1.8583
Epoch: 130/200
Train loss: 1.8186
Epoch: 131/200
Train loss: 1.8954
Epoch: 132/200
Train loss: 1.7782
Time needed: 166.3645796775818 for validation audios
0.7984192967887436
Model improve: 0.0000 -> 0.7984
Epoch: 133/200
Train loss: 1.9093
Time needed: 165.8504159450531 for validation audios
0.799171926763247
Model improve: 0.7984 -> 0.7992
Epoch: 134/200
Train loss: 1.8366
Time needed: 166.35778784751892 for validation audios
0.7977266301849045
Epoch: 135/200
Train loss: 1.7688
Time needed: 167.88065791130066 for validation audios
0.799625470935082
Model improve: 0.7992 -> 0.7996
Epoch: 136/200
Train loss: 1.9136
Time needed: 168.4870524406433 for validation audios
0.7946861622088054
Epoch: 137/200
Train loss: 1.7494
Time needed: 165.2267894744873 for validation audios
0.7984434822704063
Epoch: 138/200
Train loss: 1.7294
Time needed: 165.0799641609192 for validation audios
0.797192094805068
Epoch: 139/200
Train loss: 1.8539
Time needed: 164.97253847122192 for validation audios
0.7982949579217514
Epoch: 140/200
Train loss: 1.7949
Time needed: 167.21867084503174 for validation audios
0.7988908951232522
Epoch: 141/200
Train loss: 1.7477
Time needed: 163.62931299209595 for validation audios
0.7991769829932998
Epoch: 142/200
Train loss: 1.8262
Time needed: 165.7668056488037 for validation audios
0.7975963632873384
Epoch: 143/200
Train loss: 1.8263
Time needed: 166.7792706489563 for validation audios
0.7973272135829288
Epoch: 144/200
Train loss: 1.8659
Time needed: 167.6186559200287 for validation audios
0.7965055584816655
Epoch: 145/200
Train loss: 1.9140
Time needed: 166.45111656188965 for validation audios
0.7967098545552442
Epoch: 146/200
Train loss: 2.0824
Time needed: 167.0281617641449 for validation audios
0.7958649962888482
Epoch: 147/200
Train loss: 1.9586
Time needed: 164.50171375274658 for validation audios
0.7979009093638137
Epoch: 148/200
Train loss: 1.8001
Time needed: 167.56727123260498 for validation audios
0.7966636708904902
Epoch: 149/200
Train loss: 1.8108
Time needed: 170.02573013305664 for validation audios
0.7962765462571608
Epoch: 150/200
Train loss: 1.7281
Time needed: 170.4770700931549 for validation audios
0.7980835980833203
Epoch: 151/200
Train loss: 1.7874
Time needed: 167.00688433647156 for validation audios
0.7992237155464248
Epoch: 152/200
Train loss: 1.8123
Time needed: 165.4438672065735 for validation audios
0.7983975197631709
Epoch: 153/200
Train loss: 1.6155
Time needed: 165.29358005523682 for validation audios
0.7981522326069189
Epoch: 154/200
Train loss: 1.9822
Time needed: 165.03691816329956 for validation audios
0.7974282487338504
Epoch: 155/200
Train loss: 1.8487
Time needed: 165.47955965995789 for validation audios
0.7986275202064689
Epoch: 156/200
Train loss: 1.8850
Time needed: 164.55893921852112 for validation audios
0.799386290404301
Epoch: 157/200
Train loss: 1.8234
Time needed: 169.59512615203857 for validation audios
0.7985858027855302
Epoch: 158/200
Train loss: 1.7479
Time needed: 165.46091675758362 for validation audios
0.8007616957810617
Model improve: 0.7996 -> 0.8008
Epoch: 159/200
Train loss: 1.8735
Time needed: 165.2032971382141 for validation audios
0.7984002893413586
Epoch: 160/200
Train loss: 1.6994
Time needed: 165.9863739013672 for validation audios
0.7984146157467399
Epoch: 161/200
Train loss: 1.8059
Time needed: 167.75521779060364 for validation audios
0.800191195479927
Epoch: 162/200
Train loss: 1.7796
Time needed: 165.62680792808533 for validation audios
0.798288426401711
Epoch: 163/200
Train loss: 1.6407
Time needed: 167.2301380634308 for validation audios
0.7993989166809671
Epoch: 164/200
Train loss: 1.6535
Time needed: 172.8791582584381 for validation audios
0.7994704130119751
Epoch: 165/200
Train loss: 1.6701
Time needed: 165.88618087768555 for validation audios
0.8001072723285272
Epoch: 166/200
Train loss: 1.8277
Time needed: 164.2678701877594 for validation audios
0.8001634298980638
Epoch: 167/200
Train loss: 1.8928
Time needed: 166.87542843818665 for validation audios
0.7996133240470786
Epoch: 168/200
Train loss: 1.8777
Time needed: 164.6821837425232 for validation audios
0.7999018642168518
Epoch: 169/200
Train loss: 2.0999
Time needed: 166.09746384620667 for validation audios
0.7990436209875191
Epoch: 170/200
Train loss: 1.8450
Time needed: 165.39524745941162 for validation audios
0.7978320549645207
Epoch: 171/200
Train loss: 1.7321
Time needed: 164.30021286010742 for validation audios
0.7978409255337593
Epoch: 172/200
Train loss: 1.7776
Time needed: 167.16130375862122 for validation audios
0.7996012120952946
Epoch: 173/200
Train loss: 1.7390
Time needed: 165.1986858844757 for validation audios
0.800792523789849
Model improve: 0.8008 -> 0.8008
Epoch: 174/200
Train loss: 1.8270
Time needed: 165.12157726287842 for validation audios
0.8010405765811571
Model improve: 0.8008 -> 0.8010
Epoch: 175/200
Train loss: 1.7693
Time needed: 164.6468803882599 for validation audios
0.7999946139047281
Epoch: 176/200
Train loss: 1.7477
Time needed: 165.8542606830597 for validation audios
0.7984631470109358
Epoch: 177/200
Train loss: 1.7144
Time needed: 165.48153948783875 for validation audios
0.7987726554778757
Epoch: 178/200
Train loss: 1.7316
Time needed: 165.39824056625366 for validation audios
0.8010792180934473
Model improve: 0.8010 -> 0.8011
Epoch: 179/200
Train loss: 1.6349
Time needed: 166.43506455421448 for validation audios
0.8004710636365571
Epoch: 180/200
Train loss: 1.6680
Time needed: 168.0688135623932 for validation audios
0.8003507848345316
Epoch: 181/200
Train loss: 1.8634
Time needed: 165.377592086792 for validation audios
0.7985455643132179
Epoch: 182/200
Train loss: 1.8677
Time needed: 168.26219511032104 for validation audios
0.7983501377010184
Epoch: 183/200
Train loss: 1.6554
Time needed: 165.85377264022827 for validation audios
0.7999238856521549
Epoch: 184/200
Train loss: 1.6730
Time needed: 165.62058973312378 for validation audios
0.7994685671130117
Epoch: 185/200
Train loss: 1.7384
Time needed: 164.46874165534973 for validation audios
0.800029529388965
Epoch: 186/200
Train loss: 1.7714
Time needed: 172.56147694587708 for validation audios
0.8002947989358322
Epoch: 187/200
Train loss: 1.7952
Time needed: 165.1949691772461 for validation audios
0.7974236443486347
Epoch: 188/200
Train loss: 1.6563
Time needed: 165.98992013931274 for validation audios
0.7992169128051242
Epoch: 189/200
Train loss: 1.7501
Time needed: 167.94515705108643 for validation audios
0.8001732936909263
Epoch: 190/200
Train loss: 1.6337
Time needed: 163.7900402545929 for validation audios
0.8000476579514819
Epoch: 191/200
Train loss: 1.7736
Time needed: 166.3130693435669 for validation audios
0.7981785330214247
Epoch: 192/200
Train loss: 1.7337
Time needed: 167.08677172660828 for validation audios
0.7975923382603399
Epoch: 193/200
Train loss: 1.6732
Time needed: 165.95343470573425 for validation audios
0.79907739636496
Epoch: 194/200
Train loss: 1.9759
Time needed: 167.9714879989624 for validation audios
0.7974006653868903
Epoch: 195/200
Train loss: 1.7075
Time needed: 166.3418333530426 for validation audios
0.7986116341164256
Epoch: 196/200
Train loss: 1.7714
Time needed: 166.00028014183044 for validation audios
0.7990298934643362
Epoch: 197/200
Train loss: 1.7855
Time needed: 165.54694509506226 for validation audios
0.7995210769529548
Epoch: 198/200
Train loss: 1.7956
Time needed: 166.61174821853638 for validation audios
0.8005507278437095
Epoch: 199/200
Train loss: 1.7383
Time needed: 166.32102251052856 for validation audios
0.8007005494120847
Epoch: 200/200
Train loss: 1.6844
Time needed: 165.80588364601135 for validation audios
0.8000201311948079
Date :05/02/2023, 23:43:25
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 0
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13553
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 44.2651
Epoch: 2/200
Train loss: 5.7696
Epoch: 3/200
Train loss: 5.2698
Epoch: 4/200
Train loss: 4.9642
Epoch: 5/200
Train loss: 4.6639
Epoch: 6/200
Date :05/02/2023, 23:50:42
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 0
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13553
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/60
Train loss: 25.9478
Epoch: 2/60
Train loss: 5.3220
Epoch: 3/60
Train loss: 4.8032
Epoch: 4/60
Train loss: 4.3610
Epoch: 5/60
Train loss: 4.0746
Epoch: 6/60
Train loss: 3.8864
Epoch: 7/60
Train loss: 3.7167
Epoch: 8/60
Train loss: 3.6435
Epoch: 9/60
Train loss: 3.4791
Epoch: 10/60
Train loss: 3.4711
Epoch: 11/60
Train loss: 3.3073
Epoch: 12/60
Train loss: 3.2330
Epoch: 13/60
Train loss: 3.2526
Epoch: 14/60
Train loss: 3.1961
Epoch: 15/60
Train loss: 3.0770
Epoch: 16/60
Train loss: 3.0507
Epoch: 17/60
Train loss: 2.9714
Epoch: 18/60
Train loss: 3.0514
Epoch: 19/60
Train loss: 2.9689
Epoch: 20/60
Train loss: 2.9537
Epoch: 21/60
Train loss: 2.9416
Epoch: 22/60
Train loss: 2.9371
Epoch: 23/60
Train loss: 2.9675
Epoch: 24/60
Train loss: 2.8123
Epoch: 25/60
Train loss: 2.6329
Epoch: 26/60
Train loss: 2.7131
Epoch: 27/60
Train loss: 2.7001
Epoch: 28/60
Train loss: 2.7971
Epoch: 29/60
Train loss: 2.6746
Epoch: 30/60
Train loss: 2.7179
Epoch: 31/60
Train loss: 1.1129
Epoch: 32/60
Train loss: 1.0391
Epoch: 33/60
Train loss: 0.9981
Epoch: 34/60
Train loss: 0.9656
Epoch: 35/60
Train loss: 0.9414
Epoch: 36/60
Train loss: 0.9051
Epoch: 37/60
Train loss: 0.8902
Epoch: 38/60
Train loss: 0.8597
Epoch: 39/60
Train loss: 0.8579
Epoch: 40/60
Train loss: 0.8285
Epoch: 41/60
Train loss: 0.8213
Epoch: 42/60
Date :05/03/2023, 00:47:54
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 0
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13553
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/60
Train loss: 25.9543
Epoch: 2/60
Date :05/03/2023, 00:49:47
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/60
Train loss: 20.2318
Epoch: 2/60
Train loss: 5.6766
Epoch: 3/60
Train loss: 4.9118
Epoch: 4/60
Train loss: 4.2929
Epoch: 5/60
Train loss: 3.9370
Epoch: 6/60
Train loss: 3.7120
Epoch: 7/60
Train loss: 3.5843
Epoch: 8/60
Train loss: 3.3525
Epoch: 9/60
Train loss: 3.3732
Epoch: 10/60
Train loss: 3.2126
Epoch: 11/60
Train loss: 3.1494
Epoch: 12/60
Train loss: 3.0815
Epoch: 13/60
Train loss: 3.0386
Epoch: 14/60
Train loss: 3.0508
Epoch: 15/60
Train loss: 2.9746
Epoch: 16/60
Train loss: 2.9707
Epoch: 17/60
Train loss: 2.7822
Epoch: 18/60
Train loss: 2.7494
Epoch: 19/60
Train loss: 2.7354
Epoch: 20/60
Train loss: 2.7502
Epoch: 21/60
Train loss: 2.7473
Epoch: 22/60
Train loss: 2.6705
Epoch: 23/60
Train loss: 2.7198
Epoch: 24/60
Train loss: 2.6575
Epoch: 25/60
Train loss: 2.6672
Epoch: 26/60
Date :05/03/2023, 01:37:15
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/60
Date :05/03/2023, 01:37:29
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/60
Train loss: 20.1634
Epoch: 2/60
Train loss: 5.6926
Epoch: 3/60
Train loss: 4.9349
Epoch: 4/60
Train loss: 4.3123
Epoch: 5/60
Train loss: 3.9501
Epoch: 6/60
Train loss: 3.7215
Epoch: 7/60
Train loss: 3.5911
Epoch: 8/60
Train loss: 3.3570
Epoch: 9/60
Train loss: 3.3769
Epoch: 10/60
Train loss: 3.2143
Epoch: 11/60
Train loss: 3.1512
Epoch: 12/60
Train loss: 3.0830
Epoch: 13/60
Train loss: 3.0395
Epoch: 14/60
Train loss: 3.0518
Epoch: 15/60
Train loss: 2.9743
Epoch: 16/60
Train loss: 2.9707
Epoch: 17/60
Train loss: 2.7831
Epoch: 18/60
Train loss: 2.7502
Epoch: 19/60
Train loss: 2.7361
Epoch: 20/60
Train loss: 2.7503
Epoch: 21/60
Train loss: 2.7472
Epoch: 22/60
Train loss: 2.6708
Epoch: 23/60
Train loss: 2.7198
Epoch: 24/60
Train loss: 2.6587
Epoch: 25/60
Train loss: 2.6668
Epoch: 26/60
Train loss: 1.9205
Epoch: 27/60
Train loss: 2.0198
Epoch: 28/60
Train loss: 1.8940
Epoch: 29/60
Train loss: 1.9784
Epoch: 30/60
Train loss: 1.9301
Epoch: 31/60
Train loss: 1.8550
Epoch: 32/60
Train loss: 1.9229
Epoch: 33/60
Train loss: 1.7471
Epoch: 34/60
Train loss: 1.9408
Epoch: 35/60
Train loss: 1.8813
Epoch: 36/60
Train loss: 1.8977
Epoch: 37/60
Train loss: 1.8898
Epoch: 38/60
Train loss: 1.7356
Epoch: 39/60
Train loss: 1.8240
Epoch: 40/60
Train loss: 1.9074
Epoch: 41/60
Train loss: 1.7895
Epoch: 42/60
Train loss: 1.7892
Time needed: 171.64699387550354 for validation audios
0.7974626464605956
Model improve: 0.0000 -> 0.7975
Epoch: 43/60
Train loss: 1.7617
Time needed: 167.80557990074158 for validation audios
0.7976473138878429
Model improve: 0.7975 -> 0.7976
Epoch: 44/60
Train loss: 1.8546
Time needed: 168.47316241264343 for validation audios
0.7967278445019859
Epoch: 45/60
Train loss: 1.7225
Time needed: 170.29912209510803 for validation audios
0.7987717673496846
Model improve: 0.7976 -> 0.7988
Epoch: 46/60
Train loss: 1.6910
Time needed: 170.47511196136475 for validation audios
0.7975375234640649
Epoch: 47/60
Train loss: 0.5677
Time needed: 167.9668197631836 for validation audios
0.7992940751222266
Model improve: 0.7988 -> 0.7993
Epoch: 48/60
Train loss: 0.5665
Time needed: 168.41610860824585 for validation audios
0.7990011092869194
Epoch: 49/60
Train loss: 0.5406
Time needed: 169.64529943466187 for validation audios
0.799307575257137
Model improve: 0.7993 -> 0.7993
Epoch: 50/60
Train loss: 0.5405
Time needed: 169.53416275978088 for validation audios
0.7998146479796868
Model improve: 0.7993 -> 0.7998
Epoch: 51/60
Train loss: 0.5378
Time needed: 171.2181179523468 for validation audios
0.7990552858943758
Epoch: 52/60
Train loss: 0.5186
Time needed: 171.64210295677185 for validation audios
0.7991841615127535
Epoch: 53/60
Date :05/03/2023, 03:48:18
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 16
validbs: 64
epochwarmup: 0
totalepoch: 25
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.4
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Train loss: 9.5168
Epoch: 2/25
Train loss: 4.0234
Epoch: 3/25
Train loss: 3.4767
Epoch: 4/25
Train loss: 3.1253
Epoch: 5/25
Train loss: 3.0113
Epoch: 6/25
Train loss: 2.7990
Epoch: 7/25
Train loss: 2.7167
Epoch: 8/25
Train loss: 2.6415
Epoch: 9/25
Train loss: 2.5874
Epoch: 10/25
Train loss: 2.4864
Epoch: 11/25
Train loss: 2.4621
Epoch: 12/25
Train loss: 2.3865
Epoch: 13/25
Train loss: 2.3473
Epoch: 14/25
Train loss: 2.3157
Epoch: 15/25
Train loss: 2.2911
Epoch: 16/25
Train loss: 2.2770
Epoch: 17/25
Train loss: 2.1956
Epoch: 18/25
Train loss: 2.2349
Epoch: 19/25
Train loss: 2.2530
Epoch: 20/25
Train loss: 2.1766
Epoch: 21/25
Train loss: 2.2500
Epoch: 22/25
Train loss: 2.1693
Time needed: 175.018816947937 for validation audios
0.7910487929721108
Model improve: 0.0000 -> 0.7910
Epoch: 23/25
Date :05/03/2023, 05:16:21
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Train loss: 17.0988
Date :05/03/2023, 05:19:20
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Train loss: 17.0928
Time needed: 0.14854049682617188 for validation audios
0.6717806065108743
Model improve: 0.0000 -> 0.6718
Epoch: 2/100
Train loss: 5.1777
Time needed: 0.14973020553588867 for validation audios
0.8020735984426529
Model improve: 0.6718 -> 0.8021
Epoch: 3/100
Train loss: 4.1589
Time needed: 0.1542809009552002 for validation audios
0.8448757215009175
Model improve: 0.8021 -> 0.8449
Epoch: 4/100
Date :05/03/2023, 05:27:10
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Date :05/03/2023, 05:27:34
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Train loss: 21.5235
Time needed: 0.14829182624816895 for validation audios
0.5999839142380425
Model improve: 0.0000 -> 0.6000
Epoch: 2/100
Train loss: 4.9348
Time needed: 0.14681363105773926 for validation audios
0.7286920223163815
Model improve: 0.6000 -> 0.7287
Epoch: 3/100
Train loss: 4.2312
Time needed: 0.14806509017944336 for validation audios
0.7991374754147234
Model improve: 0.7287 -> 0.7991
Epoch: 4/100
Train loss: 3.6962
Time needed: 0.15180516242980957 for validation audios
0.8343773075410056
Model improve: 0.7991 -> 0.8344
Epoch: 5/100
Train loss: 3.4297
Time needed: 0.14609861373901367 for validation audios
0.8532972153270458
Model improve: 0.8344 -> 0.8533
Epoch: 6/100
Train loss: 3.1876
Time needed: 0.15156173706054688 for validation audios
0.8657433795713578
Model improve: 0.8533 -> 0.8657
Epoch: 7/100
Train loss: 3.1320
Time needed: 0.15070462226867676 for validation audios
0.8740410092174972
Model improve: 0.8657 -> 0.8740
Epoch: 8/100
Train loss: 2.9484
Time needed: 0.1481926441192627 for validation audios
0.8820554647151889
Model improve: 0.8740 -> 0.8821
Epoch: 9/100
Train loss: 2.8916
Time needed: 0.1510169506072998 for validation audios
0.8865905508490189
Model improve: 0.8821 -> 0.8866
Epoch: 10/100
Train loss: 2.6009
Time needed: 0.15245366096496582 for validation audios
0.8919714187201656
Model improve: 0.8866 -> 0.8920
Epoch: 11/100
Train loss: 2.6123
Time needed: 0.14940381050109863 for validation audios
0.8954897304160202
Model improve: 0.8920 -> 0.8955
Epoch: 12/100
Train loss: 2.6589
Time needed: 0.14533138275146484 for validation audios
0.8988866390787098
Model improve: 0.8955 -> 0.8989
Epoch: 13/100
Train loss: 2.5442
Time needed: 0.14967632293701172 for validation audios
0.9016530639810628
Model improve: 0.8989 -> 0.9017
Epoch: 14/100
Train loss: 2.4825
Time needed: 0.14944934844970703 for validation audios
0.9034930764284771
Model improve: 0.9017 -> 0.9035
Epoch: 15/100
Train loss: 2.3963
Time needed: 0.14508819580078125 for validation audios
0.9044929528934181
Model improve: 0.9035 -> 0.9045
Epoch: 16/100
Train loss: 2.3663
Time needed: 0.144378662109375 for validation audios
0.9054037497183174
Model improve: 0.9045 -> 0.9054
Epoch: 17/100
Train loss: 2.3696
Time needed: 0.1502857208251953 for validation audios
0.9065410153139366
Model improve: 0.9054 -> 0.9065
Epoch: 18/100
Train loss: 2.2836
Time needed: 0.14538931846618652 for validation audios
0.9084529557482374
Model improve: 0.9065 -> 0.9085
Epoch: 19/100
Train loss: 2.0831
Time needed: 0.15056252479553223 for validation audios
0.9094253471173859
Model improve: 0.9085 -> 0.9094
Epoch: 20/100
Train loss: 2.2860
Time needed: 0.14675092697143555 for validation audios
0.9099621405312799
Model improve: 0.9094 -> 0.9100
Epoch: 21/100
Train loss: 2.2385
Time needed: 0.1517484188079834 for validation audios
0.9108645136565351
Model improve: 0.9100 -> 0.9109
Epoch: 22/100
Train loss: 2.1456
Time needed: 0.1526777744293213 for validation audios
0.9118643557855374
Model improve: 0.9109 -> 0.9119
Epoch: 23/100
Train loss: 2.0736
Time needed: 0.14625883102416992 for validation audios
0.9130416368990255
Model improve: 0.9119 -> 0.9130
Epoch: 24/100
Train loss: 2.0295
Time needed: 0.15074419975280762 for validation audios
0.9136383507540499
Model improve: 0.9130 -> 0.9136
Epoch: 25/100
Train loss: 2.0714
Time needed: 0.15008807182312012 for validation audios
0.9141693394424244
Model improve: 0.9136 -> 0.9142
Epoch: 26/100
Train loss: 2.0942
Time needed: 0.14682674407958984 for validation audios
0.914918293405039
Model improve: 0.9142 -> 0.9149
Epoch: 27/100
Train loss: 2.0463
Time needed: 0.14624452590942383 for validation audios
0.9156175137116156
Model improve: 0.9149 -> 0.9156
Epoch: 28/100
Train loss: 2.1258
Time needed: 0.14778780937194824 for validation audios
0.9154682292103753
Epoch: 29/100
Train loss: 2.0403
Time needed: 0.15130829811096191 for validation audios
0.9171124523779111
Model improve: 0.9156 -> 0.9171
Epoch: 30/100
Train loss: 2.0658
Time needed: 0.14732837677001953 for validation audios
0.9167090127799791
Epoch: 31/100
Train loss: 1.8472
Time needed: 0.14836692810058594 for validation audios
0.9171262784875358
Model improve: 0.9171 -> 0.9171
Epoch: 32/100
Train loss: 1.7823
Time needed: 0.15201210975646973 for validation audios
0.9173702362878197
Model improve: 0.9171 -> 0.9174
Epoch: 33/100
Train loss: 1.9381
Time needed: 0.15720510482788086 for validation audios
0.9174282412844011
Model improve: 0.9174 -> 0.9174
Epoch: 34/100
Train loss: 1.9608
Time needed: 0.1468830108642578 for validation audios
0.9189020958356316
Model improve: 0.9174 -> 0.9189
Epoch: 35/100
Train loss: 1.9082
Time needed: 0.15023398399353027 for validation audios
0.9189293936153274
Model improve: 0.9189 -> 0.9189
Epoch: 36/100
Train loss: 1.8552
Time needed: 0.14972209930419922 for validation audios
0.9177608297000288
Epoch: 37/100
Train loss: 1.7958
Time needed: 0.14820146560668945 for validation audios
0.9172123900007882
Epoch: 38/100
Train loss: 1.8631
Time needed: 0.15221285820007324 for validation audios
0.918650258111335
Epoch: 39/100
Train loss: 1.8081
Time needed: 0.1468663215637207 for validation audios
0.9179816704046307
Epoch: 40/100
Train loss: 1.8313
Time needed: 0.15195679664611816 for validation audios
0.9185815295502668
Epoch: 41/100
Train loss: 1.7945
Time needed: 0.14884090423583984 for validation audios
0.9199089597898209
Model improve: 0.9189 -> 0.9199
Epoch: 42/100
Train loss: 1.7650
Time needed: 0.14540719985961914 for validation audios
0.9187237901656805
Epoch: 43/100
Train loss: 1.8244
Time needed: 0.1483936309814453 for validation audios
0.9174337046534555
Epoch: 44/100
Train loss: 1.7243
Time needed: 0.15040302276611328 for validation audios
0.9175155870620465
Epoch: 45/100
Train loss: 1.7754
Time needed: 0.14696621894836426 for validation audios
0.9171999315721753
Epoch: 46/100
Train loss: 1.6409
Time needed: 0.15013480186462402 for validation audios
0.9179957687582497
Epoch: 47/100
Train loss: 1.6145
Time needed: 0.14704394340515137 for validation audios
0.9190944663653577
Epoch: 48/100
Train loss: 1.7032
Time needed: 0.14595818519592285 for validation audios
0.9186184125914244
Epoch: 49/100
Train loss: 1.7600
Time needed: 0.15039491653442383 for validation audios
0.9182225494841626
Epoch: 50/100
Train loss: 1.6939
Time needed: 0.15062880516052246 for validation audios
0.9200922767169799
Model improve: 0.9199 -> 0.9201
Epoch: 51/100
Train loss: 1.6356
Time needed: 0.14563417434692383 for validation audios
0.9202179646030556
Model improve: 0.9201 -> 0.9202
Epoch: 52/100
Train loss: 1.7642
Time needed: 0.14894413948059082 for validation audios
0.9180018576678259
Epoch: 53/100
Train loss: 1.8226
Time needed: 0.1478102207183838 for validation audios
0.9174808229718955
Epoch: 54/100
Train loss: 1.5693
Time needed: 0.14949440956115723 for validation audios
0.9169348905470807
Epoch: 55/100
Train loss: 1.7154
Time needed: 0.14934778213500977 for validation audios
0.9176159227310992
Epoch: 56/100
Train loss: 1.6351
Time needed: 0.14480209350585938 for validation audios
0.9180373661031597
Epoch: 57/100
Train loss: 1.5687
Time needed: 0.14856505393981934 for validation audios
0.919417448425645
Epoch: 58/100
Train loss: 1.6596
Time needed: 0.15001583099365234 for validation audios
0.9190428576325536
Epoch: 59/100
Train loss: 1.7119
Time needed: 0.14434480667114258 for validation audios
0.9204585077774483
Model improve: 0.9202 -> 0.9205
Epoch: 60/100
Train loss: 1.6988
Time needed: 0.14412975311279297 for validation audios
0.9197676959448254
Epoch: 61/100
Train loss: 1.6967
Time needed: 0.1479794979095459 for validation audios
0.9203034968432096
Epoch: 62/100
Train loss: 1.7033
Time needed: 0.14632225036621094 for validation audios
0.9197162608363284
Epoch: 63/100
Train loss: 1.6789
Time needed: 0.15277934074401855 for validation audios
0.9202609961212501
Epoch: 64/100
Train loss: 1.6417
Time needed: 0.15273022651672363 for validation audios
0.9193556443368731
Epoch: 65/100
Train loss: 1.5685
Time needed: 0.14862465858459473 for validation audios
0.9201722631221481
Epoch: 66/100
Train loss: 1.6544
Time needed: 0.14584875106811523 for validation audios
0.9203765826214065
Epoch: 67/100
Train loss: 1.6673
Time needed: 0.15107083320617676 for validation audios
0.9202917529753464
Epoch: 68/100
Train loss: 1.7244
Time needed: 0.15212130546569824 for validation audios
0.9208001521138082
Model improve: 0.9205 -> 0.9208
Epoch: 69/100
Train loss: 1.6407
Time needed: 0.14432120323181152 for validation audios
0.9207886155984243
Epoch: 70/100
Train loss: 1.5988
Time needed: 0.14497756958007812 for validation audios
0.9201459988335197
Epoch: 71/100
Train loss: 1.6304
Time needed: 0.14824843406677246 for validation audios
0.9187626954640242
Epoch: 72/100
Train loss: 1.5757
Time needed: 0.14838075637817383 for validation audios
0.9198134612477149
Epoch: 73/100
Train loss: 1.7035
Time needed: 0.15145373344421387 for validation audios
0.920452552947594
Epoch: 74/100
Train loss: 1.6303
Time needed: 0.14917516708374023 for validation audios
0.9190557801617831
Epoch: 75/100
Train loss: 1.4632
Time needed: 0.15339112281799316 for validation audios
0.9199365463870905
Epoch: 76/100
Train loss: 1.6487
Time needed: 0.15314316749572754 for validation audios
0.9200739043509517
Epoch: 77/100
Train loss: 1.4563
Time needed: 0.1457502841949463 for validation audios
0.9200261317227191
Epoch: 78/100
Train loss: 1.6766
Time needed: 0.1460103988647461 for validation audios
0.9193084679670728
Epoch: 79/100
Train loss: 1.4534
Time needed: 0.14695334434509277 for validation audios
0.9201433622365953
Epoch: 80/100
Train loss: 1.5752
Time needed: 0.15136384963989258 for validation audios
0.9198400446760179
Epoch: 81/100
Train loss: 1.7105
Time needed: 0.1482982635498047 for validation audios
0.9197883130057152
Epoch: 82/100
Train loss: 1.6286
Time needed: 0.1465754508972168 for validation audios
0.9200899950613725
Epoch: 83/100
Train loss: 1.5380
Time needed: 0.15015530586242676 for validation audios
0.919971419204262
Epoch: 84/100
Date :05/03/2023, 07:48:56
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 50
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/50
Epoch: 2/50
Epoch: 3/50
Epoch: 4/50
Epoch: 5/50
Epoch: 6/50
Epoch: 7/50
Epoch: 8/50
Epoch: 9/50
Epoch: 10/50
Epoch: 11/50
Epoch: 12/50
Epoch: 13/50
Epoch: 14/50
Epoch: 15/50
Epoch: 16/50
Epoch: 17/50
Epoch: 18/50
Epoch: 19/50
Epoch: 20/50
Epoch: 21/50
Epoch: 22/50
Time needed: 167.09886717796326 for validation audios
0.802539587029376
Model improve: 0.0000 -> 0.8025
Epoch: 23/50
Date :05/03/2023, 07:53:38
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 50
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
Date :05/03/2023, 07:53:52
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 50
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
Date :05/03/2023, 07:54:52
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 50
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
Date :05/03/2023, 07:55:09
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 50
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/50
Epoch: 2/50
Epoch: 3/50
Epoch: 4/50
Epoch: 5/50
Epoch: 6/50
Epoch: 7/50
Epoch: 8/50
Epoch: 9/50
Epoch: 10/50
Epoch: 11/50
Epoch: 12/50
Epoch: 13/50
Epoch: 14/50
Epoch: 15/50
Epoch: 16/50
Epoch: 17/50
Epoch: 18/50
Epoch: 19/50
Epoch: 20/50
Epoch: 21/50
Epoch: 22/50
Time needed: 169.60939049720764 for validation audios
0.8117087188249115
Model improve: 0.0000 -> 0.8117
Epoch: 23/50
Date :05/03/2023, 07:58:13
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 50
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/50
Time needed: 0.15178418159484863 for validation audios
0.9227929013786283
Model improve: 0.0000 -> 0.9228
Epoch: 2/50
Date :05/03/2023, 08:10:41
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/60
Date :05/03/2023, 08:12:54
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/60
Date :05/03/2023, 08:13:07
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/60
Date :05/03/2023, 08:13:31
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/60
Date :05/03/2023, 08:14:10
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/60
Date :05/03/2023, 08:14:35
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/60
Epoch: 2/60
Epoch: 3/60
Date :05/03/2023, 08:18:32
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/60
Date :05/03/2023, 08:19:22
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/60
Date :05/03/2023, 08:19:48
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/60
Train loss: 0.0110
Epoch: 2/60
Train loss: 0.0023
Epoch: 3/60
Train loss: 0.0021
Epoch: 4/60
Train loss: 0.0019
Epoch: 5/60
Train loss: 0.0018
Epoch: 6/60
Train loss: 0.0017
Epoch: 7/60
Train loss: 0.0016
Epoch: 8/60
Train loss: 0.0016
Epoch: 9/60
Train loss: 0.0015
Epoch: 10/60
Train loss: 0.0014
Epoch: 11/60
Train loss: 0.0014
Epoch: 12/60
Train loss: 0.0014
Epoch: 13/60
Train loss: 0.0014
Epoch: 14/60
Train loss: 0.0014
Time needed: 168.7336323261261 for validation audios
0.7236716471075485
Model improve: 0.0000 -> 0.7237
Epoch: 15/60
Train loss: 0.0013
Time needed: 171.31633138656616 for validation audios
0.7270344380746576
Model improve: 0.7237 -> 0.7270
Epoch: 16/60
Date :05/03/2023, 08:47:21
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.4
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Train loss: 0.011036
Epoch: 2/100
Train loss: 0.002361
Epoch: 3/100
Train loss: 0.002122
Epoch: 4/100
Date :05/03/2023, 08:51:58
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 16
validbs: 64
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.4
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Date :05/03/2023, 08:53:33
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 16
validbs: 64
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.4
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Date :05/03/2023, 08:54:06
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 16
validbs: 64
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.4
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Train loss: 0.004643
Epoch: 2/100
Train loss: 0.001973
Epoch: 3/100
Date :05/03/2023, 09:00:59
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 16
validbs: 64
epochwarmup: 0
totalepoch: 25
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.4
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Train loss: 0.004643
Epoch: 2/25
Train loss: 0.001973
Epoch: 3/25
Train loss: 0.001778
Epoch: 4/25
Train loss: 0.001683
Epoch: 5/25
Train loss: 0.001613
Epoch: 6/25
Train loss: 0.001543
Epoch: 7/25
Train loss: 0.001542
Epoch: 8/25
Train loss: 0.001457
Epoch: 9/25
Train loss: 0.001457
Epoch: 10/25
Train loss: 0.001395
Epoch: 11/25
Train loss: 0.001371
Epoch: 12/25
Train loss: 0.001376
Epoch: 13/25
Train loss: 0.001334
Epoch: 14/25
Train loss: 0.001308
Epoch: 15/25
Train loss: 0.001291
Epoch: 16/25
Train loss: 0.001289
Epoch: 17/25
Train loss: 0.001250
Time needed: 169.928466796875 for validation audios
0.751775419342009
Model improve: 0.0000 -> 0.7518
Epoch: 18/25
Train loss: 0.001260
Time needed: 170.18433213233948 for validation audios
0.7507779275587747
Epoch: 19/25
Train loss: 0.001240
Time needed: 170.58635568618774 for validation audios
0.7462734457002262
Epoch: 20/25
Date :05/03/2023, 10:04:12
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 16
validbs: 64
epochwarmup: 0
totalepoch: 25
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.4
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
Date :05/03/2023, 10:04:24
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 256
validbs: 1024
epochwarmup: 0
totalepoch: 200
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.4
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 0.033683
Epoch: 2/200
Train loss: 0.002548
Epoch: 3/200
Train loss: 0.002240
Epoch: 4/200
Train loss: 0.002083
Epoch: 5/200
Train loss: 0.001982
Time needed: 171.9090166091919 for validation audios
0.3689203437765938
Model improve: 0.0000 -> 0.3689
Epoch: 6/200
Date :05/03/2023, 10:13:57
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 256
validbs: 1024
epochwarmup: 0
totalepoch: 200
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.4
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Date :05/03/2023, 10:14:16
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.4
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 34.722731
Epoch: 2/200
Train loss: 5.458298
Epoch: 3/200
Train loss: 4.911625
Epoch: 4/200
Train loss: 4.542950
Epoch: 5/200
Train loss: 4.145967
Time needed: 168.9948480129242 for validation audios
0.59880571601347
Model improve: 0.0000 -> 0.5988
Epoch: 6/200
Train loss: 3.989307
Time needed: 168.86710691452026 for validation audios
0.6275152570543265
Model improve: 0.5988 -> 0.6275
Epoch: 7/200
Date :05/03/2023, 10:27:18
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.4
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Date :05/03/2023, 10:27:45
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.4
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 34.728214
Epoch: 2/200
Train loss: 5.459604
Epoch: 3/200
Train loss: 4.912531
Epoch: 4/200
Date :05/03/2023, 10:31:37
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 35.263141
Epoch: 2/200
Train loss: 5.581898
Epoch: 3/200
Train loss: 5.093320
Epoch: 4/200
Train loss: 4.783975
Epoch: 5/200
Train loss: 4.436236
Epoch: 6/200
Train loss: 4.187738
Epoch: 7/200
Train loss: 3.892926
Epoch: 8/200
Train loss: 3.834010
Epoch: 9/200
Train loss: 3.674308
Epoch: 10/200
Train loss: 3.602155
Epoch: 11/200
Train loss: 3.548121
Epoch: 12/200
Train loss: 3.427964
Epoch: 13/200
Train loss: 3.371214
Epoch: 14/200
Train loss: 3.257029
Epoch: 15/200
Train loss: 3.297645
Epoch: 16/200
Train loss: 3.235940
Time needed: 169.93237709999084 for validation audios
0.7364127902482958
Model improve: 0.0000 -> 0.7364
Epoch: 17/200
Date :05/03/2023, 10:53:49
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 35.276077
Epoch: 2/200
Train loss: 5.571030
Epoch: 3/200
Train loss: 5.078302
Epoch: 4/200
Train loss: 4.772960
Epoch: 5/200
Train loss: 4.427219
Epoch: 6/200
Train loss: 4.181320
Epoch: 7/200
Train loss: 3.887153
Epoch: 8/200
Train loss: 3.829702
Epoch: 9/200
Train loss: 3.671089
Epoch: 10/200
Train loss: 3.599967
Epoch: 11/200
Train loss: 3.545774
Epoch: 12/200
Train loss: 3.426355
Epoch: 13/200
Train loss: 3.369413
Epoch: 14/200
Train loss: 3.255464
Epoch: 15/200
Train loss: 3.296839
Epoch: 16/200
Train loss: 3.234531
Epoch: 17/200
Train loss: 3.124783
Epoch: 18/200
Train loss: 3.029483
Epoch: 19/200
Train loss: 3.222821
Epoch: 20/200
Train loss: 3.108235
Epoch: 21/200
Train loss: 2.967650
Epoch: 22/200
Train loss: 2.855365
Epoch: 23/200
Train loss: 2.882300
Epoch: 24/200
Train loss: 2.959688
Epoch: 25/200
Train loss: 2.922853
Epoch: 26/200
Train loss: 2.921893
Epoch: 27/200
Train loss: 2.902520
Epoch: 28/200
Train loss: 2.770177
Epoch: 29/200
Train loss: 2.773659
Epoch: 30/200
Train loss: 2.641298
Epoch: 31/200
Train loss: 2.773122
Epoch: 32/200
Train loss: 2.815792
Epoch: 33/200
Train loss: 2.613580
Epoch: 34/200
Train loss: 2.703076
Epoch: 35/200
Train loss: 2.694901
Epoch: 36/200
Train loss: 2.720410
Epoch: 37/200
Train loss: 2.620922
Epoch: 38/200
Train loss: 2.839349
Epoch: 39/200
Train loss: 2.528723
Epoch: 40/200
Train loss: 2.652280
Epoch: 41/200
Train loss: 2.591183
Epoch: 42/200
Train loss: 2.564650
Epoch: 43/200
Train loss: 2.672956
Epoch: 44/200
Train loss: 2.627347
Epoch: 45/200
Train loss: 2.649614
Epoch: 46/200
Train loss: 2.425604
Epoch: 47/200
Train loss: 2.611815
Epoch: 48/200
Train loss: 2.360423
Epoch: 49/200
Train loss: 2.353963
Epoch: 50/200
Train loss: 2.403233
Epoch: 51/200
Train loss: 2.445760
Epoch: 52/200
Train loss: 2.433749
Epoch: 53/200
Train loss: 2.355029
Epoch: 54/200
Train loss: 2.461604
Epoch: 55/200
Train loss: 2.437698
Epoch: 56/200
Train loss: 2.394617
Epoch: 57/200
Train loss: 2.442188
Epoch: 58/200
Train loss: 2.417928
Epoch: 59/200
Train loss: 2.410329
Epoch: 60/200
Train loss: 2.436662
Epoch: 61/200
Train loss: 2.329996
Epoch: 62/200
Train loss: 2.348256
Epoch: 63/200
Train loss: 2.355334
Epoch: 64/200
Train loss: 2.388013
Epoch: 65/200
Train loss: 2.420103
Epoch: 66/200
Train loss: 2.303922
Epoch: 67/200
Train loss: 2.384683
Epoch: 68/200
Train loss: 2.347550
Epoch: 69/200
Train loss: 2.512814
Epoch: 70/200
Train loss: 2.322861
Epoch: 71/200
Train loss: 2.187952
Epoch: 72/200
Train loss: 2.212182
Epoch: 73/200
Train loss: 2.325094
Epoch: 74/200
Train loss: 2.310272
Epoch: 75/200
Train loss: 2.357917
Epoch: 76/200
Train loss: 2.319703
Epoch: 77/200
Train loss: 2.302311
Epoch: 78/200
Train loss: 2.205606
Epoch: 79/200
Train loss: 2.070662
Epoch: 80/200
Train loss: 2.239169
Epoch: 81/200
Train loss: 2.255933
Epoch: 82/200
Train loss: 2.220697
Epoch: 83/200
Train loss: 2.193368
Epoch: 84/200
Train loss: 2.175852
Epoch: 85/200
Train loss: 2.311144
Epoch: 86/200
Train loss: 2.228675
Epoch: 87/200
Train loss: 2.114188
Epoch: 88/200
Train loss: 2.313098
Epoch: 89/200
Train loss: 2.330118
Epoch: 90/200
Train loss: 2.164207
Epoch: 91/200
Train loss: 2.278349
Epoch: 92/200
Train loss: 2.265579
Epoch: 93/200
Train loss: 2.296493
Epoch: 94/200
Train loss: 2.202286
Epoch: 95/200
Train loss: 2.256337
Epoch: 96/200
Train loss: 2.284444
Epoch: 97/200
Train loss: 2.228407
Epoch: 98/200
Train loss: 2.216950
Epoch: 99/200
Train loss: 2.139655
Epoch: 100/200
Train loss: 2.189248
Epoch: 101/200
Train loss: 2.238297
Epoch: 102/200
Train loss: 2.033608
Time needed: 166.552104473114 for validation audios
0.8076329181327837
Model improve: 0.0000 -> 0.8076
Epoch: 103/200
Train loss: 2.094917
Time needed: 166.40285062789917 for validation audios
0.8043525852232157
Epoch: 104/200
Train loss: 2.123059
Time needed: 165.63385272026062 for validation audios
0.8057913282221436
Epoch: 105/200
Train loss: 2.137468
Time needed: 165.91386795043945 for validation audios
0.805466877282919
Epoch: 106/200
Train loss: 2.158535
Date :05/03/2023, 13:10:27
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.4
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 34.739797
Epoch: 2/200
Train loss: 5.460310
Epoch: 3/200
Train loss: 4.913891
Epoch: 4/200
Train loss: 4.545379
Epoch: 5/200
Train loss: 4.148560
Epoch: 6/200
Train loss: 3.991416
Epoch: 7/200
Train loss: 3.660473
Epoch: 8/200
Train loss: 3.622963
Epoch: 9/200
Train loss: 3.413643
Epoch: 10/200
Train loss: 3.333118
Epoch: 11/200
Train loss: 3.388898
Epoch: 12/200
Train loss: 3.147200
Epoch: 13/200
Train loss: 3.160325
Epoch: 14/200
Train loss: 2.981634
Epoch: 15/200
Train loss: 3.034606
Epoch: 16/200
Train loss: 2.818426
Epoch: 17/200
Train loss: 2.959318
Epoch: 18/200
Train loss: 2.753127
Epoch: 19/200
Train loss: 2.757584
Epoch: 20/200
Train loss: 2.611551
Epoch: 21/200
Train loss: 2.619341
Epoch: 22/200
Train loss: 2.696213
Epoch: 23/200
Train loss: 2.683807
Epoch: 24/200
Train loss: 2.696707
Epoch: 25/200
Train loss: 2.495490
Epoch: 26/200
Train loss: 2.628901
Epoch: 27/200
Train loss: 2.484608
Epoch: 28/200
Train loss: 2.545824
Epoch: 29/200
Train loss: 2.412644
Epoch: 30/200
Train loss: 2.391121
Epoch: 31/200
Train loss: 2.714367
Epoch: 32/200
Train loss: 2.448465
Epoch: 33/200
Train loss: 2.489004
Epoch: 34/200
Train loss: 2.576932
Epoch: 35/200
Train loss: 2.400389
Epoch: 36/200
Train loss: 2.238837
Epoch: 37/200
Train loss: 2.314219
Epoch: 38/200
Train loss: 2.238057
Epoch: 39/200
Train loss: 2.307587
Epoch: 40/200
Train loss: 2.253945
Epoch: 41/200
Train loss: 2.292514
Epoch: 42/200
Train loss: 2.216203
Epoch: 43/200
Train loss: 2.192461
Epoch: 44/200
Train loss: 2.312190
Epoch: 45/200
Train loss: 2.215437
Epoch: 46/200
Train loss: 2.138273
Epoch: 47/200
Train loss: 2.131958
Epoch: 48/200
Train loss: 2.119557
Epoch: 49/200
Train loss: 2.306324
Epoch: 50/200
Train loss: 2.136530
Epoch: 51/200
Train loss: 2.336679
Epoch: 52/200
Train loss: 2.280211
Epoch: 53/200
Train loss: 2.049548
Epoch: 54/200
Train loss: 2.141809
Epoch: 55/200
Train loss: 2.207033
Epoch: 56/200
Train loss: 2.184625
Epoch: 57/200
Train loss: 2.002922
Epoch: 58/200
Train loss: 1.998597
Epoch: 59/200
Train loss: 2.021893
Epoch: 60/200
Train loss: 1.923638
Epoch: 61/200
Train loss: 2.062240
Epoch: 62/200
Train loss: 1.964105
Epoch: 63/200
Train loss: 2.158975
Epoch: 64/200
Train loss: 2.132812
Epoch: 65/200
Train loss: 1.925960
Epoch: 66/200
Train loss: 2.125259
Epoch: 67/200
Train loss: 1.992085
Epoch: 68/200
Train loss: 2.183054
Epoch: 69/200
Train loss: 1.965754
Epoch: 70/200
Train loss: 2.130494
Epoch: 71/200
Train loss: 2.075706
Epoch: 72/200
Train loss: 2.151017
Epoch: 73/200
Train loss: 2.013969
Epoch: 74/200
Train loss: 2.016781
Epoch: 75/200
Train loss: 2.034983
Epoch: 76/200
Train loss: 1.985975
Epoch: 77/200
Train loss: 1.836641
Epoch: 78/200
Train loss: 1.838302
Epoch: 79/200
Train loss: 1.909998
Epoch: 80/200
Train loss: 1.984154
Epoch: 81/200
Train loss: 2.054416
Epoch: 82/200
Train loss: 1.881171
Epoch: 83/200
Train loss: 1.908233
Epoch: 84/200
Train loss: 1.918681
Epoch: 85/200
Train loss: 2.024159
Epoch: 86/200
Train loss: 2.070908
Epoch: 87/200
Train loss: 1.700019
Epoch: 88/200
Train loss: 1.775791
Epoch: 89/200
Train loss: 1.995881
Epoch: 90/200
Train loss: 1.931368
Epoch: 91/200
Train loss: 1.982188
Epoch: 92/200
Train loss: 2.003055
Epoch: 93/200
Train loss: 2.020963
Epoch: 94/200
Train loss: 1.694522
Epoch: 95/200
Train loss: 1.905135
Epoch: 96/200
Train loss: 2.022757
Epoch: 97/200
Train loss: 1.916521
Epoch: 98/200
Train loss: 1.800980
Epoch: 99/200
Train loss: 1.902971
Epoch: 100/200
Train loss: 1.942737
Epoch: 101/200
Train loss: 1.800870
Epoch: 102/200
Train loss: 1.840129
Time needed: 172.44480681419373 for validation audios
0.8028388889748862
Model improve: 0.0000 -> 0.8028
Epoch: 103/200
Train loss: 1.780401
Time needed: 166.53954124450684 for validation audios
0.8043287989226209
Model improve: 0.8028 -> 0.8043
Epoch: 104/200
Train loss: 1.962664
Time needed: 167.13928055763245 for validation audios
0.8040253794631276
Epoch: 105/200
Date :05/03/2023, 15:21:46
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 2
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 34.730725
Epoch: 2/200
Train loss: 5.702686
Epoch: 3/200
Train loss: 5.272428
Epoch: 4/200
Date :05/03/2023, 15:25:36
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 1.0
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 3
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 34.017811
Epoch: 2/200
Train loss: 5.689733
Epoch: 3/200
Date :05/03/2023, 15:29:05
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 3
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 34.044031
Epoch: 2/200
Train loss: 5.643567
Epoch: 3/200
Date :05/03/2023, 15:32:12
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 3
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 35.197388
Epoch: 2/200
Train loss: 5.667630
Epoch: 3/200
Date :05/03/2023, 15:35:33
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 1.0
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Date :05/03/2023, 15:35:54
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 1.0
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 35.024862
Epoch: 2/200
Train loss: 5.600435
Epoch: 3/200
Train loss: 5.203808
Epoch: 4/200
Train loss: 4.821506
Epoch: 5/200
Train loss: 4.519970
Epoch: 6/200
Train loss: 4.305764
Epoch: 7/200
Train loss: 4.024615
Epoch: 8/200
Train loss: 3.932128
Epoch: 9/200
Train loss: 3.771263
Epoch: 10/200
Train loss: 3.774291
Epoch: 11/200
Train loss: 3.585633
Epoch: 12/200
Train loss: 3.537073
Epoch: 13/200
Train loss: 3.496376
Epoch: 14/200
Train loss: 3.415430
Epoch: 15/200
Train loss: 3.245977
Epoch: 16/200
Train loss: 3.220648
Epoch: 17/200
Train loss: 3.343542
Epoch: 18/200
Train loss: 3.240871
Epoch: 19/200
Train loss: 3.145866
Epoch: 20/200
Train loss: 3.097434
Epoch: 21/200
Train loss: 3.106504
Epoch: 22/200
Train loss: 3.006561
Epoch: 23/200
Train loss: 3.114377
Epoch: 24/200
Train loss: 2.989869
Epoch: 25/200
Train loss: 2.807266
Epoch: 26/200
Train loss: 2.799230
Epoch: 27/200
Train loss: 3.021738
Epoch: 28/200
Train loss: 2.898066
Epoch: 29/200
Train loss: 2.800648
Epoch: 30/200
Train loss: 2.875865
Epoch: 31/200
Train loss: 2.815467
Epoch: 32/200
Train loss: 2.835780
Epoch: 33/200
Train loss: 2.841386
Epoch: 34/200
Train loss: 2.838172
Epoch: 35/200
Train loss: 2.787133
Epoch: 36/200
Train loss: 2.819422
Epoch: 37/200
Train loss: 2.717533
Epoch: 38/200
Train loss: 2.710957
Epoch: 39/200
Train loss: 2.775921
Epoch: 40/200
Train loss: 2.753835
Epoch: 41/200
Train loss: 2.840974
Epoch: 42/200
Train loss: 2.502566
Epoch: 43/200
Train loss: 2.572446
Epoch: 44/200
Train loss: 2.525636
Epoch: 45/200
Train loss: 2.551539
Epoch: 46/200
Train loss: 2.471920
Epoch: 47/200
Train loss: 2.634818
Epoch: 48/200
Train loss: 2.559117
Epoch: 49/200
Train loss: 2.626476
Epoch: 50/200
Train loss: 2.571116
Epoch: 51/200
Train loss: 2.544398
Epoch: 52/200
Train loss: 2.590747
Epoch: 53/200
Train loss: 2.630739
Epoch: 54/200
Train loss: 2.458452
Epoch: 55/200
Train loss: 2.586415
Epoch: 56/200
Train loss: 2.601653
Epoch: 57/200
Train loss: 2.538088
Epoch: 58/200
Train loss: 2.454280
Epoch: 59/200
Train loss: 2.523410
Epoch: 60/200
Train loss: 2.516104
Epoch: 61/200
Train loss: 2.543164
Epoch: 62/200
Train loss: 2.420676
Epoch: 63/200
Train loss: 2.489635
Epoch: 64/200
Train loss: 2.412298
Epoch: 65/200
Train loss: 2.513741
Epoch: 66/200
Train loss: 2.433138
Epoch: 67/200
Train loss: 2.503931
Epoch: 68/200
Train loss: 2.492310
Epoch: 69/200
Train loss: 2.446188
Epoch: 70/200
Train loss: 2.343550
Epoch: 71/200
Train loss: 2.477064
Epoch: 72/200
Train loss: 2.318626
Epoch: 73/200
Train loss: 2.350119
Epoch: 74/200
Train loss: 2.180844
Epoch: 75/200
Train loss: 2.283320
Epoch: 76/200
Train loss: 2.216778
Epoch: 77/200
Train loss: 2.316508
Epoch: 78/200
Train loss: 2.372131
Epoch: 79/200
Train loss: 2.385820
Epoch: 80/200
Train loss: 2.395847
Epoch: 81/200
Train loss: 2.444231
Epoch: 82/200
Train loss: 2.399751
Epoch: 83/200
Train loss: 2.360597
Epoch: 84/200
Train loss: 2.387269
Epoch: 85/200
Train loss: 2.390824
Epoch: 86/200
Train loss: 2.325628
Epoch: 87/200
Train loss: 2.241053
Epoch: 88/200
Train loss: 2.381756
Epoch: 89/200
Train loss: 2.377818
Epoch: 90/200
Train loss: 2.183843
Epoch: 91/200
Train loss: 2.329705
Epoch: 92/200
Train loss: 2.271388
Epoch: 93/200
Train loss: 2.210328
Epoch: 94/200
Train loss: 2.281682
Epoch: 95/200
Train loss: 2.321314
Epoch: 96/200
Train loss: 2.295776
Epoch: 97/200
Train loss: 2.303381
Epoch: 98/200
Train loss: 2.262107
Epoch: 99/200
Train loss: 2.181696
Epoch: 100/200
Train loss: 2.300755
Epoch: 101/200
Train loss: 2.211455
Epoch: 102/200
Train loss: 2.287665
Epoch: 103/200
Train loss: 2.323913
Epoch: 104/200
Train loss: 2.160502
Epoch: 105/200
Train loss: 2.243113
Epoch: 106/200
Train loss: 2.223651
Epoch: 107/200
Train loss: 2.261029
Epoch: 108/200
Train loss: 2.243284
Epoch: 109/200
Train loss: 2.348053
Epoch: 110/200
Train loss: 2.244368
Epoch: 111/200
Train loss: 2.234884
Epoch: 112/200
Train loss: 2.204974
Epoch: 113/200
Train loss: 2.243775
Epoch: 114/200
Train loss: 2.111679
Epoch: 115/200
Train loss: 2.204744
Epoch: 116/200
Train loss: 2.261740
Epoch: 117/200
Train loss: 2.197251
Epoch: 118/200
Train loss: 2.273156
Epoch: 119/200
Train loss: 2.097117
Epoch: 120/200
Train loss: 2.160631
Epoch: 121/200
Train loss: 2.247383
Epoch: 122/200
Train loss: 2.217182
Time needed: 165.33044123649597 for validation audios
0.8016310795109955
Model improve: 0.0000 -> 0.8016
Epoch: 123/200
Train loss: 2.301874
Time needed: 164.6651210784912 for validation audios
0.8009201433444891
Epoch: 124/200
Train loss: 2.180543
Time needed: 168.9157636165619 for validation audios
0.8012828361470838
Epoch: 125/200
Train loss: 2.210328
Time needed: 164.4959237575531 for validation audios
0.8035657296582234
Model improve: 0.8016 -> 0.8036
Epoch: 126/200
Train loss: 2.253104
Time needed: 164.6239607334137 for validation audios
0.8026406125384695
Epoch: 127/200
Train loss: 2.127553
Time needed: 167.8160367012024 for validation audios
0.8029990478796557
Epoch: 128/200
Train loss: 2.203610
Time needed: 167.6585249900818 for validation audios
0.8050814718079217
Model improve: 0.8036 -> 0.8051
Epoch: 129/200
Train loss: 2.194559
Time needed: 165.69626212120056 for validation audios
0.8032898066343724
Epoch: 130/200
Train loss: 2.286800
Time needed: 170.81696844100952 for validation audios
0.8039516172440113
Epoch: 131/200
Train loss: 2.056290
Time needed: 164.54587864875793 for validation audios
0.8039765626541566
Epoch: 132/200
Train loss: 2.188639
Time needed: 166.23636722564697 for validation audios
0.8025996839518155
Epoch: 133/200
Train loss: 2.082333
Time needed: 166.03302335739136 for validation audios
0.8045465825641502
Epoch: 134/200
Train loss: 2.111871
Time needed: 167.68193292617798 for validation audios
0.803582841713192
Epoch: 135/200
Train loss: 2.161079
Time needed: 167.79159212112427 for validation audios
0.8040758208687548
Epoch: 136/200
Train loss: 2.286325
Time needed: 169.0092191696167 for validation audios
0.8050638810753774
Epoch: 137/200
Train loss: 2.236385
Time needed: 166.79497385025024 for validation audios
0.8051659153016463
Model improve: 0.8051 -> 0.8052
Epoch: 138/200
Train loss: 2.153649
Time needed: 165.70676398277283 for validation audios
0.8055874334152641
Model improve: 0.8052 -> 0.8056
Epoch: 139/200
Train loss: 2.153691
Time needed: 165.95038270950317 for validation audios
0.803119795063691
Epoch: 140/200
Train loss: 2.150272
Time needed: 166.34064245224 for validation audios
0.8020612283712393
Epoch: 141/200
Train loss: 2.261161
Time needed: 165.5259232521057 for validation audios
0.804238575535197
Epoch: 142/200
Train loss: 2.215089
Time needed: 165.98479962348938 for validation audios
0.8036833500229862
Epoch: 143/200
Train loss: 2.179524
Time needed: 165.49140763282776 for validation audios
0.8015290444241464
Epoch: 144/200
Train loss: 2.169592
Time needed: 168.24196076393127 for validation audios
0.8047179243449446
Epoch: 145/200
Train loss: 2.290725
Time needed: 165.09586811065674 for validation audios
0.8046954156987018
Epoch: 146/200
Train loss: 2.178582
Time needed: 165.7732241153717 for validation audios
0.8034315859261327
Epoch: 147/200
Train loss: 2.147345
Time needed: 165.0186631679535 for validation audios
0.8048178442908199
Epoch: 148/200
Train loss: 2.196777
Time needed: 166.4766411781311 for validation audios
0.8048560910700814
Epoch: 149/200
Train loss: 2.186290
Time needed: 165.43679523468018 for validation audios
0.8038853838519131
Epoch: 150/200
Train loss: 2.277209
Time needed: 166.0737714767456 for validation audios
0.8046710425046028
Epoch: 151/200
Train loss: 2.207320
Time needed: 164.97905898094177 for validation audios
0.8042925008517189
Epoch: 152/200
Train loss: 2.139392
Time needed: 164.6739685535431 for validation audios
0.8063439407138159
Model improve: 0.8056 -> 0.8063
Epoch: 153/200
Train loss: 2.192538
Time needed: 165.3743507862091 for validation audios
0.8030297496080089
Epoch: 154/200
Train loss: 2.073668
Time needed: 164.16016626358032 for validation audios
0.8068882359928264
Model improve: 0.8063 -> 0.8069
Epoch: 155/200
Train loss: 2.146391
Time needed: 163.92437291145325 for validation audios
0.8055666864395552
Epoch: 156/200
Train loss: 2.178725
Time needed: 163.05355143547058 for validation audios
0.8065738184688054
Epoch: 157/200
Train loss: 2.194870
Time needed: 164.82257223129272 for validation audios
0.8034282555641371
Epoch: 158/200
Train loss: 2.204301
Time needed: 166.03523898124695 for validation audios
0.8046444498026959
Epoch: 159/200
Train loss: 2.028999
Time needed: 166.079434633255 for validation audios
0.806104500521006
Epoch: 160/200
Train loss: 2.214891
Time needed: 166.7051763534546 for validation audios
0.8037829117142841
Epoch: 161/200
Train loss: 2.136014
Time needed: 164.03693151474 for validation audios
0.8057692922003993
Epoch: 162/200
Train loss: 2.109940
Time needed: 166.87895345687866 for validation audios
0.8054164377428114
Epoch: 163/200
Train loss: 2.245819
Time needed: 167.86340308189392 for validation audios
0.8038628746810775
Epoch: 164/200
Train loss: 2.045631
Time needed: 164.39945340156555 for validation audios
0.8065348120643964
Epoch: 165/200
Train loss: 2.149727
Time needed: 164.0092897415161 for validation audios
0.8032935852802574
Epoch: 166/200
Train loss: 2.177307
Time needed: 164.29279828071594 for validation audios
0.8060385330404691
Epoch: 167/200
Train loss: 2.152166
Time needed: 167.07828760147095 for validation audios
0.8047192821917474
Epoch: 168/200
Train loss: 2.136312
Time needed: 167.16339802742004 for validation audios
0.8046998357573073
Epoch: 169/200
Train loss: 2.207263
Time needed: 164.00547623634338 for validation audios
0.8045987588237057
Epoch: 170/200
Train loss: 2.155187
Time needed: 164.89151883125305 for validation audios
0.8064087922805742
Epoch: 171/200
Train loss: 2.224529
Time needed: 164.9947681427002 for validation audios
0.8046670303148918
Epoch: 172/200
Train loss: 2.186037
Time needed: 164.94271183013916 for validation audios
0.8056232812930016
Epoch: 173/200
Train loss: 2.193256
Time needed: 166.19388818740845 for validation audios
0.8012986118931488
Epoch: 174/200
Train loss: 2.178650
Time needed: 165.61424207687378 for validation audios
0.8040915148369701
Epoch: 175/200
Train loss: 2.093975
Time needed: 163.4530472755432 for validation audios
0.8041582194904656
Epoch: 176/200
Train loss: 2.209306
Time needed: 165.4835422039032 for validation audios
0.8052155134884078
Epoch: 177/200
Train loss: 2.164887
Time needed: 167.47339916229248 for validation audios
0.8041075227153041
Epoch: 178/200
Train loss: 2.233309
Time needed: 165.8370611667633 for validation audios
0.8048819120726004
Epoch: 179/200
Train loss: 2.109594
Time needed: 164.69865584373474 for validation audios
0.8046832449305095
Epoch: 180/200
Train loss: 2.065671
Time needed: 165.2555410861969 for validation audios
0.8051065112445019
Epoch: 181/200
Train loss: 2.032189
Time needed: 166.34469652175903 for validation audios
0.8041339643426292
Epoch: 182/200
Train loss: 2.007320
Time needed: 170.3879177570343 for validation audios
0.8068559126703242
Epoch: 183/200
Train loss: 2.257026
Time needed: 165.1559658050537 for validation audios
0.8054398029291361
Epoch: 184/200
Train loss: 2.148423
Time needed: 167.14183068275452 for validation audios
0.8052121050096379
Epoch: 185/200
Train loss: 2.101947
Time needed: 170.19841718673706 for validation audios
0.8044387437657863
Epoch: 186/200
Train loss: 2.156981
Time needed: 166.2101948261261 for validation audios
0.803781340315668
Epoch: 187/200
Train loss: 2.157304
Time needed: 165.231098651886 for validation audios
0.8050100766226369
Epoch: 188/200
Train loss: 2.237998
Time needed: 163.99115633964539 for validation audios
0.8026705474656491
Epoch: 189/200
Train loss: 2.108941
Time needed: 167.95252680778503 for validation audios
0.8047235864033466
Epoch: 190/200
Train loss: 2.235857
Time needed: 164.49973344802856 for validation audios
0.8033545194955622
Epoch: 191/200
Train loss: 2.112289
Time needed: 163.58140683174133 for validation audios
0.8043880155824769
Epoch: 192/200
Train loss: 2.141741
Time needed: 165.50578236579895 for validation audios
0.8048526733791684
Epoch: 193/200
Train loss: 2.156734
Time needed: 166.33948683738708 for validation audios
0.8058742036085046
Epoch: 194/200
Train loss: 2.181183
Time needed: 169.1137180328369 for validation audios
0.8042170580452
Epoch: 195/200
Train loss: 2.101206
Time needed: 166.71030354499817 for validation audios
0.8051515539433522
Epoch: 196/200
Train loss: 2.184126
Time needed: 166.52132081985474 for validation audios
0.8023833358649988
Epoch: 197/200
Train loss: 2.184283
Time needed: 164.9253432750702 for validation audios
0.8036386678728892
Epoch: 198/200
Train loss: 2.234978
Time needed: 164.83127403259277 for validation audios
0.8037312321973875
Epoch: 199/200
Train loss: 2.142498
Time needed: 167.3222553730011 for validation audios
0.8053415495983433
Epoch: 200/200
Train loss: 2.110642
Time needed: 165.33040189743042 for validation audios
0.8055760133438106
Date :05/03/2023, 23:40:30
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.6
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 34.771720
Epoch: 2/200
Train loss: 5.562609
Epoch: 3/200
Train loss: 5.062329
Epoch: 4/200
Train loss: 4.673621
Epoch: 5/200
Train loss: 4.336676
Epoch: 6/200
Train loss: 4.123010
Epoch: 7/200
Train loss: 3.866963
Epoch: 8/200
Train loss: 3.739307
Epoch: 9/200
Train loss: 3.603795
Epoch: 10/200
Train loss: 3.437592
Epoch: 11/200
Train loss: 3.380997
Epoch: 12/200
Train loss: 3.409864
Epoch: 13/200
Train loss: 3.342718
Epoch: 14/200
Train loss: 3.207089
Epoch: 15/200
Train loss: 3.141745
Epoch: 16/200
Train loss: 2.951484
Epoch: 17/200
Train loss: 2.975539
Epoch: 18/200
Train loss: 3.004574
Epoch: 19/200
Train loss: 2.889339
Epoch: 20/200
Train loss: 2.898076
Epoch: 21/200
Train loss: 2.889067
Epoch: 22/200
Train loss: 2.940930
Epoch: 23/200
Train loss: 2.890601
Epoch: 24/200
Train loss: 2.674927
Epoch: 25/200
Train loss: 2.720932
Epoch: 26/200
Train loss: 2.676272
Epoch: 27/200
Train loss: 2.613995
Epoch: 28/200
Train loss: 2.745013
Epoch: 29/200
Train loss: 2.644828
Epoch: 30/200
Train loss: 2.753139
Epoch: 31/200
Train loss: 2.672838
Epoch: 32/200
Train loss: 2.660095
Epoch: 33/200
Train loss: 2.563242
Epoch: 34/200
Train loss: 2.550775
Epoch: 35/200
Train loss: 2.580485
Epoch: 36/200
Train loss: 2.486241
Epoch: 37/200
Train loss: 2.591715
Epoch: 38/200
Train loss: 2.471722
Epoch: 39/200
Train loss: 2.410740
Epoch: 40/200
Train loss: 2.527005
Epoch: 41/200
Train loss: 2.529826
Epoch: 42/200
Train loss: 2.318993
Epoch: 43/200
Train loss: 2.461765
Epoch: 44/200
Train loss: 2.391431
Epoch: 45/200
Train loss: 2.433192
Epoch: 46/200
Train loss: 2.415851
Epoch: 47/200
Train loss: 2.333487
Epoch: 48/200
Train loss: 2.372558
Epoch: 49/200
Train loss: 2.513491
Epoch: 50/200
Train loss: 2.393931
Epoch: 51/200
Train loss: 2.404582
Epoch: 52/200
Train loss: 2.249100
Epoch: 53/200
Train loss: 2.221007
Epoch: 54/200
Train loss: 2.230890
Epoch: 55/200
Train loss: 2.204924
Epoch: 56/200
Train loss: 2.196657
Epoch: 57/200
Train loss: 2.309874
Epoch: 58/200
Train loss: 2.282801
Epoch: 59/200
Train loss: 2.264601
Epoch: 60/200
Train loss: 2.389438
Epoch: 61/200
Train loss: 2.148918
Epoch: 62/200
Train loss: 2.190067
Epoch: 63/200
Train loss: 2.370500
Epoch: 64/200
Train loss: 2.133012
Epoch: 65/200
Train loss: 2.293468
Epoch: 66/200
Train loss: 2.378622
Epoch: 67/200
Train loss: 2.199891
Epoch: 68/200
Train loss: 2.232204
Epoch: 69/200
Train loss: 2.098133
Epoch: 70/200
Train loss: 2.176739
Epoch: 71/200
Train loss: 2.138273
Epoch: 72/200
Train loss: 2.122183
Epoch: 73/200
Train loss: 2.256677
Epoch: 74/200
Train loss: 2.075540
Epoch: 75/200
Train loss: 2.244630
Epoch: 76/200
Train loss: 1.909846
Epoch: 77/200
Train loss: 2.228194
Epoch: 78/200
Train loss: 2.207529
Epoch: 79/200
Train loss: 1.992042
Epoch: 80/200
Train loss: 2.030783
Epoch: 81/200
Train loss: 1.988913
Epoch: 82/200
Train loss: 2.162238
Epoch: 83/200
Train loss: 2.251460
Epoch: 84/200
Train loss: 2.289317
Epoch: 85/200
Train loss: 1.993854
Epoch: 86/200
Train loss: 1.993314
Epoch: 87/200
Train loss: 2.149933
Epoch: 88/200
Train loss: 2.154988
Epoch: 89/200
Train loss: 2.036577
Epoch: 90/200
Train loss: 2.076544
Epoch: 91/200
Train loss: 2.171783
Epoch: 92/200
Train loss: 1.977363
Epoch: 93/200
Train loss: 1.983766
Epoch: 94/200
Train loss: 1.929559
Epoch: 95/200
Train loss: 1.994549
Epoch: 96/200
Train loss: 2.051472
Epoch: 97/200
Train loss: 2.003482
Epoch: 98/200
Train loss: 1.853876
Epoch: 99/200
Train loss: 1.852154
Epoch: 100/200
Train loss: 2.142371
Epoch: 101/200
Train loss: 2.012120
Epoch: 102/200
Train loss: 2.126006
Time needed: 165.80035710334778 for validation audios
0.8029921694209267
Model improve: 0.0000 -> 0.8030
Epoch: 103/200
Train loss: 2.065181
Date :05/04/2023, 01:47:21
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 512
epochwarmup: 10
totalepoch: 200
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 174.442354
Epoch: 2/200
Train loss: 83.156395
Epoch: 3/200
Train loss: 9.780949
Epoch: 4/200
Train loss: 6.317481
Epoch: 5/200
Train loss: 5.792163
Epoch: 6/200
Train loss: 5.417281
Epoch: 7/200
Train loss: 5.076693
Epoch: 8/200
Train loss: 4.865440
Epoch: 9/200
Train loss: 4.615590
Epoch: 10/200
Train loss: 4.431372
Epoch: 11/200
Train loss: 4.272113
Epoch: 12/200
Train loss: 4.098090
Epoch: 13/200
Train loss: 3.989622
Epoch: 14/200
Train loss: 3.851954
Epoch: 15/200
Train loss: 3.820599
Epoch: 16/200
Train loss: 3.739755
Epoch: 17/200
Train loss: 3.602888
Epoch: 18/200
Train loss: 3.494439
Epoch: 19/200
Train loss: 3.651224
Epoch: 20/200
Train loss: 3.522897
Epoch: 21/200
Train loss: 3.378455
Epoch: 22/200
Train loss: 3.252311
Epoch: 23/200
Train loss: 3.271296
Epoch: 24/200
Train loss: 3.324901
Epoch: 25/200
Train loss: 3.281434
Epoch: 26/200
Train loss: 3.267374
Epoch: 27/200
Train loss: 3.240992
Epoch: 28/200
Train loss: 3.107854
Epoch: 29/200
Train loss: 3.100811
Epoch: 30/200
Train loss: 2.971548
Epoch: 31/200
Train loss: 3.079366
Epoch: 32/200
Train loss: 3.107792
Epoch: 33/200
Train loss: 2.919056
Epoch: 34/200
Train loss: 2.997980
Epoch: 35/200
Train loss: 2.976961
Epoch: 36/200
Train loss: 2.992944
Epoch: 37/200
Train loss: 2.894447
Epoch: 38/200
Train loss: 3.095445
Epoch: 39/200
Train loss: 2.790114
Epoch: 40/200
Train loss: 2.901186
Epoch: 41/200
Train loss: 2.839236
Epoch: 42/200
Train loss: 2.797677
Epoch: 43/200
Train loss: 2.909850
Epoch: 44/200
Train loss: 2.852151
Epoch: 45/200
Train loss: 2.875261
Epoch: 46/200
Train loss: 2.650225
Epoch: 47/200
Train loss: 2.824594
Epoch: 48/200
Train loss: 2.573725
Epoch: 49/200
Train loss: 2.560143
Epoch: 50/200
Train loss: 2.613443
Epoch: 51/200
Train loss: 2.641229
Epoch: 52/200
Train loss: 2.625283
Epoch: 53/200
Train loss: 2.548716
Epoch: 54/200
Train loss: 2.649312
Epoch: 55/200
Train loss: 2.624169
Epoch: 56/200
Train loss: 2.571701
Epoch: 57/200
Train loss: 2.612839
Epoch: 58/200
Train loss: 2.590100
Epoch: 59/200
Train loss: 2.576626
Epoch: 60/200
Train loss: 2.599382
Epoch: 61/200
Train loss: 2.488345
Epoch: 62/200
Train loss: 2.501946
Epoch: 63/200
Train loss: 2.509641
Epoch: 64/200
Train loss: 2.536700
Epoch: 65/200
Train loss: 2.570406
Epoch: 66/200
Train loss: 2.449026
Epoch: 67/200
Train loss: 2.525322
Epoch: 68/200
Train loss: 2.484408
Epoch: 69/200
Train loss: 2.641449
Epoch: 70/200
Train loss: 2.459268
Epoch: 71/200
Train loss: 2.318031
Epoch: 72/200
Train loss: 2.339777
Epoch: 73/200
Train loss: 2.453068
Epoch: 74/200
Train loss: 2.435105
Epoch: 75/200
Train loss: 2.474413
Epoch: 76/200
Train loss: 2.438439
Epoch: 77/200
Train loss: 2.415302
Epoch: 78/200
Train loss: 2.319191
Epoch: 79/200
Train loss: 2.184711
Epoch: 80/200
Train loss: 2.352356
Epoch: 81/200
Train loss: 2.364388
Epoch: 82/200
Train loss: 2.323660
Epoch: 83/200
Train loss: 2.294145
Epoch: 84/200
Train loss: 2.279348
Epoch: 85/200
Train loss: 2.410752
Epoch: 86/200
Train loss: 2.328962
Epoch: 87/200
Train loss: 2.208798
Epoch: 88/200
Train loss: 2.413373
Epoch: 89/200
Train loss: 2.426779
Epoch: 90/200
Train loss: 2.257331
Epoch: 91/200
Train loss: 2.367273
Epoch: 92/200
Train loss: 2.356694
Epoch: 93/200
Train loss: 2.384503
Epoch: 94/200
Train loss: 2.287602
Epoch: 95/200
Train loss: 2.344459
Epoch: 96/200
Train loss: 2.368445
Epoch: 97/200
Train loss: 2.310725
Epoch: 98/200
Train loss: 2.296705
Epoch: 99/200
Train loss: 2.215479
Epoch: 100/200
Train loss: 2.268126
Epoch: 101/200
Train loss: 2.317531
Epoch: 102/200
Train loss: 2.110251
Time needed: 168.2796447277069 for validation audios
0.8067059891408637
Model improve: 0.0000 -> 0.8067
Epoch: 103/200
Train loss: 2.169498
Time needed: 167.9885504245758 for validation audios
0.8033339914619702
Epoch: 104/200
Train loss: 2.198092
Date :05/04/2023, 03:55:46
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 34.411722
Epoch: 2/200
Train loss: 5.711993
Epoch: 3/200
Train loss: 5.307320
Epoch: 4/200
Train loss: 4.994330
Epoch: 5/200
Train loss: 4.634115
Epoch: 6/200
Train loss: 4.354757
Epoch: 7/200
Train loss: 4.032927
Epoch: 8/200
Train loss: 3.943569
Epoch: 9/200
Train loss: 3.780733
Epoch: 10/200
Train loss: 3.687218
Epoch: 11/200
Train loss: 3.619956
Epoch: 12/200
Train loss: 3.495996
Epoch: 13/200
Train loss: 3.435492
Epoch: 14/200
Train loss: 3.312593
Epoch: 15/200
Train loss: 3.347894
Epoch: 16/200
Train loss: 3.276642
Epoch: 17/200
Train loss: 3.173986
Epoch: 18/200
Train loss: 3.070212
Epoch: 19/200
Train loss: 3.260250
Epoch: 20/200
Train loss: 3.141855
Epoch: 21/200
Train loss: 3.006270
Epoch: 22/200
Train loss: 2.887820
Epoch: 23/200
Train loss: 2.910696
Epoch: 24/200
Train loss: 2.990226
Epoch: 25/200
Train loss: 2.949585
Epoch: 26/200
Train loss: 2.949098
Epoch: 27/200
Train loss: 2.927486
Epoch: 28/200
Train loss: 2.792440
Epoch: 29/200
Train loss: 2.799231
Epoch: 30/200
Train loss: 2.661236
Epoch: 31/200
Train loss: 2.795235
Epoch: 32/200
Train loss: 2.835685
Epoch: 33/200
Train loss: 2.630860
Epoch: 34/200
Train loss: 2.717768
Epoch: 35/200
Train loss: 2.713712
Epoch: 36/200
Train loss: 2.736873
Epoch: 37/200
Train loss: 2.639543
Epoch: 38/200
Train loss: 2.854982
Epoch: 39/200
Train loss: 2.544015
Epoch: 40/200
Train loss: 2.672838
Epoch: 41/200
Train loss: 2.608359
Epoch: 42/200
Train loss: 2.581763
Epoch: 43/200
Train loss: 2.685919
Epoch: 44/200
Train loss: 2.643122
Epoch: 45/200
Train loss: 2.667854
Epoch: 46/200
Train loss: 2.441363
Epoch: 47/200
Train loss: 2.626690
Epoch: 48/200
Train loss: 2.375970
Epoch: 49/200
Train loss: 2.367616
Epoch: 50/200
Train loss: 2.420402
Epoch: 51/200
Train loss: 2.457572
Epoch: 52/200
Train loss: 2.450792
Epoch: 53/200
Train loss: 2.369034
Epoch: 54/200
Train loss: 2.480173
Epoch: 55/200
Train loss: 2.452369
Epoch: 56/200
Train loss: 2.409085
Epoch: 57/200
Train loss: 2.465022
Epoch: 58/200
Train loss: 2.434312
Epoch: 59/200
Train loss: 2.422089
Epoch: 60/200
Train loss: 2.451055
Epoch: 61/200
Train loss: 2.341246
Epoch: 62/200
Train loss: 2.359327
Epoch: 63/200
Train loss: 2.365110
Epoch: 64/200
Train loss: 2.396493
Epoch: 65/200
Train loss: 2.434220
Epoch: 66/200
Train loss: 2.317310
Epoch: 67/200
Train loss: 2.396458
Epoch: 68/200
Train loss: 2.361536
Epoch: 69/200
Train loss: 2.528811
Epoch: 70/200
Train loss: 2.340923
Epoch: 71/200
Train loss: 2.198154
Epoch: 72/200
Train loss: 2.228196
Epoch: 73/200
Train loss: 2.339098
Epoch: 74/200
Train loss: 2.320432
Epoch: 75/200
Train loss: 2.366804
Epoch: 76/200
Train loss: 2.331938
Epoch: 77/200
Train loss: 2.312919
Epoch: 78/200
Train loss: 2.215791
Epoch: 79/200
Train loss: 2.082959
Epoch: 80/200
Train loss: 2.252348
Epoch: 81/200
Train loss: 2.269389
Epoch: 82/200
Train loss: 2.236010
Epoch: 83/200
Train loss: 2.203047
Epoch: 84/200
Train loss: 2.194468
Epoch: 85/200
Train loss: 2.323108
Epoch: 86/200
Train loss: 2.248355
Epoch: 87/200
Train loss: 2.123489
Epoch: 88/200
Train loss: 2.324223
Epoch: 89/200
Train loss: 2.347489
Epoch: 90/200
Train loss: 2.179479
Epoch: 91/200
Train loss: 2.292763
Epoch: 92/200
Train loss: 2.281476
Epoch: 93/200
Train loss: 2.307150
Epoch: 94/200
Train loss: 2.214855
Epoch: 95/200
Train loss: 2.271384
Epoch: 96/200
Train loss: 2.297977
Epoch: 97/200
Train loss: 2.243576
Epoch: 98/200
Train loss: 2.225836
Epoch: 99/200
Train loss: 2.152910
Epoch: 100/200
Train loss: 2.204677
Epoch: 101/200
Train loss: 2.248821
Epoch: 102/200
Train loss: 2.041892
Time needed: 208.13554692268372 for validation audios
0.8016162533849011
Model improve: 0.0000 -> 0.8016
Epoch: 103/200
Date :05/04/2023, 06:54:34
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 35.265443
Epoch: 2/200
Train loss: 5.579841
Epoch: 3/200
Train loss: 5.089957
Epoch: 4/200
Train loss: 4.781321
Epoch: 5/200
Train loss: 4.433061
Epoch: 6/200
Train loss: 4.185622
Epoch: 7/200
Train loss: 3.889838
Epoch: 8/200
Train loss: 3.831865
Epoch: 9/200
Train loss: 3.671455
Epoch: 10/200
Train loss: 3.600572
Epoch: 11/200
Train loss: 3.546104
Epoch: 12/200
Date :05/04/2023, 07:08:01
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 35.657442
Epoch: 2/200
Train loss: 5.602016
Epoch: 3/200
Date :05/04/2023, 07:10:59
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 34.994569
Epoch: 2/200
Train loss: 5.564823
Epoch: 3/200
Train loss: 5.056626
Epoch: 4/200
Train loss: 4.778146
Epoch: 5/200
Train loss: 4.441138
Epoch: 6/200
Train loss: 4.185479
Epoch: 7/200
Train loss: 3.879604
Epoch: 8/200
Train loss: 3.826222
Epoch: 9/200
Train loss: 3.668893
Epoch: 10/200
Train loss: 3.605806
Epoch: 11/200
Train loss: 3.531594
Epoch: 12/200
Train loss: 3.425083
Epoch: 13/200
Train loss: 3.378134
Epoch: 14/200
Train loss: 3.284883
Epoch: 15/200
Train loss: 3.285166
Epoch: 16/200
Train loss: 3.233766
Epoch: 17/200
Train loss: 3.107133
Epoch: 18/200
Train loss: 3.015028
Epoch: 19/200
Train loss: 3.224344
Epoch: 20/200
Train loss: 3.080661
Epoch: 21/200
Train loss: 2.960687
Epoch: 22/200
Train loss: 2.853386
Epoch: 23/200
Train loss: 2.880131
Epoch: 24/200
Train loss: 2.953456
Epoch: 25/200
Train loss: 2.931631
Epoch: 26/200
Train loss: 2.935793
Epoch: 27/200
Train loss: 2.886703
Epoch: 28/200
Train loss: 2.760137
Epoch: 29/200
Train loss: 2.784248
Epoch: 30/200
Train loss: 2.636895
Epoch: 31/200
Train loss: 2.772635
Epoch: 32/200
Train loss: 2.828993
Epoch: 33/200
Train loss: 2.614546
Epoch: 34/200
Train loss: 2.708962
Epoch: 35/200
Train loss: 2.695732
Epoch: 36/200
Train loss: 2.728742
Epoch: 37/200
Train loss: 2.609120
Epoch: 38/200
Train loss: 2.861230
Epoch: 39/200
Train loss: 2.533061
Epoch: 40/200
Train loss: 2.655528
Epoch: 41/200
Train loss: 2.599870
Epoch: 42/200
Train loss: 2.561750
Epoch: 43/200
Train loss: 2.660457
Epoch: 44/200
Train loss: 2.633769
Epoch: 45/200
Train loss: 2.656278
Epoch: 46/200
Train loss: 2.412255
Epoch: 47/200
Train loss: 2.589948
Epoch: 48/200
Train loss: 2.360556
Epoch: 49/200
Train loss: 2.349206
Epoch: 50/200
Train loss: 2.401216
Epoch: 51/200
Train loss: 2.449806
Epoch: 52/200
Train loss: 2.449102
Epoch: 53/200
Train loss: 2.354478
Epoch: 54/200
Train loss: 2.474446
Epoch: 55/200
Train loss: 2.433751
Epoch: 56/200
Train loss: 2.395644
Epoch: 57/200
Train loss: 2.424631
Epoch: 58/200
Train loss: 2.398942
Epoch: 59/200
Train loss: 2.408627
Epoch: 60/200
Train loss: 2.430630
Epoch: 61/200
Train loss: 2.335787
Epoch: 62/200
Train loss: 2.352852
Epoch: 63/200
Train loss: 2.364563
Epoch: 64/200
Train loss: 2.393276
Epoch: 65/200
Train loss: 2.429514
Epoch: 66/200
Train loss: 2.318135
Epoch: 67/200
Train loss: 2.392918
Epoch: 68/200
Train loss: 2.351246
Epoch: 69/200
Train loss: 2.514548
Epoch: 70/200
Train loss: 2.326634
Epoch: 71/200
Train loss: 2.192630
Epoch: 72/200
Train loss: 2.222122
Epoch: 73/200
Train loss: 2.326663
Epoch: 74/200
Train loss: 2.322121
Epoch: 75/200
Train loss: 2.376280
Epoch: 76/200
Train loss: 2.336616
Epoch: 77/200
Train loss: 2.300448
Epoch: 78/200
Train loss: 2.209054
Epoch: 79/200
Train loss: 2.060705
Epoch: 80/200
Train loss: 2.227370
Epoch: 81/200
Train loss: 2.247077
Epoch: 82/200
Train loss: 2.247841
Epoch: 83/200
Train loss: 2.200044
Epoch: 84/200
Train loss: 2.190214
Epoch: 85/200
Train loss: 2.322103
Epoch: 86/200
Train loss: 2.235057
Epoch: 87/200
Train loss: 2.122060
Epoch: 88/200
Train loss: 2.299740
Epoch: 89/200
Train loss: 2.336759
Epoch: 90/200
Train loss: 2.183040
Epoch: 91/200
Train loss: 2.294577
Epoch: 92/200
Train loss: 2.274253
Epoch: 93/200
Train loss: 2.308642
Epoch: 94/200
Train loss: 2.221827
Epoch: 95/200
Train loss: 2.262855
Epoch: 96/200
Train loss: 2.282009
Epoch: 97/200
Train loss: 2.244576
Epoch: 98/200
Train loss: 2.228139
Epoch: 99/200
Train loss: 2.168544
Epoch: 100/200
Train loss: 2.198398
Epoch: 101/200
Train loss: 2.248243
Epoch: 102/200
Train loss: 2.051493
Time needed: 166.7934970855713 for validation audios
0.8061480326628055
Model improve: 0.0000 -> 0.8061
Epoch: 103/200
Train loss: 2.104129
Time needed: 166.76426696777344 for validation audios
0.8044667706294479
Epoch: 104/200
Train loss: 2.113153
Time needed: 168.34289073944092 for validation audios
0.806132033866511
Epoch: 105/200
Train loss: 2.155972
Time needed: 166.05598711967468 for validation audios
0.8069744737836376
Model improve: 0.8061 -> 0.8070
Epoch: 106/200
Train loss: 2.173042
Time needed: 167.3710434436798 for validation audios
0.8055282147207464
Epoch: 107/200
Train loss: 2.228287
Time needed: 165.82379484176636 for validation audios
0.8047471604582261
Epoch: 108/200
Train loss: 2.203976
Time needed: 166.71191096305847 for validation audios
0.8048172299530924
Epoch: 109/200
Train loss: 2.181916
Time needed: 165.03656721115112 for validation audios
0.8058223981472769
Epoch: 110/200
Train loss: 2.227036
Time needed: 237.3144087791443 for validation audios
0.8037562837662954
Epoch: 111/200
Train loss: 2.043313
Time needed: 167.68254351615906 for validation audios
0.8058272002520204
Epoch: 112/200
Train loss: 2.145266
Time needed: 166.84613966941833 for validation audios
0.8027486165501658
Epoch: 113/200
Train loss: 2.130584
Time needed: 165.20566511154175 for validation audios
0.8054393633082206
Epoch: 114/200
Train loss: 2.246382
Time needed: 165.90356421470642 for validation audios
0.8053279788314748
Epoch: 115/200
Train loss: 2.134884
Time needed: 166.29891753196716 for validation audios
0.8073904154993248
Model improve: 0.8070 -> 0.8074
Epoch: 116/200
Train loss: 2.176121
Time needed: 165.75739645957947 for validation audios
0.8051205427155448
Epoch: 117/200
Train loss: 2.125924
Time needed: 166.4020290374756 for validation audios
0.8054420093267114
Epoch: 118/200
Train loss: 2.097093
Time needed: 165.9073121547699 for validation audios
0.8042972992828998
Epoch: 119/200
Train loss: 2.197159
Time needed: 166.59871077537537 for validation audios
0.8028335995235036
Epoch: 120/200
Train loss: 2.101334
Time needed: 166.45046758651733 for validation audios
0.8044556252806432
Epoch: 121/200
Train loss: 2.110007
Date :05/04/2023, 10:29:44
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 1
13839
Date :05/04/2023, 10:29:55
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 103.067435
Epoch: 2/200
Date :05/04/2023, 10:31:32
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Date :05/04/2023, 10:32:11
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 30.763174
Epoch: 2/200
Date :05/04/2023, 10:33:53
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 30.771244
Epoch: 2/200
Train loss: 6.749905
Epoch: 3/200
Train loss: 6.164810
Epoch: 4/200
Train loss: 6.014728
Time needed: 165.59665942192078 for validation audios
0.24213645434681202
Model improve: 0.0000 -> 0.2421
Epoch: 5/200
Date :05/04/2023, 10:41:32
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 48.963005
Epoch: 2/200
Date :05/04/2023, 10:43:25
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 104.830072
Epoch: 2/200
Date :05/04/2023, 10:45:13
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Date :05/04/2023, 10:47:06
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 35.099567
Epoch: 2/200
Train loss: 5.560993
Epoch: 3/200
Train loss: 5.048676
Epoch: 4/200
Train loss: 4.776300
Epoch: 5/200
Train loss: 4.445060
Epoch: 6/200
Train loss: 4.195263
Epoch: 7/200
Train loss: 3.889207
Epoch: 8/200
Train loss: 3.836433
Epoch: 9/200
Train loss: 3.678487
Epoch: 10/200
Train loss: 3.617539
Epoch: 11/200
Train loss: 3.542932
Epoch: 12/200
Train loss: 3.438491
Epoch: 13/200
Train loss: 3.391606
Epoch: 14/200
Train loss: 3.298737
Epoch: 15/200
Train loss: 3.298329
Epoch: 16/200
Train loss: 3.247111
Epoch: 17/200
Train loss: 3.120130
Epoch: 18/200
Train loss: 3.028822
Epoch: 19/200
Train loss: 3.238424
Epoch: 20/200
Train loss: 3.094629
Epoch: 21/200
Train loss: 2.973124
Epoch: 22/200
Train loss: 2.866776
Epoch: 23/200
Train loss: 2.894044
Epoch: 24/200
Train loss: 2.964718
Epoch: 25/200
Train loss: 2.944366
Epoch: 26/200
Train loss: 2.947670
Epoch: 27/200
Train loss: 2.898878
Epoch: 28/200
Train loss: 2.772050
Epoch: 29/200
Train loss: 2.795650
Epoch: 30/200
Train loss: 2.649618
Epoch: 31/200
Train loss: 2.782388
Epoch: 32/200
Train loss: 2.838725
Epoch: 33/200
Train loss: 2.625994
Epoch: 34/200
Train loss: 2.718852
Epoch: 35/200
Train loss: 2.706254
Epoch: 36/200
Train loss: 2.738200
Epoch: 37/200
Train loss: 2.619078
Epoch: 38/200
Train loss: 2.869747
Epoch: 39/200
Train loss: 2.542160
Epoch: 40/200
Train loss: 2.663616
Epoch: 41/200
Train loss: 2.608169
Epoch: 42/200
Train loss: 2.570397
Epoch: 43/200
Train loss: 2.668338
Epoch: 44/200
Train loss: 2.640872
Epoch: 45/200
Train loss: 2.662968
Epoch: 46/200
Train loss: 2.419671
Epoch: 47/200
Train loss: 2.596162
Epoch: 48/200
Train loss: 2.366803
Epoch: 49/200
Train loss: 2.356808
Epoch: 50/200
Train loss: 2.408924
Epoch: 51/200
Train loss: 2.457190
Epoch: 52/200
Train loss: 2.454420
Epoch: 53/200
Train loss: 2.360556
Epoch: 54/200
Train loss: 2.479998
Epoch: 55/200
Train loss: 2.440025
Epoch: 56/200
Train loss: 2.401508
Epoch: 57/200
Train loss: 2.428614
Epoch: 58/200
Train loss: 2.403393
Epoch: 59/200
Train loss: 2.413541
Epoch: 60/200
Train loss: 2.435808
Epoch: 61/200
Train loss: 2.339326
Epoch: 62/200
Train loss: 2.355903
Epoch: 63/200
Train loss: 2.367426
Epoch: 64/200
Train loss: 2.395125
Epoch: 65/200
Train loss: 2.432403
Epoch: 66/200
Train loss: 2.321586
Epoch: 67/200
Train loss: 2.395745
Epoch: 68/200
Train loss: 2.354490
Epoch: 69/200
Train loss: 2.517160
Epoch: 70/200
Train loss: 2.328947
Epoch: 71/200
Train loss: 2.194304
Epoch: 72/200
Train loss: 2.224093
Epoch: 73/200
Train loss: 2.328688
Epoch: 74/200
Train loss: 2.324099
Epoch: 75/200
Train loss: 2.377264
Epoch: 76/200
Train loss: 2.338689
Epoch: 77/200
Train loss: 2.301644
Epoch: 78/200
Train loss: 2.210381
Epoch: 79/200
Train loss: 2.062474
Epoch: 80/200
Train loss: 2.226228
Epoch: 81/200
Train loss: 2.247430
Epoch: 82/200
Train loss: 2.250410
Epoch: 83/200
Train loss: 2.201719
Epoch: 84/200
Train loss: 2.190777
Epoch: 85/200
Train loss: 2.322639
Epoch: 86/200
Train loss: 2.236065
Epoch: 87/200
Train loss: 2.121908
Epoch: 88/200
Train loss: 2.299416
Epoch: 89/200
Train loss: 2.336170
Epoch: 90/200
Train loss: 2.182121
Epoch: 91/200
Train loss: 2.292957
Epoch: 92/200
Train loss: 2.275058
Epoch: 93/200
Train loss: 2.308645
Epoch: 94/200
Train loss: 2.221930
Epoch: 95/200
Train loss: 2.262689
Epoch: 96/200
Train loss: 2.280348
Epoch: 97/200
Train loss: 2.244136
Epoch: 98/200
Train loss: 2.228015
Epoch: 99/200
Train loss: 2.166601
Epoch: 100/200
Train loss: 2.196990
Epoch: 101/200
Train loss: 2.246885
Epoch: 102/200
Train loss: 2.051386
Time needed: 167.2986958026886 for validation audios
0.8066415856658817
Model improve: 0.0000 -> 0.8066
Epoch: 103/200
Train loss: 2.103216
Time needed: 171.8463613986969 for validation audios
0.8047649569222123
Epoch: 104/200
Date :05/04/2023, 12:57:42
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 35.109298
Epoch: 2/200
Train loss: 5.559196
Epoch: 3/200
Train loss: 5.048197
Epoch: 4/200
Train loss: 4.774855
Epoch: 5/200
Train loss: 4.440319
Epoch: 6/200
Train loss: 4.188117
Epoch: 7/200
Train loss: 3.884698
Epoch: 8/200
Train loss: 3.832969
Epoch: 9/200
Train loss: 3.674786
Epoch: 10/200
Train loss: 3.613980
Epoch: 11/200
Train loss: 3.538741
Epoch: 12/200
Train loss: 3.433058
Epoch: 13/200
Train loss: 3.386318
Epoch: 14/200
Train loss: 3.293253
Epoch: 15/200
Train loss: 3.293257
Epoch: 16/200
Train loss: 3.241312
Epoch: 17/200
Train loss: 3.113741
Epoch: 18/200
Train loss: 3.021958
Epoch: 19/200
Train loss: 3.232415
Epoch: 20/200
Train loss: 3.089054
Epoch: 21/200
Train loss: 2.967867
Epoch: 22/200
Train loss: 2.860218
Epoch: 23/200
Train loss: 2.888143
Epoch: 24/200
Train loss: 2.960704
Epoch: 25/200
Train loss: 2.938979
Epoch: 26/200
Train loss: 2.943114
Epoch: 27/200
Train loss: 2.894420
Epoch: 28/200
Train loss: 2.766784
Epoch: 29/200
Train loss: 2.791675
Epoch: 30/200
Train loss: 2.644373
Epoch: 31/200
Train loss: 2.778026
Epoch: 32/200
Train loss: 2.834325
Epoch: 33/200
Train loss: 2.621547
Epoch: 34/200
Train loss: 2.714887
Epoch: 35/200
Train loss: 2.702085
Epoch: 36/200
Train loss: 2.734012
Epoch: 37/200
Train loss: 2.615184
Epoch: 38/200
Train loss: 2.867798
Epoch: 39/200
Train loss: 2.538105
Epoch: 40/200
Train loss: 2.659781
Epoch: 41/200
Train loss: 2.605645
Epoch: 42/200
Train loss: 2.567228
Epoch: 43/200
Train loss: 2.665894
Epoch: 44/200
Train loss: 2.638057
Epoch: 45/200
Train loss: 2.660131
Epoch: 46/200
Train loss: 2.417576
Epoch: 47/200
Train loss: 2.595257
Epoch: 48/200
Train loss: 2.363866
Epoch: 49/200
Train loss: 2.354889
Epoch: 50/200
Train loss: 2.407911
Epoch: 51/200
Train loss: 2.455225
Epoch: 52/200
Train loss: 2.452981
Epoch: 53/200
Train loss: 2.359427
Epoch: 54/200
Train loss: 2.478515
Epoch: 55/200
Train loss: 2.438172
Epoch: 56/200
Train loss: 2.400069
Epoch: 57/200
Train loss: 2.428650
Epoch: 58/200
Train loss: 2.403836
Epoch: 59/200
Train loss: 2.411874
Epoch: 60/200
Train loss: 2.434513
Epoch: 61/200
Train loss: 2.338190
Epoch: 62/200
Train loss: 2.356435
Epoch: 63/200
Train loss: 2.367549
Epoch: 64/200
Train loss: 2.395664
Epoch: 65/200
Train loss: 2.432897
Epoch: 66/200
Train loss: 2.321564
Epoch: 67/200
Train loss: 2.396078
Epoch: 68/200
Train loss: 2.353661
Epoch: 69/200
Train loss: 2.518299
Epoch: 70/200
Train loss: 2.330191
Epoch: 71/200
Train loss: 2.195104
Epoch: 72/200
Train loss: 2.224415
Epoch: 73/200
Train loss: 2.329625
Epoch: 74/200
Train loss: 2.326001
Epoch: 75/200
Train loss: 2.380599
Epoch: 76/200
Train loss: 2.340997
Epoch: 77/200
Train loss: 2.304010
Epoch: 78/200
Train loss: 2.212410
Epoch: 79/200
Train loss: 2.063310
Epoch: 80/200
Train loss: 2.228638
Epoch: 81/200
Train loss: 2.251075
Epoch: 82/200
Train loss: 2.251081
Epoch: 83/200
Train loss: 2.202709
Epoch: 84/200
Train loss: 2.191910
Epoch: 85/200
Train loss: 2.324922
Epoch: 86/200
Train loss: 2.237417
Epoch: 87/200
Train loss: 2.124713
Epoch: 88/200
Train loss: 2.302253
Epoch: 89/200
Train loss: 2.341230
Epoch: 90/200
Train loss: 2.185014
Epoch: 91/200
Train loss: 2.296263
Epoch: 92/200
Train loss: 2.276969
Epoch: 93/200
Train loss: 2.311928
Epoch: 94/200
Train loss: 2.223049
Epoch: 95/200
Train loss: 2.265349
Epoch: 96/200
Train loss: 2.285930
Epoch: 97/200
Train loss: 2.248039
Epoch: 98/200
Train loss: 2.229888
Epoch: 99/200
Train loss: 2.170152
Epoch: 100/200
Train loss: 2.200064
Epoch: 101/200
Train loss: 2.248862
Epoch: 102/200
Train loss: 2.053878
Epoch: 103/200
Train loss: 2.111390
Epoch: 104/200
Train loss: 2.104241
Epoch: 105/200
Train loss: 2.140637
Epoch: 106/200
Train loss: 2.179020
Epoch: 107/200
Train loss: 2.235170
Epoch: 108/200
Train loss: 2.202293
Epoch: 109/200
Train loss: 2.188787
Epoch: 110/200
Train loss: 2.253386
Epoch: 111/200
Train loss: 2.032210
Epoch: 112/200
Train loss: 2.154249
Epoch: 113/200
Train loss: 2.138423
Epoch: 114/200
Train loss: 2.246342
Epoch: 115/200
Train loss: 2.127108
Epoch: 116/200
Train loss: 2.180837
Epoch: 117/200
Train loss: 2.123605
Epoch: 118/200
Train loss: 2.086409
Epoch: 119/200
Train loss: 2.203954
Epoch: 120/200
Train loss: 2.119233
Epoch: 121/200
Train loss: 2.092384
Epoch: 122/200
Train loss: 2.179104
Epoch: 123/200
Train loss: 2.287494
Epoch: 124/200
Train loss: 2.296786
Epoch: 125/200
Train loss: 2.228933
Epoch: 126/200
Train loss: 2.070178
Epoch: 127/200
Train loss: 2.145929
Epoch: 128/200
Train loss: 2.020920
Epoch: 129/200
Train loss: 2.096150
Epoch: 130/200
Train loss: 1.980869
Epoch: 131/200
Train loss: 2.080875
Epoch: 132/200
Train loss: 2.220222
Time needed: 166.27170968055725 for validation audios
0.8052571393598509
Model improve: 0.0000 -> 0.8053
Epoch: 133/200
Train loss: 2.018239
Time needed: 170.63882970809937 for validation audios
0.8089371204018061
Model improve: 0.8053 -> 0.8089
Epoch: 134/200
Train loss: 2.140371
Time needed: 175.92136907577515 for validation audios
0.8075197458729642
Epoch: 135/200
Train loss: 2.003762
Time needed: 175.1011095046997 for validation audios
0.8086894428889178
Epoch: 136/200
Train loss: 2.082377
Time needed: 168.65011405944824 for validation audios
0.8079790867585006
Epoch: 137/200
Train loss: 2.104380
Time needed: 167.77904152870178 for validation audios
0.8047123862791148
Epoch: 138/200
Train loss: 2.000702
Time needed: 166.59414958953857 for validation audios
0.8091580065734982
Model improve: 0.8089 -> 0.8092
Epoch: 139/200
Train loss: 2.122971
Time needed: 165.13556218147278 for validation audios
0.807587912600888
Epoch: 140/200
Train loss: 2.078450
Time needed: 167.08162569999695 for validation audios
0.8072799345969554
Epoch: 141/200
Train loss: 2.083174
Time needed: 167.1657464504242 for validation audios
0.8080949036692603
Epoch: 142/200
Train loss: 2.037262
Time needed: 166.29005432128906 for validation audios
0.8082177847231161
Epoch: 143/200
Train loss: 2.187209
Time needed: 168.80402946472168 for validation audios
0.8069325829858092
Epoch: 144/200
Train loss: 2.137523
Time needed: 166.93046522140503 for validation audios
0.8088925891722977
Epoch: 145/200
Train loss: 2.003321
Time needed: 165.64581608772278 for validation audios
0.8081651181864505
Epoch: 146/200
Train loss: 2.161432
Time needed: 166.12635564804077 for validation audios
0.8074090154349304
Epoch: 147/200
Train loss: 2.147212
Time needed: 167.42971014976501 for validation audios
0.8082248067320458
Epoch: 148/200
Train loss: 2.219990
Time needed: 167.8522870540619 for validation audios
0.8076132886012048
Epoch: 149/200
Train loss: 2.082832
Time needed: 166.54255509376526 for validation audios
0.807938912777274
Epoch: 150/200
Train loss: 1.893142
Time needed: 169.20790219306946 for validation audios
0.8074598504967354
Epoch: 151/200
Train loss: 2.080501
Time needed: 165.0632767677307 for validation audios
0.8082400914061919
Epoch: 152/200
Train loss: 1.985557
Time needed: 166.41624855995178 for validation audios
0.8072741555325819
Epoch: 153/200
Train loss: 1.956594
Time needed: 166.98107838630676 for validation audios
0.8079356384284582
Epoch: 154/200
Train loss: 2.061668
Time needed: 172.60772800445557 for validation audios
0.8095399085623075
Model improve: 0.8092 -> 0.8095
Epoch: 155/200
Train loss: 2.125603
Time needed: 167.3456425666809 for validation audios
0.8062732232191988
Epoch: 156/200
Train loss: 2.092460
Time needed: 164.42860794067383 for validation audios
0.8081302137045224
Epoch: 157/200
Train loss: 2.082844
Time needed: 164.95578241348267 for validation audios
0.8063562083049769
Epoch: 158/200
Train loss: 1.914715
Time needed: 172.537846326828 for validation audios
0.8099743039046473
Model improve: 0.8095 -> 0.8100
Epoch: 159/200
Train loss: 2.126784
Time needed: 165.31504464149475 for validation audios
0.8073321051765203
Epoch: 160/200
Train loss: 1.952672
Time needed: 165.54368925094604 for validation audios
0.8094052598698019
Epoch: 161/200
Train loss: 2.185152
Time needed: 165.34081435203552 for validation audios
0.8087708892911292
Epoch: 162/200
Train loss: 2.056347
Time needed: 168.0898425579071 for validation audios
0.8072963885810027
Epoch: 163/200
Train loss: 2.036742
Time needed: 166.3183753490448 for validation audios
0.8085706488591302
Epoch: 164/200
Train loss: 2.160564
Time needed: 166.04717826843262 for validation audios
0.8058100517932907
Epoch: 165/200
Train loss: 2.160787
Time needed: 165.79143071174622 for validation audios
0.8062769813285999
Epoch: 166/200
Train loss: 2.126516
Time needed: 167.5948886871338 for validation audios
0.8087135635647442
Epoch: 167/200
Train loss: 2.171253
Time needed: 166.3125102519989 for validation audios
0.807286275023148
Epoch: 168/200
Train loss: 2.077506
Time needed: 166.6832549571991 for validation audios
0.8095405893624341
Epoch: 169/200
Train loss: 2.152962
Time needed: 168.50003623962402 for validation audios
0.8092718927926337
Epoch: 170/200
Train loss: 2.019619
Time needed: 168.85867071151733 for validation audios
0.8074556644816385
Epoch: 171/200
Train loss: 2.157066
Time needed: 167.1772654056549 for validation audios
0.8056983346799961
Epoch: 172/200
Train loss: 2.009897
Time needed: 165.03269171714783 for validation audios
0.8081578775532827
Epoch: 173/200
Train loss: 2.139888
Time needed: 166.18270611763 for validation audios
0.8083686324305194
Epoch: 174/200
Train loss: 2.004358
Time needed: 167.75197315216064 for validation audios
0.806389373469238
Epoch: 175/200
Train loss: 2.133520
Time needed: 166.94000554084778 for validation audios
0.8044859817027219
Epoch: 176/200
Train loss: 1.934644
Time needed: 167.65587902069092 for validation audios
0.8106623105236523
Model improve: 0.8100 -> 0.8107
Epoch: 177/200
Train loss: 2.011134
Time needed: 167.23776173591614 for validation audios
0.8080943694237323
Epoch: 178/200
Train loss: 2.019682
Time needed: 165.8621003627777 for validation audios
0.8090642088238611
Epoch: 179/200
Train loss: 1.984152
Time needed: 165.08997130393982 for validation audios
0.8099892237594114
Epoch: 180/200
Train loss: 2.175070
Time needed: 165.44376492500305 for validation audios
0.8064045739384849
Epoch: 181/200
Train loss: 2.040855
Time needed: 166.72579073905945 for validation audios
0.8097560297579155
Epoch: 182/200
Train loss: 1.953325
Time needed: 167.98509097099304 for validation audios
0.8078812892497798
Epoch: 183/200
Train loss: 2.012858
Time needed: 169.33789801597595 for validation audios
0.8100110733813446
Model improve: 0.8107 -> 0.8100
Epoch: 184/200
Train loss: 2.149706
Time needed: 166.66095352172852 for validation audios
0.807975615262788
Epoch: 185/200
Train loss: 2.029119
Time needed: 169.74381518363953 for validation audios
0.8104771388959697
Model improve: 0.8100 -> 0.8105
Epoch: 186/200
Train loss: 2.172993
Time needed: 167.7932596206665 for validation audios
0.8092068713247736
Epoch: 187/200
Train loss: 1.970005
Time needed: 165.56149125099182 for validation audios
0.8100899274548954
Model improve: 0.8105 -> 0.8101
Epoch: 188/200
Train loss: 2.186515
Time needed: 166.81330680847168 for validation audios
0.8073833896531644
Epoch: 189/200
Train loss: 2.135061
Time needed: 164.75739812850952 for validation audios
0.8071877394173584
Epoch: 190/200
Train loss: 2.028096
Time needed: 165.3827314376831 for validation audios
0.8110297897432345
Model improve: 0.8101 -> 0.8110
Epoch: 191/200
Train loss: 2.148623
Time needed: 166.64702582359314 for validation audios
0.8066681497481549
Epoch: 192/200
Train loss: 2.092819
Time needed: 165.5431091785431 for validation audios
0.8093397712032045
Epoch: 193/200
Train loss: 2.102443
Time needed: 164.8263566493988 for validation audios
0.8071673283458795
Epoch: 194/200
Train loss: 2.126028
Time needed: 163.6160228252411 for validation audios
0.8090122109733349
Epoch: 195/200
Train loss: 2.104929
Time needed: 168.95389437675476 for validation audios
0.8076204444188839
Epoch: 196/200
Train loss: 2.117109
Time needed: 171.57139897346497 for validation audios
0.8079013116455991
Epoch: 197/200
Train loss: 2.071051
Time needed: 167.42635488510132 for validation audios
0.8084188473167057
Epoch: 198/200
Train loss: 2.017609
Time needed: 165.796733379364 for validation audios
0.8077354387879421
Epoch: 199/200
Train loss: 2.127466
Time needed: 164.66289234161377 for validation audios
0.8067168733915184
Epoch: 200/200
Train loss: 2.060741
Time needed: 180.4148678779602 for validation audios
0.8080086766044268
Fold: 2
13807
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 34.435601
Epoch: 2/200
Train loss: 5.682254
Epoch: 3/200
Train loss: 5.248760
Epoch: 4/200
Train loss: 4.933252
Epoch: 5/200
Train loss: 4.563270
Epoch: 6/200
Train loss: 4.295767
Epoch: 7/200
Train loss: 4.022433
Epoch: 8/200
Train loss: 3.855583
Epoch: 9/200
Train loss: 3.840954
Epoch: 10/200
Train loss: 3.696142
Epoch: 11/200
Train loss: 3.606814
Epoch: 12/200
Train loss: 3.411283
Epoch: 13/200
Train loss: 3.371139
Epoch: 14/200
Train loss: 3.330956
Epoch: 15/200
Train loss: 3.230244
Epoch: 16/200
Train loss: 3.178917
Epoch: 17/200
Train loss: 3.132649
Epoch: 18/200
Train loss: 3.155938
Epoch: 19/200
Train loss: 3.122861
Epoch: 20/200
Train loss: 3.030179
Epoch: 21/200
Train loss: 2.987444
Epoch: 22/200
Train loss: 2.977297
Epoch: 23/200
Train loss: 2.942675
Epoch: 24/200
Train loss: 2.945117
Epoch: 25/200
Train loss: 2.975196
Epoch: 26/200
Train loss: 2.850715
Epoch: 27/200
Train loss: 2.877567
Epoch: 28/200
Train loss: 2.816138
Epoch: 29/200
Train loss: 2.713444
Epoch: 30/200
Train loss: 2.749655
Epoch: 31/200
Train loss: 2.725951
Epoch: 32/200
Train loss: 2.691233
Epoch: 33/200
Train loss: 2.657436
Epoch: 34/200
Train loss: 2.624655
Epoch: 35/200
Train loss: 2.600612
Epoch: 36/200
Train loss: 2.607409
Epoch: 37/200
Train loss: 2.761872
Epoch: 38/200
Train loss: 2.659051
Epoch: 39/200
Train loss: 2.537188
Epoch: 40/200
Train loss: 2.485724
Epoch: 41/200
Train loss: 2.533482
Epoch: 42/200
Train loss: 2.525387
Epoch: 43/200
Train loss: 2.598216
Epoch: 44/200
Train loss: 2.504353
Epoch: 45/200
Train loss: 2.666805
Epoch: 46/200
Train loss: 2.558784
Epoch: 47/200
Train loss: 2.519573
Epoch: 48/200
Train loss: 2.466035
Epoch: 49/200
Train loss: 2.572647
Epoch: 50/200
Train loss: 2.456674
Epoch: 51/200
Train loss: 2.420698
Epoch: 52/200
Train loss: 2.361960
Epoch: 53/200
Train loss: 2.514541
Epoch: 54/200
Train loss: 2.374308
Epoch: 55/200
Train loss: 2.411376
Epoch: 56/200
Train loss: 2.462049
Epoch: 57/200
Train loss: 2.516809
Epoch: 58/200
Train loss: 2.322999
Epoch: 59/200
Train loss: 2.351412
Epoch: 60/200
Train loss: 2.543632
Epoch: 61/200
Train loss: 2.282499
Epoch: 62/200
Train loss: 2.361172
Epoch: 63/200
Train loss: 2.452049
Epoch: 64/200
Train loss: 2.391121
Epoch: 65/200
Train loss: 2.319900
Epoch: 66/200
Train loss: 2.365607
Epoch: 67/200
Train loss: 2.305193
Epoch: 68/200
Train loss: 2.304892
Epoch: 69/200
Train loss: 2.045640
Epoch: 70/200
Train loss: 2.433570
Epoch: 71/200
Train loss: 2.222644
Epoch: 72/200
Train loss: 2.387133
Epoch: 73/200
Train loss: 2.430852
Epoch: 74/200
Train loss: 2.411138
Epoch: 75/200
Train loss: 2.157578
Epoch: 76/200
Train loss: 2.248600
Epoch: 77/200
Train loss: 2.383634
Epoch: 78/200
Train loss: 2.346382
Epoch: 79/200
Train loss: 2.329298
Epoch: 80/200
Train loss: 2.238007
Epoch: 81/200
Train loss: 2.160640
Epoch: 82/200
Train loss: 2.257956
Epoch: 83/200
Train loss: 2.350588
Epoch: 84/200
Train loss: 2.276047
Epoch: 85/200
Train loss: 2.236142
Epoch: 86/200
Train loss: 2.173869
Epoch: 87/200
Train loss: 2.328666
Epoch: 88/200
Train loss: 2.356994
Epoch: 89/200
Train loss: 2.213764
Epoch: 90/200
Train loss: 2.259079
Epoch: 91/200
Train loss: 2.370883
Epoch: 92/200
Train loss: 2.211963
Epoch: 93/200
Train loss: 2.267074
Epoch: 94/200
Train loss: 2.363609
Epoch: 95/200
Train loss: 2.124391
Epoch: 96/200
Train loss: 2.292449
Epoch: 97/200
Train loss: 2.176024
Epoch: 98/200
Train loss: 2.142532
Epoch: 99/200
Train loss: 2.158299
Epoch: 100/200
Train loss: 2.204058
Epoch: 101/200
Train loss: 2.073221
Epoch: 102/200
Train loss: 2.044361
Epoch: 103/200
Train loss: 2.114317
Epoch: 104/200
Train loss: 2.269692
Epoch: 105/200
Train loss: 2.082385
Epoch: 106/200
Train loss: 2.161223
Epoch: 107/200
Train loss: 2.129604
Epoch: 108/200
Train loss: 1.976852
Epoch: 109/200
Train loss: 2.136886
Epoch: 110/200
Train loss: 2.156451
Epoch: 111/200
Train loss: 2.231107
Epoch: 112/200
Train loss: 2.230180
Epoch: 113/200
Train loss: 2.102629
Epoch: 114/200
Train loss: 2.222073
Epoch: 115/200
Train loss: 2.042174
Epoch: 116/200
Train loss: 2.031413
Epoch: 117/200
Train loss: 2.125499
Epoch: 118/200
Train loss: 2.118862
Epoch: 119/200
Train loss: 2.152086
Epoch: 120/200
Train loss: 2.200689
Epoch: 121/200
Train loss: 2.133121
Epoch: 122/200
Train loss: 2.127412
Epoch: 123/200
Train loss: 2.176178
Epoch: 124/200
Train loss: 2.114146
Epoch: 125/200
Train loss: 2.229649
Epoch: 126/200
Train loss: 2.090577
Epoch: 127/200
Train loss: 2.044505
Epoch: 128/200
Train loss: 2.188710
Epoch: 129/200
Train loss: 2.148262
Epoch: 130/200
Train loss: 2.085375
Epoch: 131/200
Train loss: 2.039693
Epoch: 132/200
Train loss: 2.111116
Time needed: 169.86400961875916 for validation audios
0.7788014456574207
Model improve: 0.0000 -> 0.7788
Epoch: 133/200
Train loss: 2.090634
Time needed: 169.08224892616272 for validation audios
0.779586567477674
Model improve: 0.7788 -> 0.7796
Epoch: 134/200
Train loss: 2.066918
Time needed: 176.6155891418457 for validation audios
0.7842808258861537
Model improve: 0.7796 -> 0.7843
Epoch: 135/200
Train loss: 2.156065
Time needed: 174.34520196914673 for validation audios
0.7803688073349229
Epoch: 136/200
Train loss: 2.189485
Time needed: 169.6425371170044 for validation audios
0.780597227152058
Epoch: 137/200
Train loss: 2.161393
Time needed: 171.78656578063965 for validation audios
0.7826268436515261
Epoch: 138/200
Train loss: 2.158217
Time needed: 169.46127033233643 for validation audios
0.783775284677592
Epoch: 139/200
Train loss: 2.166970
Time needed: 170.7847340106964 for validation audios
0.776933045477527
Epoch: 140/200
Train loss: 2.116991
Time needed: 170.96289801597595 for validation audios
0.7831588485230697
Epoch: 141/200
Train loss: 2.028202
Time needed: 172.03309631347656 for validation audios
0.7812756041311776
Epoch: 142/200
Train loss: 1.955615
Time needed: 173.23827362060547 for validation audios
0.7833524412015181
Epoch: 143/200
Train loss: 2.189467
Time needed: 171.8206126689911 for validation audios
0.7832489230166989
Epoch: 144/200
Train loss: 1.974437
Time needed: 170.54495811462402 for validation audios
0.7837117843390002
Epoch: 145/200
Train loss: 2.121305
Time needed: 182.43668127059937 for validation audios
0.7842477672015296
Epoch: 146/200
Train loss: 2.082890
Date :05/04/2023, 23:40:16
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 0
13830
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 36.239865
Epoch: 2/200
Train loss: 5.611568
Epoch: 3/200
Train loss: 5.116972
Epoch: 4/200
Train loss: 4.825533
Epoch: 5/200
Train loss: 4.487850
Epoch: 6/200
Train loss: 4.205692
Epoch: 7/200
Train loss: 3.912705
Epoch: 8/200
Train loss: 3.849558
Epoch: 9/200
Train loss: 3.690500
Epoch: 10/200
Train loss: 3.600756
Epoch: 11/200
Train loss: 3.542058
Epoch: 12/200
Train loss: 3.432346
Epoch: 13/200
Train loss: 3.399154
Epoch: 14/200
Train loss: 3.258680
Epoch: 15/200
Train loss: 3.299403
Epoch: 16/200
Train loss: 3.242185
Epoch: 17/200
Train loss: 3.129179
Epoch: 18/200
Train loss: 3.029439
Epoch: 19/200
Train loss: 3.240423
Epoch: 20/200
Train loss: 3.113677
Epoch: 21/200
Train loss: 2.980096
Epoch: 22/200
Train loss: 2.848187
Epoch: 23/200
Train loss: 2.904176
Epoch: 24/200
Train loss: 2.956817
Epoch: 25/200
Train loss: 2.937312
Epoch: 26/200
Train loss: 2.925026
Epoch: 27/200
Train loss: 2.903133
Epoch: 28/200
Train loss: 2.751191
Epoch: 29/200
Train loss: 2.801346
Epoch: 30/200
Train loss: 2.636267
Epoch: 31/200
Train loss: 2.785395
Epoch: 32/200
Train loss: 2.821937
Epoch: 33/200
Train loss: 2.617193
Epoch: 34/200
Train loss: 2.707946
Epoch: 35/200
Train loss: 2.718471
Epoch: 36/200
Train loss: 2.733458
Epoch: 37/200
Train loss: 2.615960
Epoch: 38/200
Train loss: 2.855215
Epoch: 39/200
Train loss: 2.542431
Epoch: 40/200
Train loss: 2.657775
Epoch: 41/200
Train loss: 2.605244
Epoch: 42/200
Train loss: 2.585142
Epoch: 43/200
Train loss: 2.683611
Epoch: 44/200
Train loss: 2.632571
Epoch: 45/200
Train loss: 2.660402
Epoch: 46/200
Train loss: 2.414142
Epoch: 47/200
Train loss: 2.612066
Epoch: 48/200
Train loss: 2.336661
Epoch: 49/200
Train loss: 2.360459
Epoch: 50/200
Train loss: 2.420338
Epoch: 51/200
Train loss: 2.461146
Epoch: 52/200
Train loss: 2.442176
Epoch: 53/200
Train loss: 2.366282
Epoch: 54/200
Train loss: 2.460622
Epoch: 55/200
Train loss: 2.444078
Epoch: 56/200
Train loss: 2.432170
Epoch: 57/200
Train loss: 2.418627
Epoch: 58/200
Train loss: 2.434100
Epoch: 59/200
Train loss: 2.408464
Epoch: 60/200
Train loss: 2.445134
Epoch: 61/200
Train loss: 2.325538
Epoch: 62/200
Train loss: 2.338488
Epoch: 63/200
Train loss: 2.384966
Epoch: 64/200
Train loss: 2.383945
Epoch: 65/200
Train loss: 2.417554
Epoch: 66/200
Train loss: 2.297772
Epoch: 67/200
Train loss: 2.403405
Epoch: 68/200
Train loss: 2.368679
Epoch: 69/200
Train loss: 2.518521
Epoch: 70/200
Train loss: 2.310490
Epoch: 71/200
Train loss: 2.212771
Epoch: 72/200
Train loss: 2.205676
Epoch: 73/200
Train loss: 2.320193
Epoch: 74/200
Train loss: 2.326008
Epoch: 75/200
Train loss: 2.374917
Epoch: 76/200
Train loss: 2.341270
Epoch: 77/200
Train loss: 2.303752
Epoch: 78/200
Train loss: 2.225582
Epoch: 79/200
Train loss: 2.070050
Epoch: 80/200
Train loss: 2.230457
Epoch: 81/200
Train loss: 2.247377
Epoch: 82/200
Train loss: 2.249646
Epoch: 83/200
Train loss: 2.177899
Epoch: 84/200
Train loss: 2.171697
Epoch: 85/200
Train loss: 2.306700
Epoch: 86/200
Train loss: 2.245262
Epoch: 87/200
Train loss: 2.112349
Epoch: 88/200
Train loss: 2.317166
Epoch: 89/200
Train loss: 2.332922
Epoch: 90/200
Train loss: 2.189779
Epoch: 91/200
Train loss: 2.281964
Epoch: 92/200
Train loss: 2.274180
Epoch: 93/200
Train loss: 2.301878
Epoch: 94/200
Train loss: 2.199796
Epoch: 95/200
Train loss: 2.249817
Epoch: 96/200
Train loss: 2.297851
Epoch: 97/200
Train loss: 2.252305
Epoch: 98/200
Train loss: 2.221929
Epoch: 99/200
Train loss: 2.150655
Epoch: 100/200
Train loss: 2.214216
Epoch: 101/200
Train loss: 2.253972
Epoch: 102/200
Train loss: 2.041222
Epoch: 103/200
Train loss: 2.112383
Epoch: 104/200
Train loss: 2.093350
Epoch: 105/200
Train loss: 2.160630
Epoch: 106/200
Train loss: 2.170873
Epoch: 107/200
Train loss: 2.238395
Epoch: 108/200
Train loss: 2.209224
Epoch: 109/200
Train loss: 2.174061
Epoch: 110/200
Train loss: 2.253572
Epoch: 111/200
Train loss: 2.027955
Epoch: 112/200
Train loss: 2.121707
Epoch: 113/200
Train loss: 2.129623
Epoch: 114/200
Train loss: 2.232036
Epoch: 115/200
Train loss: 2.130286
Epoch: 116/200
Train loss: 2.164189
Epoch: 117/200
Train loss: 2.138212
Epoch: 118/200
Train loss: 2.106721
Epoch: 119/200
Train loss: 2.205809
Epoch: 120/200
Train loss: 2.094315
Epoch: 121/200
Train loss: 2.095402
Epoch: 122/200
Train loss: 2.170446
Time needed: 175.0180504322052 for validation audios
0.7887622823365509
Model improve: 0.0000 -> 0.7888
Epoch: 123/200
Train loss: 2.275128
Time needed: 173.4331977367401 for validation audios
0.7863848283711995
Model improve: 0.7888 -> 0.7864
Epoch: 124/200
Train loss: 2.288903
Time needed: 173.84624576568604 for validation audios
0.787517839969986
Model improve: 0.7864 -> 0.7875
Epoch: 125/200
Train loss: 2.238957
Time needed: 173.52961039543152 for validation audios
0.7900967638162636
Model improve: 0.7875 -> 0.7901
Epoch: 126/200
Train loss: 2.068567
Time needed: 172.94729685783386 for validation audios
0.7900362121498532
Model improve: 0.7901 -> 0.7900
Epoch: 127/200
Train loss: 2.148171
Time needed: 173.63092160224915 for validation audios
0.7871303021341852
Model improve: 0.7900 -> 0.7871
Epoch: 128/200
Train loss: 2.020449
Time needed: 172.96162462234497 for validation audios
0.7905069637324506
Model improve: 0.7871 -> 0.7905
Epoch: 129/200
Train loss: 2.074767
Time needed: 174.93677425384521 for validation audios
0.7914955769090839
Model improve: 0.7905 -> 0.7915
Epoch: 130/200
Train loss: 1.972601
Time needed: 173.16270780563354 for validation audios
0.7870565076159071
Model improve: 0.7915 -> 0.7871
Epoch: 131/200
Train loss: 2.084315
Time needed: 171.38537573814392 for validation audios
0.7898598183393549
Model improve: 0.7871 -> 0.7899
Epoch: 132/200
Train loss: 2.233760
Time needed: 173.3697428703308 for validation audios
0.7880543414879647
Model improve: 0.7899 -> 0.7881
Epoch: 133/200
Train loss: 2.021745
Time needed: 173.22240686416626 for validation audios
0.7901421059410245
Model improve: 0.7881 -> 0.7901
Epoch: 134/200
Train loss: 2.159174
Time needed: 173.726553440094 for validation audios
0.7897685419541468
Model improve: 0.7901 -> 0.7898
Epoch: 135/200
Train loss: 1.998352
Time needed: 172.36422967910767 for validation audios
0.7910180686112152
Model improve: 0.7898 -> 0.7910
Epoch: 136/200
Train loss: 2.077630
Time needed: 172.67950797080994 for validation audios
0.7897380452747386
Model improve: 0.7910 -> 0.7897
Epoch: 137/200
Train loss: 2.105354
Time needed: 173.32184720039368 for validation audios
0.787255253132239
Model improve: 0.7897 -> 0.7873
Epoch: 138/200
Train loss: 2.004442
Time needed: 174.70650577545166 for validation audios
0.7905424433389139
Model improve: 0.7873 -> 0.7905
Epoch: 139/200
Train loss: 2.153768
Time needed: 174.21771478652954 for validation audios
0.7885440790073476
Model improve: 0.7905 -> 0.7885
Epoch: 140/200
Train loss: 2.074911
Time needed: 173.32685256004333 for validation audios
0.7890606362299103
Model improve: 0.7885 -> 0.7891
Epoch: 141/200
Train loss: 2.086424
Time needed: 173.20424270629883 for validation audios
0.7895956066579604
Model improve: 0.7891 -> 0.7896
Epoch: 142/200
Train loss: 2.032267
Time needed: 174.0406837463379 for validation audios
0.7894436521319558
Model improve: 0.7896 -> 0.7894
Epoch: 143/200
Train loss: 2.177232
Time needed: 174.34581971168518 for validation audios
0.7890480138507848
Model improve: 0.7894 -> 0.7890
Epoch: 144/200
Train loss: 2.143372
Time needed: 172.58559775352478 for validation audios
0.7923329307047422
Model improve: 0.7890 -> 0.7923
Epoch: 145/200
Train loss: 2.015013
Time needed: 173.83134126663208 for validation audios
0.7920960644141241
Model improve: 0.7923 -> 0.7921
Epoch: 146/200
Train loss: 2.149035
Time needed: 173.42208218574524 for validation audios
0.7900775194755109
Model improve: 0.7921 -> 0.7901
Epoch: 147/200
Train loss: 2.151600
Time needed: 172.37934708595276 for validation audios
0.7891760420602306
Model improve: 0.7901 -> 0.7892
Epoch: 148/200
Train loss: 2.199690
Time needed: 173.21558332443237 for validation audios
0.7887147208577233
Model improve: 0.7892 -> 0.7887
Epoch: 149/200
Train loss: 2.072098
Time needed: 171.5898597240448 for validation audios
0.7890333333158513
Model improve: 0.7887 -> 0.7890
Epoch: 150/200
Train loss: 1.898623
Time needed: 173.66969108581543 for validation audios
0.7895663753961335
Model improve: 0.7890 -> 0.7896
Epoch: 151/200
Train loss: 2.089139
Time needed: 173.95634865760803 for validation audios
0.7893905723270933
Model improve: 0.7896 -> 0.7894
Epoch: 152/200
Train loss: 1.997705
Time needed: 172.54865908622742 for validation audios
0.789418632822188
Model improve: 0.7894 -> 0.7894
Epoch: 153/200
Train loss: 1.960785
Time needed: 171.39107513427734 for validation audios
0.7902673354467158
Model improve: 0.7894 -> 0.7903
Epoch: 154/200
Train loss: 2.067335
Time needed: 172.82859468460083 for validation audios
0.7904877261791018
Model improve: 0.7903 -> 0.7905
Epoch: 155/200
Train loss: 2.124815
Time needed: 172.33070707321167 for validation audios
0.7880834253848699
Model improve: 0.7905 -> 0.7881
Epoch: 156/200
Train loss: 2.100615
Time needed: 175.9625542163849 for validation audios
0.7892159394907993
Model improve: 0.7881 -> 0.7892
Epoch: 157/200
Train loss: 2.072895
Time needed: 172.5962164402008 for validation audios
0.7881378682425124
Model improve: 0.7892 -> 0.7881
Epoch: 158/200
Train loss: 1.905601
Time needed: 171.3088936805725 for validation audios
0.7922299606615091
Model improve: 0.7881 -> 0.7922
Epoch: 159/200
Train loss: 2.120312
Time needed: 170.62322068214417 for validation audios
0.7897576967491879
Model improve: 0.7922 -> 0.7898
Epoch: 160/200
Train loss: 1.947293
Time needed: 171.90674424171448 for validation audios
0.79175928829159
Model improve: 0.7898 -> 0.7918
Epoch: 161/200
Train loss: 2.195689
Time needed: 172.06269788742065 for validation audios
0.7903684438489883
Model improve: 0.7918 -> 0.7904
Epoch: 162/200
Train loss: 2.049989
Time needed: 172.39802837371826 for validation audios
0.7885756279239884
Model improve: 0.7904 -> 0.7886
Epoch: 163/200
Train loss: 2.016774
Time needed: 172.50130677223206 for validation audios
0.7901188610168529
Model improve: 0.7886 -> 0.7901
Epoch: 164/200
Train loss: 2.157466
Time needed: 171.64413285255432 for validation audios
0.7880237124578375
Model improve: 0.7901 -> 0.7880
Epoch: 165/200
Train loss: 2.164977
Time needed: 172.7430818080902 for validation audios
0.7873313936808146
Model improve: 0.7880 -> 0.7873
Epoch: 166/200
Train loss: 2.117347
Time needed: 172.08816742897034 for validation audios
0.790955978005655
Model improve: 0.7873 -> 0.7910
Epoch: 167/200
Train loss: 2.161719
Time needed: 175.6144609451294 for validation audios
0.7886507256721166
Model improve: 0.7910 -> 0.7887
Epoch: 168/200
Train loss: 2.088545
Time needed: 173.5777235031128 for validation audios
0.7913709978147325
Model improve: 0.7887 -> 0.7914
Epoch: 169/200
Train loss: 2.132978
Time needed: 173.77616953849792 for validation audios
0.7911468728782779
Model improve: 0.7914 -> 0.7911
Epoch: 170/200
Train loss: 2.023076
Time needed: 173.47277164459229 for validation audios
0.7888072116631354
Model improve: 0.7911 -> 0.7888
Epoch: 171/200
Train loss: 2.149906
Time needed: 172.37036967277527 for validation audios
0.7867336009466663
Model improve: 0.7888 -> 0.7867
Epoch: 172/200
Train loss: 2.009810
Time needed: 171.93917798995972 for validation audios
0.7901673728640303
Model improve: 0.7867 -> 0.7902
Epoch: 173/200
Train loss: 2.135891
Time needed: 174.1132423877716 for validation audios
0.7901660857789431
Model improve: 0.7902 -> 0.7902
Epoch: 174/200
Train loss: 2.018939
Time needed: 172.6730444431305 for validation audios
0.7888494705000249
Model improve: 0.7902 -> 0.7888
Epoch: 175/200
Train loss: 2.123474
Time needed: 173.70969915390015 for validation audios
0.7870195903169351
Model improve: 0.7888 -> 0.7870
Epoch: 176/200
Train loss: 1.927357
Time needed: 174.36984181404114 for validation audios
0.7931016047293814
Model improve: 0.7870 -> 0.7931
Epoch: 177/200
Train loss: 2.002457
Time needed: 173.55695343017578 for validation audios
0.7896631041199971
Model improve: 0.7931 -> 0.7897
Epoch: 178/200
Train loss: 2.028309
Time needed: 172.27126240730286 for validation audios
0.7923520655746057
Model improve: 0.7897 -> 0.7924
Epoch: 179/200
Train loss: 1.973250
Time needed: 175.3721935749054 for validation audios
0.7914475078901253
Model improve: 0.7924 -> 0.7914
Epoch: 180/200
Train loss: 2.164154
Time needed: 173.96480226516724 for validation audios
0.7881717247948659
Model improve: 0.7914 -> 0.7882
Epoch: 181/200
Train loss: 2.035249
Time needed: 176.09587597846985 for validation audios
0.7916872869061129
Model improve: 0.7882 -> 0.7917
Epoch: 182/200
Train loss: 1.954227
Time needed: 172.73551440238953 for validation audios
0.7903117870384215
Model improve: 0.7917 -> 0.7903
Epoch: 183/200
Train loss: 2.007091
Time needed: 170.6843502521515 for validation audios
0.7917892584614901
Model improve: 0.7903 -> 0.7918
Epoch: 184/200
Train loss: 2.162754
Time needed: 171.76691484451294 for validation audios
0.7895592982288332
Model improve: 0.7918 -> 0.7896
Epoch: 185/200
Train loss: 2.031738
Time needed: 176.64037418365479 for validation audios
0.7920511781680305
Model improve: 0.7896 -> 0.7921
Epoch: 186/200
Train loss: 2.174558
Time needed: 176.33931922912598 for validation audios
0.7898337895732646
Model improve: 0.7921 -> 0.7898
Epoch: 187/200
Train loss: 1.966351
Time needed: 175.17450714111328 for validation audios
0.7911387712096489
Model improve: 0.7898 -> 0.7911
Epoch: 188/200
Train loss: 2.177086
Time needed: 175.17576813697815 for validation audios
0.7897773813706339
Model improve: 0.7911 -> 0.7898
Epoch: 189/200
Train loss: 2.138575
Time needed: 174.72282576560974 for validation audios
0.7878116163981966
Model improve: 0.7898 -> 0.7878
Epoch: 190/200
Train loss: 2.012310
Time needed: 172.53899693489075 for validation audios
0.7926728556855541
Model improve: 0.7878 -> 0.7927
Epoch: 191/200
Train loss: 2.149417
Time needed: 171.6661422252655 for validation audios
0.7886264864107311
Model improve: 0.7927 -> 0.7886
Epoch: 192/200
Train loss: 2.073054
Time needed: 174.1641983985901 for validation audios
0.7909908177209743
Model improve: 0.7886 -> 0.7910
Epoch: 193/200
Train loss: 2.114082
Time needed: 175.12784457206726 for validation audios
0.7890832864605897
Model improve: 0.7910 -> 0.7891
Epoch: 194/200
Train loss: 2.113508
Time needed: 175.25770711898804 for validation audios
0.7915776652713782
Model improve: 0.7891 -> 0.7916
Epoch: 195/200
Train loss: 2.113145
Time needed: 172.83323621749878 for validation audios
0.789463751476851
Model improve: 0.7916 -> 0.7895
Epoch: 196/200
Train loss: 2.109888
Time needed: 173.9684042930603 for validation audios
0.7891174958656895
Model improve: 0.7895 -> 0.7891
Epoch: 197/200
Date :05/05/2023, 07:09:10
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 3
13816
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 35.617581
Epoch: 2/200
Train loss: 5.637967
Epoch: 3/200
Train loss: 5.189187
Epoch: 4/200
Train loss: 4.886096
Epoch: 5/200
Train loss: 4.564254
Epoch: 6/200
Train loss: 4.262962
Epoch: 7/200
Train loss: 3.932015
Epoch: 8/200
Train loss: 3.898032
Epoch: 9/200
Train loss: 3.671010
Epoch: 10/200
Train loss: 3.619665
Epoch: 11/200
Train loss: 3.527772
Epoch: 12/200
Train loss: 3.451391
Epoch: 13/200
Train loss: 3.375098
Epoch: 14/200
Train loss: 3.309586
Epoch: 15/200
Train loss: 3.275843
Epoch: 16/200
Train loss: 3.238867
Epoch: 17/200
Train loss: 3.117707
Epoch: 18/200
Train loss: 2.982354
Epoch: 19/200
Train loss: 3.224613
Epoch: 20/200
Train loss: 3.127620
Epoch: 21/200
Train loss: 2.980220
Epoch: 22/200
Train loss: 2.862818
Epoch: 23/200
Train loss: 2.913071
Epoch: 24/200
Train loss: 2.880733
Epoch: 25/200
Train loss: 2.986725
Epoch: 26/200
Train loss: 2.862183
Epoch: 27/200
Train loss: 2.874007
Epoch: 28/200
Train loss: 2.847514
Epoch: 29/200
Train loss: 2.725436
Epoch: 30/200
Train loss: 2.626341
Epoch: 31/200
Train loss: 2.740118
Epoch: 32/200
Train loss: 2.818032
Epoch: 33/200
Train loss: 2.600700
Epoch: 34/200
Train loss: 2.659812
Epoch: 35/200
Train loss: 2.710960
Epoch: 36/200
Train loss: 2.744617
Epoch: 37/200
Train loss: 2.570749
Epoch: 38/200
Train loss: 2.792389
Epoch: 39/200
Train loss: 2.665506
Epoch: 40/200
Train loss: 2.564568
Epoch: 41/200
Train loss: 2.689740
Epoch: 42/200
Train loss: 2.490426
Epoch: 43/200
Train loss: 2.666832
Epoch: 44/200
Train loss: 2.554925
Epoch: 45/200
Train loss: 2.617805
Epoch: 46/200
Train loss: 2.558480
Epoch: 47/200
Train loss: 2.538188
Epoch: 48/200
Train loss: 2.451701
Epoch: 49/200
Train loss: 2.344729
Epoch: 50/200
Train loss: 2.441468
Epoch: 51/200
Train loss: 2.352665
Epoch: 52/200
Train loss: 2.454685
Epoch: 53/200
Train loss: 2.317471
Epoch: 54/200
Train loss: 2.533706
Epoch: 55/200
Train loss: 2.429038
Epoch: 56/200
Train loss: 2.274476
Epoch: 57/200
Train loss: 2.488900
Epoch: 58/200
Train loss: 2.340879
Epoch: 59/200
Train loss: 2.479234
Epoch: 60/200
Train loss: 2.391181
Epoch: 61/200
Train loss: 2.420376
Epoch: 62/200
Train loss: 2.283151
Epoch: 63/200
Train loss: 2.311674
Epoch: 64/200
Train loss: 2.415804
Epoch: 65/200
Train loss: 2.328391
Epoch: 66/200
Train loss: 2.446532
Epoch: 67/200
Train loss: 2.253594
Epoch: 68/200
Train loss: 2.455789
Epoch: 69/200
Train loss: 2.346611
Epoch: 70/200
Train loss: 2.386226
Epoch: 71/200
Train loss: 2.283434
Epoch: 72/200
Train loss: 2.245297
Epoch: 73/200
Train loss: 2.122566
Epoch: 74/200
Train loss: 2.423654
Epoch: 75/200
Train loss: 2.394909
Epoch: 76/200
Train loss: 2.234603
Epoch: 77/200
Train loss: 2.327986
Epoch: 78/200
Train loss: 2.295233
Epoch: 79/200
Train loss: 2.139717
Epoch: 80/200
Train loss: 2.170171
Epoch: 81/200
Train loss: 2.233289
Epoch: 82/200
Train loss: 2.287472
Epoch: 83/200
Train loss: 2.140438
Epoch: 84/200
Train loss: 2.138329
Epoch: 85/200
Train loss: 2.289552
Epoch: 86/200
Train loss: 2.296168
Epoch: 87/200
Train loss: 2.264073
Epoch: 88/200
Train loss: 2.062577
Epoch: 89/200
Train loss: 2.322076
Epoch: 90/200
Train loss: 2.303301
Epoch: 91/200
Train loss: 2.262015
Epoch: 92/200
Train loss: 2.210786
Epoch: 93/200
Train loss: 2.310883
Epoch: 94/200
Train loss: 2.219804
Epoch: 95/200
Train loss: 2.250746
Epoch: 96/200
Train loss: 2.224296
Epoch: 97/200
Train loss: 2.295117
Epoch: 98/200
Train loss: 2.236596
Epoch: 99/200
Train loss: 2.237254
Epoch: 100/200
Train loss: 2.128655
Epoch: 101/200
Train loss: 2.241446
Epoch: 102/200
Train loss: 2.218007
Epoch: 103/200
Train loss: 2.024166
Epoch: 104/200
Train loss: 2.094055
Epoch: 105/200
Train loss: 2.103193
Epoch: 106/200
Train loss: 2.151252
Epoch: 107/200
Train loss: 2.165519
Epoch: 108/200
Train loss: 2.225847
Epoch: 109/200
Train loss: 2.191771
Epoch: 110/200
Train loss: 2.158250
Epoch: 111/200
Train loss: 2.242149
Epoch: 112/200
Train loss: 2.043756
Epoch: 113/200
Train loss: 2.103579
Epoch: 114/200
Train loss: 2.153918
Epoch: 115/200
Train loss: 2.189117
Epoch: 116/200
Train loss: 2.178380
Epoch: 117/200
Train loss: 2.158410
Epoch: 118/200
Train loss: 2.101776
Epoch: 119/200
Train loss: 2.063418
Epoch: 120/200
Train loss: 2.217041
Epoch: 121/200
Train loss: 2.116068
Epoch: 122/200
Train loss: 2.122709
Time needed: 171.30394530296326 for validation audios
0.7768762774399669
Model improve: 0.0000 -> 0.7769
Epoch: 123/200
Train loss: 2.164518
Time needed: 173.51527857780457 for validation audios
0.7790220405111222
Model improve: 0.7769 -> 0.7790
Epoch: 124/200
Train loss: 2.222768
Time needed: 171.1283094882965 for validation audios
0.7792022887197755
Model improve: 0.7790 -> 0.7792
Epoch: 125/200
Train loss: 2.276383
Time needed: 173.66226410865784 for validation audios
0.7825399148182551
Model improve: 0.7792 -> 0.7825
Epoch: 126/200
Train loss: 2.267190
Time needed: 172.56836986541748 for validation audios
0.7805557727677793
Epoch: 127/200
Train loss: 2.077639
Time needed: 173.15613722801208 for validation audios
0.7780552544537742
Epoch: 128/200
Train loss: 2.106657
Time needed: 172.2874367237091 for validation audios
0.7805791197833724
Epoch: 129/200
Train loss: 2.114868
Time needed: 170.7653205394745 for validation audios
0.778750419449619
Epoch: 130/200
Train loss: 2.019662
Time needed: 172.49846696853638 for validation audios
0.7800054570644329
Epoch: 131/200
Train loss: 1.922843
Time needed: 172.0229208469391 for validation audios
0.7802277721612896
Epoch: 132/200
Train loss: 2.148522
Time needed: 173.09141516685486 for validation audios
0.7787793583653239
Epoch: 133/200
Train loss: 2.195784
Time needed: 172.51983547210693 for validation audios
0.7785963393919825
Epoch: 134/200
Train loss: 2.053709
Time needed: 171.0466628074646 for validation audios
0.7799811561678819
Epoch: 135/200
Train loss: 2.081358
Time needed: 171.12929916381836 for validation audios
0.7780946024762848
Epoch: 136/200
Train loss: 2.088896
Time needed: 172.1744704246521 for validation audios
0.779317808686726
Epoch: 137/200
Train loss: 2.036906
Time needed: 172.76140236854553 for validation audios
0.7779838354085025
Epoch: 138/200
Train loss: 2.026161
Time needed: 175.90982913970947 for validation audios
0.7798323661281731
Epoch: 139/200
Train loss: 2.124982
Time needed: 173.4816370010376 for validation audios
0.7797765899976596
Epoch: 140/200
Train loss: 2.046642
Time needed: 175.47140860557556 for validation audios
0.777525850634212
Epoch: 141/200
Train loss: 2.041457
Time needed: 171.98023581504822 for validation audios
0.7797613094366048
Epoch: 142/200
Train loss: 2.127355
Time needed: 179.46203112602234 for validation audios
0.779300527413444
Epoch: 143/200
Train loss: 1.972370
Time needed: 171.3208770751953 for validation audios
0.7808301731850754
Epoch: 144/200
Train loss: 2.197846
Time needed: 176.4613995552063 for validation audios
0.7787504493990361
Epoch: 145/200
Train loss: 2.153973
Time needed: 172.66063356399536 for validation audios
0.7796109392671399
Epoch: 146/200
Train loss: 1.989207
Time needed: 172.63896250724792 for validation audios
0.7818656621506507
Epoch: 147/200
Train loss: 2.154912
Time needed: 172.12438130378723 for validation audios
0.7802314533816631
Epoch: 148/200
Train loss: 2.114495
Time needed: 176.73531222343445 for validation audios
0.7792122901338194
Epoch: 149/200
Train loss: 2.207831
Time needed: 171.26432943344116 for validation audios
0.7784213104304729
Epoch: 150/200
Train loss: 2.092716
Time needed: 171.87355184555054 for validation audios
0.7805470071651459
Epoch: 151/200
Train loss: 1.888231
Time needed: 171.7406928539276 for validation audios
0.7813050442189008
Epoch: 152/200
Train loss: 2.071949
Time needed: 173.28685569763184 for validation audios
0.7797168590918262
Epoch: 153/200
Train loss: 1.977496
Time needed: 173.4255986213684 for validation audios
0.7802841163473877
Epoch: 154/200
Train loss: 2.005538
Time needed: 171.52820491790771 for validation audios
0.7824254916318775
Epoch: 155/200
Train loss: 2.109273
Time needed: 171.27312302589417 for validation audios
0.7783270455482179
Epoch: 156/200
Train loss: 2.041594
Time needed: 173.13133549690247 for validation audios
0.7800898823147626
Epoch: 157/200
Train loss: 2.096661
Time needed: 172.40627670288086 for validation audios
0.7793682130739606
Epoch: 158/200
Train loss: 2.109904
Time needed: 171.52649331092834 for validation audios
0.7801209970987412
Epoch: 159/200
Train loss: 1.976593
Time needed: 174.055917263031 for validation audios
0.7797802031237481
Epoch: 160/200
Train loss: 1.968952
Time needed: 173.12145686149597 for validation audios
0.7806135128366877
Epoch: 161/200
Train loss: 2.109719
Time needed: 171.7688488960266 for validation audios
0.7802409704449959
Epoch: 162/200
Train loss: 2.104715
Time needed: 174.689062833786 for validation audios
0.7766095947669341
Epoch: 163/200
Train loss: 2.028644
Time needed: 173.23616886138916 for validation audios
0.780527041123276
Epoch: 164/200
Date :05/05/2023, 12:21:12
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 3
13816
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Train loss: 35.622882
Epoch: 2/100
Train loss: 5.639579
Epoch: 3/100
Train loss: 5.191400
Epoch: 4/100
Train loss: 4.889701
Epoch: 5/100
Train loss: 4.566670
Epoch: 6/100
Train loss: 4.267842
Epoch: 7/100
Train loss: 3.937098
Epoch: 8/100
Train loss: 3.903464
Epoch: 9/100
Train loss: 3.677459
Epoch: 10/100
Train loss: 3.626295
Epoch: 11/100
Train loss: 3.535009
Epoch: 12/100
Train loss: 3.458177
Epoch: 13/100
Train loss: 3.382262
Epoch: 14/100
Train loss: 3.316913
Epoch: 15/100
Train loss: 3.283860
Epoch: 16/100
Train loss: 3.246894
Epoch: 17/100
Train loss: 3.128157
Epoch: 18/100
Train loss: 2.992953
Epoch: 19/100
Train loss: 3.235454
Epoch: 20/100
Train loss: 3.139780
Epoch: 21/100
Train loss: 2.993273
Epoch: 22/100
Train loss: 2.876831
Epoch: 23/100
Train loss: 2.928319
Epoch: 24/100
Train loss: 2.895492
Epoch: 25/100
Train loss: 3.002546
Epoch: 26/100
Train loss: 2.879313
Epoch: 27/100
Train loss: 2.890774
Epoch: 28/100
Train loss: 2.865624
Epoch: 29/100
Train loss: 2.744405
Epoch: 30/100
Train loss: 2.646991
Epoch: 31/100
Train loss: 2.760446
Epoch: 32/100
Train loss: 2.840223
Epoch: 33/100
Train loss: 2.624222
Epoch: 34/100
Train loss: 2.682715
Epoch: 35/100
Train loss: 2.735106
Epoch: 36/100
Train loss: 2.769171
Epoch: 37/100
Train loss: 2.598323
Epoch: 38/100
Train loss: 2.817722
Epoch: 39/100
Train loss: 2.693390
Epoch: 40/100
Train loss: 2.593482
Epoch: 41/100
Train loss: 2.716569
Epoch: 42/100
Train loss: 2.520468
Epoch: 43/100
Train loss: 2.697413
Epoch: 44/100
Train loss: 2.586625
Epoch: 45/100
Train loss: 2.650208
Epoch: 46/100
Train loss: 2.592619
Epoch: 47/100
Train loss: 2.572463
Epoch: 48/100
Train loss: 2.488392
Epoch: 49/100
Train loss: 2.384279
Epoch: 50/100
Train loss: 2.481209
Epoch: 51/100
Train loss: 0.827171
Epoch: 52/100
Train loss: 0.793324
Epoch: 53/100
Train loss: 0.755161
Epoch: 54/100
Train loss: 0.747535
Epoch: 55/100
Train loss: 0.723771
Epoch: 56/100
Train loss: 0.713640
Epoch: 57/100
Train loss: 0.681817
Epoch: 58/100
Train loss: 0.670021
Epoch: 59/100
Train loss: 0.658075
Epoch: 60/100
Train loss: 0.667534
Epoch: 61/100
Train loss: 0.646692
Epoch: 62/100
Train loss: 0.631683
Time needed: 172.30926871299744 for validation audios
0.7729276214801449
Model improve: 0.0000 -> 0.7729
Epoch: 63/100
Train loss: 0.621344
Time needed: 173.51105308532715 for validation audios
0.7732888643204973
Model improve: 0.7729 -> 0.7733
Epoch: 64/100
Train loss: 0.613171
Time needed: 173.72258019447327 for validation audios
0.7726015790601427
Epoch: 65/100
Train loss: 0.575580
Time needed: 174.39317345619202 for validation audios
0.7733524607534736
Model improve: 0.7733 -> 0.7734
Epoch: 66/100
Date :05/05/2023, 13:53:45
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.5
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 3
13816
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Train loss: 35.633996
Epoch: 2/100
Train loss: 5.628421
Epoch: 3/100
Date :05/05/2023, 13:56:35
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 10
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.5
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 3
13816
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Train loss: 174.762831
Epoch: 2/100
Train loss: 83.252957
Epoch: 3/100
Train loss: 9.587032
Epoch: 4/100
Train loss: 6.326947
Epoch: 5/100
Train loss: 5.753889
Epoch: 6/100
Train loss: 5.387841
Epoch: 7/100
Date :05/05/2023, 14:05:30
Duration: 3
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.5
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 3
13816
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Date :05/05/2023, 14:07:51
Duration: 3
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.5
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 3
13816
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Train loss: 38.111879
Epoch: 2/100
Train loss: 5.708450
Epoch: 3/100
Train loss: 5.281389
Epoch: 4/100
Train loss: 4.936615
Epoch: 5/100
Train loss: 4.667386
Epoch: 6/100
Train loss: 4.401131
Epoch: 7/100
Train loss: 4.139752
Epoch: 8/100
Train loss: 4.069769
Epoch: 9/100
Train loss: 3.960228
Epoch: 10/100
Train loss: 3.789196
Epoch: 11/100
Train loss: 3.734000
Epoch: 12/100
Train loss: 3.528058
Epoch: 13/100
Train loss: 3.543715
Epoch: 14/100
Train loss: 3.629167
Epoch: 15/100
Train loss: 3.457815
Epoch: 16/100
Train loss: 3.407076
Epoch: 17/100
Train loss: 3.281585
Epoch: 18/100
Train loss: 3.183261
Epoch: 19/100
Train loss: 3.289716
Epoch: 20/100
Train loss: 3.168568
Epoch: 21/100
Train loss: 3.184947
Epoch: 22/100
Train loss: 3.234002
Epoch: 23/100
Train loss: 3.355022
Epoch: 24/100
Train loss: 3.117959
Epoch: 25/100
Train loss: 3.159175
Epoch: 26/100
Train loss: 3.084268
Epoch: 27/100
Train loss: 2.911636
Epoch: 28/100
Train loss: 3.057909
Epoch: 29/100
Train loss: 2.858439
Epoch: 30/100
Train loss: 2.973640
Epoch: 31/100
Train loss: 3.011395
Epoch: 32/100
Train loss: 2.988506
Epoch: 33/100
Train loss: 3.072317
Epoch: 34/100
Train loss: 2.915591
Epoch: 35/100
Train loss: 2.875523
Epoch: 36/100
Train loss: 2.999959
Epoch: 37/100
Train loss: 2.885304
Epoch: 38/100
Train loss: 2.861156
Epoch: 39/100
Train loss: 2.794992
Epoch: 40/100
Train loss: 2.761462
Epoch: 41/100
Train loss: 2.746289
Epoch: 42/100
Train loss: 2.834572
Epoch: 43/100
Train loss: 2.745077
Epoch: 44/100
Train loss: 2.699254
Epoch: 45/100
Train loss: 2.700597
Epoch: 46/100
Train loss: 2.699802
Epoch: 47/100
Train loss: 2.628152
Epoch: 48/100
Train loss: 2.712352
Epoch: 49/100
Train loss: 2.671025
Epoch: 50/100
Train loss: 2.574842
Epoch: 51/100
Train loss: 2.798391
Epoch: 52/100
Train loss: 2.687086
Epoch: 53/100
Train loss: 2.670921
Epoch: 54/100
Train loss: 2.746647
Epoch: 55/100
Train loss: 2.719188
Epoch: 56/100
Train loss: 2.562693
Epoch: 57/100
Train loss: 2.654603
Epoch: 58/100
Train loss: 2.670262
Epoch: 59/100
Train loss: 2.608273
Epoch: 60/100
Train loss: 2.719211
Epoch: 61/100
Train loss: 2.551835
Epoch: 62/100
Train loss: 2.515973
Time needed: 173.92407417297363 for validation audios
0.7692850803520729
Model improve: 0.0000 -> 0.7693
Epoch: 63/100
Train loss: 2.578289
Time needed: 174.17970395088196 for validation audios
0.7681938472444161
Epoch: 64/100
Date :05/05/2023, 15:12:54
Duration: 3
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 400
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 3
13816
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/400
Date :05/05/2023, 15:13:32
Duration: 3
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 400
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 3
13816
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/400
Date :05/05/2023, 15:13:55
Duration: 3
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 400
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 3
13816
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/400
Date :05/05/2023, 15:14:13
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 400
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 3
13816
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/400
Train loss: 34.363145
Epoch: 2/400
Train loss: 6.005469
Epoch: 3/400
Date :05/05/2023, 15:16:58
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 400
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 3
13816
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/400
Train loss: 34.714307
Epoch: 2/400
Train loss: 5.595468
Epoch: 3/400
Train loss: 5.139175
Epoch: 4/400
Train loss: 4.846570
Epoch: 5/400
Train loss: 4.535919
Epoch: 6/400
Train loss: 4.242862
Epoch: 7/400
Train loss: 3.918549
Epoch: 8/400
Train loss: 3.894201
Epoch: 9/400
Train loss: 3.671519
Epoch: 10/400
Train loss: 3.623957
Epoch: 11/400
Train loss: 3.545142
Epoch: 12/400
Train loss: 3.469080
Epoch: 13/400
Date :05/05/2023, 15:34:10
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 400
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 3
13816
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/400
Date :05/05/2023, 15:35:31
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 400
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 3
13816
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/400
Train loss: 35.154479
Epoch: 2/400
Train loss: 5.759386
Epoch: 3/400
Date :05/05/2023, 15:38:35
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 400
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 3
13816
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/400
Date :05/05/2023, 15:39:00
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 400
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 3
13816
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/400
Train loss: 34.560046
Epoch: 2/400
Train loss: 5.945790
Epoch: 3/400
Date :05/05/2023, 15:42:21
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 400
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 3
13816
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/400
Train loss: 35.625355
Epoch: 2/400
Train loss: 5.634668
Epoch: 3/400
Train loss: 5.184678
Epoch: 4/400
Train loss: 4.881746
Epoch: 5/400
Train loss: 4.559574
Epoch: 6/400
Train loss: 4.259829
Epoch: 7/400
Train loss: 3.928065
Epoch: 8/400
Train loss: 3.895365
Epoch: 9/400
Train loss: 3.669126
Epoch: 10/400
Train loss: 3.617724
Epoch: 11/400
Train loss: 3.526098
Epoch: 12/400
Train loss: 3.450806
Epoch: 13/400
Train loss: 3.372821
Epoch: 14/400
Train loss: 3.307339
Epoch: 15/400
Train loss: 3.273534
Epoch: 16/400
Train loss: 3.236907
Epoch: 17/400
Train loss: 3.115419
Epoch: 18/400
Train loss: 2.980459
Epoch: 19/400
Train loss: 3.222355
Epoch: 20/400
Train loss: 3.124607
Epoch: 21/400
Train loss: 2.977806
Epoch: 22/400
Train loss: 2.859755
Epoch: 23/400
Train loss: 2.910187
Epoch: 24/400
Train loss: 2.875355
Epoch: 25/400
Train loss: 2.984175
Epoch: 26/400
Train loss: 2.858782
Epoch: 27/400
Train loss: 2.869375
Epoch: 28/400
Train loss: 2.842628
Epoch: 29/400
Train loss: 2.722159
Epoch: 30/400
Train loss: 2.623168
Epoch: 31/400
Train loss: 2.735590
Epoch: 32/400
Train loss: 2.813127
Epoch: 33/400
Train loss: 2.595333
Epoch: 34/400
Train loss: 2.654167
Epoch: 35/400
Train loss: 2.707363
Epoch: 36/400
Train loss: 2.739900
Epoch: 37/400
Train loss: 2.565421
Epoch: 38/400
Train loss: 2.787039
Epoch: 39/400
Train loss: 2.661293
Epoch: 40/400
Train loss: 2.559551
Epoch: 41/400
Train loss: 2.683038
Epoch: 42/400
Train loss: 2.485596
Epoch: 43/400
Train loss: 2.660678
Epoch: 44/400
Train loss: 2.549574
Epoch: 45/400
Train loss: 2.613396
Epoch: 46/400
Train loss: 2.551438
Epoch: 47/400
Train loss: 2.531824
Epoch: 48/400
Train loss: 2.442968
Epoch: 49/400
Train loss: 2.337820
Epoch: 50/400
Train loss: 2.434741
Epoch: 51/400
Train loss: 2.345674
Epoch: 52/400
Train loss: 2.448372
Epoch: 53/400
Train loss: 2.309039
Epoch: 54/400
Train loss: 2.526031
Epoch: 55/400
Train loss: 2.421940
Epoch: 56/400
Train loss: 2.267258
Epoch: 57/400
Train loss: 2.480931
Epoch: 58/400
Train loss: 2.333820
Epoch: 59/400
Train loss: 2.470888
Epoch: 60/400
Train loss: 2.382782
Epoch: 61/400
Train loss: 2.411429
Epoch: 62/400
Train loss: 2.275617
Epoch: 63/400
Train loss: 2.303581
Epoch: 64/400
Train loss: 2.408383
Epoch: 65/400
Train loss: 2.319710
Epoch: 66/400
Train loss: 2.440708
Epoch: 67/400
Train loss: 2.244124
Epoch: 68/400
Train loss: 2.447156
Epoch: 69/400
Train loss: 2.337909
Epoch: 70/400
Train loss: 2.376854
Epoch: 71/400
Train loss: 2.274251
Epoch: 72/400
Train loss: 2.236976
Epoch: 73/400
Train loss: 2.113185
Epoch: 74/400
Train loss: 2.414412
Epoch: 75/400
Train loss: 2.384756
Epoch: 76/400
Train loss: 2.224480
Epoch: 77/400
Train loss: 2.318160
Epoch: 78/400
Train loss: 2.285720
Epoch: 79/400
Train loss: 2.129819
Epoch: 80/400
Train loss: 2.160722
Epoch: 81/400
Train loss: 2.221644
Epoch: 82/400
Train loss: 2.275895
Epoch: 83/400
Train loss: 2.129937
Epoch: 84/400
Train loss: 2.124841
Epoch: 85/400
Train loss: 2.281097
Epoch: 86/400
Train loss: 2.284178
Epoch: 87/400
Train loss: 2.254136
Epoch: 88/400
Train loss: 2.051510
Epoch: 89/400
Train loss: 2.311527
Epoch: 90/400
Train loss: 2.289480
Epoch: 91/400
Train loss: 2.250366
Epoch: 92/400
Train loss: 2.197571
Epoch: 93/400
Train loss: 2.298972
Epoch: 94/400
Train loss: 2.206707
Epoch: 95/400
Train loss: 2.236536
Epoch: 96/400
Train loss: 2.208862
Epoch: 97/400
Train loss: 2.286586
Epoch: 98/400
Train loss: 2.223516
Epoch: 99/400
Train loss: 2.220691
Epoch: 100/400
Train loss: 2.114876
Epoch: 101/400
Train loss: 2.225104
Epoch: 102/400
Train loss: 2.206132
Epoch: 103/400
Train loss: 2.007248
Epoch: 104/400
Train loss: 2.081022
Epoch: 105/400
Train loss: 2.086759
Epoch: 106/400
Train loss: 2.137120
Epoch: 107/400
Train loss: 2.148201
Epoch: 108/400
Train loss: 2.211510
Epoch: 109/400
Train loss: 2.174197
Epoch: 110/400
Train loss: 2.140804
Epoch: 111/400
Train loss: 2.225334
Epoch: 112/400
Train loss: 2.025809
Epoch: 113/400
Train loss: 2.086852
Epoch: 114/400
Train loss: 2.133614
Epoch: 115/400
Train loss: 2.170008
Epoch: 116/400
Train loss: 2.159156
Epoch: 117/400
Train loss: 2.139883
Epoch: 118/400
Train loss: 2.084006
Epoch: 119/400
Train loss: 2.045262
Epoch: 120/400
Train loss: 2.195376
Epoch: 121/400
Train loss: 2.095711
Epoch: 122/400
Train loss: 2.104521
Epoch: 123/400
Train loss: 2.142180
Epoch: 124/400
Train loss: 2.227715
Epoch: 125/400
Train loss: 2.263039
Epoch: 126/400
Train loss: 2.253473
Epoch: 127/400
Train loss: 2.066136
Epoch: 128/400
Train loss: 2.065946
Epoch: 129/400
Train loss: 2.088489
Epoch: 130/400
Train loss: 2.006751
Epoch: 131/400
Train loss: 1.910624
Epoch: 132/400
Train loss: 2.141387
Epoch: 133/400
Train loss: 2.185380
Epoch: 134/400
Train loss: 2.017164
Epoch: 135/400
Train loss: 2.051775
Epoch: 136/400
Train loss: 2.066009
Epoch: 137/400
Train loss: 2.006878
Epoch: 138/400
Train loss: 2.006584
Epoch: 139/400
Train loss: 2.079194
Epoch: 140/400
Train loss: 2.007620
Epoch: 141/400
Train loss: 2.035806
Epoch: 142/400
Train loss: 2.065470
Epoch: 143/400
Train loss: 1.939421
Epoch: 144/400
Train loss: 2.136228
Epoch: 145/400
Train loss: 2.137977
Epoch: 146/400
Train loss: 1.949567
Epoch: 147/400
Train loss: 2.107462
Epoch: 148/400
Train loss: 2.068839
Epoch: 149/400
Train loss: 2.170547
Epoch: 150/400
Train loss: 2.068806
Epoch: 151/400
Train loss: 1.851544
Epoch: 152/400
Train loss: 2.042990
Epoch: 153/400
Train loss: 1.922859
Epoch: 154/400
Train loss: 1.933082
Epoch: 155/400
Train loss: 2.053862
Epoch: 156/400
Train loss: 2.001399
Epoch: 157/400
Train loss: 2.056729
Epoch: 158/400
Train loss: 2.055731
Epoch: 159/400
Train loss: 1.896834
Epoch: 160/400
Train loss: 1.926830
Epoch: 161/400
Train loss: 2.056331
Epoch: 162/400
Train loss: 2.035789
Epoch: 163/400
Train loss: 1.985980
Epoch: 164/400
Train loss: 1.997608
Epoch: 165/400
Train loss: 1.992782
Epoch: 166/400
Train loss: 2.136579
Epoch: 167/400
Train loss: 2.111905
Epoch: 168/400
Train loss: 2.034007
Epoch: 169/400
Train loss: 2.115173
Epoch: 170/400
Train loss: 2.018197
Epoch: 171/400
Train loss: 2.020401
Epoch: 172/400
Train loss: 1.983876
Epoch: 173/400
Train loss: 2.028710
Epoch: 174/400
Train loss: 2.056692
Epoch: 175/400
Train loss: 1.955262
Epoch: 176/400
Train loss: 1.951173
Epoch: 177/400
Train loss: 2.066405
Epoch: 178/400
Train loss: 1.809063
Epoch: 179/400
Train loss: 2.001343
Epoch: 180/400
Train loss: 1.889554
Epoch: 181/400
Train loss: 1.919695
Epoch: 182/400
Train loss: 2.073959
Epoch: 183/400
Train loss: 1.942192
Epoch: 184/400
Train loss: 1.874768
Epoch: 185/400
Train loss: 1.957671
Epoch: 186/400
Train loss: 2.046398
Epoch: 187/400
Train loss: 2.035625
Epoch: 188/400
Train loss: 1.972531
Epoch: 189/400
Train loss: 1.980119
Epoch: 190/400
Train loss: 1.999364
Epoch: 191/400
Train loss: 2.135401
Epoch: 192/400
Train loss: 1.952968
Epoch: 193/400
Train loss: 2.004971
Epoch: 194/400
Train loss: 2.032561
Epoch: 195/400
Train loss: 1.948679
Epoch: 196/400
Train loss: 2.050481
Epoch: 197/400
Train loss: 2.059132
Epoch: 198/400
Train loss: 1.899864
Epoch: 199/400
Train loss: 1.929181
Epoch: 200/400
Train loss: 2.028802
Epoch: 201/400
Train loss: 2.003544
Epoch: 202/400
Train loss: 1.955552
Epoch: 203/400
Train loss: 2.014646
Epoch: 204/400
Train loss: 1.954032
Epoch: 205/400
Train loss: 2.028261
Epoch: 206/400
Train loss: 1.997712
Epoch: 207/400
Train loss: 1.965203
Epoch: 208/400
Train loss: 1.876700
Epoch: 209/400
Train loss: 1.855472
Epoch: 210/400
Train loss: 1.800386
Epoch: 211/400
Train loss: 2.049495
Epoch: 212/400
Train loss: 2.039697
Epoch: 213/400
Train loss: 1.951270
Epoch: 214/400
Train loss: 1.881826
Epoch: 215/400
Train loss: 1.952184
Epoch: 216/400
Train loss: 1.930730
Epoch: 217/400
Train loss: 1.870445
Epoch: 218/400
Train loss: 1.949892
Epoch: 219/400
Train loss: 1.851655
Epoch: 220/400
Train loss: 2.001974
Epoch: 221/400
Train loss: 1.951256
Epoch: 222/400
Train loss: 1.925851
Epoch: 223/400
Train loss: 1.951123
Epoch: 224/400
Train loss: 1.904465
Epoch: 225/400
Train loss: 1.955451
Epoch: 226/400
Train loss: 1.893833
Epoch: 227/400
Train loss: 1.976356
Epoch: 228/400
Train loss: 1.962844
Epoch: 229/400
Train loss: 1.962363
Epoch: 230/400
Train loss: 1.926082
Epoch: 231/400
Train loss: 1.874000
Epoch: 232/400
Train loss: 1.877639
Epoch: 233/400
Train loss: 1.880516
Epoch: 234/400
Train loss: 1.884172
Epoch: 235/400
Train loss: 1.859155
Epoch: 236/400
Train loss: 1.835516
Epoch: 237/400
Train loss: 1.838500
Epoch: 238/400
Train loss: 1.872921
Epoch: 239/400
Train loss: 2.014276
Epoch: 240/400
Train loss: 1.931805
Epoch: 241/400
Train loss: 1.826013
Epoch: 242/400
Train loss: 1.831496
Epoch: 243/400
Train loss: 1.849917
Epoch: 244/400
Train loss: 1.890457
Epoch: 245/400
Train loss: 1.946671
Epoch: 246/400
Train loss: 1.899395
Epoch: 247/400
Train loss: 2.025218
Epoch: 248/400
Train loss: 1.959575
Epoch: 249/400
Train loss: 1.887070
Epoch: 250/400
Train loss: 1.873729
Epoch: 251/400
Train loss: 2.031040
Epoch: 252/400
Train loss: 1.783040
Epoch: 253/400
Train loss: 1.927360
Epoch: 254/400
Train loss: 1.872712
Epoch: 255/400
Train loss: 1.896830
Epoch: 256/400
Train loss: 1.901606
Epoch: 257/400
Train loss: 1.856209
Epoch: 258/400
Train loss: 1.944700
Epoch: 259/400
Train loss: 1.944535
Epoch: 260/400
Train loss: 1.827815
Epoch: 261/400
Train loss: 1.907865
Epoch: 262/400
Train loss: 1.997219
Epoch: 263/400
Train loss: 1.795798
Epoch: 264/400
Train loss: 1.896821
Epoch: 265/400
Train loss: 1.987864
Epoch: 266/400
Train loss: 1.922323
Epoch: 267/400
Train loss: 1.876808
Epoch: 268/400
Train loss: 1.903207
Epoch: 269/400
Train loss: 1.821314
Epoch: 270/400
Train loss: 1.879539
Epoch: 271/400
Train loss: 1.660551
Epoch: 272/400
Train loss: 1.953704
Epoch: 273/400
Train loss: 1.833350
Epoch: 274/400
Train loss: 1.933897
Epoch: 275/400
Train loss: 1.989676
Epoch: 276/400
Train loss: 1.953511
Epoch: 277/400
Train loss: 1.808944
Epoch: 278/400
Train loss: 1.861526
Epoch: 279/400
Train loss: 1.982652
Epoch: 280/400
Train loss: 1.865447
Epoch: 281/400
Train loss: 1.986699
Epoch: 282/400
Train loss: 1.864020
Epoch: 283/400
Train loss: 1.746260
Epoch: 284/400
Train loss: 1.962635
Epoch: 285/400
Train loss: 1.917193
Epoch: 286/400
Train loss: 1.939639
Epoch: 287/400
Train loss: 1.891350
Epoch: 288/400
Train loss: 1.889512
Epoch: 289/400
Train loss: 1.924788
Epoch: 290/400
Train loss: 1.969626
Epoch: 291/400
Train loss: 1.904925
Epoch: 292/400
Train loss: 1.903259
Epoch: 293/400
Train loss: 1.998084
Epoch: 294/400
Train loss: 1.937970
Epoch: 295/400
Train loss: 1.967774
Epoch: 296/400
Train loss: 1.986126
Epoch: 297/400
Train loss: 1.784240
Epoch: 298/400
Train loss: 1.943530
Epoch: 299/400
Train loss: 1.870425
Epoch: 300/400
Train loss: 1.831692
Epoch: 301/400
Train loss: 1.811442
Epoch: 302/400
Train loss: 1.876198
Epoch: 303/400
Train loss: 1.755365
Epoch: 304/400
Train loss: 1.800533
Epoch: 305/400
Train loss: 1.813609
Epoch: 306/400
Train loss: 1.943268
Epoch: 307/400
Train loss: 1.755174
Epoch: 308/400
Train loss: 1.829441
Epoch: 309/400
Train loss: 1.828118
Epoch: 310/400
Train loss: 1.708294
Epoch: 311/400
Train loss: 1.856129
Epoch: 312/400
Train loss: 1.877984
Epoch: 313/400
Train loss: 1.939619
Epoch: 314/400
Train loss: 1.948246
Epoch: 315/400
Train loss: 1.811984
Epoch: 316/400
Train loss: 1.946216
Epoch: 317/400
Train loss: 1.771961
Epoch: 318/400
Train loss: 1.785980
Epoch: 319/400
Train loss: 1.843236
Epoch: 320/400
Train loss: 1.821714
Epoch: 321/400
Train loss: 1.913537
Epoch: 322/400
Train loss: 1.972432
Time needed: 170.35490369796753 for validation audios
0.7799027351915814
Model improve: 0.0000 -> 0.7799
Epoch: 323/400
Train loss: 1.824318
Time needed: 170.25569033622742 for validation audios
0.7823508211581904
Epoch: 324/400
Train loss: 1.910611
Time needed: 172.05983996391296 for validation audios
0.7799706853947088
Model improve: 0.7799 -> 0.7800
Epoch: 325/400
Train loss: 1.885386
Time needed: 171.6848087310791 for validation audios
0.7824074749110629
Epoch: 326/400
Train loss: 1.905908
Time needed: 171.2871377468109 for validation audios
0.7810634292723883
Epoch: 327/400
Train loss: 1.959487
Time needed: 172.01134610176086 for validation audios
0.781519537132505
Epoch: 328/400
Train loss: 1.753874
Time needed: 171.1830677986145 for validation audios
0.7832687928362284
Epoch: 329/400
Train loss: 1.838068
Time needed: 170.52795004844666 for validation audios
0.7810774328401862
Epoch: 330/400
Train loss: 1.933629
Time needed: 172.9605634212494 for validation audios
0.7806601851364261
Epoch: 331/400
Train loss: 1.827440
Time needed: 173.76798820495605 for validation audios
0.7820132927529032
Epoch: 332/400
Train loss: 1.821122
Time needed: 173.1074914932251 for validation audios
0.7820737132314053
Epoch: 333/400
Train loss: 1.854500
Time needed: 172.8046886920929 for validation audios
0.7820960762354194
Epoch: 334/400
Train loss: 1.807407
Time needed: 173.23060250282288 for validation audios
0.7832745782173623
Epoch: 335/400
Train loss: 1.910694
Time needed: 171.8162989616394 for validation audios
0.7803973596678888
Epoch: 336/400
Train loss: 1.797406
Time needed: 172.5410873889923 for validation audios
0.7822080533969432
Epoch: 337/400
Train loss: 1.900354
Time needed: 170.90650415420532 for validation audios
0.7813546061133162
Epoch: 338/400
Train loss: 1.948369
Time needed: 172.69585752487183 for validation audios
0.7802206672512338
Epoch: 339/400
Train loss: 1.879722
Time needed: 174.84370803833008 for validation audios
0.7835762032513292
Epoch: 340/400
Train loss: 1.951872
Time needed: 172.33403706550598 for validation audios
0.7812504169831161
Epoch: 341/400
Train loss: 1.932314
Time needed: 172.61126375198364 for validation audios
0.780558904014379
Epoch: 342/400
Train loss: 1.892065
Time needed: 173.0999698638916 for validation audios
0.7803896996536629
Epoch: 343/400
Train loss: 1.774897
Time needed: 170.70395588874817 for validation audios
0.7818167276638485
Epoch: 344/400
Train loss: 1.741458
Time needed: 170.26164269447327 for validation audios
0.7818778557639782
Epoch: 345/400
Train loss: 1.966657
Time needed: 172.72653484344482 for validation audios
0.7826959397605286
Epoch: 346/400
Train loss: 1.747154
Time needed: 174.9481806755066 for validation audios
0.783111130065553
Epoch: 347/400
Train loss: 1.903825
Time needed: 173.53600144386292 for validation audios
0.7818351540631718
Epoch: 348/400
Train loss: 1.874582
Time needed: 172.02778720855713 for validation audios
0.7811604794686444
Epoch: 349/400
Train loss: 1.855481
Date :05/05/2023, 23:47:13
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 4
13830
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 36.871065
Epoch: 2/200
Train loss: 5.598358
Epoch: 3/200
Train loss: 5.098851
Epoch: 4/200
Train loss: 4.805102
Epoch: 5/200
Train loss: 4.476275
Epoch: 6/200
Train loss: 4.212310
Epoch: 7/200
Train loss: 3.934490
Epoch: 8/200
Train loss: 3.868797
Epoch: 9/200
Train loss: 3.711605
Epoch: 10/200
Train loss: 3.647821
Epoch: 11/200
Train loss: 3.594371
Epoch: 12/200
Train loss: 3.471153
Epoch: 13/200
Train loss: 3.449185
Epoch: 14/200
Train loss: 3.314748
Epoch: 15/200
Train loss: 3.355315
Epoch: 16/200
Train loss: 3.282690
Epoch: 17/200
Train loss: 3.183372
Epoch: 18/200
Train loss: 3.086807
Epoch: 19/200
Train loss: 3.301971
Epoch: 20/200
Train loss: 3.170352
Epoch: 21/200
Train loss: 3.027394
Epoch: 22/200
Train loss: 2.914147
Epoch: 23/200
Train loss: 2.958881
Epoch: 24/200
Train loss: 3.022099
Epoch: 25/200
Train loss: 2.971737
Epoch: 26/200
Train loss: 2.975736
Epoch: 27/200
Train loss: 2.948545
Epoch: 28/200
Train loss: 2.802826
Epoch: 29/200
Train loss: 2.835206
Epoch: 30/200
Train loss: 2.689214
Epoch: 31/200
Train loss: 2.818112
Epoch: 32/200
Train loss: 2.872648
Epoch: 33/200
Train loss: 2.664105
Epoch: 34/200
Train loss: 2.757673
Epoch: 35/200
Train loss: 2.757168
Epoch: 36/200
Train loss: 2.783477
Epoch: 37/200
Train loss: 2.665258
Epoch: 38/200
Train loss: 2.894250
Epoch: 39/200
Train loss: 2.567707
Epoch: 40/200
Train loss: 2.727739
Epoch: 41/200
Train loss: 2.633800
Epoch: 42/200
Train loss: 2.630423
Epoch: 43/200
Train loss: 2.720062
Epoch: 44/200
Train loss: 2.687062
Epoch: 45/200
Train loss: 2.690844
Epoch: 46/200
Train loss: 2.455420
Epoch: 47/200
Train loss: 2.633713
Epoch: 48/200
Train loss: 2.373844
Epoch: 49/200
Train loss: 2.383658
Epoch: 50/200
Train loss: 2.441156
Epoch: 51/200
Train loss: 2.494805
Epoch: 52/200
Train loss: 2.491327
Epoch: 53/200
Train loss: 2.386597
Epoch: 54/200
Train loss: 2.493181
Epoch: 55/200
Train loss: 2.483876
Epoch: 56/200
Train loss: 2.431946
Epoch: 57/200
Train loss: 2.451739
Epoch: 58/200
Train loss: 2.441957
Epoch: 59/200
Train loss: 2.459566
Epoch: 60/200
Train loss: 2.466034
Epoch: 61/200
Train loss: 2.358619
Epoch: 62/200
Train loss: 2.378816
Epoch: 63/200
Train loss: 2.394409
Epoch: 64/200
Train loss: 2.394139
Epoch: 65/200
Train loss: 2.448597
Epoch: 66/200
Train loss: 2.334400
Epoch: 67/200
Train loss: 2.420562
Epoch: 68/200
Train loss: 2.379336
Epoch: 69/200
Train loss: 2.545032
Epoch: 70/200
Train loss: 2.342707
Epoch: 71/200
Train loss: 2.208676
Epoch: 72/200
Train loss: 2.222508
Epoch: 73/200
Train loss: 2.348915
Epoch: 74/200
Train loss: 2.330369
Epoch: 75/200
Train loss: 2.401939
Epoch: 76/200
Train loss: 2.347459
Epoch: 77/200
Train loss: 2.338971
Epoch: 78/200
Train loss: 2.247190
Epoch: 79/200
Train loss: 2.097012
Epoch: 80/200
Train loss: 2.231715
Epoch: 81/200
Train loss: 2.264170
Epoch: 82/200
Train loss: 2.262749
Epoch: 83/200
Train loss: 2.217699
Epoch: 84/200
Train loss: 2.190956
Epoch: 85/200
Train loss: 2.336315
Epoch: 86/200
Train loss: 2.248021
Epoch: 87/200
Train loss: 2.127777
Epoch: 88/200
Train loss: 2.310825
Epoch: 89/200
Train loss: 2.337997
Epoch: 90/200
Train loss: 2.211154
Epoch: 91/200
Train loss: 2.311294
Epoch: 92/200
Train loss: 2.270286
Epoch: 93/200
Train loss: 2.318500
Epoch: 94/200
Train loss: 2.245898
Epoch: 95/200
Train loss: 2.277453
Epoch: 96/200
Train loss: 2.298603
Epoch: 97/200
Train loss: 2.255382
Epoch: 98/200
Train loss: 2.234909
Epoch: 99/200
Train loss: 2.162546
Epoch: 100/200
Train loss: 2.210861
Epoch: 101/200
Train loss: 2.261778
Epoch: 102/200
Train loss: 2.063550
Epoch: 103/200
Train loss: 2.116065
Epoch: 104/200
Train loss: 2.110244
Epoch: 105/200
Train loss: 2.174799
Epoch: 106/200
Train loss: 2.185945
Epoch: 107/200
Train loss: 2.239557
Epoch: 108/200
Train loss: 2.215511
Epoch: 109/200
Train loss: 2.203466
Epoch: 110/200
Train loss: 2.260409
Epoch: 111/200
Train loss: 2.050563
Epoch: 112/200
Train loss: 2.140505
Epoch: 113/200
Train loss: 2.142194
Epoch: 114/200
Train loss: 2.259977
Epoch: 115/200
Train loss: 2.157444
Epoch: 116/200
Train loss: 2.185614
Epoch: 117/200
Train loss: 2.142830
Epoch: 118/200
Train loss: 2.122057
Epoch: 119/200
Train loss: 2.216449
Epoch: 120/200
Train loss: 2.102161
Epoch: 121/200
Train loss: 2.094775
Epoch: 122/200
Train loss: 2.185204
Time needed: 163.24163341522217 for validation audios
0.8026676242104855
Epoch: 123/200
Train loss: 2.287137
Time needed: 166.61318135261536 for validation audios
0.8009636735041247
Epoch: 124/200
Train loss: 2.305233
Time needed: 163.28354573249817 for validation audios
0.8022949879544986
Epoch: 125/200
Train loss: 2.238114
Time needed: 163.51931858062744 for validation audios
0.803235242846056
Epoch: 126/200
Train loss: 2.094970
Time needed: 162.7243618965149 for validation audios
0.8048800257809023
Epoch: 127/200
Train loss: 2.166481
Time needed: 169.63969564437866 for validation audios
0.8024209825239661
Epoch: 128/200
Train loss: 2.057895
Time needed: 164.46848893165588 for validation audios
0.8042490300124187
Epoch: 129/200
Train loss: 2.081479
Time needed: 164.27285885810852 for validation audios
0.8049130022425789
Epoch: 130/200
Train loss: 2.003966
Time needed: 163.87067079544067 for validation audios
0.8016095911862993
Epoch: 131/200
Train loss: 2.094836
Time needed: 164.26501321792603 for validation audios
0.8042778084064734
Epoch: 132/200
Train loss: 2.214918
Time needed: 163.59303426742554 for validation audios
0.8016562430306527
Epoch: 133/200
Train loss: 2.038575
Time needed: 164.94546556472778 for validation audios
0.8035251242598344
Epoch: 134/200
Train loss: 2.158667
Time needed: 163.2277946472168 for validation audios
0.8028110904039117
Epoch: 135/200
Train loss: 1.994560
Time needed: 166.42685317993164 for validation audios
0.8045205662769648
Epoch: 136/200
Train loss: 2.098449
Time needed: 163.25228810310364 for validation audios
0.804359594958916
Epoch: 137/200
Train loss: 2.136375
Time needed: 164.51099157333374 for validation audios
0.8011557091245418
Epoch: 138/200
Train loss: 2.001027
Time needed: 165.67541193962097 for validation audios
0.8049084592781844
Epoch: 139/200
Train loss: 2.141378
Time needed: 167.1353087425232 for validation audios
0.8017777976457399
Epoch: 140/200
Train loss: 2.074550
Time needed: 164.91614866256714 for validation audios
0.8025702862900408
Epoch: 141/200
Train loss: 2.089181
Time needed: 166.00995254516602 for validation audios
0.8043029923829201
Epoch: 142/200
Train loss: 2.038264
Time needed: 164.6747465133667 for validation audios
0.8029307175316804
Epoch: 143/200
Train loss: 2.188503
Time needed: 166.1203329563141 for validation audios
0.8031030636147388
Epoch: 144/200
Train loss: 2.137532
Time needed: 164.45509552955627 for validation audios
0.80482270391711
Epoch: 145/200
Train loss: 2.008179
Time needed: 163.86887979507446 for validation audios
0.8038624287701267
Epoch: 146/200
Train loss: 2.160744
Time needed: 163.72079277038574 for validation audios
0.8037318890022787
Epoch: 147/200
Train loss: 2.146705
Time needed: 166.72082901000977 for validation audios
0.8043751161274583
Epoch: 148/200
Train loss: 2.200499
Time needed: 166.33371877670288 for validation audios
0.8043094070658947
Epoch: 149/200
Train loss: 2.060214
Time needed: 163.96180534362793 for validation audios
0.8031181301934287
Epoch: 150/200
Train loss: 1.913346
Time needed: 163.47707104682922 for validation audios
0.8033063871734819
Epoch: 151/200
Train loss: 2.085664
Time needed: 165.0310378074646 for validation audios
0.8032643004620543
Epoch: 152/200
Train loss: 2.010110
Time needed: 164.03031182289124 for validation audios
0.8029703277746547
Epoch: 153/200
Train loss: 1.982402
Time needed: 164.3863742351532 for validation audios
0.8041354090710698
Epoch: 154/200
Train loss: 2.085280
Time needed: 164.79006791114807 for validation audios
0.8043399370957195
Epoch: 155/200
Train loss: 2.130784
Time needed: 164.12391591072083 for validation audios
0.8010599773770034
Epoch: 156/200
Train loss: 2.103981
Time needed: 165.19187664985657 for validation audios
0.803021824725679
Epoch: 157/200
Train loss: 2.077472
Time needed: 163.10084199905396 for validation audios
0.8029136683626416
Epoch: 158/200
Train loss: 1.933841
Time needed: 166.09465503692627 for validation audios
0.8060801162445707
Epoch: 159/200
Train loss: 2.151441
Time needed: 165.2765519618988 for validation audios
0.8033130119575043
Epoch: 160/200
Train loss: 1.981216
Time needed: 166.6661238670349 for validation audios
0.8047136292685548
Epoch: 161/200
Train loss: 2.185834
Time needed: 165.74967527389526 for validation audios
0.8035147974616174
Epoch: 162/200
Date :05/06/2023, 04:47:56
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 2
13807
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 35.248328
Epoch: 2/200
Train loss: 5.571556
Epoch: 3/200
Train loss: 5.064398
Epoch: 4/200
Train loss: 4.762431
Epoch: 5/200
Train loss: 4.445183
Epoch: 6/200
Train loss: 4.163168
Epoch: 7/200
Train loss: 3.874287
Epoch: 8/200
Train loss: 3.852199
Epoch: 9/200
Train loss: 3.630696
Epoch: 10/200
Train loss: 3.583204
Epoch: 11/200
Train loss: 3.513556
Epoch: 12/200
Train loss: 3.454199
Epoch: 13/200
Train loss: 3.343684
Epoch: 14/200
Train loss: 3.289942
Epoch: 15/200
Train loss: 3.255703
Epoch: 16/200
Train loss: 3.217066
Epoch: 17/200
Train loss: 3.131117
Epoch: 18/200
Train loss: 2.998189
Epoch: 19/200
Train loss: 3.214117
Epoch: 20/200
Train loss: 3.112178
Epoch: 21/200
Train loss: 2.979476
Epoch: 22/200
Train loss: 2.875089
Epoch: 23/200
Train loss: 2.899454
Epoch: 24/200
Train loss: 2.894964
Epoch: 25/200
Train loss: 3.001442
Epoch: 26/200
Train loss: 2.866725
Epoch: 27/200
Train loss: 2.882640
Epoch: 28/200
Train loss: 2.858227
Epoch: 29/200
Train loss: 2.713863
Epoch: 30/200
Train loss: 2.641520
Epoch: 31/200
Train loss: 2.738910
Epoch: 32/200
Train loss: 2.793580
Epoch: 33/200
Train loss: 2.594319
Epoch: 34/200
Train loss: 2.678380
Epoch: 35/200
Train loss: 2.739290
Epoch: 36/200
Train loss: 2.756621
Epoch: 37/200
Train loss: 2.577995
Epoch: 38/200
Train loss: 2.806839
Epoch: 39/200
Train loss: 2.637612
Epoch: 40/200
Train loss: 2.575034
Epoch: 41/200
Train loss: 2.670729
Epoch: 42/200
Train loss: 2.492302
Epoch: 43/200
Train loss: 2.657816
Epoch: 44/200
Train loss: 2.564289
Epoch: 45/200
Train loss: 2.623328
Epoch: 46/200
Train loss: 2.527536
Epoch: 47/200
Train loss: 2.536998
Epoch: 48/200
Train loss: 2.431199
Epoch: 49/200
Train loss: 2.333409
Epoch: 50/200
Train loss: 2.425907
Epoch: 51/200
Train loss: 2.340346
Epoch: 52/200
Train loss: 2.440650
Epoch: 53/200
Train loss: 2.338508
Epoch: 54/200
Train loss: 2.537745
Epoch: 55/200
Train loss: 2.425791
Epoch: 56/200
Train loss: 2.297273
Epoch: 57/200
Train loss: 2.498730
Epoch: 58/200
Train loss: 2.331489
Epoch: 59/200
Train loss: 2.467300
Epoch: 60/200
Train loss: 2.386354
Epoch: 61/200
Train loss: 2.409646
Epoch: 62/200
Train loss: 2.271521
Epoch: 63/200
Train loss: 2.320528
Epoch: 64/200
Train loss: 2.407080
Epoch: 65/200
Train loss: 2.322368
Epoch: 66/200
Train loss: 2.445686
Epoch: 67/200
Train loss: 2.263421
Epoch: 68/200
Train loss: 2.456499
Epoch: 69/200
Train loss: 2.344398
Epoch: 70/200
Train loss: 2.395838
Epoch: 71/200
Train loss: 2.288663
Epoch: 72/200
Train loss: 2.259302
Epoch: 73/200
Train loss: 2.128739
Epoch: 74/200
Train loss: 2.415829
Epoch: 75/200
Train loss: 2.362694
Epoch: 76/200
Train loss: 2.261669
Epoch: 77/200
Train loss: 2.343777
Epoch: 78/200
Train loss: 2.313825
Epoch: 79/200
Train loss: 2.145199
Epoch: 80/200
Train loss: 2.167887
Epoch: 81/200
Train loss: 2.232309
Epoch: 82/200
Train loss: 2.272062
Epoch: 83/200
Train loss: 2.136799
Epoch: 84/200
Train loss: 2.122079
Epoch: 85/200
Train loss: 2.284685
Epoch: 86/200
Train loss: 2.277619
Epoch: 87/200
Train loss: 2.250089
Epoch: 88/200
Train loss: 2.063676
Epoch: 89/200
Train loss: 2.310729
Epoch: 90/200
Train loss: 2.289705
Epoch: 91/200
Train loss: 2.290838
Epoch: 92/200
Train loss: 2.228958
Epoch: 93/200
Train loss: 2.292191
Epoch: 94/200
Train loss: 2.234148
Epoch: 95/200
Train loss: 2.256899
Epoch: 96/200
Train loss: 2.239299
Epoch: 97/200
Train loss: 2.282222
Epoch: 98/200
Train loss: 2.231222
Epoch: 99/200
Train loss: 2.218572
Epoch: 100/200
Train loss: 2.127714
Epoch: 101/200
Train loss: 2.239050
Epoch: 102/200
Train loss: 2.198085
Epoch: 103/200
Train loss: 2.026650
Epoch: 104/200
Train loss: 2.083103
Epoch: 105/200
Train loss: 2.111308
Epoch: 106/200
Train loss: 2.152367
Epoch: 107/200
Train loss: 2.147277
Epoch: 108/200
Train loss: 2.220114
Epoch: 109/200
Train loss: 2.200273
Epoch: 110/200
Train loss: 2.165387
Epoch: 111/200
Train loss: 2.233700
Epoch: 112/200
Train loss: 2.023822
Epoch: 113/200
Train loss: 2.096862
Epoch: 114/200
Train loss: 2.139435
Epoch: 115/200
Train loss: 2.216731
Epoch: 116/200
Train loss: 2.164270
Epoch: 117/200
Train loss: 2.152840
Epoch: 118/200
Train loss: 2.102859
Epoch: 119/200
Train loss: 2.062208
Epoch: 120/200
Train loss: 2.215361
Epoch: 121/200
Train loss: 2.111635
Epoch: 122/200
Train loss: 2.127429
Time needed: 172.27231693267822 for validation audios
0.7764007198198102
Model improve: 0.0000 -> 0.7764
Epoch: 123/200
Train loss: 2.161524
Time needed: 171.418559551239 for validation audios
0.7788344602778996
Model improve: 0.7764 -> 0.7788
Epoch: 124/200
Train loss: 2.240923
Time needed: 169.66305446624756 for validation audios
0.7779386323808395
Epoch: 125/200
Train loss: 2.280029
Time needed: 177.64603185653687 for validation audios
0.7801429378228724
Model improve: 0.7788 -> 0.7801
Epoch: 126/200
Train loss: 2.280773
Time needed: 176.76761722564697 for validation audios
0.7804296214279335
Model improve: 0.7801 -> 0.7804
Epoch: 127/200
Train loss: 2.065698
Time needed: 170.420419216156 for validation audios
0.7781584785482643
Epoch: 128/200
Train loss: 2.090985
Time needed: 171.3302571773529 for validation audios
0.7809398679175216
Model improve: 0.7804 -> 0.7809
Epoch: 129/200
Train loss: 2.115028
Time needed: 168.79291248321533 for validation audios
0.7799189179068445
Epoch: 130/200
Train loss: 2.016105
Time needed: 169.86596131324768 for validation audios
0.7814561204690716
Model improve: 0.7809 -> 0.7815
Epoch: 131/200
Train loss: 1.926116
Time needed: 170.80900931358337 for validation audios
0.781191046089311
Epoch: 132/200
Train loss: 2.142462
Time needed: 168.9083559513092 for validation audios
0.7802141576870041
Epoch: 133/200
Train loss: 2.173909
Time needed: 171.03094625473022 for validation audios
0.7797022800295832
Epoch: 134/200
Train loss: 2.042923
Time needed: 172.20401978492737 for validation audios
0.7823317319058565
Model improve: 0.7815 -> 0.7823
Epoch: 135/200
Train loss: 2.081767
Time needed: 171.20059323310852 for validation audios
0.7805841316929485
Epoch: 136/200
Train loss: 2.099201
Time needed: 170.91800546646118 for validation audios
0.7809744360248345
Epoch: 137/200
Train loss: 2.044391
Time needed: 170.2545235157013 for validation audios
0.7804258638894566
Epoch: 138/200
Train loss: 2.020786
Time needed: 173.30029439926147 for validation audios
0.7820643770471033
Epoch: 139/200
Train loss: 2.108612
Time needed: 177.3608055114746 for validation audios
0.7820305611553623
Epoch: 140/200
Train loss: 2.043353
Time needed: 170.16796207427979 for validation audios
0.7801141840824921
Epoch: 141/200
Train loss: 2.044609
Time needed: 171.54055166244507 for validation audios
0.7818318522919863
Epoch: 142/200
Train loss: 2.101897
Time needed: 171.14039182662964 for validation audios
0.7820642029862584
Epoch: 143/200
Train loss: 1.964019
Time needed: 171.451318025589 for validation audios
0.7826919243352731
Model improve: 0.7823 -> 0.7827
Epoch: 144/200
Train loss: 2.188007
Time needed: 171.0691385269165 for validation audios
0.7802384898587549
Epoch: 145/200
Train loss: 2.151758
Time needed: 168.381685256958 for validation audios
0.7799052418889779
Epoch: 146/200
Train loss: 1.990166
Time needed: 170.5413384437561 for validation audios
0.7826723527415484
Epoch: 147/200
Train loss: 2.148869
Time needed: 173.06724762916565 for validation audios
0.7807416412408811
Epoch: 148/200
Train loss: 2.093935
Time needed: 170.0985507965088 for validation audios
0.7793311653998016
Epoch: 149/200
Train loss: 2.200911
Time needed: 170.48832035064697 for validation audios
0.7785943386286317
Epoch: 150/200
Train loss: 2.124473
Time needed: 171.24136972427368 for validation audios
0.7808861818514443
Epoch: 151/200
Train loss: 1.889027
Time needed: 173.19766998291016 for validation audios
0.7823655204031584
Epoch: 152/200
Train loss: 2.071920
Time needed: 171.56451106071472 for validation audios
0.7813480232944149
Epoch: 153/200
Train loss: 1.972191
Time needed: 169.217182636261 for validation audios
0.7810177749441468
Epoch: 154/200
Train loss: 1.982767
Time needed: 170.40416502952576 for validation audios
0.7835065529021511
Model improve: 0.7827 -> 0.7835
Epoch: 155/200
Train loss: 2.105027
Time needed: 175.67098736763 for validation audios
0.7799739025850959
Epoch: 156/200
Train loss: 2.039241
Time needed: 169.37402772903442 for validation audios
0.7817605672099437
Epoch: 157/200
Train loss: 2.100496
Time needed: 173.16343021392822 for validation audios
0.7796450808634567
Epoch: 158/200
Train loss: 2.116354
Time needed: 170.93703389167786 for validation audios
0.7816071101454283
Epoch: 159/200
Train loss: 1.958448
Time needed: 170.94009280204773 for validation audios
0.7814985582250096
Epoch: 160/200
Train loss: 1.962927
Time needed: 171.93233466148376 for validation audios
0.782105468975788
Epoch: 161/200
Train loss: 2.112558
Time needed: 171.87786149978638 for validation audios
0.7817579534032789
Epoch: 162/200
Train loss: 2.102678
Time needed: 170.48069071769714 for validation audios
0.778990915025508
Epoch: 163/200
Train loss: 2.029771
Time needed: 170.6497678756714 for validation audios
0.7824185136875909
Epoch: 164/200
Train loss: 2.050269
Time needed: 171.38733077049255 for validation audios
0.7831018401548278
Epoch: 165/200
Train loss: 2.035822
Time needed: 172.62655186653137 for validation audios
0.780249207629412
Epoch: 166/200
Train loss: 2.185055
Time needed: 170.05872678756714 for validation audios
0.7807892797330771
Epoch: 167/200
Train loss: 2.181380
Time needed: 170.2502155303955 for validation audios
0.7779993766532179
Epoch: 168/200
Train loss: 2.091484
Time needed: 170.4376242160797 for validation audios
0.7816278354884645
Epoch: 169/200
Train loss: 2.184263
Time needed: 170.28535795211792 for validation audios
0.7792628968075567
Epoch: 170/200
Train loss: 2.076469
Time needed: 170.12020182609558 for validation audios
0.7809435243865781
Epoch: 171/200
Train loss: 2.085573
Time needed: 170.90812230110168 for validation audios
0.7812778935422969
Epoch: 172/200
Train loss: 2.044172
Time needed: 170.41212582588196 for validation audios
0.7818906969810233
Epoch: 173/200
Train loss: 2.095099
Time needed: 174.99562883377075 for validation audios
0.782798271231643
Epoch: 174/200
Train loss: 2.130200
Time needed: 169.64458918571472 for validation audios
0.7802304037042134
Epoch: 175/200
Train loss: 2.016144
Time needed: 169.43686985969543 for validation audios
0.7817373693541032
Epoch: 176/200
Train loss: 2.027501
Time needed: 168.94638466835022 for validation audios
0.7814759416153289
Epoch: 177/200
Train loss: 2.131582
Time needed: 171.35065484046936 for validation audios
0.7804347066638778
Epoch: 178/200
Train loss: 1.870807
Time needed: 170.58590841293335 for validation audios
0.7806188137072352
Epoch: 179/200
Train loss: 2.067643
Time needed: 170.89565086364746 for validation audios
0.7814653293255458
Epoch: 180/200
Train loss: 1.967870
Time needed: 172.18709087371826 for validation audios
0.7833042427139213
Epoch: 181/200
Train loss: 2.000619
Time needed: 172.89425230026245 for validation audios
0.7810362235803112
Epoch: 182/200
Train loss: 2.149269
Date :05/06/2023, 11:15:15
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 0
14388
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 34.515504
Epoch: 2/200
Train loss: 5.529441
Epoch: 3/200
Train loss: 5.042804
Epoch: 4/200
Train loss: 4.721678
Epoch: 5/200
Train loss: 4.398622
Epoch: 6/200
Train loss: 4.142224
Epoch: 7/200
Date :05/06/2023, 11:23:34
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 0
13406
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Train loss: 21.527296
Epoch: 2/100
Date :05/06/2023, 11:25:24
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 0
13406
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Train loss: 36.044367
Epoch: 2/100
Train loss: 5.780751
Epoch: 3/100
Train loss: 5.303962
Epoch: 4/100
Train loss: 4.959794
Epoch: 5/100
Train loss: 4.621123
Epoch: 6/100
Train loss: 4.314061
Epoch: 7/100
Train loss: 4.075919
Epoch: 8/100
Train loss: 3.944355
Epoch: 9/100
Train loss: 3.768274
Epoch: 10/100
Train loss: 3.659102
Epoch: 11/100
Train loss: 3.650407
Epoch: 12/100
Train loss: 3.482146
Epoch: 13/100
Train loss: 3.462920
Epoch: 14/100
Train loss: 3.393152
Epoch: 15/100
Train loss: 3.347869
Epoch: 16/100
Train loss: 3.335514
Epoch: 17/100
Train loss: 3.158753
Epoch: 18/100
Train loss: 3.152896
Epoch: 19/100
Train loss: 3.175906
Epoch: 20/100
Train loss: 3.191961
Epoch: 21/100
Train loss: 3.127780
Epoch: 22/100
Train loss: 3.006583
Epoch: 23/100
Train loss: 2.850417
Epoch: 24/100
Train loss: 3.015025
Epoch: 25/100
Train loss: 3.040482
Epoch: 26/100
Train loss: 2.912170
Epoch: 27/100
Train loss: 2.965960
Epoch: 28/100
Train loss: 2.923166
Epoch: 29/100
Train loss: 2.814931
Epoch: 30/100
Train loss: 2.831197
Epoch: 31/100
Train loss: 2.706027
Epoch: 32/100
Train loss: 2.826133
Epoch: 33/100
Train loss: 2.871941
Epoch: 34/100
Train loss: 2.678617
Epoch: 35/100
Train loss: 2.720704
Epoch: 36/100
Train loss: 2.796157
Epoch: 37/100
Train loss: 2.834796
Epoch: 38/100
Train loss: 2.627029
Epoch: 39/100
Train loss: 2.867562
Epoch: 40/100
Train loss: 2.744495
Epoch: 41/100
Train loss: 2.565225
Epoch: 42/100
Train loss: 2.695901
Epoch: 43/100
Train loss: 2.647585
Epoch: 44/100
Train loss: 2.661919
Epoch: 45/100
Train loss: 2.662763
Epoch: 46/100
Train loss: 2.762092
Epoch: 47/100
Train loss: 2.667238
Epoch: 48/100
Train loss: 2.528889
Epoch: 49/100
Train loss: 2.538729
Epoch: 50/100
Train loss: 2.448737
Epoch: 51/100
Train loss: 2.410572
Epoch: 52/100
Train loss: 2.491605
Epoch: 53/100
Train loss: 2.510517
Epoch: 54/100
Train loss: 2.493666
Epoch: 55/100
Train loss: 2.444035
Epoch: 56/100
Train loss: 2.529135
Epoch: 57/100
Train loss: 2.575199
Epoch: 58/100
Train loss: 2.399092
Epoch: 59/100
Train loss: 2.580057
Epoch: 60/100
Train loss: 2.505770
Epoch: 61/100
Train loss: 2.529304
Epoch: 62/100
Train loss: 2.481338
Time needed: 145.07963132858276 for validation audios
0.7969530422028347
Model improve: 0.0000 -> 0.7970
Epoch: 63/100
Train loss: 2.466440
Time needed: 145.2070028781891 for validation audios
0.797716933415526
Model improve: 0.7970 -> 0.7977
Epoch: 64/100
Train loss: 2.364442
Time needed: 146.4654951095581 for validation audios
0.798025056791905
Model improve: 0.7977 -> 0.7980
Epoch: 65/100
Train loss: 2.495212
Time needed: 147.39490365982056 for validation audios
0.7981162166577176
Model improve: 0.7980 -> 0.7981
Epoch: 66/100
Train loss: 2.470501
Time needed: 144.9693169593811 for validation audios
0.7978662677078435
Epoch: 67/100
Train loss: 2.463777
Date :05/06/2023, 12:53:24
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 0
13019
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Train loss: 38.680314
Epoch: 2/100
Date :05/06/2023, 12:55:06
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 0
13019
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Train loss: 38.646796
Epoch: 2/100
Train loss: 5.818451
Epoch: 3/100
Train loss: 5.362032
Epoch: 4/100
Train loss: 5.018460
Epoch: 5/100
Train loss: 4.698725
Epoch: 6/100
Train loss: 4.339039
Epoch: 7/100
Train loss: 4.197232
Epoch: 8/100
Train loss: 3.988733
Epoch: 9/100
Train loss: 3.828785
Epoch: 10/100
Train loss: 3.666946
Epoch: 11/100
Train loss: 3.685881
Epoch: 12/100
Train loss: 3.525930
Epoch: 13/100
Train loss: 3.546332
Epoch: 14/100
Train loss: 3.427313
Epoch: 15/100
Train loss: 3.339384
Epoch: 16/100
Train loss: 3.386719
Epoch: 17/100
Train loss: 3.280368
Epoch: 18/100
Train loss: 3.229748
Epoch: 19/100
Train loss: 3.080555
Epoch: 20/100
Train loss: 3.301248
Epoch: 21/100
Train loss: 3.121429
Epoch: 22/100
Train loss: 3.182234
Epoch: 23/100
Train loss: 2.924694
Epoch: 24/100
Train loss: 2.953613
Epoch: 25/100
Train loss: 2.992823
Epoch: 26/100
Train loss: 3.121524
Epoch: 27/100
Train loss: 2.864460
Epoch: 28/100
Train loss: 2.977974
Epoch: 29/100
Train loss: 2.918412
Epoch: 30/100
Train loss: 2.849568
Epoch: 31/100
Train loss: 2.842261
Epoch: 32/100
Train loss: 2.698065
Epoch: 33/100
Train loss: 2.856671
Epoch: 34/100
Train loss: 2.896946
Epoch: 35/100
Train loss: 2.672242
Epoch: 36/100
Train loss: 2.755774
Epoch: 37/100
Train loss: 2.775107
Epoch: 38/100
Train loss: 2.867387
Epoch: 39/100
Train loss: 2.671186
Epoch: 40/100
Train loss: 2.800017
Epoch: 41/100
Train loss: 2.881905
Epoch: 42/100
Train loss: 2.576455
Epoch: 43/100
Train loss: 2.739265
Epoch: 44/100
Train loss: 2.641006
Epoch: 45/100
Train loss: 2.672118
Epoch: 46/100
Train loss: 2.750809
Epoch: 47/100
Train loss: 2.737170
Epoch: 48/100
Train loss: 2.712357
Epoch: 49/100
Train loss: 2.508688
Epoch: 50/100
Train loss: 2.658967
Epoch: 51/100
Train loss: 2.523186
Epoch: 52/100
Train loss: 2.393306
Epoch: 53/100
Train loss: 2.547141
Epoch: 54/100
Train loss: 2.472678
Epoch: 55/100
Train loss: 2.547051
Epoch: 56/100
Train loss: 2.396371
Epoch: 57/100
Train loss: 2.595502
Epoch: 58/100
Train loss: 2.497726
Epoch: 59/100
Train loss: 2.443975
Epoch: 60/100
Train loss: 2.570215
Epoch: 61/100
Train loss: 2.491589
Epoch: 62/100
Train loss: 2.529931
Time needed: 171.50730752944946 for validation audios
0.7787366293266244
Model improve: 0.0000 -> 0.7787
Epoch: 63/100
Train loss: 2.498137
Time needed: 173.6675090789795 for validation audios
0.779831902647469
Model improve: 0.7787 -> 0.7798
Epoch: 64/100
Train loss: 2.499820
Time needed: 172.11385560035706 for validation audios
0.7788404665593373
Epoch: 65/100
Train loss: 2.536178
Date :05/06/2023, 14:15:33
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 30
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 0
14072
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Train loss: 21.427816
Epoch: 2/100
Train loss: 5.794772
Epoch: 3/100
Train loss: 5.087913
Epoch: 4/100
Train loss: 4.435128
Epoch: 5/100
Train loss: 4.058099
Epoch: 6/100
Train loss: 3.877150
Epoch: 7/100
Train loss: 3.673507
Epoch: 8/100
Train loss: 3.568449
Epoch: 9/100
Train loss: 3.425911
Epoch: 10/100
Train loss: 3.434963
Epoch: 11/100
Train loss: 3.168214
Epoch: 12/100
Train loss: 3.276185
Epoch: 13/100
Train loss: 3.159205
Epoch: 14/100
Train loss: 3.055017
Epoch: 15/100
Train loss: 2.958779
Epoch: 16/100
Train loss: 3.042599
Epoch: 17/100
Train loss: 2.934669
Epoch: 18/100
Train loss: 2.935733
Epoch: 19/100
Train loss: 2.989469
Epoch: 20/100
Train loss: 2.871145
Epoch: 21/100
Train loss: 2.869509
Epoch: 22/100
Train loss: 2.857047
Epoch: 23/100
Train loss: 2.776124
Epoch: 24/100
Train loss: 2.616138
Epoch: 25/100
Train loss: 2.653485
Epoch: 26/100
Train loss: 2.632202
Epoch: 27/100
Train loss: 2.717836
Epoch: 28/100
Train loss: 2.606892
Epoch: 29/100
Train loss: 2.674046
Epoch: 30/100
Train loss: 2.616534
Epoch: 31/100
Train loss: 2.565202
Epoch: 32/100
Train loss: 2.608967
Epoch: 33/100
Train loss: 2.583890
Epoch: 34/100
Train loss: 2.665607
Epoch: 35/100
Train loss: 2.485865
Epoch: 36/100
Train loss: 2.506583
Epoch: 37/100
Train loss: 2.573681
Epoch: 38/100
Train loss: 2.526803
Epoch: 39/100
Train loss: 2.371452
Epoch: 40/100
Train loss: 2.460614
Epoch: 41/100
Train loss: 2.371649
Epoch: 42/100
Train loss: 2.477878
Epoch: 43/100
Train loss: 2.367803
Epoch: 44/100
Train loss: 2.519056
Epoch: 45/100
Train loss: 2.463444
Epoch: 46/100
Train loss: 2.498241
Epoch: 47/100
Train loss: 2.426412
Epoch: 48/100
Train loss: 2.482842
Epoch: 49/100
Train loss: 2.357114
Epoch: 50/100
Train loss: 2.453734
Epoch: 51/100
Train loss: 2.242514
Epoch: 52/100
Train loss: 2.310054
Epoch: 53/100
Train loss: 2.424175
Epoch: 54/100
Train loss: 2.323782
Epoch: 55/100
Train loss: 2.314194
Epoch: 56/100
Train loss: 2.376619
Epoch: 57/100
Train loss: 2.372962
Epoch: 58/100
Train loss: 2.305039
Epoch: 59/100
Train loss: 2.309330
Epoch: 60/100
Train loss: 2.342645
Epoch: 61/100
Train loss: 2.491174
Epoch: 62/100
Train loss: 2.331210
Time needed: 173.28662633895874 for validation audios
0.7843468194983637
Date :05/06/2023, 15:41:28
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 200
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 30
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 0
14072
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 21.432869
Epoch: 2/200
Train loss: 5.794684
Epoch: 3/200
Train loss: 5.087150
Epoch: 4/200
Train loss: 4.433929
Epoch: 5/200
Train loss: 4.059257
Epoch: 6/200
Train loss: 3.878365
Epoch: 7/200
Train loss: 3.673713
Epoch: 8/200
Train loss: 3.568338
Epoch: 9/200
Train loss: 3.424579
Epoch: 10/200
Train loss: 3.434199
Epoch: 11/200
Train loss: 3.167235
Epoch: 12/200
Train loss: 3.274679
Epoch: 13/200
Train loss: 3.157370
Epoch: 14/200
Train loss: 3.052961
Epoch: 15/200
Train loss: 2.956795
Epoch: 16/200
Train loss: 3.041096
Epoch: 17/200
Train loss: 2.932506
Epoch: 18/200
Train loss: 2.932867
Epoch: 19/200
Train loss: 2.986866
Epoch: 20/200
Train loss: 2.868400
Epoch: 21/200
Train loss: 2.866804
Epoch: 22/200
Train loss: 2.854524
Epoch: 23/200
Train loss: 2.772171
Epoch: 24/200
Train loss: 2.612234
Epoch: 25/200
Train loss: 2.649937
Epoch: 26/200
Train loss: 2.628069
Epoch: 27/200
Train loss: 2.713583
Epoch: 28/200
Train loss: 2.601530
Epoch: 29/200
Train loss: 2.671234
Epoch: 30/200
Train loss: 2.612041
Epoch: 31/200
Train loss: 2.561200
Epoch: 32/200
Train loss: 2.606530
Epoch: 33/200
Train loss: 2.579580
Epoch: 34/200
Train loss: 2.661539
Epoch: 35/200
Train loss: 2.482911
Epoch: 36/200
Train loss: 2.502303
Epoch: 37/200
Train loss: 2.571913
Epoch: 38/200
Train loss: 2.522950
Epoch: 39/200
Train loss: 2.367378
Epoch: 40/200
Train loss: 2.456170
Epoch: 41/200
Train loss: 2.367312
Epoch: 42/200
Train loss: 2.473914
Epoch: 43/200
Train loss: 2.364581
Epoch: 44/200
Train loss: 2.515295
Epoch: 45/200
Train loss: 2.459093
Epoch: 46/200
Train loss: 2.495122
Epoch: 47/200
Train loss: 2.424011
Epoch: 48/200
Train loss: 2.479730
Epoch: 49/200
Train loss: 2.353668
Epoch: 50/200
Train loss: 2.450120
Epoch: 51/200
Train loss: 2.236761
Epoch: 52/200
Train loss: 2.305587
Epoch: 53/200
Train loss: 2.419323
Epoch: 54/200
Train loss: 2.318108
Epoch: 55/200
Train loss: 2.309032
Epoch: 56/200
Train loss: 2.368913
Epoch: 57/200
Train loss: 2.366515
Epoch: 58/200
Train loss: 2.298363
Epoch: 59/200
Train loss: 2.301402
Epoch: 60/200
Train loss: 2.333937
Epoch: 61/200
Train loss: 2.485060
Epoch: 62/200
Train loss: 2.319867
Epoch: 63/200
Train loss: 2.298599
Epoch: 64/200
Train loss: 2.172764
Epoch: 65/200
Train loss: 2.347634
Epoch: 66/200
Train loss: 2.251663
Epoch: 67/200
Train loss: 2.228469
Epoch: 68/200
Train loss: 2.197186
Epoch: 69/200
Train loss: 2.258023
Epoch: 70/200
Train loss: 2.214572
Epoch: 71/200
Train loss: 2.329821
Epoch: 72/200
Train loss: 2.211312
Epoch: 73/200
Train loss: 2.332769
Epoch: 74/200
Train loss: 2.134231
Epoch: 75/200
Train loss: 2.172530
Epoch: 76/200
Train loss: 2.157533
Epoch: 77/200
Train loss: 2.269211
Epoch: 78/200
Train loss: 2.144300
Epoch: 79/200
Train loss: 2.178089
Epoch: 80/200
Train loss: 2.257123
Epoch: 81/200
Train loss: 2.245006
Epoch: 82/200
Train loss: 2.282074
Epoch: 83/200
Train loss: 2.252612
Epoch: 84/200
Train loss: 2.209111
Epoch: 85/200
Train loss: 2.226372
Epoch: 86/200
Train loss: 2.143840
Epoch: 87/200
Train loss: 2.130320
Epoch: 88/200
Train loss: 2.164152
Epoch: 89/200
Train loss: 2.189004
Epoch: 90/200
Train loss: 2.098500
Epoch: 91/200
Train loss: 2.189661
Epoch: 92/200
Train loss: 2.186772
Epoch: 93/200
Train loss: 2.172781
Epoch: 94/200
Train loss: 2.206356
Epoch: 95/200
Train loss: 2.240336
Epoch: 96/200
Train loss: 2.178996
Epoch: 97/200
Train loss: 2.185554
Epoch: 98/200
Train loss: 2.135589
Epoch: 99/200
Train loss: 2.195610
Epoch: 100/200
Train loss: 2.153879
Epoch: 101/200
Train loss: 2.185689
Epoch: 102/200
Train loss: 2.082872
Epoch: 103/200
Train loss: 2.072110
Epoch: 104/200
Train loss: 2.185747
Epoch: 105/200
Train loss: 2.082438
Epoch: 106/200
Train loss: 2.106653
Epoch: 107/200
Train loss: 2.085481
Epoch: 108/200
Train loss: 2.158883
Epoch: 109/200
Train loss: 2.098793
Epoch: 110/200
Train loss: 2.111645
Epoch: 111/200
Train loss: 2.125456
Epoch: 112/200
Train loss: 2.119036
Epoch: 113/200
Train loss: 2.053293
Epoch: 114/200
Train loss: 2.091870
Epoch: 115/200
Train loss: 1.980574
Epoch: 116/200
Train loss: 2.043491
Epoch: 117/200
Train loss: 2.210416
Epoch: 118/200
Train loss: 1.978214
Epoch: 119/200
Train loss: 2.004395
Epoch: 120/200
Train loss: 2.086668
Epoch: 121/200
Train loss: 2.160751
Epoch: 122/200
Train loss: 2.067055
Time needed: 172.81616973876953 for validation audios
0.788543387981248
Model improve: 0.0000 -> 0.7885
Epoch: 123/200
Train loss: 2.145191
Time needed: 173.65567302703857 for validation audios
0.7903899712202855
Model improve: 0.7885 -> 0.7904
Epoch: 124/200
Train loss: 2.007239
Time needed: 174.4635090827942 for validation audios
0.7891672851294083
Epoch: 125/200
Train loss: 2.075866
Time needed: 172.7461063861847 for validation audios
0.7883072806644673
Epoch: 126/200
Train loss: 2.079028
Time needed: 171.91649723052979 for validation audios
0.7880520714189286
Epoch: 127/200
Train loss: 2.077205
Time needed: 170.53502297401428 for validation audios
0.7887860590040332
Epoch: 128/200
Train loss: 2.118477
Time needed: 171.87947487831116 for validation audios
0.7901720940796276
Epoch: 129/200
Train loss: 2.000310
Time needed: 170.76804780960083 for validation audios
0.7909751340592364
Model improve: 0.7904 -> 0.7910
Epoch: 130/200
Train loss: 2.143632
Time needed: 175.00409841537476 for validation audios
0.788522300597927
Epoch: 131/200
Train loss: 2.062755
Time needed: 172.5376739501953 for validation audios
0.7882259585355682
Epoch: 132/200
Train loss: 1.977185
Time needed: 173.6561462879181 for validation audios
0.7914517413270712
Model improve: 0.7910 -> 0.7915
Epoch: 133/200
Train loss: 1.999109
Time needed: 172.59130692481995 for validation audios
0.7864464573552392
Epoch: 134/200
Train loss: 2.026763
Time needed: 172.19409441947937 for validation audios
0.7899840471007072
Epoch: 135/200
Train loss: 2.118652
Time needed: 174.94398975372314 for validation audios
0.7896566001601811
Epoch: 136/200
Train loss: 2.056251
Time needed: 174.23435187339783 for validation audios
0.7896455420493355
Epoch: 137/200
Train loss: 2.118511
Time needed: 172.96574234962463 for validation audios
0.7888270432652843
Epoch: 138/200
Train loss: 2.030994
Time needed: 173.25357723236084 for validation audios
0.7898226135038691
Epoch: 139/200
Train loss: 2.063243
Time needed: 173.89431619644165 for validation audios
0.7900704406974025
Epoch: 140/200
Train loss: 2.051884
Time needed: 174.38145542144775 for validation audios
0.7901100356313653
Epoch: 141/200
Train loss: 2.053046
Time needed: 174.29331350326538 for validation audios
0.7885887783960516
Epoch: 142/200
Train loss: 2.102808
Time needed: 171.33346819877625 for validation audios
0.7907329011941796
Epoch: 143/200
Train loss: 2.136595
Time needed: 171.6609125137329 for validation audios
0.7867262901261297
Epoch: 144/200
Train loss: 2.061043
Time needed: 171.23630547523499 for validation audios
0.787257283859141
Epoch: 145/200
Train loss: 2.104232
Time needed: 171.10256791114807 for validation audios
0.7913464431373206
Epoch: 146/200
Train loss: 2.032007
Time needed: 173.32259345054626 for validation audios
0.7880898998270791
Epoch: 147/200
Train loss: 1.995009
Time needed: 171.4750838279724 for validation audios
0.7880971680247776
Epoch: 148/200
Train loss: 1.993564
Time needed: 174.47440576553345 for validation audios
0.791720266924983
Model improve: 0.7915 -> 0.7917
Epoch: 149/200
Train loss: 1.976721
Time needed: 173.58444166183472 for validation audios
0.7883531474582793
Epoch: 150/200
Train loss: 2.009653
Time needed: 172.13801956176758 for validation audios
0.7901111049590824
Epoch: 151/200
Train loss: 1.970233
Time needed: 174.59050512313843 for validation audios
0.7917861074814353
Model improve: 0.7917 -> 0.7918
Epoch: 152/200
Train loss: 1.958920
Time needed: 173.3234043121338 for validation audios
0.7879256528244188
Epoch: 153/200
Train loss: 2.087746
Time needed: 171.36547708511353 for validation audios
0.7900409587887188
Epoch: 154/200
Train loss: 2.046358
Time needed: 172.66899824142456 for validation audios
0.7888548937115517
Epoch: 155/200
Train loss: 1.946035
Time needed: 171.39405131340027 for validation audios
0.7913707893418848
Epoch: 156/200
Train loss: 2.028020
Time needed: 170.91186547279358 for validation audios
0.7907790245002438
Epoch: 157/200
Train loss: 2.002060
Time needed: 173.81937718391418 for validation audios
0.7916334988785805
Epoch: 158/200
Train loss: 2.057082
Time needed: 171.55069231987 for validation audios
0.7879719551956398
Epoch: 159/200
Train loss: 2.071121
Time needed: 171.34220695495605 for validation audios
0.7896031092564686
Epoch: 160/200
Train loss: 2.027485
Time needed: 173.63591194152832 for validation audios
0.7938246863360439
Model improve: 0.7918 -> 0.7938
Epoch: 161/200
Train loss: 2.021071
Time needed: 173.35918307304382 for validation audios
0.7902070930907737
Epoch: 162/200
Train loss: 2.042882
Time needed: 173.1189513206482 for validation audios
0.7893061363493324
Epoch: 163/200
Train loss: 1.970539
Time needed: 173.07217931747437 for validation audios
0.7891069731526448
Epoch: 164/200
Train loss: 2.017409
Time needed: 173.93166875839233 for validation audios
0.7909799477199873
Epoch: 165/200
Train loss: 2.038814
Time needed: 174.0572154521942 for validation audios
0.7896204531588944
Epoch: 166/200
Train loss: 2.130820
Time needed: 170.58109521865845 for validation audios
0.7909717058801051
Epoch: 167/200
Train loss: 2.038238
Time needed: 170.83056950569153 for validation audios
0.7916826253924052
Epoch: 168/200
Train loss: 1.904103
Time needed: 170.84538459777832 for validation audios
0.7912067340744028
Epoch: 169/200
Train loss: 2.004894
Time needed: 172.62985730171204 for validation audios
0.790495315203362
Epoch: 170/200
Train loss: 2.045248
Time needed: 172.98612904548645 for validation audios
0.7889412173167951
Epoch: 171/200
Train loss: 2.044539
Time needed: 170.5717170238495 for validation audios
0.7892261161931268
Epoch: 172/200
Train loss: 1.993989
Time needed: 173.13823056221008 for validation audios
0.7908004855420848
Epoch: 173/200
Train loss: 1.955407
Time needed: 174.0367259979248 for validation audios
0.7914451992280078
Epoch: 174/200
Train loss: 1.981035
Time needed: 170.35363864898682 for validation audios
0.7902901117978551
Epoch: 175/200
Train loss: 1.961073
Time needed: 172.85025310516357 for validation audios
0.7921427129585682
Epoch: 176/200
Train loss: 1.977635
Time needed: 172.8520050048828 for validation audios
0.7897389437053604
Epoch: 177/200
Train loss: 2.010333
Time needed: 174.2222776412964 for validation audios
0.7903656472276132
Epoch: 178/200
Train loss: 2.098176
Time needed: 171.76468777656555 for validation audios
0.7905590520734526
Epoch: 179/200
Train loss: 2.012635
Time needed: 170.87971591949463 for validation audios
0.7908375141215925
Epoch: 180/200
Train loss: 2.069470
Time needed: 174.63100290298462 for validation audios
0.7906792230704528
Epoch: 181/200
Train loss: 1.912289
Time needed: 174.5759880542755 for validation audios
0.7903018479697521
Epoch: 182/200
Train loss: 1.973802
Time needed: 173.61620783805847 for validation audios
0.7896134969103406
Epoch: 183/200
Train loss: 2.074682
Time needed: 174.342218875885 for validation audios
0.790797132164835
Epoch: 184/200
Train loss: 1.990065
Date :05/06/2023, 22:41:50
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 200
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 0
17596
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 17.701378
Epoch: 2/200
Train loss: 5.749446
Epoch: 3/200
Train loss: 4.739534
Epoch: 4/200
Train loss: 4.109016
Epoch: 5/200
Train loss: 3.823756
Epoch: 6/200
Train loss: 3.630407
Epoch: 7/200
Train loss: 3.401186
Epoch: 8/200
Train loss: 3.416456
Epoch: 9/200
Train loss: 3.155136
Epoch: 10/200
Train loss: 3.185275
Epoch: 11/200
Train loss: 3.099947
Epoch: 12/200
Train loss: 2.928849
Epoch: 13/200
Train loss: 2.985351
Epoch: 14/200
Train loss: 2.957620
Epoch: 15/200
Train loss: 2.929072
Epoch: 16/200
Train loss: 2.850323
Epoch: 17/200
Train loss: 2.813386
Epoch: 18/200
Train loss: 2.806936
Epoch: 19/200
Train loss: 2.663488
Epoch: 20/200
Train loss: 2.605468
Epoch: 21/200
Train loss: 2.661618
Epoch: 22/200
Train loss: 2.606551
Epoch: 23/200
Train loss: 2.667266
Epoch: 24/200
Train loss: 2.596407
Epoch: 25/200
Train loss: 2.575477
Epoch: 26/200
Train loss: 2.549002
Epoch: 27/200
Train loss: 2.591745
Epoch: 28/200
Train loss: 2.506501
Epoch: 29/200
Train loss: 2.488068
Epoch: 30/200
Train loss: 2.530440
Epoch: 31/200
Train loss: 2.458212
Epoch: 32/200
Train loss: 2.384280
Epoch: 33/200
Train loss: 2.356529
Epoch: 34/200
Train loss: 2.450498
Epoch: 35/200
Train loss: 2.435527
Epoch: 36/200
Train loss: 2.453193
Epoch: 37/200
Train loss: 2.453937
Epoch: 38/200
Train loss: 2.513071
Epoch: 39/200
Train loss: 2.353407
Epoch: 40/200
Train loss: 2.412435
Epoch: 41/200
Train loss: 2.246090
Epoch: 42/200
Train loss: 2.368562
Epoch: 43/200
Train loss: 2.363928
Epoch: 44/200
Train loss: 2.313114
Epoch: 45/200
Train loss: 2.360161
Epoch: 46/200
Train loss: 2.341443
Epoch: 47/200
Train loss: 2.295021
Epoch: 48/200
Train loss: 2.318352
Epoch: 49/200
Train loss: 2.449513
Epoch: 50/200
Train loss: 2.305742
Epoch: 51/200
Train loss: 2.208889
Epoch: 52/200
Train loss: 2.288383
Epoch: 53/200
Train loss: 2.274264
Epoch: 54/200
Train loss: 2.215543
Epoch: 55/200
Train loss: 2.214010
Epoch: 56/200
Train loss: 2.223775
Epoch: 57/200
Train loss: 2.249294
Epoch: 58/200
Train loss: 2.303347
Epoch: 59/200
Train loss: 2.175159
Epoch: 60/200
Train loss: 2.164607
Epoch: 61/200
Train loss: 2.187861
Epoch: 62/200
Train loss: 2.212922
Epoch: 63/200
Train loss: 2.145072
Epoch: 64/200
Train loss: 2.237763
Epoch: 65/200
Train loss: 2.252637
Epoch: 66/200
Train loss: 2.304426
Epoch: 67/200
Train loss: 2.189828
Epoch: 68/200
Train loss: 2.219724
Epoch: 69/200
Train loss: 2.190918
Epoch: 70/200
Train loss: 2.091621
Epoch: 71/200
Train loss: 2.157671
Epoch: 72/200
Train loss: 2.108261
Epoch: 73/200
Train loss: 2.200066
Epoch: 74/200
Train loss: 2.190778
Epoch: 75/200
Train loss: 2.193656
Epoch: 76/200
Train loss: 2.191321
Epoch: 77/200
Train loss: 2.229706
Epoch: 78/200
Train loss: 2.109940
Epoch: 79/200
Train loss: 2.198957
Epoch: 80/200
Train loss: 2.159357
Epoch: 81/200
Train loss: 2.137397
Epoch: 82/200
Train loss: 2.030711
Epoch: 83/200
Train loss: 2.211488
Epoch: 84/200
Train loss: 2.097651
Epoch: 85/200
Train loss: 2.100932
Epoch: 86/200
Train loss: 2.089491
Epoch: 87/200
Train loss: 2.130679
Epoch: 88/200
Train loss: 2.122940
Epoch: 89/200
Train loss: 2.095994
Epoch: 90/200
Train loss: 2.117214
Epoch: 91/200
Train loss: 2.045455
Epoch: 92/200
Train loss: 2.001156
Epoch: 93/200
Train loss: 2.072346
Epoch: 94/200
Train loss: 2.093933
Epoch: 95/200
Train loss: 1.999142
Epoch: 96/200
Train loss: 2.081785
Epoch: 97/200
Train loss: 2.176945
Epoch: 98/200
Train loss: 2.052139
Epoch: 99/200
Train loss: 2.010444
Epoch: 100/200
Train loss: 2.056948
Epoch: 101/200
Train loss: 2.122348
Epoch: 102/200
Train loss: 2.052259
Epoch: 103/200
Train loss: 2.007580
Epoch: 104/200
Train loss: 2.117359
Epoch: 105/200
Train loss: 2.030276
Epoch: 106/200
Train loss: 1.986147
Epoch: 107/200
Train loss: 2.024420
Epoch: 108/200
Train loss: 2.078571
Epoch: 109/200
Train loss: 2.076449
Epoch: 110/200
Train loss: 2.094589
Epoch: 111/200
Train loss: 2.008252
Epoch: 112/200
Train loss: 2.080882
Epoch: 113/200
Train loss: 2.085504
Epoch: 114/200
Train loss: 2.076575
Epoch: 115/200
Train loss: 2.092824
Epoch: 116/200
Train loss: 2.083571
Epoch: 117/200
Train loss: 2.040888
Epoch: 118/200
Train loss: 1.985885
Epoch: 119/200
Train loss: 1.909453
Epoch: 120/200
Train loss: 2.051389
Epoch: 121/200
Train loss: 1.928220
Epoch: 122/200
Train loss: 2.020248
Time needed: 170.88212752342224 for validation audios
0.7869331144629681
Model improve: 0.0000 -> 0.7869
Epoch: 123/200
Train loss: 2.045389
Time needed: 175.73496460914612 for validation audios
0.787088123190328
Model improve: 0.7869 -> 0.7871
Epoch: 124/200
Date :05/07/2023, 02:05:57
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 200
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 0
thrdownsample: 200
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 0
Date :05/07/2023, 02:06:16
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 200
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 0
thrdownsample: 200
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 0
11571
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 24.042103
Epoch: 2/200
Train loss: 5.582411
Epoch: 3/200
Train loss: 4.990209
Epoch: 4/200
Train loss: 4.478657
Epoch: 5/200
Train loss: 4.122092
Epoch: 6/200
Train loss: 3.921923
Epoch: 7/200
Train loss: 3.793317
Epoch: 8/200
Train loss: 3.672150
Epoch: 9/200
Train loss: 3.544788
Epoch: 10/200
Train loss: 3.455844
Epoch: 11/200
Train loss: 3.410522
Epoch: 12/200
Train loss: 3.391794
Epoch: 13/200
Train loss: 3.187434
Epoch: 14/200
Train loss: 3.154356
Epoch: 15/200
Train loss: 3.189008
Epoch: 16/200
Train loss: 3.173236
Epoch: 17/200
Train loss: 3.011072
Epoch: 18/200
Train loss: 2.982650
Epoch: 19/200
Train loss: 3.041557
Epoch: 20/200
Train loss: 2.879749
Epoch: 21/200
Train loss: 2.953977
Epoch: 22/200
Train loss: 2.877208
Epoch: 23/200
Train loss: 3.010410
Epoch: 24/200
Train loss: 2.831691
Epoch: 25/200
Train loss: 2.796044
Epoch: 26/200
Train loss: 2.870762
Epoch: 27/200
Train loss: 2.898237
Epoch: 28/200
Train loss: 2.722563
Epoch: 29/200
Train loss: 2.613798
Epoch: 30/200
Train loss: 2.588894
Epoch: 31/200
Train loss: 2.676027
Epoch: 32/200
Train loss: 2.642084
Epoch: 33/200
Train loss: 2.630181
Epoch: 34/200
Train loss: 2.593980
Epoch: 35/200
Train loss: 2.641769
Epoch: 36/200
Train loss: 2.620556
Epoch: 37/200
Train loss: 2.520353
Epoch: 38/200
Train loss: 2.603481
Epoch: 39/200
Train loss: 2.573788
Epoch: 40/200
Train loss: 2.523891
Epoch: 41/200
Train loss: 2.613938
Epoch: 42/200
Train loss: 2.595776
Epoch: 43/200
Train loss: 2.405263
Epoch: 44/200
Train loss: 2.528567
Epoch: 45/200
Train loss: 2.527518
Epoch: 46/200
Train loss: 2.496690
Epoch: 47/200
Train loss: 2.382515
Epoch: 48/200
Train loss: 2.379231
Epoch: 49/200
Train loss: 2.410710
Epoch: 50/200
Train loss: 2.326105
Epoch: 51/200
Train loss: 2.512643
Epoch: 52/200
Train loss: 2.357535
Epoch: 53/200
Train loss: 2.458541
Epoch: 54/200
Train loss: 2.447745
Epoch: 55/200
Train loss: 2.419079
Epoch: 56/200
Train loss: 2.483919
Epoch: 57/200
Train loss: 2.406045
Epoch: 58/200
Train loss: 2.453132
Epoch: 59/200
Train loss: 2.351925
Epoch: 60/200
Train loss: 2.395082
Epoch: 61/200
Train loss: 2.361993
Epoch: 62/200
Train loss: 2.240722
Epoch: 63/200
Train loss: 2.293611
Epoch: 64/200
Train loss: 2.358146
Epoch: 65/200
Train loss: 2.363435
Epoch: 66/200
Train loss: 2.409928
Epoch: 67/200
Train loss: 2.211657
Epoch: 68/200
Train loss: 2.364028
Epoch: 69/200
Train loss: 2.325350
Epoch: 70/200
Train loss: 2.325187
Epoch: 71/200
Train loss: 2.251737
Epoch: 72/200
Train loss: 2.292200
Epoch: 73/200
Train loss: 2.278927
Epoch: 74/200
Train loss: 2.443354
Epoch: 75/200
Train loss: 2.383808
Epoch: 76/200
Train loss: 2.275151
Epoch: 77/200
Train loss: 2.228531
Epoch: 78/200
Train loss: 2.144072
Epoch: 79/200
Train loss: 2.315683
Epoch: 80/200
Train loss: 2.218396
Epoch: 81/200
Train loss: 2.180253
Epoch: 82/200
Train loss: 2.219117
Epoch: 83/200
Train loss: 2.164918
Epoch: 84/200
Train loss: 2.266920
Epoch: 85/200
Train loss: 2.166370
Epoch: 86/200
Train loss: 2.289423
Epoch: 87/200
Train loss: 2.195961
Epoch: 88/200
Train loss: 2.272479
Epoch: 89/200
Train loss: 2.304733
Epoch: 90/200
Train loss: 2.053994
Epoch: 91/200
Train loss: 2.128915
Epoch: 92/200
Train loss: 2.153350
Epoch: 93/200
Train loss: 2.230985
Epoch: 94/200
Train loss: 2.173095
Epoch: 95/200
Train loss: 2.123660
Epoch: 96/200
Train loss: 2.158490
Epoch: 97/200
Train loss: 2.240950
Epoch: 98/200
Train loss: 2.136682
Epoch: 99/200
Train loss: 2.303606
Epoch: 100/200
Train loss: 2.236946
Epoch: 101/200
Train loss: 2.225258
Epoch: 102/200
Train loss: 2.175948
Epoch: 103/200
Train loss: 2.199543
Epoch: 104/200
Train loss: 2.160988
Epoch: 105/200
Train loss: 2.182842
Epoch: 106/200
Train loss: 2.048057
Epoch: 107/200
Train loss: 2.110613
Epoch: 108/200
Train loss: 2.182927
Epoch: 109/200
Train loss: 2.082231
Epoch: 110/200
Train loss: 2.108914
Epoch: 111/200
Train loss: 2.167169
Epoch: 112/200
Train loss: 2.194086
Epoch: 113/200
Train loss: 2.134936
Epoch: 114/200
Train loss: 2.208016
Epoch: 115/200
Train loss: 2.182123
Epoch: 116/200
Train loss: 2.215936
Epoch: 117/200
Train loss: 2.146669
Epoch: 118/200
Train loss: 2.143548
Epoch: 119/200
Train loss: 2.105069
Epoch: 120/200
Train loss: 2.179224
Epoch: 121/200
Train loss: 2.168094
Epoch: 122/200
Train loss: 2.160655
Time needed: 172.90350198745728 for validation audios
0.7851941088516888
Model improve: 0.0000 -> 0.7852
Epoch: 123/200
Date :05/07/2023, 04:23:15
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 200
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 30
thrdownsample: 300
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 0
15242
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Date :05/07/2023, 04:23:24
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 200
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 30
thrdownsample: 200
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 0
14072
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Date :05/07/2023, 04:23:38
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 200
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 30
thrdownsample: 300
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 0
15242
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 19.887473
Epoch: 2/200
Train loss: 5.519536
Epoch: 3/200
Train loss: 4.795189
Epoch: 4/200
Train loss: 4.206912
Epoch: 5/200
Train loss: 3.906587
Epoch: 6/200
Train loss: 3.696178
Epoch: 7/200
Train loss: 3.560337
Epoch: 8/200
Train loss: 3.337361
Epoch: 9/200
Train loss: 3.403237
Epoch: 10/200
Train loss: 3.140475
Epoch: 11/200
Train loss: 3.174115
Epoch: 12/200
Train loss: 3.104134
Epoch: 13/200
Train loss: 3.029244
Epoch: 14/200
Train loss: 2.915838
Epoch: 15/200
Train loss: 2.909527
Epoch: 16/200
Train loss: 2.938547
Epoch: 17/200
Train loss: 2.888131
Epoch: 18/200
Train loss: 2.860543
Epoch: 19/200
Train loss: 2.767460
Epoch: 20/200
Train loss: 2.879025
Epoch: 21/200
Train loss: 2.750578
Epoch: 22/200
Train loss: 2.635142
Epoch: 23/200
Train loss: 2.591521
Epoch: 24/200
Train loss: 2.593020
Epoch: 25/200
Train loss: 2.638706
Epoch: 26/200
Train loss: 2.610403
Epoch: 27/200
Train loss: 2.623825
Epoch: 28/200
Train loss: 2.546578
Epoch: 29/200
Train loss: 2.597598
Epoch: 30/200
Train loss: 2.568673
Epoch: 31/200
Train loss: 2.605690
Epoch: 32/200
Train loss: 2.517641
Epoch: 33/200
Train loss: 2.432226
Epoch: 34/200
Train loss: 2.591260
Epoch: 35/200
Train loss: 2.478208
Epoch: 36/200
Train loss: 2.368847
Epoch: 37/200
Train loss: 2.416961
Epoch: 38/200
Train loss: 2.363376
Epoch: 39/200
Train loss: 2.456430
Epoch: 40/200
Train loss: 2.388009
Epoch: 41/200
Train loss: 2.470444
Epoch: 42/200
Train loss: 2.446727
Epoch: 43/200
Train loss: 2.449434
Epoch: 44/200
Train loss: 2.455897
Epoch: 45/200
Train loss: 2.358747
Epoch: 46/200
Train loss: 2.395359
Epoch: 47/200
Train loss: 2.225149
Epoch: 48/200
Train loss: 2.323039
Epoch: 49/200
Train loss: 2.364942
Epoch: 50/200
Train loss: 2.348497
Epoch: 51/200
Train loss: 2.248135
Epoch: 52/200
Train loss: 2.389497
Epoch: 53/200
Train loss: 2.299537
Epoch: 54/200
Train loss: 2.325117
Epoch: 55/200
Train loss: 2.250968
Epoch: 56/200
Train loss: 2.426729
Epoch: 57/200
Train loss: 2.332057
Epoch: 58/200
Train loss: 2.266189
Epoch: 59/200
Train loss: 2.175242
Epoch: 60/200
Train loss: 2.303254
Epoch: 61/200
Train loss: 2.253673
Epoch: 62/200
Train loss: 2.187388
Epoch: 63/200
Train loss: 2.215822
Epoch: 64/200
Train loss: 2.223224
Epoch: 65/200
Train loss: 2.216686
Epoch: 66/200
Train loss: 2.247334
Epoch: 67/200
Train loss: 2.296637
Epoch: 68/200
Train loss: 2.141980
Epoch: 69/200
Train loss: 2.162220
Epoch: 70/200
Train loss: 2.160969
Epoch: 71/200
Train loss: 2.243915
Epoch: 72/200
Train loss: 2.122902
Epoch: 73/200
Train loss: 2.213820
Epoch: 74/200
Train loss: 2.147109
Epoch: 75/200
Train loss: 2.280709
Epoch: 76/200
Train loss: 2.285207
Epoch: 77/200
Train loss: 2.152827
Epoch: 78/200
Train loss: 2.214418
Epoch: 79/200
Train loss: 2.169587
Epoch: 80/200
Train loss: 2.108048
Epoch: 81/200
Train loss: 2.131022
Epoch: 82/200
Train loss: 2.173398
Epoch: 83/200
Train loss: 2.087080
Epoch: 84/200
Train loss: 2.149743
Epoch: 85/200
Train loss: 2.198843
Epoch: 86/200
Train loss: 2.212432
Epoch: 87/200
Train loss: 2.143657
Epoch: 88/200
Train loss: 2.199288
Epoch: 89/200
Train loss: 2.145794
Epoch: 90/200
Train loss: 2.141027
Epoch: 91/200
Train loss: 2.183445
Epoch: 92/200
Train loss: 2.148290
Epoch: 93/200
Train loss: 2.166068
Epoch: 94/200
Train loss: 2.028356
Epoch: 95/200
Train loss: 2.133320
Epoch: 96/200
Train loss: 2.133653
Epoch: 97/200
Train loss: 2.075935
Epoch: 98/200
Train loss: 2.081970
Epoch: 99/200
Train loss: 2.102833
Epoch: 100/200
Train loss: 2.093440
Epoch: 101/200
Train loss: 2.062817
Epoch: 102/200
Train loss: 2.169893
Epoch: 103/200
Train loss: 2.091780
Epoch: 104/200
Train loss: 2.051989
Epoch: 105/200
Train loss: 2.055991
Epoch: 106/200
Train loss: 2.013826
Epoch: 107/200
Train loss: 2.014120
Epoch: 108/200
Train loss: 2.106715
Epoch: 109/200
Train loss: 1.975017
Epoch: 110/200
Train loss: 2.072871
Epoch: 111/200
Train loss: 2.149925
Epoch: 112/200
Train loss: 2.069037
Epoch: 113/200
Train loss: 2.105050
Epoch: 114/200
Train loss: 1.987512
Epoch: 115/200
Train loss: 2.064193
Epoch: 116/200
Train loss: 2.079565
Epoch: 117/200
Train loss: 2.059071
Epoch: 118/200
Train loss: 2.058169
Epoch: 119/200
Train loss: 2.063464
Epoch: 120/200
Train loss: 2.042304
Epoch: 121/200
Train loss: 2.021842
Epoch: 122/200
Train loss: 1.979680
Time needed: 171.81833362579346 for validation audios
0.7896087399693226
Model improve: 0.0000 -> 0.7896
Epoch: 123/200
Train loss: 1.991519
Time needed: 171.57589769363403 for validation audios
0.7912102780718453
Model improve: 0.7896 -> 0.7912
Epoch: 124/200
Train loss: 2.130992
Time needed: 174.3755509853363 for validation audios
0.7891610071493389
Epoch: 125/200
Train loss: 1.967291
Time needed: 181.57449674606323 for validation audios
0.7896468669629849
Epoch: 126/200
Train loss: 2.098395
Time needed: 183.90227437019348 for validation audios
0.7897353374881528
Epoch: 127/200
Train loss: 2.027654
Time needed: 172.8081238269806 for validation audios
0.7891926066565736
Epoch: 128/200
Train loss: 2.049402
Time needed: 171.95527505874634 for validation audios
0.7897709503860116
Epoch: 129/200
Train loss: 2.052382
Time needed: 170.97620606422424 for validation audios
0.7894477519087333
Epoch: 130/200
Train loss: 2.054332
Time needed: 171.7100727558136 for validation audios
0.792204170528752
Model improve: 0.7912 -> 0.7922
Epoch: 131/200
Train loss: 2.066562
Time needed: 171.79438519477844 for validation audios
0.7898369131357805
Epoch: 132/200
Train loss: 2.095067
Time needed: 172.19527626037598 for validation audios
0.7884840406463036
Epoch: 133/200
Train loss: 2.133427
Time needed: 172.45475459098816 for validation audios
0.7897140179346618
Epoch: 134/200
Train loss: 2.004623
Time needed: 172.4770896434784 for validation audios
0.7923696657312478
Model improve: 0.7922 -> 0.7924
Epoch: 135/200
Train loss: 2.026787
Time needed: 171.92004418373108 for validation audios
0.7899801599158366
Epoch: 136/200
Train loss: 1.958928
Date :05/07/2023, 14:56:46
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 200
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 30
thrdownsample: 300
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 1
15267
Date :05/07/2023, 14:57:53
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 200
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 30
thrdownsample: 300
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 1
15267
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Date :05/07/2023, 14:58:17
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 200
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 30
thrdownsample: 300
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 1
15267
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 24.008552
Epoch: 2/200
Train loss: 5.340500
Epoch: 3/200
Train loss: 4.529540
Epoch: 4/200
Train loss: 3.971195
Epoch: 5/200
Train loss: 3.733391
Epoch: 6/200
Train loss: 3.549750
Epoch: 7/200
Train loss: 3.442062
Epoch: 8/200
Train loss: 3.201313
Epoch: 9/200
Train loss: 3.302931
Epoch: 10/200
Train loss: 3.063486
Epoch: 11/200
Train loss: 3.082214
Epoch: 12/200
Train loss: 3.020889
Epoch: 13/200
Train loss: 2.946520
Epoch: 14/200
Train loss: 2.837748
Epoch: 15/200
Train loss: 2.837346
Epoch: 16/200
Train loss: 2.882216
Epoch: 17/200
Train loss: 2.846622
Epoch: 18/200
Train loss: 2.784693
Epoch: 19/200
Train loss: 2.756985
Epoch: 20/200
Train loss: 2.838372
Epoch: 21/200
Train loss: 2.707095
Epoch: 22/200
Train loss: 2.591748
Epoch: 23/200
Train loss: 2.560201
Epoch: 24/200
Train loss: 2.572809
Epoch: 25/200
Train loss: 2.604029
Epoch: 26/200
Train loss: 2.571926
Epoch: 27/200
Train loss: 2.614987
Epoch: 28/200
Train loss: 2.522539
Epoch: 29/200
Train loss: 2.577051
Epoch: 30/200
Train loss: 2.539477
Epoch: 31/200
Train loss: 2.586495
Epoch: 32/200
Train loss: 2.505153
Epoch: 33/200
Train loss: 2.407746
Epoch: 34/200
Train loss: 2.553512
Epoch: 35/200
Train loss: 2.469287
Epoch: 36/200
Train loss: 2.353557
Epoch: 37/200
Train loss: 2.406697
Epoch: 38/200
Train loss: 2.341171
Epoch: 39/200
Train loss: 2.437359
Epoch: 40/200
Train loss: 2.398799
Epoch: 41/200
Train loss: 2.447111
Epoch: 42/200
Train loss: 2.451437
Epoch: 43/200
Train loss: 2.428808
Epoch: 44/200
Train loss: 2.458425
Epoch: 45/200
Train loss: 2.347012
Epoch: 46/200
Train loss: 2.415060
Epoch: 47/200
Train loss: 2.225769
Epoch: 48/200
Train loss: 2.308414
Epoch: 49/200
Train loss: 2.378427
Epoch: 50/200
Train loss: 2.346639
Epoch: 51/200
Train loss: 2.265872
Epoch: 52/200
Train loss: 2.390927
Epoch: 53/200
Train loss: 2.305863
Epoch: 54/200
Train loss: 2.314001
Epoch: 55/200
Train loss: 2.276187
Epoch: 56/200
Train loss: 2.446965
Epoch: 57/200
Train loss: 2.341020
Epoch: 58/200
Train loss: 2.271958
Epoch: 59/200
Train loss: 2.166822
Epoch: 60/200
Train loss: 2.306948
Epoch: 61/200
Train loss: 2.247269
Epoch: 62/200
Train loss: 2.186980
Epoch: 63/200
Train loss: 2.246591
Epoch: 64/200
Train loss: 2.234575
Epoch: 65/200
Train loss: 2.243379
Epoch: 66/200
Train loss: 2.253009
Epoch: 67/200
Train loss: 2.303765
Epoch: 68/200
Train loss: 2.150285
Epoch: 69/200
Train loss: 2.167678
Epoch: 70/200
Train loss: 2.167160
Epoch: 71/200
Train loss: 2.278625
Epoch: 72/200
Train loss: 2.136940
Epoch: 73/200
Train loss: 2.230969
Epoch: 74/200
Train loss: 2.161981
Epoch: 75/200
Train loss: 2.290860
Epoch: 76/200
Train loss: 2.311150
Epoch: 77/200
Train loss: 2.155619
Epoch: 78/200
Train loss: 2.226839
Epoch: 79/200
Train loss: 2.199874
Epoch: 80/200
Train loss: 2.124350
Epoch: 81/200
Train loss: 2.155937
Epoch: 82/200
Train loss: 2.200069
Epoch: 83/200
Train loss: 2.126382
Epoch: 84/200
Train loss: 2.183921
Epoch: 85/200
Train loss: 2.226875
Epoch: 86/200
Train loss: 2.231171
Epoch: 87/200
Train loss: 2.188648
Epoch: 88/200
Train loss: 2.219624
Epoch: 89/200
Train loss: 2.170175
Epoch: 90/200
Train loss: 2.151969
Epoch: 91/200
Train loss: 2.201391
Epoch: 92/200
Train loss: 2.175634
Epoch: 93/200
Train loss: 2.193872
Epoch: 94/200
Train loss: 2.040315
Epoch: 95/200
Train loss: 2.161311
Epoch: 96/200
Train loss: 2.141681
Epoch: 97/200
Train loss: 2.102996
Epoch: 98/200
Train loss: 2.110458
Epoch: 99/200
Train loss: 2.120679
Epoch: 100/200
Train loss: 2.131042
Epoch: 101/200
Train loss: 2.094537
Epoch: 102/200
Train loss: 2.195111
Epoch: 103/200
Train loss: 2.113881
Epoch: 104/200
Train loss: 2.081207
Epoch: 105/200
Train loss: 2.083780
Epoch: 106/200
Train loss: 2.029677
Epoch: 107/200
Train loss: 2.063890
Epoch: 108/200
Train loss: 2.146254
Epoch: 109/200
Train loss: 2.002574
Epoch: 110/200
Train loss: 2.098178
Epoch: 111/200
Train loss: 2.172451
Epoch: 112/200
Train loss: 2.101297
Epoch: 113/200
Train loss: 2.130017
Epoch: 114/200
Train loss: 2.030808
Epoch: 115/200
Train loss: 2.084429
Epoch: 116/200
Train loss: 2.117898
Epoch: 117/200
Train loss: 2.104928
Epoch: 118/200
Train loss: 2.110909
Epoch: 119/200
Train loss: 2.101109
Epoch: 120/200
Train loss: 2.092341
Epoch: 121/200
Train loss: 2.052442
Epoch: 122/200
Train loss: 2.008233
Time needed: 169.07755827903748 for validation audios
0.7958167454403512
Model improve: 0.0000 -> 0.7958
Epoch: 123/200
Train loss: 2.048891
Time needed: 164.2433521747589 for validation audios
0.8001840972906864
Model improve: 0.7958 -> 0.8002
Epoch: 124/200
Train loss: 2.162992
Time needed: 167.60087871551514 for validation audios
0.7964522700774431
Epoch: 125/200
Train loss: 2.015214
Time needed: 165.5867612361908 for validation audios
0.7984898892186459
Epoch: 126/200
Train loss: 2.145879
Time needed: 165.20705199241638 for validation audios
0.7999621841213743
Epoch: 127/200
Train loss: 2.053860
Time needed: 163.7560818195343 for validation audios
0.7986812675922387
Epoch: 128/200
Train loss: 2.095573
Time needed: 166.77408719062805 for validation audios
0.7986756673554387
Epoch: 129/200
Train loss: 2.092403
Time needed: 163.40526008605957 for validation audios
0.7996223855771398
Epoch: 130/200
Train loss: 2.111563
Time needed: 166.00303959846497 for validation audios
0.79950365062371
Epoch: 131/200
Train loss: 2.115965
Time needed: 165.40361952781677 for validation audios
0.799066579744582
Epoch: 132/200
Train loss: 2.141790
Time needed: 167.54123497009277 for validation audios
0.7989855554009005
Epoch: 133/200
Train loss: 2.178572
Time needed: 163.56538605690002 for validation audios
0.7980026388509894
Epoch: 134/200
Train loss: 2.044007
Time needed: 166.70791339874268 for validation audios
0.8010219456086841
Model improve: 0.8002 -> 0.8010
Epoch: 135/200
Train loss: 2.070395
Time needed: 165.18404841423035 for validation audios
0.7966904622683669
Epoch: 136/200
Train loss: 2.013576
Time needed: 166.9980547428131 for validation audios
0.7992163688363447
Epoch: 137/200
Train loss: 1.937497
Time needed: 169.95520973205566 for validation audios
0.8009653656725881
Epoch: 138/200
Train loss: 2.066810
Time needed: 165.29186511039734 for validation audios
0.7979565243958475
Epoch: 139/200
Train loss: 2.004070
Time needed: 166.47426390647888 for validation audios
0.7977104400632407
Epoch: 140/200
Train loss: 2.001844
Time needed: 165.33712530136108 for validation audios
0.7980631014241074
Epoch: 141/200
Train loss: 2.100010
Time needed: 165.40601420402527 for validation audios
0.7970697697143446
Epoch: 142/200
Train loss: 2.064084
Time needed: 166.12962818145752 for validation audios
0.797976372110498
Epoch: 143/200
Train loss: 1.963525
Time needed: 166.05416297912598 for validation audios
0.7962923442987908
Epoch: 144/200
Train loss: 2.038180
Time needed: 166.03960061073303 for validation audios
0.7997496951983738
Epoch: 145/200
Train loss: 2.091559
Time needed: 164.03274488449097 for validation audios
0.798819226020614
Epoch: 146/200
Train loss: 2.066331
Time needed: 166.52284979820251 for validation audios
0.7981185547159172
Epoch: 147/200
Train loss: 2.095295
Time needed: 165.04323768615723 for validation audios
0.79664840866208
Epoch: 148/200
Train loss: 1.994505
Time needed: 165.89976620674133 for validation audios
0.7977867753220298
Epoch: 149/200
Train loss: 2.055091
Time needed: 165.50757598876953 for validation audios
0.8033205040755887
Model improve: 0.8010 -> 0.8033
Epoch: 150/200
Train loss: 1.998181
Time needed: 164.5699918270111 for validation audios
0.7989459036733407
Epoch: 151/200
Train loss: 2.038145
Time needed: 165.5742630958557 for validation audios
0.8023538697636458
Epoch: 152/200
Train loss: 2.071827
Time needed: 165.37680339813232 for validation audios
0.7990080044679089
Epoch: 153/200
Train loss: 2.102403
Time needed: 165.97677087783813 for validation audios
0.80059525873089
Epoch: 154/200
Train loss: 2.049043
Time needed: 166.5639111995697 for validation audios
0.798217284240143
Epoch: 155/200
Train loss: 2.039823
Time needed: 165.503267288208 for validation audios
0.7949865806304882
Epoch: 156/200
Train loss: 1.998638
Time needed: 164.26257014274597 for validation audios
0.7987153888141753
Epoch: 157/200
Train loss: 2.041112
Time needed: 165.08292841911316 for validation audios
0.7981844580173212
Epoch: 158/200
Train loss: 2.058463
Time needed: 165.81023263931274 for validation audios
0.7988328552944771
Epoch: 159/200
Train loss: 1.953170
Time needed: 166.76887321472168 for validation audios
0.7997657589073284
Epoch: 160/200
Train loss: 2.018338
Time needed: 165.99416089057922 for validation audios
0.7987016113160522
Epoch: 161/200
Train loss: 2.015276
Time needed: 164.21420669555664 for validation audios
0.8004969391724541
Epoch: 162/200
Train loss: 1.989975
Time needed: 166.04425692558289 for validation audios
0.7987939008531268
Epoch: 163/200
Train loss: 2.021308
Time needed: 165.89980721473694 for validation audios
0.7981505718726193
Epoch: 164/200
Train loss: 2.105975
Time needed: 167.50257182121277 for validation audios
0.8008630601367827
Epoch: 165/200
Train loss: 2.067708
Time needed: 165.87170505523682 for validation audios
0.7966108302321606
Epoch: 166/200
Train loss: 1.989380
Time needed: 165.06580328941345 for validation audios
0.7988963692124745
Epoch: 167/200
Train loss: 1.994279
Time needed: 165.21837663650513 for validation audios
0.7999273254524116
Epoch: 168/200
Train loss: 2.048961
Time needed: 165.07238793373108 for validation audios
0.7994328290266238
Epoch: 169/200
Train loss: 2.030188
Time needed: 163.44595193862915 for validation audios
0.798049468069105
Epoch: 170/200
Train loss: 2.067481
Time needed: 166.9036045074463 for validation audios
0.8035598506031988
Model improve: 0.8033 -> 0.8036
Epoch: 171/200
Train loss: 2.136656
Time needed: 164.24172735214233 for validation audios
0.8001326339217355
Epoch: 172/200
Train loss: 2.113247
Time needed: 163.93817019462585 for validation audios
0.7977949452107364
Epoch: 173/200
Train loss: 2.054602
Time needed: 168.49302005767822 for validation audios
0.7987883670388574
Epoch: 174/200
Train loss: 2.040175
Time needed: 164.1546778678894 for validation audios
0.7987260983187976
Epoch: 175/200
Train loss: 2.004236
Time needed: 166.35645174980164 for validation audios
0.7999435318489074
Epoch: 176/200
Train loss: 2.008351
Time needed: 164.52287006378174 for validation audios
0.8006463089114602
Epoch: 177/200
Train loss: 2.021131
Time needed: 164.09885025024414 for validation audios
0.7984177427370299
Epoch: 178/200
Train loss: 1.995229
Time needed: 163.56787967681885 for validation audios
0.800976598035081
Epoch: 179/200
Train loss: 1.975062
Time needed: 163.63862895965576 for validation audios
0.7995416632459551
Epoch: 180/200
Train loss: 1.947515
Time needed: 166.51365494728088 for validation audios
0.8024675666348428
Epoch: 181/200
Train loss: 2.058327
Time needed: 166.12762022018433 for validation audios
0.7995933232591785
Epoch: 182/200
Train loss: 2.046141
Time needed: 166.56948804855347 for validation audios
0.7981541325830575
Epoch: 183/200
Train loss: 2.100835
Time needed: 166.3491542339325 for validation audios
0.8000155638196816
Epoch: 184/200
Train loss: 2.015735
Time needed: 165.18643069267273 for validation audios
0.7968982909810176
Epoch: 185/200
Train loss: 1.939225
Time needed: 165.8668646812439 for validation audios
0.7984434386198047
Epoch: 186/200
Train loss: 1.970964
Time needed: 166.47904467582703 for validation audios
0.7993211506577187
Epoch: 187/200
Train loss: 1.968514
Time needed: 169.71425485610962 for validation audios
0.7946825933978519
Epoch: 188/200
Train loss: 2.041186
Time needed: 165.20404982566833 for validation audios
0.8010302533768765
Epoch: 189/200
Train loss: 1.976379
Time needed: 164.26098227500916 for validation audios
0.8021398175281736
Epoch: 190/200
Train loss: 2.013151
Time needed: 165.55354142189026 for validation audios
0.7983370358285623
Epoch: 191/200
Train loss: 2.020008
Time needed: 164.46275305747986 for validation audios
0.7999936535648895
Epoch: 192/200
Train loss: 2.076346
Time needed: 163.46057057380676 for validation audios
0.7998062881137616
Epoch: 193/200
Train loss: 2.063813
Time needed: 165.5748975276947 for validation audios
0.7967231434877912
Epoch: 194/200
Train loss: 2.087303
Time needed: 164.92838096618652 for validation audios
0.7973465808377997
Epoch: 195/200
Train loss: 2.041050
Time needed: 168.2325222492218 for validation audios
0.8024136707299315
Epoch: 196/200
Train loss: 1.954575
Time needed: 165.66809606552124 for validation audios
0.8019417478131454
Epoch: 197/200
Train loss: 2.030455
Time needed: 164.70151090621948 for validation audios
0.7977504780991093
Epoch: 198/200
Train loss: 2.046205
Time needed: 163.71628189086914 for validation audios
0.798298718336758
Epoch: 199/200
Train loss: 2.001559
Time needed: 167.5851650238037 for validation audios
0.7967537938342434
Epoch: 200/200
Train loss: 1.970716
Time needed: 166.27388501167297 for validation audios
0.7971706687960299
Date :05/07/2023, 23:28:02
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 200
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
thrdownsample: 300
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 25.656706
Epoch: 2/200
Train loss: 4.947811
Epoch: 3/200
Train loss: 4.324832
Epoch: 4/200
Train loss: 3.886640
Epoch: 5/200
Train loss: 3.652209
Epoch: 6/200
Train loss: 3.497681
Epoch: 7/200
Train loss: 3.368207
Epoch: 8/200
Train loss: 3.315057
Epoch: 9/200
Train loss: 3.113222
Epoch: 10/200
Train loss: 3.226139
Epoch: 11/200
Train loss: 2.965802
Epoch: 12/200
Train loss: 2.988605
Epoch: 13/200
Train loss: 2.985131
Epoch: 14/200
Train loss: 2.898813
Epoch: 15/200
Train loss: 2.774138
Epoch: 16/200
Train loss: 2.871928
Epoch: 17/200
Train loss: 2.758912
Epoch: 18/200
Train loss: 2.813613
Epoch: 19/200
Train loss: 2.812474
Epoch: 20/200
Train loss: 2.680972
Epoch: 21/200
Train loss: 2.667442
Epoch: 22/200
Train loss: 2.761727
Epoch: 23/200
Train loss: 2.630517
Epoch: 24/200
Train loss: 2.599302
Epoch: 25/200
Train loss: 2.481224
Epoch: 26/200
Train loss: 2.574611
Epoch: 27/200
Train loss: 2.531885
Epoch: 28/200
Train loss: 2.557252
Epoch: 29/200
Train loss: 2.540429
Epoch: 30/200
Train loss: 2.556460
Epoch: 31/200
Train loss: 2.464195
Epoch: 32/200
Train loss: 2.479678
Epoch: 33/200
Train loss: 2.468962
Epoch: 34/200
Train loss: 2.485159
Epoch: 35/200
Train loss: 2.543807
Epoch: 36/200
Train loss: 2.336449
Epoch: 37/200
Train loss: 2.467615
Epoch: 38/200
Train loss: 2.491099
Epoch: 39/200
Train loss: 2.373525
Epoch: 40/200
Train loss: 2.267982
Epoch: 41/200
Train loss: 2.358334
Epoch: 42/200
Train loss: 2.314345
Epoch: 43/200
Train loss: 2.390107
Epoch: 44/200
Train loss: 2.349805
Epoch: 45/200
Train loss: 2.377382
Epoch: 46/200
Train loss: 2.412935
Epoch: 47/200
Train loss: 2.367195
Epoch: 48/200
Train loss: 2.407654
Epoch: 49/200
Train loss: 2.352413
Epoch: 50/200
Train loss: 2.309379
Epoch: 51/200
Train loss: 2.276010
Epoch: 52/200
Train loss: 2.233156
Epoch: 53/200
Train loss: 2.299597
Epoch: 54/200
Train loss: 2.345042
Epoch: 55/200
Train loss: 2.320935
Epoch: 56/200
Train loss: 2.203718
Epoch: 57/200
Train loss: 2.309190
Epoch: 58/200
Train loss: 2.278932
Epoch: 59/200
Train loss: 2.239371
Epoch: 60/200
Train loss: 2.280799
Epoch: 61/200
Train loss: 2.268535
Epoch: 62/200
Train loss: 2.405164
Epoch: 63/200
Train loss: 2.300999
Epoch: 64/200
Train loss: 2.210769
Epoch: 65/200
Train loss: 2.163343
Epoch: 66/200
Train loss: 2.261717
Epoch: 67/200
Train loss: 2.192341
Epoch: 68/200
Train loss: 2.163002
Epoch: 69/200
Train loss: 2.158033
Epoch: 70/200
Train loss: 2.223184
Epoch: 71/200
Train loss: 2.165398
Epoch: 72/200
Train loss: 2.269625
Epoch: 73/200
Train loss: 2.177868
Epoch: 74/200
Train loss: 2.286207
Epoch: 75/200
Train loss: 2.096931
Epoch: 76/200
Train loss: 2.135590
Epoch: 77/200
Train loss: 2.122214
Epoch: 78/200
Train loss: 2.225941
Epoch: 79/200
Train loss: 2.099306
Epoch: 80/200
Train loss: 2.126884
Epoch: 81/200
Train loss: 2.209983
Epoch: 82/200
Train loss: 2.189526
Epoch: 83/200
Train loss: 2.244159
Epoch: 84/200
Train loss: 2.222364
Epoch: 85/200
Train loss: 2.147855
Epoch: 86/200
Train loss: 2.182620
Epoch: 87/200
Train loss: 2.154027
Epoch: 88/200
Train loss: 2.103409
Epoch: 89/200
Train loss: 2.117606
Epoch: 90/200
Train loss: 2.143618
Epoch: 91/200
Train loss: 2.084911
Epoch: 92/200
Train loss: 2.158909
Epoch: 93/200
Train loss: 2.167186
Epoch: 94/200
Train loss: 2.156780
Epoch: 95/200
Train loss: 2.149386
Epoch: 96/200
Train loss: 2.171801
Epoch: 97/200
Train loss: 2.187062
Epoch: 98/200
Train loss: 2.163474
Epoch: 99/200
Train loss: 2.096808
Epoch: 100/200
Train loss: 2.163381
Epoch: 101/200
Train loss: 2.173289
Epoch: 102/200
Train loss: 2.152434
Epoch: 103/200
Train loss: 2.078640
Epoch: 104/200
Train loss: 1.978636
Epoch: 105/200
Train loss: 2.204381
Epoch: 106/200
Train loss: 2.069421
Epoch: 107/200
Train loss: 2.114495
Epoch: 108/200
Train loss: 2.056094
Epoch: 109/200
Train loss: 2.092472
Epoch: 110/200
Train loss: 2.112888
Epoch: 111/200
Train loss: 2.063100
Epoch: 112/200
Train loss: 2.100279
Epoch: 113/200
Train loss: 2.146694
Epoch: 114/200
Train loss: 2.106029
Epoch: 115/200
Train loss: 2.029682
Epoch: 116/200
Train loss: 2.020942
Epoch: 117/200
Train loss: 1.987261
Epoch: 118/200
Train loss: 2.074613
Epoch: 119/200
Train loss: 2.099730
Epoch: 120/200
Train loss: 1.973867
Epoch: 121/200
Train loss: 2.057507
Epoch: 122/200
Train loss: 2.063255
Time needed: 167.37693166732788 for validation audios
0.7984450368847715
Model improve: 0.0000 -> 0.7984
Epoch: 123/200
Train loss: 2.168429
Time needed: 166.3290810585022 for validation audios
0.7967307624760976
Epoch: 124/200
Train loss: 2.047361
Date :05/08/2023, 02:27:16
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 200
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
thrdownsample: 300
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 25.583316
Epoch: 2/200
Train loss: 4.933561
Epoch: 3/200
Train loss: 4.311377
Epoch: 4/200
Train loss: 3.871505
Epoch: 5/200
Train loss: 3.643508
Epoch: 6/200
Train loss: 3.493366
Epoch: 7/200
Train loss: 3.375574
Epoch: 8/200
Date :05/08/2023, 02:37:23
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
thrdownsample: 300
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 44.072250
Epoch: 2/200
Train loss: 5.420290
Epoch: 3/200
Train loss: 4.884949
Epoch: 4/200
Train loss: 4.586591
Epoch: 5/200
Train loss: 4.222204
Epoch: 6/200
Train loss: 3.956866
Epoch: 7/200
Train loss: 3.674022
Epoch: 8/200
Train loss: 3.647703
Epoch: 9/200
Train loss: 3.490375
Epoch: 10/200
Train loss: 3.431744
Epoch: 11/200
Train loss: 3.371176
Epoch: 12/200
Train loss: 3.278639
Epoch: 13/200
Train loss: 3.230288
Epoch: 14/200
Train loss: 3.106968
Epoch: 15/200
Train loss: 3.156897
Epoch: 16/200
Train loss: 3.112876
Epoch: 17/200
Train loss: 2.988270
Epoch: 18/200
Train loss: 2.890746
Epoch: 19/200
Train loss: 3.121634
Epoch: 20/200
Train loss: 2.985115
Epoch: 21/200
Train loss: 2.857310
Epoch: 22/200
Train loss: 2.737179
Epoch: 23/200
Train loss: 2.771953
Epoch: 24/200
Train loss: 2.856914
Epoch: 25/200
Train loss: 2.847300
Epoch: 26/200
Train loss: 2.833284
Epoch: 27/200
Train loss: 2.800347
Epoch: 28/200
Train loss: 2.679178
Epoch: 29/200
Train loss: 2.704771
Epoch: 30/200
Train loss: 2.551337
Epoch: 31/200
Train loss: 2.700210
Epoch: 32/200
Train loss: 2.752680
Epoch: 33/200
Train loss: 2.532366
Epoch: 34/200
Train loss: 2.624163
Epoch: 35/200
Train loss: 2.640095
Epoch: 36/200
Train loss: 2.672907
Epoch: 37/200
Train loss: 2.550333
Epoch: 38/200
Train loss: 2.782339
Epoch: 39/200
Train loss: 2.478971
Epoch: 40/200
Train loss: 2.604327
Epoch: 41/200
Train loss: 2.553020
Epoch: 42/200
Train loss: 2.527427
Epoch: 43/200
Train loss: 2.621514
Epoch: 44/200
Train loss: 2.600625
Epoch: 45/200
Train loss: 2.623297
Epoch: 46/200
Train loss: 2.370830
Epoch: 47/200
Train loss: 2.566568
Epoch: 48/200
Train loss: 2.303940
Epoch: 49/200
Train loss: 2.308558
Epoch: 50/200
Train loss: 2.365934
Epoch: 51/200
Train loss: 2.418238
Epoch: 52/200
Train loss: 2.417660
Epoch: 53/200
Train loss: 2.335744
Epoch: 54/200
Train loss: 2.421457
Epoch: 55/200
Train loss: 2.416149
Epoch: 56/200
Train loss: 2.382542
Epoch: 57/200
Train loss: 2.413028
Epoch: 58/200
Train loss: 2.375365
Epoch: 59/200
Train loss: 2.383933
Epoch: 60/200
Train loss: 2.412005
Epoch: 61/200
Train loss: 2.316822
Epoch: 62/200
Train loss: 2.322600
Epoch: 63/200
Train loss: 2.367354
Epoch: 64/200
Train loss: 2.367837
Epoch: 65/200
Train loss: 2.416863
Epoch: 66/200
Train loss: 2.281560
Epoch: 67/200
Train loss: 2.382850
Epoch: 68/200
Train loss: 2.341702
Epoch: 69/200
Train loss: 2.519531
Epoch: 70/200
Train loss: 2.313838
Epoch: 71/200
Train loss: 2.169886
Epoch: 72/200
Train loss: 2.200763
Epoch: 73/200
Train loss: 2.311344
Epoch: 74/200
Train loss: 2.306007
Epoch: 75/200
Train loss: 2.360806
Epoch: 76/200
Train loss: 2.341319
Epoch: 77/200
Train loss: 2.310123
Epoch: 78/200
Train loss: 2.195234
Epoch: 79/200
Train loss: 2.072390
Epoch: 80/200
Train loss: 2.234695
Epoch: 81/200
Train loss: 2.245297
Epoch: 82/200
Train loss: 2.246259
Epoch: 83/200
Train loss: 2.200409
Epoch: 84/200
Train loss: 2.177189
Epoch: 85/200
Train loss: 2.311749
Epoch: 86/200
Train loss: 2.234634
Epoch: 87/200
Train loss: 2.122205
Epoch: 88/200
Train loss: 2.321323
Epoch: 89/200
Train loss: 2.332396
Epoch: 90/200
Train loss: 2.189692
Epoch: 91/200
Train loss: 2.302791
Epoch: 92/200
Train loss: 2.273700
Epoch: 93/200
Train loss: 2.314384
Epoch: 94/200
Train loss: 2.211235
Epoch: 95/200
Train loss: 2.271499
Epoch: 96/200
Train loss: 2.292001
Epoch: 97/200
Train loss: 2.260578
Epoch: 98/200
Train loss: 2.222458
Epoch: 99/200
Train loss: 2.159166
Epoch: 100/200
Train loss: 2.206244
Epoch: 101/200
Train loss: 2.266551
Epoch: 102/200
Train loss: 2.063791
Time needed: 191.74698734283447 for validation audios
0.8060710544557557
Model improve: 0.0000 -> 0.8061
Epoch: 103/200
Train loss: 2.115822
Time needed: 167.61252403259277 for validation audios
0.7984895154817696
Epoch: 104/200
Train loss: 2.116172
Time needed: 168.10763478279114 for validation audios
0.8018701174389524
Epoch: 105/200
Train loss: 2.179832
Time needed: 167.03863716125488 for validation audios
0.8028843578789866
Epoch: 106/200
Train loss: 2.187387
Time needed: 167.43749260902405 for validation audios
0.8018147547790107
Epoch: 107/200
Train loss: 2.253970
Time needed: 170.8988001346588 for validation audios
0.7971542667540348
Epoch: 108/200
Train loss: 2.234397
Time needed: 168.58233785629272 for validation audios
0.7979811090651475
Epoch: 109/200
Train loss: 2.197657
Time needed: 167.34188961982727 for validation audios
0.8000583105488127
Epoch: 110/200
Train loss: 2.240739
Time needed: 167.56103539466858 for validation audios
0.8005545376428818
Epoch: 111/200
Train loss: 2.040573
Time needed: 167.11004066467285 for validation audios
0.8030555209753718
Epoch: 112/200
Train loss: 2.168945
Time needed: 169.98525547981262 for validation audios
0.797535763474538
Epoch: 113/200
Train loss: 2.160676
Time needed: 169.3897578716278 for validation audios
0.802335340383753
Epoch: 114/200
Train loss: 2.261018
Time needed: 168.31136441230774 for validation audios
0.7998824939594242
Epoch: 115/200
Train loss: 2.153480
Date :05/08/2023, 05:33:09
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 30
thrdownsample: 300
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 1
15267
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 40.895728
Epoch: 2/200
Train loss: 5.829992
Epoch: 3/200
Train loss: 5.289497
Epoch: 4/200
Train loss: 4.807500
Epoch: 5/200
Train loss: 4.367934
Epoch: 6/200
Train loss: 4.031777
Epoch: 7/200
Train loss: 3.815031
Epoch: 8/200
Train loss: 3.568870
Epoch: 9/200
Train loss: 3.521592
Epoch: 10/200
Train loss: 3.457709
Epoch: 11/200
Train loss: 3.360326
Epoch: 12/200
Train loss: 3.310224
Epoch: 13/200
Train loss: 3.219787
Epoch: 14/200
Train loss: 3.237610
Epoch: 15/200
Train loss: 3.020544
Epoch: 16/200
Train loss: 2.998062
Epoch: 17/200
Train loss: 3.167603
Epoch: 18/200
Train loss: 3.077631
Epoch: 19/200
Train loss: 2.924014
Epoch: 20/200
Train loss: 2.763815
Epoch: 21/200
Train loss: 2.835498
Epoch: 22/200
Train loss: 2.991830
Epoch: 23/200
Train loss: 2.793114
Epoch: 24/200
Train loss: 2.846495
Epoch: 25/200
Train loss: 2.774886
Epoch: 26/200
Train loss: 2.738216
Epoch: 27/200
Train loss: 2.604596
Epoch: 28/200
Train loss: 2.716029
Epoch: 29/200
Train loss: 2.782540
Epoch: 30/200
Train loss: 2.543124
Epoch: 31/200
Train loss: 2.709994
Epoch: 32/200
Train loss: 2.693017
Epoch: 33/200
Train loss: 2.614491
Epoch: 34/200
Train loss: 2.701716
Epoch: 35/200
Train loss: 2.707761
Epoch: 36/200
Train loss: 2.538659
Epoch: 37/200
Train loss: 2.641736
Epoch: 38/200
Train loss: 2.483941
Epoch: 39/200
Train loss: 2.676456
Epoch: 40/200
Train loss: 2.645312
Epoch: 41/200
Train loss: 2.610754
Epoch: 42/200
Train loss: 2.462368
Epoch: 43/200
Train loss: 2.457275
Epoch: 44/200
Train loss: 2.361390
Epoch: 45/200
Train loss: 2.385619
Epoch: 46/200
Train loss: 2.367485
Epoch: 47/200
Train loss: 2.463920
Epoch: 48/200
Train loss: 2.340188
Epoch: 49/200
Train loss: 2.454757
Epoch: 50/200
Train loss: 2.421743
Epoch: 51/200
Train loss: 2.437018
Epoch: 52/200
Train loss: 2.375819
Epoch: 53/200
Train loss: 2.486928
Epoch: 54/200
Train loss: 2.378475
Epoch: 55/200
Train loss: 2.409709
Epoch: 56/200
Train loss: 2.279630
Epoch: 57/200
Train loss: 2.413346
Epoch: 58/200
Train loss: 2.381948
Epoch: 59/200
Train loss: 2.442779
Epoch: 60/200
Train loss: 2.275307
Epoch: 61/200
Train loss: 2.471448
Epoch: 62/200
Train loss: 2.379833
Epoch: 63/200
Train loss: 2.435157
Epoch: 64/200
Train loss: 2.282000
Epoch: 65/200
Train loss: 2.216977
Epoch: 66/200
Train loss: 2.265588
Epoch: 67/200
Train loss: 2.392719
Epoch: 68/200
Train loss: 2.384291
Epoch: 69/200
Train loss: 2.309846
Epoch: 70/200
Train loss: 2.297612
Epoch: 71/200
Train loss: 2.207316
Epoch: 72/200
Train loss: 2.178441
Epoch: 73/200
Train loss: 2.236779
Epoch: 74/200
Train loss: 2.266035
Epoch: 75/200
Train loss: 2.174289
Epoch: 76/200
Train loss: 2.209598
Epoch: 77/200
Train loss: 2.314045
Epoch: 78/200
Train loss: 2.236638
Epoch: 79/200
Train loss: 2.161421
Epoch: 80/200
Train loss: 2.330280
Epoch: 81/200
Train loss: 2.315546
Epoch: 82/200
Train loss: 2.269192
Epoch: 83/200
Train loss: 2.216192
Epoch: 84/200
Train loss: 2.378065
Epoch: 85/200
Train loss: 2.207762
Epoch: 86/200
Train loss: 2.358056
Epoch: 87/200
Train loss: 2.277922
Epoch: 88/200
Train loss: 2.288842
Epoch: 89/200
Train loss: 2.238904
Epoch: 90/200
Train loss: 2.156881
Epoch: 91/200
Train loss: 2.252987
Epoch: 92/200
Train loss: 2.287376
Epoch: 93/200
Train loss: 2.022151
Epoch: 94/200
Train loss: 2.153485
Epoch: 95/200
Train loss: 2.182363
Epoch: 96/200
Train loss: 2.175486
Epoch: 97/200
Train loss: 2.193452
Epoch: 98/200
Train loss: 2.270919
Epoch: 99/200
Train loss: 2.205237
Epoch: 100/200
Train loss: 2.231866
Epoch: 101/200
Train loss: 2.108980
Epoch: 102/200
Train loss: 2.120384
Time needed: 170.46249413490295 for validation audios
0.8002068699850583
Model improve: 0.0000 -> 0.8002
Epoch: 103/200
Train loss: 2.242081
Time needed: 166.55813026428223 for validation audios
0.7988008151283138
Epoch: 104/200
Train loss: 2.243216
Time needed: 166.4272539615631 for validation audios
0.7991927660469649
Epoch: 105/200
Train loss: 2.174347
Time needed: 165.9063103199005 for validation audios
0.8018345448671779
Model improve: 0.8002 -> 0.8018
Epoch: 106/200
Train loss: 2.176757
Time needed: 166.27222108840942 for validation audios
0.7990041910250391
Epoch: 107/200
Train loss: 2.081409
Time needed: 166.5815098285675 for validation audios
0.8014077763485713
Epoch: 108/200
Train loss: 2.268868
Time needed: 166.51549911499023 for validation audios
0.7970971813550789
Epoch: 109/200
Train loss: 2.123884
Time needed: 166.864328622818 for validation audios
0.7989340566646662
Epoch: 110/200
Train loss: 2.142877
Time needed: 167.409348487854 for validation audios
0.798292887939078
Epoch: 111/200
Train loss: 2.171285
Time needed: 169.034850358963 for validation audios
0.8007351480266146
Epoch: 112/200
Train loss: 2.418288
Time needed: 166.47413158416748 for validation audios
0.8002531256620526
Epoch: 113/200
Train loss: 2.268235
Time needed: 168.1379873752594 for validation audios
0.7951349096652184
Epoch: 114/200
Train loss: 2.145260
Time needed: 166.21472430229187 for validation audios
0.7978682857330381
Epoch: 115/200
Train loss: 2.142391
Time needed: 167.0957474708557 for validation audios
0.8011372360885375
Epoch: 116/200
Train loss: 2.149124
Time needed: 167.83302688598633 for validation audios
0.7986184108138602
Epoch: 117/200
Train loss: 2.106391
Time needed: 167.45252895355225 for validation audios
0.7983964240679087
Epoch: 118/200
Train loss: 1.976102
Time needed: 167.30176162719727 for validation audios
0.7986074245207141
Epoch: 119/200
Train loss: 2.141251
Time needed: 167.98570442199707 for validation audios
0.7977053785252074
Epoch: 120/200
Train loss: 2.211100
Time needed: 171.36285853385925 for validation audios
0.7996700031530405
Epoch: 121/200
Train loss: 2.085647
Time needed: 168.72716426849365 for validation audios
0.7993036936965693
Epoch: 122/200
Train loss: 2.135888
Time needed: 167.58460450172424 for validation audios
0.7975403355981573
Epoch: 123/200
Train loss: 2.029426
Time needed: 168.22678399085999 for validation audios
0.7992118428696892
Epoch: 124/200
Train loss: 2.083509
Time needed: 166.9521141052246 for validation audios
0.8035046811034087
Model improve: 0.8018 -> 0.8035
Epoch: 125/200
Train loss: 2.119581
Time needed: 169.31157898902893 for validation audios
0.7994307128143261
Epoch: 126/200
Train loss: 2.079664
Time needed: 168.45790266990662 for validation audios
0.8014977610658587
Epoch: 127/200
Train loss: 2.085592
Time needed: 168.909526348114 for validation audios
0.7975373003107489
Epoch: 128/200
Train loss: 2.125563
Time needed: 168.21790838241577 for validation audios
0.7992891063562233
Epoch: 129/200
Train loss: 2.052367
Time needed: 167.4804561138153 for validation audios
0.8002815894717155
Epoch: 130/200
Train loss: 2.193688
Time needed: 168.7076780796051 for validation audios
0.7985625727227929
Epoch: 131/200
Train loss: 2.147042
Time needed: 166.34620356559753 for validation audios
0.8027498983847738
Epoch: 132/200
Train loss: 2.117664
Time needed: 166.20924711227417 for validation audios
0.7992615355690417
Epoch: 133/200
Train loss: 2.099925
Time needed: 166.20339488983154 for validation audios
0.8019111981687793
Epoch: 134/200
Train loss: 2.279201
Time needed: 170.63337922096252 for validation audios
0.7973486168595709
Epoch: 135/200
Train loss: 2.100294
Time needed: 167.48147773742676 for validation audios
0.8042586165268986
Model improve: 0.8035 -> 0.8043
Epoch: 136/200
Train loss: 1.966811
Time needed: 167.5753631591797 for validation audios
0.8012122571672312
Epoch: 137/200
Train loss: 2.092914
Time needed: 165.47481274604797 for validation audios
0.8041893021614404
Epoch: 138/200
Train loss: 2.018247
Time needed: 165.22443222999573 for validation audios
0.8017034397771564
Epoch: 139/200
Train loss: 2.022982
Time needed: 164.34424948692322 for validation audios
0.7990693686268655
Epoch: 140/200
Train loss: 2.067945
Time needed: 166.08462285995483 for validation audios
0.8007221555818896
Epoch: 141/200
Train loss: 2.132581
Time needed: 166.5352771282196 for validation audios
0.8000214298196816
Epoch: 142/200
Train loss: 2.164677
Time needed: 164.74811100959778 for validation audios
0.803984485290804
Epoch: 143/200
Train loss: 1.996668
Time needed: 166.63921189308167 for validation audios
0.8035264334353955
Epoch: 144/200
Train loss: 2.064516
Time needed: 166.4012200832367 for validation audios
0.7984419015085625
Epoch: 145/200
Train loss: 2.069026
Time needed: 166.1995508670807 for validation audios
0.8052041974705663
Model improve: 0.8043 -> 0.8052
Epoch: 146/200
Train loss: 2.195177
Time needed: 165.7663130760193 for validation audios
0.8016917454465299
Epoch: 147/200
Train loss: 2.060743
Time needed: 166.85463738441467 for validation audios
0.7975840083980119
Epoch: 148/200
Train loss: 2.069777
Time needed: 164.9227213859558 for validation audios
0.8027768165802526
Epoch: 149/200
Train loss: 2.219664
Time needed: 168.03180027008057 for validation audios
0.8016336141022169
Epoch: 150/200
Date :05/08/2023, 10:53:59
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 30
thrdownsample: 300
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 1
16079
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 38.977359
Epoch: 2/200
Train loss: 5.621873
Epoch: 3/200
Train loss: 5.101186
Epoch: 4/200
Train loss: 4.654310
Epoch: 5/200
Train loss: 4.226709
Epoch: 6/200
Train loss: 3.868948
Epoch: 7/200
Train loss: 3.731079
Epoch: 8/200
Train loss: 3.548261
Epoch: 9/200
Train loss: 3.536900
Epoch: 10/200
Train loss: 3.339809
Epoch: 11/200
Train loss: 3.337113
Epoch: 12/200
Train loss: 3.222249
Epoch: 13/200
Train loss: 3.180115
Epoch: 14/200
Train loss: 3.141882
Epoch: 15/200
Train loss: 3.003996
Epoch: 16/200
Train loss: 3.096412
Epoch: 17/200
Train loss: 3.023243
Epoch: 18/200
Train loss: 2.918843
Epoch: 19/200
Train loss: 2.788940
Epoch: 20/200
Train loss: 2.825437
Epoch: 21/200
Train loss: 2.986320
Epoch: 22/200
Train loss: 2.769447
Epoch: 23/200
Train loss: 2.856296
Epoch: 24/200
Train loss: 2.788153
Epoch: 25/200
Train loss: 2.637930
Epoch: 26/200
Train loss: 2.631987
Epoch: 27/200
Train loss: 2.780718
Epoch: 28/200
Train loss: 2.634682
Epoch: 29/200
Train loss: 2.599310
Epoch: 30/200
Train loss: 2.692111
Epoch: 31/200
Train loss: 2.728455
Epoch: 32/200
Train loss: 2.562396
Epoch: 33/200
Train loss: 2.787979
Epoch: 34/200
Train loss: 2.492600
Epoch: 35/200
Train loss: 2.644402
Epoch: 36/200
Train loss: 2.524505
Epoch: 37/200
Train loss: 2.626931
Epoch: 38/200
Train loss: 2.631928
Epoch: 39/200
Train loss: 2.609748
Epoch: 40/200
Train loss: 2.460077
Epoch: 41/200
Train loss: 2.441322
Epoch: 42/200
Train loss: 2.310457
Epoch: 43/200
Train loss: 2.396060
Epoch: 44/200
Train loss: 2.422629
Epoch: 45/200
Train loss: 2.438666
Epoch: 46/200
Train loss: 2.423253
Epoch: 47/200
Train loss: 2.452126
Epoch: 48/200
Train loss: 2.308057
Epoch: 49/200
Train loss: 2.488433
Epoch: 50/200
Train loss: 2.386655
Epoch: 51/200
Train loss: 2.442394
Epoch: 52/200
Train loss: 2.405497
Epoch: 53/200
Train loss: 2.341397
Epoch: 54/200
Train loss: 2.369798
Epoch: 55/200
Train loss: 2.392173
Epoch: 56/200
Train loss: 2.381903
Epoch: 57/200
Train loss: 2.321399
Epoch: 58/200
Train loss: 2.404253
Epoch: 59/200
Train loss: 2.399143
Epoch: 60/200
Train loss: 2.431562
Epoch: 61/200
Train loss: 2.243287
Epoch: 62/200
Train loss: 2.241237
Epoch: 63/200
Train loss: 2.339871
Epoch: 64/200
Train loss: 2.365807
Epoch: 65/200
Train loss: 2.335578
Epoch: 66/200
Train loss: 2.320654
Epoch: 67/200
Train loss: 2.330032
Epoch: 68/200
Train loss: 2.216619
Epoch: 69/200
Train loss: 2.138908
Epoch: 70/200
Train loss: 2.279517
Epoch: 71/200
Train loss: 2.205499
Epoch: 72/200
Train loss: 2.152584
Epoch: 73/200
Train loss: 2.314318
Epoch: 74/200
Train loss: 2.329705
Epoch: 75/200
Train loss: 2.097543
Epoch: 76/200
Train loss: 2.345352
Epoch: 77/200
Train loss: 2.333322
Epoch: 78/200
Train loss: 2.297548
Epoch: 79/200
Train loss: 2.202812
Epoch: 80/200
Train loss: 2.379136
Epoch: 81/200
Train loss: 2.208067
Epoch: 82/200
Train loss: 2.342204
Epoch: 83/200
Train loss: 2.328545
Epoch: 84/200
Train loss: 2.264006
Epoch: 85/200
Train loss: 2.182480
Epoch: 86/200
Train loss: 2.234447
Epoch: 87/200
Train loss: 2.235603
Epoch: 88/200
Train loss: 2.145169
Epoch: 89/200
Train loss: 2.095597
Epoch: 90/200
Train loss: 2.156275
Epoch: 91/200
Train loss: 2.118663
Epoch: 92/200
Train loss: 2.273509
Epoch: 93/200
Train loss: 2.197979
Epoch: 94/200
Train loss: 2.230271
Epoch: 95/200
Train loss: 2.266362
Epoch: 96/200
Train loss: 2.083076
Epoch: 97/200
Train loss: 2.153463
Epoch: 98/200
Train loss: 2.205863
Epoch: 99/200
Train loss: 2.257534
Epoch: 100/200
Train loss: 2.199314
Epoch: 101/200
Train loss: 2.162839
Epoch: 102/200
Train loss: 2.133362
Time needed: 165.08475065231323 for validation audios
0.7956077800670163
Model improve: 0.0000 -> 0.7956
Epoch: 103/200
Train loss: 2.186356
Time needed: 164.85271048545837 for validation audios
0.7988086052622644
Model improve: 0.7956 -> 0.7988
Epoch: 104/200
Date :05/08/2023, 13:17:25
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
thrdownsample: 300
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 44.053098
Epoch: 2/200
Train loss: 5.414804
Epoch: 3/200
Train loss: 4.879595
Epoch: 4/200
Train loss: 4.583059
Epoch: 5/200
Train loss: 4.220703
Epoch: 6/200
Train loss: 3.955062
Epoch: 7/200
Train loss: 3.671697
Epoch: 8/200
Train loss: 3.646872
Epoch: 9/200
Train loss: 3.490228
Epoch: 10/200
Train loss: 3.431197
Epoch: 11/200
Train loss: 3.370446
Epoch: 12/200
Train loss: 3.277528
Epoch: 13/200
Train loss: 3.229915
Epoch: 14/200
Train loss: 3.107405
Epoch: 15/200
Train loss: 3.155746
Epoch: 16/200
Train loss: 3.111873
Epoch: 17/200
Train loss: 2.987947
Epoch: 18/200
Train loss: 2.888927
Epoch: 19/200
Train loss: 3.122247
Epoch: 20/200
Train loss: 2.983449
Epoch: 21/200
Train loss: 2.857591
Epoch: 22/200
Train loss: 2.737360
Epoch: 23/200
Train loss: 2.770784
Epoch: 24/200
Train loss: 2.856634
Epoch: 25/200
Train loss: 2.843991
Epoch: 26/200
Train loss: 2.832304
Epoch: 27/200
Train loss: 2.801085
Epoch: 28/200
Train loss: 2.678834
Epoch: 29/200
Train loss: 2.702823
Epoch: 30/200
Train loss: 2.550011
Epoch: 31/200
Train loss: 2.697709
Epoch: 32/200
Train loss: 2.750989
Epoch: 33/200
Train loss: 2.532720
Epoch: 34/200
Train loss: 2.624587
Epoch: 35/200
Train loss: 2.639970
Epoch: 36/200
Train loss: 2.670678
Epoch: 37/200
Train loss: 2.548334
Epoch: 38/200
Train loss: 2.781915
Epoch: 39/200
Train loss: 2.476859
Epoch: 40/200
Train loss: 2.604471
Epoch: 41/200
Train loss: 2.549946
Epoch: 42/200
Train loss: 2.527485
Epoch: 43/200
Train loss: 2.623086
Epoch: 44/200
Train loss: 2.600148
Epoch: 45/200
Train loss: 2.620667
Epoch: 46/200
Train loss: 2.372260
Epoch: 47/200
Train loss: 2.565249
Epoch: 48/200
Train loss: 2.300878
Epoch: 49/200
Train loss: 2.308563
Epoch: 50/200
Train loss: 2.365942
Epoch: 51/200
Train loss: 2.417680
Epoch: 52/200
Train loss: 2.418162
Epoch: 53/200
Train loss: 2.332496
Epoch: 54/200
Train loss: 2.419654
Epoch: 55/200
Train loss: 2.415552
Epoch: 56/200
Train loss: 2.379303
Epoch: 57/200
Train loss: 2.411184
Epoch: 58/200
Train loss: 2.375198
Epoch: 59/200
Train loss: 2.380476
Epoch: 60/200
Train loss: 2.411763
Epoch: 61/200
Train loss: 2.317361
Epoch: 62/200
Train loss: 2.321141
Epoch: 63/200
Train loss: 2.366732
Epoch: 64/200
Train loss: 2.366703
Epoch: 65/200
Train loss: 2.416005
Epoch: 66/200
Train loss: 2.280489
Epoch: 67/200
Train loss: 2.381485
Epoch: 68/200
Train loss: 2.342962
Epoch: 69/200
Train loss: 2.518099
Epoch: 70/200
Train loss: 2.315126
Epoch: 71/200
Train loss: 2.169936
Epoch: 72/200
Train loss: 2.200699
Epoch: 73/200
Train loss: 2.312052
Epoch: 74/200
Train loss: 2.305211
Epoch: 75/200
Train loss: 2.359712
Epoch: 76/200
Train loss: 2.341997
Epoch: 77/200
Train loss: 2.311329
Epoch: 78/200
Train loss: 2.195317
Epoch: 79/200
Train loss: 2.071130
Epoch: 80/200
Train loss: 2.234030
Epoch: 81/200
Train loss: 2.244610
Epoch: 82/200
Train loss: 2.244926
Epoch: 83/200
Train loss: 2.198498
Epoch: 84/200
Train loss: 2.176268
Epoch: 85/200
Train loss: 2.310475
Epoch: 86/200
Train loss: 2.234124
Epoch: 87/200
Train loss: 2.119991
Epoch: 88/200
Train loss: 2.322184
Epoch: 89/200
Train loss: 2.331461
Epoch: 90/200
Train loss: 2.189516
Epoch: 91/200
Train loss: 2.302957
Epoch: 92/200
Train loss: 2.276323
Epoch: 93/200
Train loss: 2.312195
Epoch: 94/200
Train loss: 2.208688
Epoch: 95/200
Train loss: 2.271297
Epoch: 96/200
Train loss: 2.291761
Epoch: 97/200
Train loss: 2.258421
Epoch: 98/200
Train loss: 2.219526
Epoch: 99/200
Train loss: 2.158517
Epoch: 100/200
Train loss: 2.205346
Epoch: 101/200
Train loss: 2.267582
Epoch: 102/200
Train loss: 2.062609
Time needed: 163.20316195487976 for validation audios
0.8058648251113026
Model improve: 0.0000 -> 0.8059
Epoch: 103/200
Train loss: 2.116354
Time needed: 167.38722825050354 for validation audios
0.7986337341344916
Epoch: 104/200
Train loss: 2.114828
Time needed: 166.73280835151672 for validation audios
0.8017104463227828
Epoch: 105/200
Train loss: 2.177404
Time needed: 166.32988739013672 for validation audios
0.8020765971043661
Epoch: 106/200
Train loss: 2.188777
Time needed: 165.3765172958374 for validation audios
0.8013235612276363
Epoch: 107/200
Train loss: 2.250764
Time needed: 167.3328709602356 for validation audios
0.7975378863011502
Epoch: 108/200
Train loss: 2.233933
Time needed: 167.38968014717102 for validation audios
0.7977338908746846
Epoch: 109/200
Train loss: 2.197964
Time needed: 166.41108345985413 for validation audios
0.7992832551217884
Epoch: 110/200
Train loss: 2.239337
Time needed: 163.84582924842834 for validation audios
0.7996311392054499
Epoch: 111/200
Train loss: 2.039685
Time needed: 163.74058556556702 for validation audios
0.8024412981740559
Epoch: 112/200
Train loss: 2.169372
Date :05/08/2023, 15:58:49
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
Date :05/08/2023, 15:59:02
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
thrdownsample: 500
model_name: tf_efficientnetv2_b0
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 42.570084
Epoch: 2/200
Train loss: 5.485993
Epoch: 3/200
Train loss: 4.974553
Epoch: 4/200
Train loss: 4.665567
Epoch: 5/200
Train loss: 4.313871
Epoch: 6/200
Train loss: 4.042033
Epoch: 7/200
Train loss: 3.749434
Epoch: 8/200
Train loss: 3.704873
Epoch: 9/200
Train loss: 3.523096
Epoch: 10/200
Train loss: 3.464565
Epoch: 11/200
Train loss: 3.396538
Epoch: 12/200
Train loss: 3.316214
Epoch: 13/200
Train loss: 3.269722
Epoch: 14/200
Train loss: 3.161523
Epoch: 15/200
Train loss: 3.197763
Epoch: 16/200
Train loss: 3.129602
Epoch: 17/200
Train loss: 3.032844
Epoch: 18/200
Train loss: 2.934764
Epoch: 19/200
Train loss: 3.145940
Epoch: 20/200
Train loss: 3.028632
Epoch: 21/200
Train loss: 2.895270
Epoch: 22/200
Train loss: 2.763689
Epoch: 23/200
Train loss: 2.822610
Epoch: 24/200
Train loss: 2.888553
Epoch: 25/200
Train loss: 2.845153
Epoch: 26/200
Train loss: 2.868370
Epoch: 27/200
Train loss: 2.835562
Epoch: 28/200
Train loss: 2.697676
Epoch: 29/200
Train loss: 2.733848
Epoch: 30/200
Train loss: 2.570465
Epoch: 31/200
Train loss: 2.728008
Epoch: 32/200
Train loss: 2.782870
Epoch: 33/200
Train loss: 2.558882
Epoch: 34/200
Train loss: 2.649870
Epoch: 35/200
Train loss: 2.650212
Epoch: 36/200
Train loss: 2.690885
Epoch: 37/200
Train loss: 2.579434
Epoch: 38/200
Train loss: 2.820288
Epoch: 39/200
Train loss: 2.488908
Epoch: 40/200
Train loss: 2.629295
Epoch: 41/200
Train loss: 2.567855
Epoch: 42/200
Train loss: 2.537919
Epoch: 43/200
Train loss: 2.648342
Epoch: 44/200
Train loss: 2.596989
Epoch: 45/200
Train loss: 2.645410
Epoch: 46/200
Train loss: 2.385047
Epoch: 47/200
Train loss: 2.594001
Epoch: 48/200
Train loss: 2.335310
Epoch: 49/200
Train loss: 2.320178
Epoch: 50/200
Train loss: 2.397532
Epoch: 51/200
Train loss: 2.446065
Epoch: 52/200
Train loss: 2.448969
Epoch: 53/200
Train loss: 2.360077
Epoch: 54/200
Train loss: 2.450282
Epoch: 55/200
Train loss: 2.432171
Epoch: 56/200
Train loss: 2.397362
Epoch: 57/200
Train loss: 2.450719
Epoch: 58/200
Train loss: 2.415088
Epoch: 59/200
Train loss: 2.412710
Epoch: 60/200
Train loss: 2.434491
Epoch: 61/200
Train loss: 2.318530
Epoch: 62/200
Train loss: 2.355820
Epoch: 63/200
Train loss: 2.373560
Epoch: 64/200
Train loss: 2.375532
Epoch: 65/200
Train loss: 2.438916
Epoch: 66/200
Train loss: 2.315546
Epoch: 67/200
Train loss: 2.409356
Epoch: 68/200
Train loss: 2.356911
Epoch: 69/200
Train loss: 2.537051
Epoch: 70/200
Train loss: 2.338840
Epoch: 71/200
Train loss: 2.208510
Epoch: 72/200
Train loss: 2.227869
Epoch: 73/200
Train loss: 2.374558
Epoch: 74/200
Train loss: 2.339388
Epoch: 75/200
Train loss: 2.399648
Epoch: 76/200
Train loss: 2.358771
Epoch: 77/200
Train loss: 2.333746
Epoch: 78/200
Train loss: 2.253572
Epoch: 79/200
Train loss: 2.075867
Epoch: 80/200
Train loss: 2.261395
Epoch: 81/200
Train loss: 2.257270
Epoch: 82/200
Train loss: 2.270605
Epoch: 83/200
Train loss: 2.214697
Epoch: 84/200
Train loss: 2.198722
Epoch: 85/200
Train loss: 2.336180
Epoch: 86/200
Train loss: 2.266150
Epoch: 87/200
Train loss: 2.127167
Epoch: 88/200
Train loss: 2.338915
Epoch: 89/200
Train loss: 2.362773
Epoch: 90/200
Train loss: 2.205510
Epoch: 91/200
Train loss: 2.321871
Epoch: 92/200
Train loss: 2.312557
Epoch: 93/200
Train loss: 2.339595
Epoch: 94/200
Train loss: 2.251539
Epoch: 95/200
Train loss: 2.290899
Epoch: 96/200
Train loss: 2.325904
Epoch: 97/200
Train loss: 2.282513
Epoch: 98/200
Train loss: 2.263925
Epoch: 99/200
Train loss: 2.188820
Epoch: 100/200
Train loss: 2.232563
Epoch: 101/200
Train loss: 2.297603
Epoch: 102/200
Train loss: 2.077287
Time needed: 157.49855875968933 for validation audios
0.7949156451031665
Model improve: 0.0000 -> 0.7949
Epoch: 103/200
Train loss: 2.130146
Time needed: 155.9327733516693 for validation audios
0.7914114175284853
Epoch: 104/200
Train loss: 2.142229
Time needed: 155.68669271469116 for validation audios
0.7947522466932571
Epoch: 105/200
Train loss: 2.197704
Time needed: 158.76965856552124 for validation audios
0.7935019862522255
Epoch: 106/200
Train loss: 2.218907
Time needed: 157.0553171634674 for validation audios
0.7945881946783556
Epoch: 107/200
Train loss: 2.259624
Time needed: 156.09837794303894 for validation audios
0.7924625983386857
Epoch: 108/200
Train loss: 2.249766
Time needed: 157.16041207313538 for validation audios
0.7912863060487302
Epoch: 109/200
Train loss: 2.212616
Time needed: 156.86171674728394 for validation audios
0.7943776256480067
Epoch: 110/200
Train loss: 2.265752
Time needed: 155.3801486492157 for validation audios
0.7929476319891144
Epoch: 111/200
Train loss: 2.076679
Time needed: 156.6898753643036 for validation audios
0.7962630756877719
Model improve: 0.7949 -> 0.7963
Epoch: 112/200
Train loss: 2.182570
Time needed: 161.8332839012146 for validation audios
0.7914495720075643
Epoch: 113/200
Train loss: 2.177987
Time needed: 163.67809677124023 for validation audios
0.7939015834882398
Epoch: 114/200
Train loss: 2.267515
Time needed: 156.79479598999023 for validation audios
0.7913693160390668
Epoch: 115/200
Train loss: 2.171921
Time needed: 163.57545065879822 for validation audios
0.7943670417431543
Epoch: 116/200
Train loss: 2.224759
Time needed: 160.10990953445435 for validation audios
0.7937985319632553
Epoch: 117/200
Train loss: 2.174969
Time needed: 156.4048137664795 for validation audios
0.7915816591526754
Epoch: 118/200
Train loss: 2.139486
Time needed: 158.23937606811523 for validation audios
0.7904214336291471
Epoch: 119/200
Train loss: 2.255199
Time needed: 158.26707577705383 for validation audios
0.7906569822081032
Epoch: 120/200
Train loss: 2.133350
Time needed: 160.1050636768341 for validation audios
0.7939018635984585
Epoch: 121/200
Train loss: 2.129943
Time needed: 158.96287846565247 for validation audios
0.7942741727642624
Epoch: 122/200
Train loss: 2.227328
Time needed: 156.47410464286804 for validation audios
0.7958354179660347
Epoch: 123/200
Train loss: 2.329316
Time needed: 155.41556549072266 for validation audios
0.7937598114211981
Epoch: 124/200
Train loss: 2.337850
Time needed: 157.30145621299744 for validation audios
0.794264027636589
Epoch: 125/200
Train loss: 2.276842
Time needed: 156.026118516922 for validation audios
0.7969749204004842
Model improve: 0.7963 -> 0.7970
Epoch: 126/200
Train loss: 2.125482
Time needed: 157.81080770492554 for validation audios
0.7979916514510524
Model improve: 0.7970 -> 0.7980
Epoch: 127/200
Train loss: 2.190407
Time needed: 155.11175560951233 for validation audios
0.7942220369427626
Epoch: 128/200
Train loss: 2.081164
Time needed: 157.38934183120728 for validation audios
0.7988134223178565
Model improve: 0.7980 -> 0.7988
Epoch: 129/200
Train loss: 2.114620
Time needed: 159.38619565963745 for validation audios
0.7990404346626112
Model improve: 0.7988 -> 0.7990
Epoch: 130/200
Train loss: 2.027275
Time needed: 157.15501832962036 for validation audios
0.7932699406457195
Epoch: 131/200
Train loss: 2.134555
Time needed: 154.94883918762207 for validation audios
0.7964850186366502
Epoch: 132/200
Train loss: 2.262112
Time needed: 156.6038670539856 for validation audios
0.7924573744878447
Epoch: 133/200
Train loss: 2.072316
Time needed: 158.17722535133362 for validation audios
0.7986604942781872
Epoch: 134/200
Train loss: 2.191283
Time needed: 157.11646103858948 for validation audios
0.7975422493768025
Epoch: 135/200
Train loss: 2.043546
Time needed: 157.6089951992035 for validation audios
0.7991412059258056
Model improve: 0.7990 -> 0.7991
Epoch: 136/200
Train loss: 2.123720
Time needed: 154.8707914352417 for validation audios
0.7975326797615995
Epoch: 137/200
Train loss: 2.156984
Time needed: 156.74284887313843 for validation audios
0.796333784356511
Epoch: 138/200
Train loss: 2.044330
Time needed: 159.53152322769165 for validation audios
0.798988065419485
Epoch: 139/200
Train loss: 2.179379
Time needed: 157.84796500205994 for validation audios
0.7959472102766021
Epoch: 140/200
Train loss: 2.116510
Time needed: 156.1475851535797 for validation audios
0.7961970951014082
Epoch: 141/200
Train loss: 2.121192
Time needed: 157.35890889167786 for validation audios
0.7965471282879699
Epoch: 142/200
Train loss: 2.060384
Time needed: 157.63356375694275 for validation audios
0.7971307020501924
Epoch: 143/200
Train loss: 2.246806
Time needed: 155.76307249069214 for validation audios
0.7966748600988413
Epoch: 144/200
Train loss: 2.201010
Time needed: 158.65650939941406 for validation audios
0.7987060156305132
Epoch: 145/200
Train loss: 2.040662
Time needed: 156.4180610179901 for validation audios
0.7979363651900933
Epoch: 146/200
Train loss: 2.211619
Time needed: 157.72217059135437 for validation audios
0.7964006838296349
Epoch: 147/200
Train loss: 2.197276
Time needed: 156.57536339759827 for validation audios
0.7979118875575306
Epoch: 148/200
Train loss: 2.263742
Time needed: 163.41394662857056 for validation audios
0.7963305545034537
Epoch: 149/200
Train loss: 2.108429
Time needed: 156.5424041748047 for validation audios
0.7966878322006805
Epoch: 150/200
Train loss: 1.945244
Time needed: 157.77068662643433 for validation audios
0.7977569386042926
Epoch: 151/200
Train loss: 2.140107
Time needed: 157.17581033706665 for validation audios
0.7977252886063992
Epoch: 152/200
Train loss: 2.039929
Time needed: 155.89447450637817 for validation audios
0.7967591819724057
Epoch: 153/200
Train loss: 1.999632
Time needed: 157.19701600074768 for validation audios
0.7983886444591344
Epoch: 154/200
Train loss: 2.100576
Time needed: 154.13161492347717 for validation audios
0.7977685178189029
Epoch: 155/200
Train loss: 2.183025
Time needed: 157.33561325073242 for validation audios
0.7943673453556155
Epoch: 156/200
Train loss: 2.132921
Time needed: 156.8880650997162 for validation audios
0.7964857378282778
Epoch: 157/200
Train loss: 2.116312
Time needed: 156.7414891719818 for validation audios
0.7954420587966107
Epoch: 158/200
Train loss: 1.948378
Time needed: 156.36507773399353 for validation audios
0.8001171869744317
Model improve: 0.7991 -> 0.8001
Epoch: 159/200
Train loss: 2.179578
Time needed: 157.0266318321228 for validation audios
0.7953990300856325
Epoch: 160/200
Train loss: 1.990355
Time needed: 155.5943250656128 for validation audios
0.798866055285234
Epoch: 161/200
Train loss: 2.249536
Time needed: 156.19660019874573 for validation audios
0.7976843920999086
Epoch: 162/200
Train loss: 2.102348
Time needed: 154.82730269432068 for validation audios
0.7943361539617756
Epoch: 163/200
Train loss: 2.075510
Time needed: 155.64086484909058 for validation audios
0.7966423226575564
Epoch: 164/200
Train loss: 2.218828
Time needed: 157.65929245948792 for validation audios
0.7945973494840352
Epoch: 165/200
Train loss: 2.218705
Time needed: 154.89882159233093 for validation audios
0.7939194756495463
Epoch: 166/200
Train loss: 2.175172
Time needed: 154.619722366333 for validation audios
0.7977041091707218
Epoch: 167/200
Train loss: 2.225911
Time needed: 159.09368419647217 for validation audios
0.7957259612701646
Epoch: 168/200
Train loss: 2.142289
Time needed: 155.83722472190857 for validation audios
0.7980828399097276
Epoch: 169/200
Train loss: 2.189327
Time needed: 155.90361857414246 for validation audios
0.7975767536099997
Epoch: 170/200
Train loss: 2.066895
Time needed: 157.55882334709167 for validation audios
0.7956786125547735
Epoch: 171/200
Train loss: 2.201962
Time needed: 158.00275564193726 for validation audios
0.7928890059172814
Epoch: 172/200
Train loss: 2.073555
Time needed: 157.51027274131775 for validation audios
0.7971892886082269
Epoch: 173/200
Train loss: 2.217841
Time needed: 155.55877542495728 for validation audios
0.7965204779870754
Epoch: 174/200
Train loss: 2.058246
Time needed: 155.7319779396057 for validation audios
0.7962388697089866
Epoch: 175/200
Train loss: 2.182795
Time needed: 155.86911153793335 for validation audios
0.7926718997829441
Epoch: 176/200
Train loss: 1.979256
Time needed: 158.2459192276001 for validation audios
0.8015325562966661
Model improve: 0.8001 -> 0.8015
Epoch: 177/200
Train loss: 2.057798
Time needed: 156.8338589668274 for validation audios
0.7956741639038456
Epoch: 178/200
Train loss: 2.079711
Time needed: 156.02793216705322 for validation audios
0.7989739736041593
Epoch: 179/200
Train loss: 2.035834
Time needed: 162.7676432132721 for validation audios
0.7985633624525358
Epoch: 180/200
Train loss: 2.234234
Time needed: 155.23520874977112 for validation audios
0.7941594922643542
Epoch: 181/200
Train loss: 2.093497
Time needed: 158.42641949653625 for validation audios
0.7983586782571351
Epoch: 182/200
Train loss: 2.017330
Time needed: 160.20038223266602 for validation audios
0.7957770136492631
Epoch: 183/200
Train loss: 2.065794
Time needed: 159.7055537700653 for validation audios
0.7994289251556644
Epoch: 184/200
Train loss: 2.212199
Time needed: 156.81487154960632 for validation audios
0.7958016633840614
Epoch: 185/200
Train loss: 2.074650
Time needed: 156.6659255027771 for validation audios
0.7991919263847287
Epoch: 186/200
Train loss: 2.241175
Time needed: 156.58680510520935 for validation audios
0.795718092174812
Epoch: 187/200
Train loss: 2.039944
Time needed: 156.802170753479 for validation audios
0.7976125041147293
Epoch: 188/200
Train loss: 2.229780
Time needed: 157.49533319473267 for validation audios
0.7962329787377123
Epoch: 189/200
Train loss: 2.192113
Time needed: 156.63666796684265 for validation audios
0.7931906598532804
Epoch: 190/200
Train loss: 2.092435
Time needed: 156.96245431900024 for validation audios
0.8010248806366344
Epoch: 191/200
Train loss: 2.198483
Time needed: 156.42136144638062 for validation audios
0.7941594383420113
Epoch: 192/200
Train loss: 2.137322
Time needed: 158.09261202812195 for validation audios
0.7969081275618375
Epoch: 193/200
Train loss: 2.172037
Time needed: 158.88562750816345 for validation audios
0.7950739537292989
Epoch: 194/200
Train loss: 2.171312
Time needed: 157.2817234992981 for validation audios
0.7975527639244232
Epoch: 195/200
Train loss: 2.150404
Time needed: 154.96093559265137 for validation audios
0.7960825200278252
Epoch: 196/200
Train loss: 2.165786
Time needed: 156.85015726089478 for validation audios
0.7949204390038281
Epoch: 197/200
Train loss: 2.099244
Time needed: 155.2594621181488 for validation audios
0.7968783864404647
Epoch: 198/200
Train loss: 2.063333
Time needed: 156.5985095500946 for validation audios
0.7959551174810787
Epoch: 199/200
Train loss: 2.190822
Time needed: 156.3232023715973 for validation audios
0.7930945795783844
Epoch: 200/200
Train loss: 2.105011
Time needed: 157.94078946113586 for validation audios
0.7961453946011336
Date :05/09/2023, 00:11:14
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
thrdownsample: 500
model_name: tf_efficientnetv2_b0
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 42.529534
Epoch: 2/200
Train loss: 5.486431
Epoch: 3/200
Train loss: 4.976294
Epoch: 4/200
Train loss: 4.666716
Epoch: 5/200
Train loss: 4.311837
Epoch: 6/200
Train loss: 4.040302
Epoch: 7/200
Train loss: 3.749338
Epoch: 8/200
Train loss: 3.703867
Epoch: 9/200
Train loss: 3.521580
Epoch: 10/200
Train loss: 3.464344
Epoch: 11/200
Train loss: 3.396229
Epoch: 12/200
Train loss: 3.316013
Epoch: 13/200
Train loss: 3.270647
Epoch: 14/200
Train loss: 3.162940
Epoch: 15/200
Train loss: 3.198291
Epoch: 16/200
Train loss: 3.128563
Epoch: 17/200
Train loss: 3.032973
Epoch: 18/200
Train loss: 2.935557
Epoch: 19/200
Train loss: 3.144128
Epoch: 20/200
Train loss: 3.030361
Epoch: 21/200
Train loss: 2.896792
Epoch: 22/200
Train loss: 2.764297
Epoch: 23/200
Train loss: 2.823613
Epoch: 24/200
Train loss: 2.889547
Epoch: 25/200
Train loss: 2.846277
Epoch: 26/200
Train loss: 2.869467
Epoch: 27/200
Train loss: 2.837784
Epoch: 28/200
Train loss: 2.698835
Epoch: 29/200
Train loss: 2.735547
Epoch: 30/200
Train loss: 2.572347
Epoch: 31/200
Train loss: 2.726575
Epoch: 32/200
Train loss: 2.783415
Epoch: 33/200
Train loss: 2.560093
Epoch: 34/200
Train loss: 2.652403
Epoch: 35/200
Train loss: 2.652512
Epoch: 36/200
Train loss: 2.690799
Epoch: 37/200
Train loss: 2.580880
Epoch: 38/200
Train loss: 2.818964
Epoch: 39/200
Train loss: 2.491209
Epoch: 40/200
Train loss: 2.635174
Epoch: 41/200
Train loss: 2.567433
Epoch: 42/200
Train loss: 2.539645
Epoch: 43/200
Train loss: 2.649799
Epoch: 44/200
Train loss: 2.597369
Epoch: 45/200
Train loss: 2.646850
Epoch: 46/200
Train loss: 2.386349
Epoch: 47/200
Train loss: 2.594712
Epoch: 48/200
Train loss: 2.337607
Epoch: 49/200
Train loss: 2.322692
Epoch: 50/200
Train loss: 2.397913
Epoch: 51/200
Train loss: 2.448334
Epoch: 52/200
Train loss: 2.450720
Epoch: 53/200
Train loss: 2.360475
Epoch: 54/200
Train loss: 2.450753
Epoch: 55/200
Train loss: 2.432908
Epoch: 56/200
Train loss: 2.397600
Epoch: 57/200
Train loss: 2.453059
Epoch: 58/200
Train loss: 2.417032
Epoch: 59/200
Train loss: 2.413253
Epoch: 60/200
Train loss: 2.435992
Epoch: 61/200
Train loss: 2.320222
Epoch: 62/200
Train loss: 2.355245
Epoch: 63/200
Train loss: 2.374412
Epoch: 64/200
Train loss: 2.375927
Epoch: 65/200
Train loss: 2.439594
Epoch: 66/200
Train loss: 2.316872
Epoch: 67/200
Train loss: 2.410467
Epoch: 68/200
Train loss: 2.358797
Epoch: 69/200
Train loss: 2.537309
Epoch: 70/200
Train loss: 2.342357
Epoch: 71/200
Train loss: 2.210000
Epoch: 72/200
Train loss: 2.227767
Epoch: 73/200
Train loss: 2.375444
Epoch: 74/200
Train loss: 2.341261
Epoch: 75/200
Train loss: 2.399618
Epoch: 76/200
Train loss: 2.359539
Epoch: 77/200
Train loss: 2.336775
Epoch: 78/200
Train loss: 2.255646
Epoch: 79/200
Train loss: 2.075497
Epoch: 80/200
Train loss: 2.262193
Epoch: 81/200
Train loss: 2.256414
Epoch: 82/200
Train loss: 2.271473
Epoch: 83/200
Train loss: 2.215555
Epoch: 84/200
Train loss: 2.199695
Epoch: 85/200
Train loss: 2.336549
Epoch: 86/200
Train loss: 2.266380
Epoch: 87/200
Train loss: 2.129598
Epoch: 88/200
Train loss: 2.338711
Epoch: 89/200
Train loss: 2.364650
Epoch: 90/200
Train loss: 2.206385
Epoch: 91/200
Train loss: 2.320226
Epoch: 92/200
Train loss: 2.314130
Epoch: 93/200
Train loss: 2.341672
Epoch: 94/200
Train loss: 2.253576
Epoch: 95/200
Train loss: 2.290660
Epoch: 96/200
Train loss: 2.328440
Epoch: 97/200
Train loss: 2.283853
Epoch: 98/200
Train loss: 2.268671
Epoch: 99/200
Train loss: 2.192718
Epoch: 100/200
Train loss: 2.233235
Epoch: 101/200
Train loss: 2.300347
Epoch: 102/200
Train loss: 2.079292
Time needed: 155.59967708587646 for validation audios
0.7966084557011012
Model improve: 0.0000 -> 0.7966
Epoch: 103/200
Train loss: 2.130909
Date :05/09/2023, 02:25:03
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
thrdownsample: 500
model_name: tf_efficientnetv2_b2
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Date :05/09/2023, 02:25:11
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
thrdownsample: 500
model_name: tf_efficientnetv2_b2
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Date :05/09/2023, 02:26:39
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
thrdownsample: 500
model_name: tf_efficientnetv2_b2
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 1
13839
Date :05/09/2023, 02:29:23
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
thrdownsample: 500
model_name: tf_efficientnetv2_b2
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 27.106783
Epoch: 2/200
Train loss: 5.537725
Epoch: 3/200
Train loss: 4.954735
Epoch: 4/200
Train loss: 4.615552
Epoch: 5/200
Train loss: 4.249347
Epoch: 6/200
Train loss: 3.986861
Epoch: 7/200
Train loss: 3.670967
Epoch: 8/200
Train loss: 3.623851
Epoch: 9/200
Train loss: 3.441429
Epoch: 10/200
Train loss: 3.393161
Epoch: 11/200
Train loss: 3.299008
Epoch: 12/200
Train loss: 3.214523
Epoch: 13/200
Train loss: 3.159277
Epoch: 14/200
Train loss: 3.042481
Epoch: 15/200
Train loss: 3.076608
Epoch: 16/200
Train loss: 3.014748
Epoch: 17/200
Train loss: 2.898963
Epoch: 18/200
Train loss: 2.801060
Epoch: 19/200
Train loss: 2.998160
Epoch: 20/200
Train loss: 2.883630
Epoch: 21/200
Train loss: 2.744700
Epoch: 22/200
Train loss: 2.639966
Epoch: 23/200
Date :05/09/2023, 02:56:22
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 50
thrdownsample: 500
model_name: tf_efficientnetv2_b2
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Train loss: 9.737134
Epoch: 2/25
Date :05/09/2023, 02:59:16
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
thrdownsample: 500
model_name: tf_efficientnetv2_b2
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Date :05/09/2023, 03:00:32
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
thrdownsample: 500
model_name: tf_efficientnetv2_b2
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Date :05/09/2023, 03:04:11
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
thrdownsample: 500
model_name: tf_efficientnetv2_b2
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Date :05/09/2023, 03:08:26
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
thrdownsample: 500
model_name: tf_efficientnetv2_b2
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
0.8446974971506648
Train loss: 6.805210
Epoch: 2/25
0.8744294152113514
Train loss: 2.898444
Epoch: 3/25
0.8773318561977386
Train loss: 1.963211
Epoch: 4/25
Date :05/09/2023, 03:18:40
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 0
nmels: 128
fmax: None
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
thrdownsample: 500
model_name: tf_efficientnetv2_b2
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
0.7788835962804439
Train loss: 7.543606, valid loss: 3.0337
Epoch: 2/25
Date :05/09/2023, 03:23:10
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
thrdownsample: 500
model_name: tf_efficientnetv2_b2
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
0.7735352450386922
Train loss: 7.773103, valid loss: 3.0763
Epoch: 2/25
Date :05/09/2023, 03:28:07
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
thrdownsample: 500
model_name: tf_efficientnetv2_b2
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
0.7950779625007401
Train loss: 7.584763, valid loss: 2.9641
Epoch: 2/25
Date :05/09/2023, 03:32:03
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 0.0004
Date :05/09/2023, 03:32:21
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 50
thrdownsample: 500
model_name: tf_efficientnetv2_b2
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
0.7913992187113696
Train loss: 9.719284, valid loss: 3.3271
Epoch: 2/25
0.8589879159899968
Train loss: 4.091966, valid loss: 2.2339
Epoch: 3/25
0.8765809361190503
Train loss: 3.215851, valid loss: 1.9806
Epoch: 4/25
0.8793327058742373
Train loss: 2.601585, valid loss: 1.9473
Epoch: 5/25
0.8766653681065725
Train loss: 2.110455, valid loss: 2.0038
Epoch: 6/25
0.8732311205749402
Train loss: 1.770490, valid loss: 2.0969
Epoch: 7/25
Date :05/09/2023, 03:50:45
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
thrdownsample: 500
model_name: tf_efficientnetv2_b2
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
0.8446084363670909
Train loss: 6.807057, valid loss: 2.3369
Epoch: 2/25
0.8740583450178994
Train loss: 2.899402, valid loss: 1.9324
Epoch: 3/25
0.8757871342897559
Train loss: 1.962101, valid loss: 1.8814
Epoch: 4/25
0.8714777139397235
Train loss: 1.481925, valid loss: 1.9591
Epoch: 5/25
0.8717724871227234
Train loss: 1.280303, valid loss: 1.9656
Epoch: 6/25
0.8720862945113345
Train loss: 1.166247, valid loss: 2.0708
Epoch: 7/25
Date :05/10/2023, 02:54:43
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
thrdownsample: 500
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
0.8428546698496655
Train loss: 6.828407, valid loss: 2.3304
Epoch: 2/25
Date :05/10/2023, 02:59:27
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
thrdownsample: 500
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/60
0.8427537546496634
Train loss: 6.829291, valid loss: 2.3342
Val cmap first 5 secs: 0.8428
Epoch: 2/60
0.8705136617331846
Train loss: 2.960987, valid loss: 1.9505
Val cmap first 5 secs: 0.8705
Epoch: 3/60
Date :05/10/2023, 03:06:49
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
thrdownsample: 500
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/60
0.8408311931409119
Train loss: 6.787866, valid loss: 2.3511
Val cmap first 5 secs: 0.8408
Epoch: 2/60
Date :05/10/2023, 03:10:09
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 0
nmels: 128
fmax: None
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
thrdownsample: 500
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/60
0.8429265553888412
Train loss: 6.827459, valid loss: 2.3275
Val cmap first 5 secs: 0.8429
Epoch: 2/60
0.8708820833363824
Train loss: 2.957969, valid loss: 1.9559
Val cmap first 5 secs: 0.8709
Epoch: 3/60
0.8675857348447905
Train loss: 2.099880, valid loss: 1.9904
Val cmap first 5 secs: 0.8676
Epoch: 4/60
Date :05/10/2023, 03:20:46
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.001
weightdecay: 0.0
thrupsample: 10
thrdownsample: 500
model_name: tf_efficientnetv2_b0
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Epoch: 2/200
Date :05/10/2023, 03:23:42
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.001
weightdecay: 0.0
thrupsample: 10
thrdownsample: 500
model_name: tf_efficientnetv2_b0
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 18.152413
Epoch: 2/200
Date :05/10/2023, 03:25:30
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
thrdownsample: 500
model_name: tf_efficientnetv2_b0
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 29.360631
Epoch: 2/200
Train loss: 5.222773
Epoch: 3/200
Train loss: 4.692300
Epoch: 4/200
Train loss: 4.299876
Epoch: 5/200
Train loss: 3.985983
Epoch: 6/200
Train loss: 3.719051
Epoch: 7/200
Train loss: 3.478467
Epoch: 8/200
Train loss: 3.253003
Epoch: 9/200
Train loss: 3.035990
Epoch: 10/200
Train loss: 2.823202
Epoch: 11/200
Train loss: 2.612541
Epoch: 12/200
Train loss: 2.403651
Epoch: 13/200
Train loss: 2.201005
Epoch: 14/200
Train loss: 2.008382
Epoch: 15/200
Train loss: 1.834533
Epoch: 16/200
Train loss: 1.699613
Epoch: 17/200
Train loss: 1.625873
Epoch: 18/200
Train loss: 1.583906
Epoch: 19/200
Train loss: 1.516602
Epoch: 20/200
Train loss: 1.470684
Epoch: 21/200
Train loss: 1.460805
Epoch: 22/200
Train loss: 1.427169
Epoch: 23/200
Train loss: 1.380097
Epoch: 24/200
Train loss: 1.341254
Epoch: 25/200
Train loss: 1.296987
Epoch: 26/200
Train loss: 1.280018
Epoch: 27/200
Train loss: 1.283897
Epoch: 28/200
Train loss: 1.288033
Epoch: 29/200
Train loss: 1.285455
Epoch: 30/200
Train loss: 1.279487
Epoch: 31/200
Train loss: 1.266786
Epoch: 32/200
Train loss: 1.261608
Epoch: 33/200
Train loss: 1.236348
Epoch: 34/200
Train loss: 1.212555
Epoch: 35/200
Train loss: 1.195595
Epoch: 36/200
Train loss: 1.197424
Epoch: 37/200
Train loss: 1.196560
Epoch: 38/200
Train loss: 1.199574
Epoch: 39/200
Train loss: 1.196561
Epoch: 40/200
Train loss: 1.182571
Epoch: 41/200
Train loss: 1.161508
Epoch: 42/200
Train loss: 1.155463
Epoch: 43/200
Train loss: 1.149059
Epoch: 44/200
Train loss: 1.159270
Epoch: 45/200
Train loss: 1.163661
Epoch: 46/200
Train loss: 1.161481
Epoch: 47/200
Train loss: 1.150888
Epoch: 48/200
Train loss: 1.140212
Epoch: 49/200
Train loss: 1.139232
Epoch: 50/200
Train loss: 1.132530
Epoch: 51/200
Train loss: 1.130316
Epoch: 52/200
Train loss: 1.129095
Epoch: 53/200
Train loss: 1.130267
Epoch: 54/200
Train loss: 1.129170
Epoch: 55/200
Train loss: 1.117356
Epoch: 56/200
Train loss: 1.112656
Epoch: 57/200
Train loss: 1.110130
Epoch: 58/200
Train loss: 1.110215
Epoch: 59/200
Train loss: 1.117064
Epoch: 60/200
Train loss: 1.116963
Epoch: 61/200
Train loss: 1.112800
Epoch: 62/200
Train loss: 1.101679
Epoch: 63/200
Train loss: 1.095657
Epoch: 64/200
Train loss: 1.093110
Epoch: 65/200
Train loss: 1.092145
Epoch: 66/200
Train loss: 1.087362
Epoch: 67/200
Train loss: 1.086603
Epoch: 68/200
Train loss: 1.085521
Epoch: 69/200
Train loss: 1.084150
Epoch: 70/200
Train loss: 1.084707
Epoch: 71/200
Train loss: 1.084366
Epoch: 72/200
Train loss: 1.083627
Epoch: 73/200
Train loss: 1.085554
Epoch: 74/200
Train loss: 1.082921
Epoch: 75/200
Train loss: 1.081445
Epoch: 76/200
Train loss: 1.080858
Epoch: 77/200
Train loss: 1.077800
Epoch: 78/200
Train loss: 1.075244
Epoch: 79/200
Train loss: 1.072320
Epoch: 80/200
Train loss: 1.066880
Epoch: 81/200
Train loss: 1.067277
Epoch: 82/200
Train loss: 1.064947
Epoch: 83/200
Train loss: 1.056488
Epoch: 84/200
Train loss: 1.053667
Epoch: 85/200
Train loss: 1.051174
Epoch: 86/200
Train loss: 1.052921
Epoch: 87/200
Train loss: 1.052520
Epoch: 88/200
Train loss: 1.051779
Epoch: 89/200
Train loss: 1.045102
Epoch: 90/200
Train loss: 1.040942
Epoch: 91/200
Train loss: 1.041551
Epoch: 92/200
Train loss: 1.041903
Epoch: 93/200
Train loss: 1.043140
Epoch: 94/200
Train loss: 1.042810
Epoch: 95/200
Train loss: 1.038694
Epoch: 96/200
Train loss: 1.036787
Epoch: 97/200
Train loss: 1.034426
Epoch: 98/200
Train loss: 1.029535
Epoch: 99/200
Train loss: 1.024751
Epoch: 100/200
Train loss: 1.023021
Epoch: 101/200
Train loss: 1.020804
Epoch: 102/200
Train loss: 1.019434
Time needed: 202.5326075553894 for validation audios
0.6627184041204193
Model improve: 0.0000 -> 0.6627
Epoch: 103/200
Train loss: 1.020716
Time needed: 196.00541853904724 for validation audios
0.6644862837149699
Model improve: 0.6627 -> 0.6645
Epoch: 104/200
Train loss: 1.019626
Time needed: 195.99477219581604 for validation audios
0.6614251307160686
Epoch: 105/200
Train loss: 1.020257
Time needed: 192.78456830978394 for validation audios
0.6610086112957625
Epoch: 106/200
Train loss: 1.020507
Time needed: 191.7015941143036 for validation audios
0.663152569596693
Epoch: 107/200
Train loss: 1.017087
Time needed: 192.56283378601074 for validation audios
0.6639289588650882
Epoch: 108/200
Train loss: 1.015326
Time needed: 192.1257996559143 for validation audios
0.6635892113882449
Epoch: 109/200
Train loss: 1.012508
Time needed: 194.06367754936218 for validation audios
0.6614461945212518
Epoch: 110/200
Train loss: 1.010491
Time needed: 192.9758653640747 for validation audios
0.6595038483237319
Epoch: 111/200
Train loss: 1.009325
Time needed: 192.2432358264923 for validation audios
0.6602167052624731
Epoch: 112/200
Train loss: 1.008276
Time needed: 190.09850573539734 for validation audios
0.6619731301099612
Epoch: 113/200
Train loss: 1.009107
Time needed: 193.62441396713257 for validation audios
0.6596416951899845
Epoch: 114/200
Train loss: 1.010917
Time needed: 195.38044905662537 for validation audios
0.6605066697993416
Epoch: 115/200
Train loss: 1.010539
Time needed: 193.8722858428955 for validation audios
0.6620821246304092
Epoch: 116/200
Train loss: 1.008713
Date :05/10/2023, 07:21:19
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
thrdownsample: 500
model_name: tf_efficientnetv2_b0
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Train loss: 15.900538
Epoch: 2/100
Train loss: 4.618419
Epoch: 3/100
Train loss: 4.020465
Epoch: 4/100
Train loss: 3.586277
Epoch: 5/100
Train loss: 3.219340
Epoch: 6/100
Train loss: 2.878381
Epoch: 7/100
Train loss: 2.547706
Epoch: 8/100
Train loss: 2.231156
Epoch: 9/100
Train loss: 1.943798
Epoch: 10/100
Train loss: 1.714607
Epoch: 11/100
Train loss: 1.604285
Epoch: 12/100
Date :05/10/2023, 07:35:24
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
thrdownsample: 500
model_name: tf_efficientnetv2_b0
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Train loss: 15.899976
Epoch: 2/100
Train loss: 4.617943
Epoch: 3/100
Date :05/10/2023, 07:38:59
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
thrdownsample: 500
model_name: tf_efficientnetv2_b0
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Train loss: 15.894315
Train loss: 15.894315, valid loss: 4.1922
Val cmap first 5 secs: 0.6486
Epoch: 2/100
Train loss: 4.618493
Train loss: 4.618493, valid loss: 3.1117
Val cmap first 5 secs: 0.7652
Epoch: 3/100
Train loss: 4.020509
Train loss: 4.020509, valid loss: 2.5727
Val cmap first 5 secs: 0.8140
Epoch: 4/100
Train loss: 3.586083
Train loss: 3.586083, valid loss: 2.2849
Val cmap first 5 secs: 0.8342
Epoch: 5/100
Train loss: 3.219859
Train loss: 3.219859, valid loss: 2.1235
Val cmap first 5 secs: 0.8447
Epoch: 6/100
Train loss: 2.878076
Train loss: 2.878076, valid loss: 2.0437
Val cmap first 5 secs: 0.8490
Epoch: 7/100
Train loss: 2.547755
Train loss: 2.547755, valid loss: 2.0209
Val cmap first 5 secs: 0.8489
Epoch: 8/100
Train loss: 2.230467
Train loss: 2.230467, valid loss: 2.0499
Val cmap first 5 secs: 0.8470
Epoch: 9/100
Date :05/10/2023, 07:52:50
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 3
13816
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 26.585868
Time needed: 175.4966139793396 for validation audios
0.29521825892954834
Model improve: 0.0000 -> 0.2952
Epoch: 2/200
Train loss: 5.198812
Time needed: 173.92855143547058 for validation audios
0.3995744241521697
Model improve: 0.2952 -> 0.3996
Epoch: 3/200
Train loss: 4.711012
Time needed: 178.37342405319214 for validation audios
0.49029942331789406
Model improve: 0.3996 -> 0.4903
Epoch: 4/200
Train loss: 4.483010
Time needed: 175.95451879501343 for validation audios
0.5456893297724734
Model improve: 0.4903 -> 0.5457
Epoch: 5/200
Train loss: 4.186174
Time needed: 175.3661663532257 for validation audios
0.5870739930949984
Model improve: 0.5457 -> 0.5871
Epoch: 6/200
Train loss: 3.972941
Time needed: 174.63476061820984 for validation audios
0.6166465900460114
Model improve: 0.5871 -> 0.6166
Epoch: 7/200
Train loss: 3.661710
Time needed: 174.96667790412903 for validation audios
0.6393191495652637
Model improve: 0.6166 -> 0.6393
Epoch: 8/200
Train loss: 3.732503
Time needed: 176.21936011314392 for validation audios
0.6564553092064286
Model improve: 0.6393 -> 0.6565
Epoch: 9/200
Train loss: 3.488487
Time needed: 175.20667719841003 for validation audios
0.6713105294230353
Model improve: 0.6565 -> 0.6713
Epoch: 10/200
Date :05/10/2023, 08:30:30
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 3
13816
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Date :05/10/2023, 08:30:56
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 3
13816
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 33.140397
Epoch: 2/200
Train loss: 5.354954
Epoch: 3/200
Train loss: 4.879579
Epoch: 4/200
Train loss: 4.655475
Epoch: 5/200
Train loss: 4.381668
Epoch: 6/200
Train loss: 4.144377
Epoch: 7/200
Train loss: 3.846244
Epoch: 8/200
Train loss: 3.873319
Epoch: 9/200
Train loss: 3.662908
Epoch: 10/200
Train loss: 3.628461
Epoch: 11/200
Train loss: 3.543502
Epoch: 12/200
Train loss: 3.474370
Epoch: 13/200
Train loss: 3.405109
Epoch: 14/200
Train loss: 3.340974
Epoch: 15/200
Train loss: 3.309150
Epoch: 16/200
Train loss: 3.277719
Time needed: 218.55162692070007 for validation audios
0.7042426349197345
Model improve: 0.0000 -> 0.7042
Epoch: 17/200
Date :05/10/2023, 09:03:04
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 31.541388
Epoch: 2/200
Train loss: 5.357346
Epoch: 3/200
Train loss: 4.888447
Epoch: 4/200
Train loss: 4.684573
Epoch: 5/200
Train loss: 4.386410
Epoch: 6/200
Train loss: 4.184503
Epoch: 7/200
Train loss: 3.904462
Epoch: 8/200
Train loss: 3.895280
Epoch: 9/200
Train loss: 3.753566
Epoch: 10/200
Train loss: 3.700243
Epoch: 11/200
Train loss: 3.614247
Epoch: 12/200
Train loss: 3.501071
Epoch: 13/200
Train loss: 3.468699
Epoch: 14/200
Train loss: 3.360284
Epoch: 15/200
Train loss: 3.363075
Epoch: 16/200
Train loss: 3.311813
Time needed: 165.68245577812195 for validation audios
0.7224629903338742
Model improve: 0.0000 -> 0.7225
Epoch: 17/200
Train loss: 3.180261
Time needed: 165.38831186294556 for validation audios
0.7284059330206959
Model improve: 0.7225 -> 0.7284
Epoch: 18/200
Train loss: 3.087628
Time needed: 165.56997394561768 for validation audios
0.7372537365599783
Model improve: 0.7284 -> 0.7373
Epoch: 19/200
Train loss: 3.295323
Date :05/10/2023, 09:34:11
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 31.361459
Epoch: 2/200
Train loss: 5.356586
Epoch: 3/200
Train loss: 4.887051
Epoch: 4/200
Train loss: 4.683443
Epoch: 5/200
Train loss: 4.385212
Epoch: 6/200
Train loss: 4.182741
Epoch: 7/200
Train loss: 3.903035
Epoch: 8/200
Train loss: 3.893171
Epoch: 9/200
Train loss: 3.751859
Epoch: 10/200
Train loss: 3.697983
Epoch: 11/200
Train loss: 3.611150
Epoch: 12/200
Train loss: 3.499452
Epoch: 13/200
Train loss: 3.466905
Epoch: 14/200
Train loss: 3.358928
Epoch: 15/200
Train loss: 3.361796
Epoch: 16/200
Train loss: 3.310565
Epoch: 17/200
Train loss: 3.180148
Epoch: 18/200
Train loss: 3.078064
Epoch: 19/200
Train loss: 3.295808
Epoch: 20/200
Train loss: 3.167021
Epoch: 21/200
Train loss: 3.021942
Epoch: 22/200
Train loss: 2.908646
Epoch: 23/200
Train loss: 2.945685
Epoch: 24/200
Train loss: 3.021571
Epoch: 25/200
Train loss: 3.001166
Epoch: 26/200
Train loss: 3.013012
Epoch: 27/200
Train loss: 2.952765
Epoch: 28/200
Train loss: 2.831888
Epoch: 29/200
Train loss: 2.847312
Epoch: 30/200
Train loss: 2.698004
Epoch: 31/200
Train loss: 2.838712
Epoch: 32/200
Train loss: 2.890723
Epoch: 33/200
Train loss: 2.682338
Epoch: 34/200
Train loss: 2.778483
Epoch: 35/200
Train loss: 2.762155
Epoch: 36/200
Train loss: 2.801495
Epoch: 37/200
Train loss: 2.674224
Epoch: 38/200
Train loss: 2.937645
Epoch: 39/200
Train loss: 2.599542
Epoch: 40/200
Train loss: 2.731633
Epoch: 41/200
Train loss: 2.666118
Epoch: 42/200
Train loss: 2.639902
Epoch: 43/200
Train loss: 2.735041
Epoch: 44/200
Train loss: 2.717917
Epoch: 45/200
Train loss: 2.733064
Epoch: 46/200
Train loss: 2.470612
Epoch: 47/200
Train loss: 2.669930
Epoch: 48/200
Train loss: 2.427991
Epoch: 49/200
Train loss: 2.410473
Epoch: 50/200
Train loss: 2.467926
Epoch: 51/200
Train loss: 2.523430
Epoch: 52/200
Train loss: 2.527378
Epoch: 53/200
Train loss: 2.428411
Epoch: 54/200
Train loss: 2.556176
Epoch: 55/200
Train loss: 2.516461
Epoch: 56/200
Train loss: 2.471613
Epoch: 57/200
Train loss: 2.512787
Epoch: 58/200
Train loss: 2.483645
Epoch: 59/200
Train loss: 2.490531
Epoch: 60/200
Train loss: 2.513833
Epoch: 61/200
Train loss: 2.415006
Epoch: 62/200
Train loss: 2.429758
Epoch: 63/200
Train loss: 2.443242
Epoch: 64/200
Train loss: 2.477341
Epoch: 65/200
Train loss: 2.506450
Epoch: 66/200
Train loss: 2.395871
Epoch: 67/200
Train loss: 2.480398
Epoch: 68/200
Train loss: 2.440507
Epoch: 69/200
Train loss: 2.601581
Epoch: 70/200
Train loss: 2.413433
Epoch: 71/200
Train loss: 2.262532
Epoch: 72/200
Train loss: 2.303349
Epoch: 73/200
Train loss: 2.412965
Epoch: 74/200
Train loss: 2.393480
Epoch: 75/200
Train loss: 2.463386
Epoch: 76/200
Train loss: 2.422868
Epoch: 77/200
Train loss: 2.373015
Epoch: 78/200
Train loss: 2.295878
Epoch: 79/200
Train loss: 2.138568
Epoch: 80/200
Train loss: 2.302852
Epoch: 81/200
Train loss: 2.333452
Epoch: 82/200
Train loss: 2.318782
Epoch: 83/200
Train loss: 2.285621
Epoch: 84/200
Train loss: 2.268014
Epoch: 85/200
Train loss: 2.401460
Epoch: 86/200
Train loss: 2.320496
Epoch: 87/200
Train loss: 2.198920
Epoch: 88/200
Train loss: 2.383419
Epoch: 89/200
Train loss: 2.426820
Epoch: 90/200
Train loss: 2.264325
Epoch: 91/200
Train loss: 2.387040
Epoch: 92/200
Train loss: 2.364530
Epoch: 93/200
Train loss: 2.404184
Epoch: 94/200
Train loss: 2.314840
Epoch: 95/200
Train loss: 2.354792
Epoch: 96/200
Train loss: 2.377675
Epoch: 97/200
Train loss: 2.334674
Epoch: 98/200
Train loss: 2.308207
Epoch: 99/200
Train loss: 2.244711
Epoch: 100/200
Train loss: 2.296513
Epoch: 101/200
Train loss: 2.337950
Epoch: 102/200
Train loss: 2.130487
Time needed: 167.43937921524048 for validation audios
0.7974810128279556
Epoch: 103/200
Train loss: 2.182027
Time needed: 164.96402883529663 for validation audios
0.7938427694152259
Epoch: 104/200
Train loss: 2.192034
Time needed: 166.0869357585907 for validation audios
0.7976794060636159
Epoch: 105/200
Train loss: 2.246348
Time needed: 168.71770405769348 for validation audios
0.7980861060689375
Epoch: 106/200
Train loss: 2.265352
Time needed: 167.89718174934387 for validation audios
0.7950321245030396
Epoch: 107/200
Date :05/10/2023, 11:53:31
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Train loss: 18.892881
Epoch: 2/100
Train loss: 4.903726
Epoch: 3/100
Train loss: 4.409528
Epoch: 4/100
Train loss: 4.049776
Epoch: 5/100
Train loss: 3.866756
Epoch: 6/100
Train loss: 3.698807
Epoch: 7/100
Train loss: 3.561521
Epoch: 8/100
Train loss: 3.515243
Epoch: 9/100
Train loss: 3.304929
Epoch: 10/100
Train loss: 3.408810
Epoch: 11/100
Train loss: 3.127635
Epoch: 12/100
Train loss: 3.157433
Epoch: 13/100
Train loss: 3.156644
Epoch: 14/100
Train loss: 3.061257
Epoch: 15/100
Train loss: 2.930912
Epoch: 16/100
Train loss: 3.028641
Epoch: 17/100
Train loss: 2.906444
Epoch: 18/100
Train loss: 2.949862
Epoch: 19/100
Train loss: 2.953902
Epoch: 20/100
Train loss: 2.824758
Epoch: 21/100
Train loss: 2.807902
Epoch: 22/100
Train loss: 2.903168
Epoch: 23/100
Train loss: 2.758907
Epoch: 24/100
Train loss: 2.728312
Epoch: 25/100
Train loss: 2.603246
Epoch: 26/100
Train loss: 2.693920
Epoch: 27/100
Train loss: 2.650078
Epoch: 28/100
Train loss: 2.670556
Epoch: 29/100
Train loss: 2.664384
Epoch: 30/100
Train loss: 2.673218
Epoch: 31/100
Train loss: 2.571633
Epoch: 32/100
Train loss: 2.598723
Time needed: 169.3682849407196 for validation audios
0.7809730610907106
Model improve: 0.0000 -> 0.7810
Epoch: 33/100
Train loss: 2.607226
Time needed: 168.7435646057129 for validation audios
0.7832198567288959
Model improve: 0.7810 -> 0.7832
Epoch: 34/100
Train loss: 2.618013
Time needed: 173.12907934188843 for validation audios
0.7827090391305972
Epoch: 35/100
Train loss: 2.667696
Time needed: 168.84391379356384 for validation audios
0.7844983219758959
Model improve: 0.7832 -> 0.7845
Epoch: 36/100
Date :05/10/2023, 12:51:58
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b2
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Date :05/10/2023, 12:54:42
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b2
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Date :05/10/2023, 12:55:49
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b2
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Train loss: 12.473594
Epoch: 2/25
Train loss: 4.450592
Epoch: 3/25
Train loss: 3.994641
Epoch: 4/25
Train loss: 3.749063
Epoch: 5/25
Train loss: 3.575350
Epoch: 6/25
Train loss: 3.364424
Epoch: 7/25
Train loss: 3.312588
Epoch: 8/25
Train loss: 3.203104
Epoch: 9/25
Train loss: 3.111058
Epoch: 10/25
Train loss: 3.094763
Epoch: 11/25
Train loss: 3.060605
Epoch: 12/25
Train loss: 2.959319
Time needed: 171.51752281188965 for validation audios
0.7607085726684167
Model improve: 0.0000 -> 0.7607
Epoch: 13/25
Train loss: 2.880503
Time needed: 170.69128513336182 for validation audios
0.7647767628081057
Model improve: 0.7607 -> 0.7648
Epoch: 14/25
Train loss: 2.871509
Time needed: 168.89354348182678 for validation audios
0.7692736461751577
Model improve: 0.7648 -> 0.7693
Epoch: 15/25
Train loss: 2.872778
Date :05/11/2023, 11:02:27
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 8.4866, val loss: 2.9367
First 5 secs: 0.7793
All: 0.6045
Model improve: 0.0000 -> 0.6045
Epoch: 2/60
Train loss: 3.4494, val loss: 2.2723
First 5 secs: 0.8353
All: 0.6863
Model improve: 0.6045 -> 0.6863
Epoch: 3/60
Train loss: 3.0765, val loss: 2.0932
First 5 secs: 0.8493
All: 0.7176
Model improve: 0.6863 -> 0.7176
Epoch: 4/60
Train loss: 2.9043, val loss: 2.0034
First 5 secs: 0.8607
All: 0.7353
Model improve: 0.7176 -> 0.7353
Epoch: 5/60
Train loss: 2.6945, val loss: 1.9757
First 5 secs: 0.8597
All: 0.7345
Epoch: 6/60
Train loss: 2.6236, val loss: 1.8903
First 5 secs: 0.8714
All: 0.7494
Model improve: 0.7353 -> 0.7494
Epoch: 7/60
Train loss: 2.5559, val loss: 1.8428
First 5 secs: 0.8712
All: 0.7487
Epoch: 8/60
Train loss: 2.4462, val loss: 1.8776
First 5 secs: 0.8683
All: 0.7503
Model improve: 0.7494 -> 0.7503
Epoch: 9/60
Train loss: 2.4434, val loss: 1.8625
First 5 secs: 0.8724
All: 0.7546
Model improve: 0.7503 -> 0.7546
Epoch: 10/60
Train loss: 2.2991, val loss: 1.8646
First 5 secs: 0.8751
Date :05/11/2023, 11:47:35
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b0
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 18.5399, val loss: 5.1489
First 5 secs: 0.5723
All: 0.3297
Model improve: 0.0000 -> 0.3297
Epoch: 2/60
Train loss: 5.0033, val loss: 3.6608
First 5 secs: 0.7160
All: 0.4989
Model improve: 0.3297 -> 0.4989
Epoch: 3/60
Train loss: 4.3091, val loss: 2.9452
First 5 secs: 0.7816
All: 0.5984
Model improve: 0.4989 -> 0.5984
Epoch: 4/60
Train loss: 4.0484, val loss: 2.5571
First 5 secs: 0.8163
Date :05/11/2023, 12:04:19
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.001
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b0
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 18.5432, val loss: 5.1456
First 5 secs: 0.5723
All: 0.3299
Model improve: 0.0000 -> 0.3299
Epoch: 2/200
Train loss: 5.0018, val loss: 3.6574
First 5 secs: 0.7162
All: 0.4995
Model improve: 0.3299 -> 0.4995
Epoch: 3/200
Train loss: 4.3076, val loss: 2.9471
First 5 secs: 0.7822
All: 0.5978
Model improve: 0.4995 -> 0.5978
Epoch: 4/200
Train loss: 4.0460, val loss: 2.5552
First 5 secs: 0.8165
All: 0.6502
Model improve: 0.5978 -> 0.6502
Epoch: 5/200
Train loss: 3.7018, val loss: 2.3408
First 5 secs: 0.8326
All: 0.6803
Model improve: 0.6502 -> 0.6803
Epoch: 6/200
Train loss: 3.4771, val loss: 2.2212
First 5 secs: 0.8469
All: 0.7014
Model improve: 0.6803 -> 0.7014
Epoch: 7/200
Train loss: 3.1932, val loss: 2.0957
First 5 secs: 0.8557
All: 0.7176
Model improve: 0.7014 -> 0.7176
Epoch: 8/200
Train loss: 3.2203, val loss: 1.9887
First 5 secs: 0.8618
All: 0.7323
Model improve: 0.7176 -> 0.7323
Epoch: 9/200
Train loss: 3.1015, val loss: 1.9599
First 5 secs: 0.8687
All: 0.7413
Model improve: 0.7323 -> 0.7413
Epoch: 10/200
Train loss: 3.0923, val loss: 2.0231
First 5 secs: 0.8612
All: 0.7455
Model improve: 0.7413 -> 0.7455
Epoch: 11/200
Date :05/11/2023, 12:48:10
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b0
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 31.6098
All: 0.2867
Model improve: 0.0000 -> 0.2867
Epoch: 2/200
Train loss: 5.3996
All: 0.3907
Model improve: 0.2867 -> 0.3907
Epoch: 3/200
Train loss: 4.8573
All: 0.4931
Model improve: 0.3907 -> 0.4931
Epoch: 4/200
Train loss: 4.5728
All: 0.5590
Model improve: 0.4931 -> 0.5590
Epoch: 5/200
Train loss: 4.2521
All: 0.6024
Model improve: 0.5590 -> 0.6024
Epoch: 6/200
Train loss: 4.0096
All: 0.6350
Model improve: 0.6024 -> 0.6350
Epoch: 7/200
Train loss: 3.7052
All: 0.6567
Model improve: 0.6350 -> 0.6567
Epoch: 8/200
Train loss: 3.7017
All: 0.6745
Model improve: 0.6567 -> 0.6745
Epoch: 9/200
Train loss: 3.5311
All: 0.6914
Model improve: 0.6745 -> 0.6914
Epoch: 10/200
Train loss: 3.4952
All: 0.6999
Model improve: 0.6914 -> 0.6999
Epoch: 11/200
Train loss: 3.4164
All: 0.7121
Model improve: 0.6999 -> 0.7121
Epoch: 12/200
Train loss: 3.3332
All: 0.7199
Model improve: 0.7121 -> 0.7199
Epoch: 13/200
Train loss: 3.3120
All: 0.7286
Model improve: 0.7199 -> 0.7286
Epoch: 14/200
Train loss: 3.1855
All: 0.7357
Model improve: 0.7286 -> 0.7357
Epoch: 15/200
Train loss: 3.2372
All: 0.7399
Model improve: 0.7357 -> 0.7399
Epoch: 16/200
Train loss: 3.1870
All: 0.7456
Model improve: 0.7399 -> 0.7456
Epoch: 17/200
Train loss: 3.0478
All: 0.7457
Model improve: 0.7456 -> 0.7457
Epoch: 18/200
Train loss: 2.9536
All: 0.7534
Model improve: 0.7457 -> 0.7534
Epoch: 19/200
Train loss: 3.1689
All: 0.7570
Model improve: 0.7534 -> 0.7570
Epoch: 20/200
Train loss: 3.0693
All: 0.7600
Model improve: 0.7570 -> 0.7600
Epoch: 21/200
Date :05/11/2023, 14:12:20
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b0
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 3
13816
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 31.406038
Epoch: 2/200
Train loss: 5.435259
Epoch: 3/200
Train loss: 4.888733
Epoch: 4/200
Train loss: 4.601089
Epoch: 5/200
Train loss: 4.281139
Epoch: 6/200
Train loss: 4.007290
Epoch: 7/200
Train loss: 3.692160
Epoch: 8/200
Train loss: 3.720562
Epoch: 9/200
Train loss: 3.492727
Epoch: 10/200
Train loss: 3.448619
Epoch: 11/200
Train loss: 3.398064
Epoch: 12/200
Train loss: 3.370808
Epoch: 13/200
Train loss: 3.270929
Epoch: 14/200
Train loss: 3.204490
Epoch: 15/200
Train loss: 3.186379
Epoch: 16/200
Train loss: 3.159845
Epoch: 17/200
Train loss: 3.047491
Epoch: 18/200
Train loss: 2.904710
Epoch: 19/200
Train loss: 3.147923
Epoch: 20/200
Train loss: 3.039973
Epoch: 21/200
Train loss: 2.895907
Epoch: 22/200
Train loss: 2.776382
Epoch: 23/200
Train loss: 2.823959
Epoch: 24/200
Train loss: 2.845821
Epoch: 25/200
Train loss: 2.946826
Epoch: 26/200
Train loss: 2.822944
Epoch: 27/200
Train loss: 2.824532
Epoch: 28/200
Train loss: 2.799533
Epoch: 29/200
Train loss: 2.661740
Epoch: 30/200
Train loss: 2.593758
Epoch: 31/200
Train loss: 2.697590
Epoch: 32/200
Train loss: 2.768956
Epoch: 33/200
Train loss: 2.564045
Epoch: 34/200
Train loss: 2.623986
Epoch: 35/200
Train loss: 2.698012
Epoch: 36/200
Train loss: 2.725260
Epoch: 37/200
Train loss: 2.537916
Epoch: 38/200
Train loss: 2.776936
Epoch: 39/200
Train loss: 2.603764
Epoch: 40/200
Train loss: 2.556487
Epoch: 41/200
Train loss: 2.655382
Epoch: 42/200
Train loss: 2.471088
Epoch: 43/200
Train loss: 2.645802
Epoch: 44/200
Train loss: 2.537376
Epoch: 45/200
Train loss: 2.614733
Epoch: 46/200
Train loss: 2.525991
Epoch: 47/200
Train loss: 2.531077
Epoch: 48/200
Train loss: 2.409309
Epoch: 49/200
Train loss: 2.301448
Epoch: 50/200
Train loss: 2.429897
Epoch: 51/200
Train loss: 2.334287
Epoch: 52/200
Train loss: 2.423854
Epoch: 53/200
Train loss: 2.309001
Epoch: 54/200
Train loss: 2.536565
Epoch: 55/200
Train loss: 2.440588
Epoch: 56/200
Train loss: 2.298768
Epoch: 57/200
Train loss: 2.500977
Epoch: 58/200
Train loss: 2.343528
Epoch: 59/200
Train loss: 2.489260
Epoch: 60/200
Train loss: 2.391830
Epoch: 61/200
Train loss: 2.418599
Epoch: 62/200
Train loss: 2.277683
Epoch: 63/200
Train loss: 2.322186
Epoch: 64/200
Train loss: 2.436989
Epoch: 65/200
Train loss: 2.321622
Epoch: 66/200
Train loss: 2.453749
Epoch: 67/200
Train loss: 2.271841
Epoch: 68/200
Train loss: 2.471272
Epoch: 69/200
Train loss: 2.365872
Epoch: 70/200
Train loss: 2.416689
Epoch: 71/200
Train loss: 2.293159
Epoch: 72/200
Train loss: 2.269977
Epoch: 73/200
Train loss: 2.135940
Epoch: 74/200
Train loss: 2.436302
Epoch: 75/200
Train loss: 2.400098
Epoch: 76/200
Train loss: 2.257940
Epoch: 77/200
Train loss: 2.337718
Epoch: 78/200
Train loss: 2.313340
Epoch: 79/200
Train loss: 2.151605
Epoch: 80/200
Train loss: 2.165919
Epoch: 81/200
Train loss: 2.238818
Epoch: 82/200
Train loss: 2.278779
Epoch: 83/200
Train loss: 2.141743
Epoch: 84/200
Train loss: 2.133795
Epoch: 85/200
Train loss: 2.298133
Epoch: 86/200
Train loss: 2.290454
Epoch: 87/200
Train loss: 2.279174
Epoch: 88/200
Train loss: 2.088652
Epoch: 89/200
Train loss: 2.346617
Epoch: 90/200
Train loss: 2.299464
Epoch: 91/200
Train loss: 2.292694
Epoch: 92/200
Train loss: 2.249989
Epoch: 93/200
Train loss: 2.325087
Epoch: 94/200
Train loss: 2.260705
Epoch: 95/200
Train loss: 2.278866
Epoch: 96/200
Train loss: 2.255587
Epoch: 97/200
Train loss: 2.319747
Epoch: 98/200
Train loss: 2.246729
Epoch: 99/200
Train loss: 2.238670
Epoch: 100/200
Train loss: 2.147823
Epoch: 101/200
Train loss: 2.250248
Epoch: 102/200
Train loss: 2.247870
Time needed: 164.42155718803406 for validation audios
0.7691124484857922
Model improve: 0.0000 -> 0.7691
Epoch: 103/200
Train loss: 2.044392
Date :05/11/2023, 16:14:22
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b0
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 3
13816
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Date :05/11/2023, 16:15:34
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b0
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 31.611367
Epoch: 2/200
Train loss: 5.398403
Epoch: 3/200
Train loss: 4.860086
Epoch: 4/200
Train loss: 4.578546
Epoch: 5/200
Train loss: 4.245188
Epoch: 6/200
Train loss: 4.019319
Epoch: 7/200
Train loss: 3.747204
Epoch: 8/200
Train loss: 3.702915
Epoch: 9/200
Train loss: 3.545433
Epoch: 10/200
Train loss: 3.512433
Epoch: 11/200
Train loss: 3.439446
Epoch: 12/200
Train loss: 3.339572
Epoch: 13/200
Train loss: 3.294123
Epoch: 14/200
Train loss: 3.189754
Epoch: 15/200
Train loss: 3.233410
Epoch: 16/200
Train loss: 3.170793
Epoch: 17/200
Train loss: 3.057186
Epoch: 18/200
Train loss: 2.954984
Epoch: 19/200
Train loss: 3.155756
Epoch: 20/200
Train loss: 3.055789
Epoch: 21/200
Train loss: 2.914750
Epoch: 22/200
Train loss: 2.788315
Epoch: 23/200
Train loss: 2.846675
Epoch: 24/200
Train loss: 2.932293
Epoch: 25/200
Train loss: 2.852577
Epoch: 26/200
Train loss: 2.889933
Epoch: 27/200
Train loss: 2.882012
Epoch: 28/200
Train loss: 2.710138
Epoch: 29/200
Train loss: 2.745368
Epoch: 30/200
Train loss: 2.583686
Epoch: 31/200
Train loss: 2.735235
Epoch: 32/200
Train loss: 2.809896
Epoch: 33/200
Train loss: 2.579163
Epoch: 34/200
Train loss: 2.664595
Epoch: 35/200
Train loss: 2.687633
Epoch: 36/200
Train loss: 2.708140
Epoch: 37/200
Train loss: 2.580895
Epoch: 38/200
Train loss: 2.860185
Epoch: 39/200
Train loss: 2.522108
Epoch: 40/200
Train loss: 2.655398
Epoch: 41/200
Train loss: 2.576948
Epoch: 42/200
Train loss: 2.550136
Epoch: 43/200
Train loss: 2.664034
Epoch: 44/200
Train loss: 2.622123
Epoch: 45/200
Train loss: 2.652444
Epoch: 46/200
Train loss: 2.395627
Epoch: 47/200
Train loss: 2.613886
Epoch: 48/200
Train loss: 2.354719
Epoch: 49/200
Train loss: 2.341316
Epoch: 50/200
Train loss: 2.402153
Epoch: 51/200
Train loss: 2.458471
Epoch: 52/200
Train loss: 2.453805
Epoch: 53/200
Train loss: 2.366782
Epoch: 54/200
Train loss: 2.486619
Epoch: 55/200
Train loss: 2.458725
Epoch: 56/200
Train loss: 2.433972
Epoch: 57/200
Train loss: 2.448034
Epoch: 58/200
Train loss: 2.441787
Epoch: 59/200
Train loss: 2.433059
Epoch: 60/200
Train loss: 2.454407
Epoch: 61/200
Train loss: 2.338783
Epoch: 62/200
Train loss: 2.372863
Epoch: 63/200
Train loss: 2.389698
Epoch: 64/200
Train loss: 2.391779
Epoch: 65/200
Train loss: 2.465522
Epoch: 66/200
Train loss: 2.335770
Epoch: 67/200
Train loss: 2.431621
Epoch: 68/200
Train loss: 2.370575
Epoch: 69/200
Train loss: 2.548868
Epoch: 70/200
Train loss: 2.353307
Epoch: 71/200
Train loss: 2.217250
Epoch: 72/200
Train loss: 2.234392
Epoch: 73/200
Train loss: 2.376835
Epoch: 74/200
Train loss: 2.364693
Epoch: 75/200
Train loss: 2.394706
Epoch: 76/200
Train loss: 2.375966
Epoch: 77/200
Train loss: 2.344526
Epoch: 78/200
Train loss: 2.269082
Epoch: 79/200
Train loss: 2.085404
Epoch: 80/200
Train loss: 2.263487
Epoch: 81/200
Train loss: 2.257459
Epoch: 82/200
Train loss: 2.264464
Epoch: 83/200
Train loss: 2.232960
Epoch: 84/200
Train loss: 2.214684
Epoch: 85/200
Train loss: 2.349108
Epoch: 86/200
Train loss: 2.274306
Epoch: 87/200
Train loss: 2.138284
Epoch: 88/200
Train loss: 2.352677
Epoch: 89/200
Train loss: 2.392690
Epoch: 90/200
Train loss: 2.203163
Epoch: 91/200
Train loss: 2.342313
Epoch: 92/200
Train loss: 2.312376
Epoch: 93/200
Train loss: 2.339748
Epoch: 94/200
Train loss: 2.259485
Epoch: 95/200
Train loss: 2.294850
Epoch: 96/200
Train loss: 2.365238
Epoch: 97/200
Train loss: 2.285460
Epoch: 98/200
Train loss: 2.263066
Epoch: 99/200
Train loss: 2.203310
Epoch: 100/200
Train loss: 2.249035
Epoch: 101/200
Train loss: 2.311038
Epoch: 102/200
Train loss: 2.086294
Time needed: 155.23564982414246 for validation audios
0.7996770974762342
Model improve: 0.0000 -> 0.7997
Epoch: 103/200
Train loss: 2.135718
Time needed: 157.1941432952881 for validation audios
0.7950781331641842
Epoch: 104/200
Train loss: 2.167876
Time needed: 154.79320573806763 for validation audios
0.7989162321286261
Epoch: 105/200
Train loss: 2.193862
Time needed: 156.57969164848328 for validation audios
0.7999722385807612
Model improve: 0.7997 -> 0.8000
Epoch: 106/200
Train loss: 2.218700
Time needed: 155.16884660720825 for validation audios
0.79797302663857
Epoch: 107/200
Train loss: 2.265811
Time needed: 155.61355662345886 for validation audios
0.7961623261786512
Epoch: 108/200
Train loss: 2.259388
Time needed: 155.57639956474304 for validation audios
0.7968897022436188
Epoch: 109/200
Train loss: 2.223081
Time needed: 157.7938096523285 for validation audios
0.7969380138719627
Epoch: 110/200
Train loss: 2.288315
Time needed: 157.26012873649597 for validation audios
0.7952746864049782
Epoch: 111/200
Train loss: 2.085346
Time needed: 157.49956464767456 for validation audios
0.7997285180703083
Epoch: 112/200
Train loss: 2.183183
Time needed: 156.84911251068115 for validation audios
0.7967250957123714
Epoch: 113/200
Train loss: 2.176637
Time needed: 156.21045351028442 for validation audios
0.7998430084710074
Epoch: 114/200
Train loss: 2.268447
Time needed: 156.77037501335144 for validation audios
0.7972541990470915
Epoch: 115/200
Train loss: 2.164153
Time needed: 156.3136076927185 for validation audios
0.8001942548379133
Model improve: 0.8000 -> 0.8002
Epoch: 116/200
Train loss: 2.227633
Time needed: 155.31464552879333 for validation audios
0.7974154509660086
Epoch: 117/200
Train loss: 2.163736
Time needed: 157.860209941864 for validation audios
0.7988315927518691
Epoch: 118/200
Train loss: 2.136364
Time needed: 157.2262623310089 for validation audios
0.7991954697752734
Epoch: 119/200
Train loss: 2.259870
Time needed: 157.41376996040344 for validation audios
0.7963043945134125
Epoch: 120/200
Train loss: 2.138695
Time needed: 155.3393235206604 for validation audios
0.7987080479672901
Epoch: 121/200
Train loss: 2.119868
Time needed: 155.73032760620117 for validation audios
0.8000939904008286
Epoch: 122/200
Train loss: 2.217721
Time needed: 156.94180250167847 for validation audios
0.7998152661487967
Epoch: 123/200
Train loss: 2.341520
Time needed: 156.8440818786621 for validation audios
0.7967674461054067
Epoch: 124/200
Train loss: 2.325351
Time needed: 156.36162853240967 for validation audios
0.7993792772036681
Epoch: 125/200
Train loss: 2.277506
Time needed: 155.2534899711609 for validation audios
0.8014615428376836
Model improve: 0.8002 -> 0.8015
Epoch: 126/200
Train loss: 2.129889
Time needed: 156.4453945159912 for validation audios
0.8026045177884873
Model improve: 0.8015 -> 0.8026
Epoch: 127/200
Train loss: 2.177497
Time needed: 155.913654088974 for validation audios
0.8002966649198647
Epoch: 128/200
Train loss: 2.076271
Time needed: 158.62632989883423 for validation audios
0.8026460351608391
Model improve: 0.8026 -> 0.8026
Epoch: 129/200
Train loss: 2.123029
Time needed: 157.53577756881714 for validation audios
0.8022147370814524
Epoch: 130/200
Train loss: 2.029047
Time needed: 155.7360715866089 for validation audios
0.7983730763251553
Epoch: 131/200
Train loss: 2.128315
Time needed: 155.81441593170166 for validation audios
0.8015421544384576
Epoch: 132/200
Train loss: 2.271129
Time needed: 157.22672724723816 for validation audios
0.7978489658017449
Epoch: 133/200
Train loss: 2.065820
Time needed: 156.6630344390869 for validation audios
0.8019249211278173
Epoch: 134/200
Train loss: 2.196232
Time needed: 156.49470710754395 for validation audios
0.8000178266578433
Epoch: 135/200
Train loss: 2.028945
Time needed: 156.1038749217987 for validation audios
0.8006543007256308
Epoch: 136/200
Train loss: 2.125455
Time needed: 157.17978715896606 for validation audios
0.8023875314417883
Epoch: 137/200
Train loss: 2.158624
Time needed: 157.28027081489563 for validation audios
0.7982206108988392
Epoch: 138/200
Train loss: 2.023276
Time needed: 157.5350251197815 for validation audios
0.8031084634856929
Model improve: 0.8026 -> 0.8031
Epoch: 139/200
Train loss: 2.190691
Time needed: 156.99659967422485 for validation audios
0.799471165630523
Epoch: 140/200
Train loss: 2.113134
Time needed: 157.52063918113708 for validation audios
0.7997625853312966
Epoch: 141/200
Train loss: 2.121433
Time needed: 156.87341737747192 for validation audios
0.8007747651205661
Epoch: 142/200
Train loss: 2.063503
Time needed: 157.2999484539032 for validation audios
0.8004557606677892
Epoch: 143/200
Train loss: 2.214524
Time needed: 155.18614435195923 for validation audios
0.798942643574737
Epoch: 144/200
Train loss: 2.179242
Time needed: 156.14569282531738 for validation audios
0.8013717907098906
Epoch: 145/200
Train loss: 2.024960
Time needed: 156.97372388839722 for validation audios
0.8009971121460144
Epoch: 146/200
Train loss: 2.210594
Time needed: 155.2495572566986 for validation audios
0.8004035942662587
Epoch: 147/200
Train loss: 2.194713
Time needed: 155.9467170238495 for validation audios
0.800857389813627
Epoch: 148/200
Train loss: 2.257024
Time needed: 156.3361518383026 for validation audios
0.8021256329986179
Epoch: 149/200
Train loss: 2.097683
Time needed: 157.08052730560303 for validation audios
0.8016457419873704
Epoch: 150/200
Train loss: 1.937748
Time needed: 157.14021754264832 for validation audios
0.7996780019205032
Epoch: 151/200
Train loss: 2.122982
Time needed: 154.48969888687134 for validation audios
0.8016154175255145
Epoch: 152/200
Train loss: 2.011997
Time needed: 157.29753804206848 for validation audios
0.7998819684250174
Epoch: 153/200
Train loss: 1.996726
Time needed: 155.64714980125427 for validation audios
0.8011552384054355
Epoch: 154/200
Train loss: 2.104082
Time needed: 155.12335920333862 for validation audios
0.8031673776640575
Model improve: 0.8031 -> 0.8032
Epoch: 155/200
Train loss: 2.178612
Time needed: 154.02327036857605 for validation audios
0.7988408592406149
Epoch: 156/200
Train loss: 2.137425
Time needed: 154.91665768623352 for validation audios
0.800843891239758
Epoch: 157/200
Train loss: 2.111913
Time needed: 156.2169487476349 for validation audios
0.7990618249336787
Epoch: 158/200
Train loss: 1.941726
Time needed: 156.2723457813263 for validation audios
0.8036945765115835
Model improve: 0.8032 -> 0.8037
Epoch: 159/200
Train loss: 2.177313
Time needed: 156.02791833877563 for validation audios
0.8011493249772561
Epoch: 160/200
Train loss: 1.976978
Time needed: 157.44901490211487 for validation audios
0.802544886058298
Epoch: 161/200
Train loss: 2.237814
Time needed: 156.9940321445465 for validation audios
0.8016559529769413
Epoch: 162/200
Train loss: 2.093961
Time needed: 157.78739547729492 for validation audios
0.7988362726697259
Epoch: 163/200
Train loss: 2.070699
Time needed: 158.71403789520264 for validation audios
0.8010557548459493
Epoch: 164/200
Train loss: 2.190209
Time needed: 156.82647228240967 for validation audios
0.7990385576629969
Epoch: 165/200
Train loss: 2.207793
Time needed: 156.431889295578 for validation audios
0.7988752834457962
Epoch: 166/200
Train loss: 2.173809
Time needed: 156.16326427459717 for validation audios
0.8016587721514578
Epoch: 167/200
Train loss: 2.227614
Time needed: 155.74391317367554 for validation audios
0.8007358143666073
Epoch: 168/200
Train loss: 2.123655
Time needed: 155.20933294296265 for validation audios
0.8031367686653929
Epoch: 169/200
Train loss: 2.178580
Time needed: 155.35272455215454 for validation audios
0.8027168241717906
Epoch: 170/200
Train loss: 2.046162
Time needed: 155.33621668815613 for validation audios
0.8013691990600179
Epoch: 171/200
Train loss: 2.204017
Time needed: 156.28388571739197 for validation audios
0.7997843545913473
Epoch: 172/200
Train loss: 2.058097
Time needed: 157.7220494747162 for validation audios
0.802067762245264
Epoch: 173/200
Train loss: 2.179879
Time needed: 156.14211773872375 for validation audios
0.8013852589207798
Epoch: 174/200
Train loss: 2.051794
Time needed: 157.26705694198608 for validation audios
0.7996153646333352
Epoch: 175/200
Train loss: 2.155681
Time needed: 154.6506712436676 for validation audios
0.7973913081585907
Epoch: 176/200
Train loss: 1.967838
Time needed: 156.47583985328674 for validation audios
0.804435919611882
Model improve: 0.8037 -> 0.8044
Epoch: 177/200
Train loss: 2.048740
Time needed: 155.68076539039612 for validation audios
0.801432635014327
Epoch: 178/200
Train loss: 2.059102
Time needed: 155.0449185371399 for validation audios
0.8023718759779377
Epoch: 179/200
Train loss: 2.026854
Time needed: 155.7607274055481 for validation audios
0.8035582988644764
Epoch: 180/200
Train loss: 2.213570
Time needed: 156.82001543045044 for validation audios
0.7991499927357516
Epoch: 181/200
Train loss: 2.087253
Time needed: 156.13490986824036 for validation audios
0.8025398236912702
Epoch: 182/200
Train loss: 1.997654
Time needed: 155.8885316848755 for validation audios
0.8002446515209166
Epoch: 183/200
Train loss: 2.046337
Time needed: 156.71785235404968 for validation audios
0.8034482830493473
Epoch: 184/200
Train loss: 2.209266
Time needed: 156.6020576953888 for validation audios
0.8016036112388798
Epoch: 185/200
Train loss: 2.069927
Date :05/11/2023, 23:38:34
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 1
13839
Date :05/11/2023, 23:42:39
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 26.492521
Epoch: 2/200
Train loss: 5.187158
Epoch: 3/200
Train loss: 4.721190
Epoch: 4/200
Train loss: 4.490301
Epoch: 5/200
Train loss: 4.184294
Epoch: 6/200
Train loss: 3.983580
Epoch: 7/200
Train loss: 3.695536
Epoch: 8/200
Train loss: 3.697140
Epoch: 9/200
Train loss: 3.553726
Epoch: 10/200
Train loss: 3.507670
Epoch: 11/200
Train loss: 3.446093
Epoch: 12/200
Train loss: 3.349986
Epoch: 13/200
Train loss: 3.295257
Epoch: 14/200
Train loss: 3.190562
Epoch: 15/200
Train loss: 3.229613
Epoch: 16/200
Train loss: 3.169606
Epoch: 17/200
Train loss: 3.047391
Epoch: 18/200
Train loss: 2.935316
Epoch: 19/200
Train loss: 3.183673
Epoch: 20/200
Train loss: 3.058034
Epoch: 21/200
Train loss: 2.905247
Epoch: 22/200
Train loss: 2.797898
Epoch: 23/200
Train loss: 2.828426
Epoch: 24/200
Train loss: 2.915885
Epoch: 25/200
Train loss: 2.877232
Epoch: 26/200
Train loss: 2.903871
Epoch: 27/200
Train loss: 2.842873
Epoch: 28/200
Train loss: 2.722594
Epoch: 29/200
Train loss: 2.757224
Epoch: 30/200
Train loss: 2.596487
Epoch: 31/200
Train loss: 2.754442
Epoch: 32/200
Train loss: 2.800325
Epoch: 33/200
Train loss: 2.587028
Epoch: 34/200
Train loss: 2.698305
Epoch: 35/200
Train loss: 2.694584
Epoch: 36/200
Train loss: 2.710948
Epoch: 37/200
Train loss: 2.594636
Epoch: 38/200
Train loss: 2.832260
Epoch: 39/200
Train loss: 2.530914
Epoch: 40/200
Train loss: 2.660469
Epoch: 41/200
Train loss: 2.578782
Epoch: 42/200
Train loss: 2.567909
Epoch: 43/200
Train loss: 2.682456
Epoch: 44/200
Train loss: 2.655839
Epoch: 45/200
Train loss: 2.659174
Epoch: 46/200
Train loss: 2.392341
Epoch: 47/200
Train loss: 2.610908
Epoch: 48/200
Train loss: 2.361960
Epoch: 49/200
Train loss: 2.323568
Epoch: 50/200
Train loss: 2.413189
Epoch: 51/200
Train loss: 2.465188
Epoch: 52/200
Train loss: 2.459680
Epoch: 53/200
Train loss: 2.365144
Epoch: 54/200
Train loss: 2.478316
Epoch: 55/200
Train loss: 2.451699
Epoch: 56/200
Train loss: 2.432310
Epoch: 57/200
Train loss: 2.469066
Epoch: 58/200
Train loss: 2.422901
Epoch: 59/200
Train loss: 2.445861
Epoch: 60/200
Train loss: 2.448584
Epoch: 61/200
Train loss: 2.358979
Epoch: 62/200
Train loss: 2.374500
Epoch: 63/200
Train loss: 2.391435
Epoch: 64/200
Train loss: 2.402476
Epoch: 65/200
Train loss: 2.463125
Epoch: 66/200
Train loss: 2.327294
Epoch: 67/200
Train loss: 2.403403
Epoch: 68/200
Train loss: 2.386821
Epoch: 69/200
Train loss: 2.566490
Epoch: 70/200
Train loss: 2.369202
Epoch: 71/200
Train loss: 2.232625
Epoch: 72/200
Train loss: 2.247229
Epoch: 73/200
Train loss: 2.364045
Epoch: 74/200
Train loss: 2.351697
Epoch: 75/200
Train loss: 2.394641
Epoch: 76/200
Train loss: 2.379693
Epoch: 77/200
Train loss: 2.337459
Epoch: 78/200
Train loss: 2.247486
Epoch: 79/200
Train loss: 2.098195
Epoch: 80/200
Train loss: 2.271543
Epoch: 81/200
Train loss: 2.276230
Epoch: 82/200
Train loss: 2.261832
Epoch: 83/200
Train loss: 2.227964
Epoch: 84/200
Train loss: 2.207833
Epoch: 85/200
Train loss: 2.343987
Epoch: 86/200
Train loss: 2.284241
Epoch: 87/200
Train loss: 2.139038
Epoch: 88/200
Train loss: 2.337243
Epoch: 89/200
Train loss: 2.351764
Epoch: 90/200
Train loss: 2.211105
Epoch: 91/200
Train loss: 2.336034
Epoch: 92/200
Train loss: 2.295158
Epoch: 93/200
Train loss: 2.341929
Epoch: 94/200
Train loss: 2.253323
Epoch: 95/200
Train loss: 2.309808
Epoch: 96/200
Train loss: 2.319068
Epoch: 97/200
Train loss: 2.274178
Epoch: 98/200
Train loss: 2.260880
Epoch: 99/200
Train loss: 2.180128
Epoch: 100/200
Train loss: 2.243760
Epoch: 101/200
Train loss: 2.281588
Epoch: 102/200
Train loss: 2.063913
Time needed: 167.8951313495636 for validation audios
0.7982761549233907
Model improve: 0.0000 -> 0.7983
Epoch: 103/200
Train loss: 2.131873
Date :05/12/2023, 01:53:45
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Date :05/12/2023, 01:54:14
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 31.350885
Epoch: 2/200
Train loss: 5.359069
Epoch: 3/200
Train loss: 4.913361
Epoch: 4/200
Train loss: 4.689944
Epoch: 5/200
Train loss: 4.393591
Epoch: 6/200
Train loss: 4.193867
Epoch: 7/200
Train loss: 3.908862
Epoch: 8/200
Train loss: 3.897296
Epoch: 9/200
Train loss: 3.760945
Epoch: 10/200
Train loss: 3.706782
Epoch: 11/200
Train loss: 3.631482
Epoch: 12/200
Train loss: 3.522286
Epoch: 13/200
Train loss: 3.465756
Epoch: 14/200
Train loss: 3.359076
Epoch: 15/200
Train loss: 3.388251
Epoch: 16/200
Train loss: 3.328024
Epoch: 17/200
Train loss: 3.197498
Epoch: 18/200
Train loss: 3.087131
Epoch: 19/200
Train loss: 3.320143
Epoch: 20/200
Train loss: 3.192721
Epoch: 21/200
Train loss: 3.037895
Epoch: 22/200
Train loss: 2.926299
Epoch: 23/200
Train loss: 2.959046
Epoch: 24/200
Train loss: 3.034313
Epoch: 25/200
Train loss: 3.001281
Epoch: 26/200
Train loss: 3.017307
Epoch: 27/200
Train loss: 2.957379
Epoch: 28/200
Train loss: 2.838106
Epoch: 29/200
Train loss: 2.869373
Epoch: 30/200
Train loss: 2.711286
Epoch: 31/200
Train loss: 2.865272
Epoch: 32/200
Train loss: 2.901442
Epoch: 33/200
Train loss: 2.689252
Epoch: 34/200
Train loss: 2.799401
Epoch: 35/200
Train loss: 2.789611
Epoch: 36/200
Train loss: 2.805374
Epoch: 37/200
Train loss: 2.688833
Epoch: 38/200
Train loss: 2.925707
Epoch: 39/200
Train loss: 2.628524
Epoch: 40/200
Train loss: 2.749847
Epoch: 41/200
Train loss: 2.670012
Epoch: 42/200
Train loss: 2.654139
Epoch: 43/200
Train loss: 2.766720
Epoch: 44/200
Train loss: 2.741322
Epoch: 45/200
Train loss: 2.746609
Epoch: 46/200
Train loss: 2.477977
Epoch: 47/200
Train loss: 2.689773
Epoch: 48/200
Train loss: 2.446433
Epoch: 49/200
Train loss: 2.405014
Epoch: 50/200
Train loss: 2.490850
Epoch: 51/200
Train loss: 2.542324
Epoch: 52/200
Train loss: 2.538222
Epoch: 53/200
Train loss: 2.446344
Epoch: 54/200
Train loss: 2.552819
Epoch: 55/200
Train loss: 2.532101
Epoch: 56/200
Train loss: 2.506453
Epoch: 57/200
Train loss: 2.542102
Epoch: 58/200
Train loss: 2.495980
Epoch: 59/200
Train loss: 2.522004
Epoch: 60/200
Train loss: 2.521592
Epoch: 61/200
Train loss: 2.433071
Epoch: 62/200
Train loss: 2.445036
Epoch: 63/200
Train loss: 2.459205
Epoch: 64/200
Train loss: 2.474389
Epoch: 65/200
Train loss: 2.534427
Epoch: 66/200
Train loss: 2.396372
Epoch: 67/200
Train loss: 2.475129
Epoch: 68/200
Train loss: 2.459694
Epoch: 69/200
Train loss: 2.637952
Epoch: 70/200
Train loss: 2.441537
Epoch: 71/200
Train loss: 2.301952
Epoch: 72/200
Train loss: 2.314064
Epoch: 73/200
Train loss: 2.434367
Epoch: 74/200
Train loss: 2.423876
Epoch: 75/200
Train loss: 2.461727
Epoch: 76/200
Train loss: 2.445193
Epoch: 77/200
Train loss: 2.408983
Epoch: 78/200
Train loss: 2.314789
Epoch: 79/200
Train loss: 2.166795
Epoch: 80/200
Train loss: 2.340940
Epoch: 81/200
Train loss: 2.339377
Epoch: 82/200
Train loss: 2.327052
Epoch: 83/200
Train loss: 2.292898
Epoch: 84/200
Train loss: 2.274900
Epoch: 85/200
Train loss: 2.406712
Epoch: 86/200
Train loss: 2.353657
Epoch: 87/200
Train loss: 2.201977
Epoch: 88/200
Train loss: 2.404217
Epoch: 89/200
Train loss: 2.417928
Epoch: 90/200
Train loss: 2.274529
Epoch: 91/200
Train loss: 2.397241
Epoch: 92/200
Train loss: 2.366135
Epoch: 93/200
Train loss: 2.407670
Epoch: 94/200
Train loss: 2.319363
Epoch: 95/200
Train loss: 2.376894
Epoch: 96/200
Train loss: 2.385348
Epoch: 97/200
Train loss: 2.342248
Epoch: 98/200
Train loss: 2.326203
Epoch: 99/200
Train loss: 2.243016
Epoch: 100/200
Train loss: 2.311076
Epoch: 101/200
Train loss: 2.348428
Epoch: 102/200
Train loss: 2.128993
Time needed: 169.43884754180908 for validation audios
0.7990543625377494
Model improve: 0.0000 -> 0.7991
Epoch: 103/200
Train loss: 2.198364
Date :05/12/2023, 04:08:58
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b2
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Seed: 1
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 36.825982
Epoch: 2/200
Train loss: 5.524977
Epoch: 3/200
Train loss: 5.028122
Epoch: 4/200
Train loss: 4.777561
Epoch: 5/200
Train loss: 4.473758
Epoch: 6/200
Train loss: 4.225348
Epoch: 7/200
Train loss: 3.950628
Epoch: 8/200
Train loss: 3.916584
Epoch: 9/200
Train loss: 3.742903
Epoch: 10/200
Train loss: 3.691855
Epoch: 11/200
Train loss: 3.598181
Epoch: 12/200
Train loss: 3.508168
Epoch: 13/200
Train loss: 3.462637
Epoch: 14/200
Train loss: 3.332078
Epoch: 15/200
Train loss: 3.351955
Epoch: 16/200
Train loss: 3.309003
Epoch: 17/200
Train loss: 3.170147
Epoch: 18/200
Train loss: 3.076011
Epoch: 19/200
Train loss: 3.276189
Epoch: 20/200
Train loss: 3.160536
Epoch: 21/200
Train loss: 3.020051
Epoch: 22/200
Train loss: 2.879224
Epoch: 23/200
Train loss: 2.930209
Epoch: 24/200
Train loss: 3.008783
Epoch: 25/200
Train loss: 2.976407
Epoch: 26/200
Train loss: 2.974330
Epoch: 27/200
Train loss: 2.948103
Epoch: 28/200
Train loss: 2.795101
Epoch: 29/200
Train loss: 2.818166
Epoch: 30/200
Train loss: 2.664255
Epoch: 31/200
Train loss: 2.795338
Epoch: 32/200
Train loss: 2.860751
Epoch: 33/200
Train loss: 2.663857
Epoch: 34/200
Train loss: 2.764902
Epoch: 35/200
Train loss: 2.734363
Epoch: 36/200
Train loss: 2.785790
Epoch: 37/200
Train loss: 2.637676
Epoch: 38/200
Train loss: 2.903292
Epoch: 39/200
Train loss: 2.555230
Epoch: 40/200
Train loss: 2.708874
Epoch: 41/200
Train loss: 2.646388
Epoch: 42/200
Train loss: 2.617825
Epoch: 43/200
Train loss: 2.717272
Epoch: 44/200
Train loss: 2.689664
Epoch: 45/200
Train loss: 2.695817
Epoch: 46/200
Train loss: 2.457069
Epoch: 47/200
Train loss: 2.651472
Epoch: 48/200
Train loss: 2.400615
Epoch: 49/200
Train loss: 2.375580
Epoch: 50/200
Train loss: 2.453216
Epoch: 51/200
Train loss: 2.509719
Epoch: 52/200
Train loss: 2.481869
Epoch: 53/200
Train loss: 2.415933
Epoch: 54/200
Train loss: 2.498448
Epoch: 55/200
Train loss: 2.476722
Epoch: 56/200
Train loss: 2.459743
Epoch: 57/200
Train loss: 2.470939
Epoch: 58/200
Train loss: 2.475601
Epoch: 59/200
Train loss: 2.454750
Epoch: 60/200
Train loss: 2.490574
Epoch: 61/200
Train loss: 2.376372
Epoch: 62/200
Train loss: 2.388053
Epoch: 63/200
Train loss: 2.414332
Epoch: 64/200
Train loss: 2.414801
Epoch: 65/200
Train loss: 2.471708
Epoch: 66/200
Train loss: 2.346078
Epoch: 67/200
Train loss: 2.451571
Epoch: 68/200
Train loss: 2.397516
Epoch: 69/200
Train loss: 2.581032
Epoch: 70/200
Train loss: 2.366309
Epoch: 71/200
Train loss: 2.240331
Epoch: 72/200
Train loss: 2.273653
Epoch: 73/200
Train loss: 2.406141
Epoch: 74/200
Train loss: 2.384072
Epoch: 75/200
Train loss: 2.431840
Epoch: 76/200
Train loss: 2.388465
Epoch: 77/200
Train loss: 2.363235
Epoch: 78/200
Train loss: 2.281583
Epoch: 79/200
Train loss: 2.116381
Epoch: 80/200
Train loss: 2.287414
Epoch: 81/200
Train loss: 2.290200
Epoch: 82/200
Train loss: 2.281751
Epoch: 83/200
Train loss: 2.246760
Epoch: 84/200
Train loss: 2.218982
Epoch: 85/200
Train loss: 2.356891
Epoch: 86/200
Train loss: 2.289771
Epoch: 87/200
Train loss: 2.179309
Epoch: 88/200
Train loss: 2.355110
Epoch: 89/200
Train loss: 2.397763
Epoch: 90/200
Train loss: 2.236241
Epoch: 91/200
Train loss: 2.349545
Epoch: 92/200
Train loss: 2.320762
Epoch: 93/200
Train loss: 2.354287
Epoch: 94/200
Train loss: 2.285283
Epoch: 95/200
Train loss: 2.314431
Epoch: 96/200
Train loss: 2.374693
Epoch: 97/200
Train loss: 2.311109
Epoch: 98/200
Train loss: 2.277834
Epoch: 99/200
Train loss: 2.196809
Epoch: 100/200
Train loss: 2.260893
Epoch: 101/200
Train loss: 2.320793
Epoch: 102/200
Train loss: 2.099805
Time needed: 172.25403904914856 for validation audios
0.8044413481815209
Date :05/14/2023, 06:46:40
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 1
13839
Date :05/14/2023, 06:47:15
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Date :05/14/2023, 06:47:39
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Date :05/14/2023, 06:48:05
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 768
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 40.7036
Epoch: 2/200
Train loss: 5.6193
Epoch: 3/200
Train loss: 5.1832
Epoch: 4/200
Train loss: 4.9184
Epoch: 5/200
Train loss: 4.6125
Epoch: 6/200
Train loss: 4.3615
Epoch: 7/200
Train loss: 4.0643
Epoch: 8/200
Train loss: 4.0105
Epoch: 9/200
Train loss: 3.8391
Epoch: 10/200
Train loss: 3.7650
Epoch: 11/200
Train loss: 3.6919
Epoch: 12/200
Train loss: 3.5729
Epoch: 13/200
Train loss: 3.5042
Epoch: 14/200
Train loss: 3.3938
Epoch: 15/200
Train loss: 3.4064
Epoch: 16/200
Train loss: 3.3696
Epoch: 17/200
Train loss: 3.2273
Epoch: 18/200
Train loss: 3.1247
Epoch: 19/200
Train loss: 3.3296
Epoch: 20/200
Train loss: 3.2070
Epoch: 21/200
Train loss: 3.0580
Epoch: 22/200
Train loss: 2.9312
Epoch: 23/200
Train loss: 2.9588
Epoch: 24/200
Train loss: 3.0341
Epoch: 25/200
Train loss: 2.9819
Epoch: 26/200
Train loss: 3.0090
Epoch: 27/200
Train loss: 2.9553
Epoch: 28/200
Train loss: 2.8260
Epoch: 29/200
Train loss: 2.8562
Epoch: 30/200
Train loss: 2.6986
Epoch: 31/200
Train loss: 2.8277
Epoch: 32/200
Train loss: 2.8966
Epoch: 33/200
Train loss: 2.6653
Epoch: 34/200
Train loss: 2.7599
Epoch: 35/200
Train loss: 2.7554
Epoch: 36/200
Train loss: 2.7902
Epoch: 37/200
Train loss: 2.6835
Epoch: 38/200
Train loss: 2.8985
Epoch: 39/200
Train loss: 2.6063
Epoch: 40/200
Train loss: 2.6950
Epoch: 41/200
Train loss: 2.6494
Epoch: 42/200
Train loss: 2.6263
Epoch: 43/200
Train loss: 2.7183
Epoch: 44/200
Train loss: 2.7114
Epoch: 45/200
Train loss: 2.7144
Epoch: 46/200
Train loss: 2.4567
Epoch: 47/200
Train loss: 2.6619
Epoch: 48/200
Train loss: 2.4238
Epoch: 49/200
Train loss: 2.4016
Epoch: 50/200
Train loss: 2.4658
Epoch: 51/200
Train loss: 2.5277
Epoch: 52/200
Train loss: 2.5070
Epoch: 53/200
Train loss: 2.4250
Epoch: 54/200
Train loss: 2.5136
Epoch: 55/200
Train loss: 2.5090
Epoch: 56/200
Train loss: 2.4594
Epoch: 57/200
Train loss: 2.4903
Epoch: 58/200
Train loss: 2.4611
Epoch: 59/200
Train loss: 2.4792
Epoch: 60/200
Train loss: 2.4916
Epoch: 61/200
Train loss: 2.3872
Epoch: 62/200
Train loss: 2.4155
Epoch: 63/200
Train loss: 2.4260
Epoch: 64/200
Train loss: 2.4463
Epoch: 65/200
Train loss: 2.5000
Epoch: 66/200
Train loss: 2.3655
Epoch: 67/200
Train loss: 2.4601
Epoch: 68/200
Train loss: 2.4306
Epoch: 69/200
Train loss: 2.5852
Epoch: 70/200
Train loss: 2.3887
Epoch: 71/200
Train loss: 2.2543
Epoch: 72/200
Train loss: 2.2781
Epoch: 73/200
Train loss: 2.3833
Epoch: 74/200
Train loss: 2.3912
Epoch: 75/200
Train loss: 2.4357
Epoch: 76/200
Train loss: 2.4051
Epoch: 77/200
Train loss: 2.3735
Epoch: 78/200
Train loss: 2.2797
Epoch: 79/200
Train loss: 2.1323
Epoch: 80/200
Train loss: 2.2987
Epoch: 81/200
Train loss: 2.3071
Epoch: 82/200
Train loss: 2.2917
Epoch: 83/200
Train loss: 2.2669
Epoch: 84/200
Train loss: 2.2336
Epoch: 85/200
Train loss: 2.3747
Epoch: 86/200
Train loss: 2.3065
Epoch: 87/200
Train loss: 2.1682
Epoch: 88/200
Train loss: 2.3662
Epoch: 89/200
Train loss: 2.4024
Epoch: 90/200
Train loss: 2.2564
Epoch: 91/200
Train loss: 2.3552
Epoch: 92/200
Train loss: 2.3322
Epoch: 93/200
Train loss: 2.3638
Epoch: 94/200
Train loss: 2.2855
Epoch: 95/200
Train loss: 2.3178
Epoch: 96/200
Train loss: 2.3421
Epoch: 97/200
Train loss: 2.3047
Epoch: 98/200
Train loss: 2.2771
Epoch: 99/200
Train loss: 2.2182
Epoch: 100/200
Train loss: 2.2730
Epoch: 101/200
Train loss: 2.3110
Epoch: 102/200
Train loss: 2.1062
Date :05/14/2023, 09:22:15
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 768
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 40.7069
Epoch: 2/200
Train loss: 5.6197
Epoch: 3/200
Train loss: 5.1836
Epoch: 4/200
Train loss: 4.9185
Epoch: 5/200
Train loss: 4.6126
Epoch: 6/200
Train loss: 4.3615
Epoch: 7/200
Train loss: 4.0641
Epoch: 8/200
Train loss: 4.0103
Epoch: 9/200
Train loss: 3.8390
Epoch: 10/200
Train loss: 3.7651
Epoch: 11/200
Train loss: 3.6922
Epoch: 12/200
Train loss: 3.5728
Epoch: 13/200
Train loss: 3.5043
Epoch: 14/200
Train loss: 3.3938
Epoch: 15/200
Train loss: 3.4063
Epoch: 16/200
Train loss: 3.3687
Epoch: 17/200
Train loss: 3.2268
Epoch: 18/200
Train loss: 3.1249
Epoch: 19/200
Train loss: 3.3297
Epoch: 20/200
Train loss: 3.2064
Epoch: 21/200
Train loss: 3.0579
Epoch: 22/200
Train loss: 2.9308
Epoch: 23/200
Train loss: 2.9584
Epoch: 24/200
Train loss: 3.0338
Epoch: 25/200
Train loss: 2.9824
Epoch: 26/200
Train loss: 3.0087
Epoch: 27/200
Train loss: 2.9548
Epoch: 28/200
Train loss: 2.8253
Epoch: 29/200
Train loss: 2.8555
Epoch: 30/200
Train loss: 2.6984
Epoch: 31/200
Train loss: 2.8272
Epoch: 32/200
Train loss: 2.8957
Epoch: 33/200
Train loss: 2.6654
Epoch: 34/200
Train loss: 2.7594
Epoch: 35/200
Train loss: 2.7553
Epoch: 36/200
Train loss: 2.7902
Epoch: 37/200
Train loss: 2.6840
Epoch: 38/200
Train loss: 2.8982
Epoch: 39/200
Train loss: 2.6053
Epoch: 40/200
Train loss: 2.6947
Epoch: 41/200
Train loss: 2.6492
Epoch: 42/200
Train loss: 2.6263
Epoch: 43/200
Train loss: 2.7175
Epoch: 44/200
Train loss: 2.7113
Epoch: 45/200
Train loss: 2.7148
Epoch: 46/200
Train loss: 2.4573
Epoch: 47/200
Train loss: 2.6623
Epoch: 48/200
Train loss: 2.4238
Epoch: 49/200
Train loss: 2.4013
Epoch: 50/200
Train loss: 2.4657
Epoch: 51/200
Train loss: 2.5273
Epoch: 52/200
Train loss: 2.5074
Epoch: 53/200
Train loss: 2.4257
Epoch: 54/200
Train loss: 2.5136
Epoch: 55/200
Train loss: 2.5091
Epoch: 56/200
Train loss: 2.4598
Epoch: 57/200
Train loss: 2.4904
Epoch: 58/200
Train loss: 2.4613
Epoch: 59/200
Train loss: 2.4785
Epoch: 60/200
Train loss: 2.4908
Epoch: 61/200
Train loss: 2.3868
Epoch: 62/200
Train loss: 2.4153
Epoch: 63/200
Train loss: 2.4262
Epoch: 64/200
Train loss: 2.4468
Epoch: 65/200
Train loss: 2.5008
Epoch: 66/200
Train loss: 2.3652
Epoch: 67/200
Train loss: 2.4592
Epoch: 68/200
Train loss: 2.4306
Epoch: 69/200
Train loss: 2.5854
Epoch: 70/200
Train loss: 2.3884
Epoch: 71/200
Train loss: 2.2545
Epoch: 72/200
Train loss: 2.2787
Epoch: 73/200
Train loss: 2.3834
Epoch: 74/200
Train loss: 2.3905
Epoch: 75/200
Train loss: 2.4359
Epoch: 76/200
Train loss: 2.4057
Epoch: 77/200
Train loss: 2.3734
Epoch: 78/200
Train loss: 2.2796
Epoch: 79/200
Train loss: 2.1323
Epoch: 80/200
Train loss: 2.2987
Epoch: 81/200
Train loss: 2.3070
Epoch: 82/200
Train loss: 2.2911
Epoch: 83/200
Train loss: 2.2668
Epoch: 84/200
Train loss: 2.2337
Epoch: 85/200
Train loss: 2.3754
Epoch: 86/200
Train loss: 2.3059
Epoch: 87/200
Train loss: 2.1680
Epoch: 88/200
Train loss: 2.3667
Epoch: 89/200
Train loss: 2.4022
Epoch: 90/200
Train loss: 2.2568
Epoch: 91/200
Train loss: 2.3554
Epoch: 92/200
Train loss: 2.3322
Epoch: 93/200
Train loss: 2.3629
Epoch: 94/200
Train loss: 2.2853
Epoch: 95/200
Train loss: 2.3181
Epoch: 96/200
Train loss: 2.3420
Epoch: 97/200
Train loss: 2.3046
Epoch: 98/200
Train loss: 2.2781
Epoch: 99/200
Train loss: 2.2180
Epoch: 100/200
Train loss: 2.2729
Epoch: 101/200
Train loss: 2.3101
Epoch: 102/200
Train loss: 2.1059
0.8043851421025054
Model improve: 0.0000 -> 0.8044
Epoch: 103/200
Train loss: 2.1642
0.8016976710432719
Epoch: 104/200
Date :05/14/2023, 11:09:51
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 768
epochwarmup: 15
totalepoch: 200
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 179.0363
Epoch: 2/200
Train loss: 103.5817
Epoch: 3/200
Date :05/14/2023, 11:12:08
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 768
epochwarmup: 15
totalepoch: 200
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 179.0134
Epoch: 2/200
Date :05/14/2023, 11:13:52
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 768
epochwarmup: 10
totalepoch: 200
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 175.7260
Epoch: 2/200
Train loss: 65.5320
Epoch: 3/200
Train loss: 7.0659
Epoch: 4/200
Train loss: 5.8992
Epoch: 5/200
Train loss: 5.5938
Epoch: 6/200
Train loss: 5.3409
Epoch: 7/200
Train loss: 5.0463
Epoch: 8/200
Train loss: 4.8603
Epoch: 9/200
Train loss: 4.6352
Epoch: 10/200
Train loss: 4.4663
Epoch: 11/200
Train loss: 4.2819
Epoch: 12/200
Train loss: 4.0930
Epoch: 13/200
Train loss: 3.9778
Epoch: 14/200
Train loss: 3.8325
Epoch: 15/200
Train loss: 3.7938
Epoch: 16/200
Train loss: 3.7273
Epoch: 17/200
Train loss: 3.5547
Epoch: 18/200
Train loss: 3.4116
Epoch: 19/200
Train loss: 3.5861
Epoch: 20/200
Train loss: 3.4396
Epoch: 21/200
Train loss: 3.3131
Epoch: 22/200
Train loss: 3.1583
Epoch: 23/200
Train loss: 3.1738
Epoch: 24/200
Train loss: 3.2405
Epoch: 25/200
Train loss: 3.1782
Epoch: 26/200
Train loss: 3.1562
Epoch: 27/200
Train loss: 3.0969
Epoch: 28/200
Train loss: 2.9703
Epoch: 29/200
Train loss: 2.9601
Epoch: 30/200
Train loss: 2.8235
Epoch: 31/200
Train loss: 2.9503
Epoch: 32/200
Train loss: 2.9797
Epoch: 33/200
Train loss: 2.7604
Epoch: 34/200
Train loss: 2.8769
Epoch: 35/200
Train loss: 2.8371
Epoch: 36/200
Train loss: 2.8802
Epoch: 37/200
Train loss: 2.7379
Epoch: 38/200
Train loss: 2.9710
Epoch: 39/200
Train loss: 2.6450
Epoch: 40/200
Train loss: 2.7642
Epoch: 41/200
Train loss: 2.6983
Epoch: 42/200
Train loss: 2.6790
Epoch: 43/200
Train loss: 2.7694
Epoch: 44/200
Train loss: 2.7639
Epoch: 45/200
Train loss: 2.7519
Epoch: 46/200
Train loss: 2.4905
Epoch: 47/200
Train loss: 2.7055
Epoch: 48/200
Train loss: 2.4517
Epoch: 49/200
Train loss: 2.4352
Epoch: 50/200
Train loss: 2.5048
Epoch: 51/200
Train loss: 2.5530
Epoch: 52/200
Train loss: 2.5303
Epoch: 53/200
Train loss: 2.4556
Epoch: 54/200
Train loss: 2.5579
Epoch: 55/200
Train loss: 2.5198
Epoch: 56/200
Train loss: 2.4827
Epoch: 57/200
Train loss: 2.5000
Epoch: 58/200
Train loss: 2.4800
Epoch: 59/200
Train loss: 2.4849
Epoch: 60/200
Train loss: 2.4978
Epoch: 61/200
Train loss: 2.3883
Epoch: 62/200
Train loss: 2.4010
Epoch: 63/200
Train loss: 2.4341
Epoch: 64/200
Train loss: 2.4463
Epoch: 65/200
Train loss: 2.4879
Epoch: 66/200
Train loss: 2.3585
Epoch: 67/200
Train loss: 2.4451
Epoch: 68/200
Train loss: 2.4032
Epoch: 69/200
Train loss: 2.5653
Epoch: 70/200
Train loss: 2.3815
Epoch: 71/200
Train loss: 2.2555
Epoch: 72/200
Train loss: 2.2561
Epoch: 73/200
Train loss: 2.3739
Epoch: 74/200
Train loss: 2.3562
Epoch: 75/200
Train loss: 2.4274
Epoch: 76/200
Train loss: 2.3902
Epoch: 77/200
Train loss: 2.3473
Epoch: 78/200
Train loss: 2.2611
Epoch: 79/200
Train loss: 2.1127
Epoch: 80/200
Train loss: 2.2826
Epoch: 81/200
Train loss: 2.2892
Epoch: 82/200
Train loss: 2.2753
Epoch: 83/200
Train loss: 2.2333
Epoch: 84/200
Train loss: 2.1997
Epoch: 85/200
Train loss: 2.3399
Epoch: 86/200
Train loss: 2.2748
Epoch: 87/200
Train loss: 2.1544
Epoch: 88/200
Train loss: 2.3347
Epoch: 89/200
Train loss: 2.3618
Epoch: 90/200
Train loss: 2.2210
Epoch: 91/200
Train loss: 2.3419
Epoch: 92/200
Train loss: 2.2955
Epoch: 93/200
Train loss: 2.3233
Epoch: 94/200
Train loss: 2.2415
Epoch: 95/200
Train loss: 2.2825
Epoch: 96/200
Train loss: 2.3106
Epoch: 97/200
Train loss: 2.2726
Epoch: 98/200
Train loss: 2.2501
Epoch: 99/200
Train loss: 2.1687
Epoch: 100/200
Train loss: 2.2410
Epoch: 101/200
Train loss: 2.2729
Epoch: 102/200
Train loss: 2.0616
0.7978674551766705
Model improve: 0.0000 -> 0.7979
Epoch: 103/200
Train loss: 2.1194
0.7947838122659869
Epoch: 104/200
Date :05/14/2023, 12:56:02
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 128
validbs: 768
epochwarmup: 0
totalepoch: 200
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 5.0
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Date :05/14/2023, 12:56:43
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 25
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 5.0
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Train loss: 12.9254
Epoch: 2/25
Train loss: 4.7660
Epoch: 3/25
Date :05/14/2023, 13:00:49
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 25
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 5.0
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Train loss: 11.0302
Val cmap: 0.763951
Valid loss: 3.655878
Epoch: 2/25
Train loss: 4.8132
Val cmap: 0.842062
Valid loss: 2.491869
Epoch: 3/25
Train loss: 4.2635
Val cmap: 0.867676
Valid loss: 2.135608
Epoch: 4/25
Date :05/14/2023, 13:09:10
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 25
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Train loss: 10.7926
Val cmap: 0.793270
Valid loss: 3.109699
Epoch: 2/25
Train loss: 3.8462
Val cmap: 0.858635
Valid loss: 2.133708
Epoch: 3/25
Train loss: 3.2447
Val cmap: 0.880141
Valid loss: 1.843336
Epoch: 4/25
Train loss: 2.8324
Val cmap: 0.892432
Valid loss: 1.665441
Epoch: 5/25
Train loss: 2.7370
Val cmap: 0.899361
Valid loss: 1.558777
Epoch: 6/25
Train loss: 2.5302
Val cmap: 0.903005
Valid loss: 1.550688
Epoch: 7/25
Train loss: 2.3571
Val cmap: 0.908256
Valid loss: 1.475672
Epoch: 8/25
Train loss: 2.3239
Val cmap: 0.910960
Valid loss: 1.435063
Epoch: 9/25
Train loss: 2.1969
Val cmap: 0.912033
Valid loss: 1.341629
Epoch: 10/25
Train loss: 2.2692
Val cmap: 0.911893
Valid loss: 1.379186
Epoch: 11/25
Train loss: 2.1153
Val cmap: 0.912388
Valid loss: 1.341217
Epoch: 12/25
Train loss: 2.0422
Val cmap: 0.913806
Valid loss: 1.380671
Epoch: 13/25
Train loss: 2.0016
Val cmap: 0.914716
Valid loss: 1.309833
Epoch: 14/25
Train loss: 1.9868
Val cmap: 0.914398
Valid loss: 1.326920
Epoch: 15/25
Train loss: 1.9115
Val cmap: 0.916073
Valid loss: 1.312074
Epoch: 16/25
Train loss: 1.8901
Val cmap: 0.917216
Valid loss: 1.244246
Epoch: 17/25
Train loss: 1.8063
Val cmap: 0.915428
Valid loss: 1.348237
0.7981713988640151
Model improve: 0.0000 -> 0.7982
Epoch: 18/25
Train loss: 1.8466
Val cmap: 0.916522
Valid loss: 1.339618
0.7990478601538678
Model improve: 0.7982 -> 0.7990
Epoch: 19/25
Train loss: 1.8524
Val cmap: 0.916288
Valid loss: 1.269353
0.8006803687553133
Model improve: 0.7990 -> 0.8007
Epoch: 20/25
Train loss: 1.8029
Val cmap: 0.918063
Valid loss: 1.213323
0.8014290077865579
Model improve: 0.8007 -> 0.8014
Epoch: 21/25
Train loss: 1.8554
Val cmap: 0.917158
Valid loss: 1.278194
0.8008713255901102
Epoch: 22/25
Train loss: 1.8271
Val cmap: 0.917037
Valid loss: 1.246266
0.8018156425314583
Model improve: 0.8014 -> 0.8018
Epoch: 23/25
Train loss: 1.7818
Val cmap: 0.917176
Valid loss: 1.276462
0.8008682251461036
Epoch: 24/25
Train loss: 1.8659
Val cmap: 0.917046
Valid loss: 1.261307
0.8006995681062521
Epoch: 25/25
Train loss: 1.7972
Val cmap: 0.916320
Valid loss: 1.308937
0.8005232455275154
Date :05/14/2023, 14:43:55
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 384
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Train loss: 10.6871
Val cmap: 0.780040
Valid loss: 3.113639
Epoch: 2/25
Train loss: 3.8093
Val cmap: 0.854766
Valid loss: 2.105008
Epoch: 3/25
Date :05/14/2023, 14:49:07
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Train loss: 10.6936
Val cmap: 0.779135
Valid loss: 3.120118
Epoch: 2/25
Train loss: 3.8109
Val cmap: 0.855222
Valid loss: 2.109168
Epoch: 3/25
Date :05/14/2023, 14:53:55
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Train loss: 10.8740
Val cmap: 0.780828
Valid loss: 3.092057
Epoch: 2/25
Train loss: 3.7885
Val cmap: 0.856185
Valid loss: 2.111990
Epoch: 3/25
Train loss: 3.0231
Val cmap: 0.880637
Valid loss: 1.861346
Epoch: 4/25
Train loss: 2.7486
Val cmap: 0.890872
Valid loss: 1.661854
Epoch: 5/25
Train loss: 2.6516
Val cmap: 0.896671
Valid loss: 1.638244
Epoch: 6/25
Date :05/14/2023, 15:05:58
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Train loss: 10.2864
Val cmap: 0.791241
Valid loss: 2.977287
Epoch: 2/25
Train loss: 3.7249
Val cmap: 0.864715
Valid loss: 2.025463
Epoch: 3/25
Train loss: 2.9535
Val cmap: 0.882320
Valid loss: 1.870964
Epoch: 4/25
Train loss: 2.7001
Val cmap: 0.894727
Valid loss: 1.638168
Epoch: 5/25
Train loss: 2.6102
Val cmap: 0.899269
Valid loss: 1.616904
Epoch: 6/25
Train loss: 2.4460
Val cmap: 0.905241
Valid loss: 1.508979
Epoch: 7/25
Train loss: 2.2210
Val cmap: 0.906741
Valid loss: 1.483568
Epoch: 8/25
Train loss: 2.1855
Val cmap: 0.907452
Valid loss: 1.433059
Epoch: 9/25
Train loss: 2.2869
Val cmap: 0.909505
Valid loss: 1.410400
Epoch: 10/25
Train loss: 2.0565
Val cmap: 0.910050
Valid loss: 1.351652
Epoch: 11/25
Train loss: 2.0424
Val cmap: 0.910410
Valid loss: 1.394862
Epoch: 12/25
Train loss: 1.9887
Val cmap: 0.913899
Valid loss: 1.353654
Epoch: 13/25
Train loss: 1.8944
Val cmap: 0.915034
Valid loss: 1.331754
Epoch: 14/25
Train loss: 1.8318
Val cmap: 0.917511
Valid loss: 1.286902
Epoch: 15/25
Train loss: 1.8980
Val cmap: 0.914596
Valid loss: 1.287792
Epoch: 16/25
Train loss: 1.8210
Val cmap: 0.916731
Valid loss: 1.302061
Epoch: 17/25
Train loss: 1.6919
Val cmap: 0.916461
Valid loss: 1.273197
0.7990002784644957
Model improve: 0.0000 -> 0.7990
Epoch: 18/25
Train loss: 1.7495
Val cmap: 0.917204
Valid loss: 1.260163
0.7976128756169102
Epoch: 19/25
Train loss: 1.8014
Val cmap: 0.917174
Valid loss: 1.264426
0.7998906825477256
Model improve: 0.7990 -> 0.7999
Epoch: 20/25
Date :05/14/2023, 15:49:53
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 192
validbs: 768
epochwarmup: 0
totalepoch: 300
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/300
Train loss: 19.9011
Epoch: 2/300
Date :05/14/2023, 15:52:24
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 192
validbs: 768
epochwarmup: 0
totalepoch: 500
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/500
Train loss: 19.9015
Epoch: 2/500
Train loss: 5.6224
Epoch: 3/500
Train loss: 4.7301
Epoch: 4/500
Train loss: 4.2414
Epoch: 5/500
Train loss: 3.8551
Epoch: 6/500
Train loss: 3.4718
Epoch: 7/500
Train loss: 3.3103
Epoch: 8/500
Train loss: 3.1531
Epoch: 9/500
Train loss: 3.0107
Epoch: 10/500
Train loss: 2.8992
Epoch: 11/500
Train loss: 3.0314
Epoch: 12/500
Train loss: 2.8306
Epoch: 13/500
Train loss: 2.9068
Epoch: 14/500
Train loss: 2.7985
Epoch: 15/500
Train loss: 2.7061
Epoch: 16/500
Train loss: 2.7770
Epoch: 17/500
Train loss: 2.6620
Epoch: 18/500
Train loss: 2.6016
Epoch: 19/500
Train loss: 2.4968
Epoch: 20/500
Train loss: 2.7732
Epoch: 21/500
Train loss: 2.7035
Epoch: 22/500
Train loss: 2.5371
Epoch: 23/500
Train loss: 2.3580
Epoch: 24/500
Train loss: 2.4369
Epoch: 25/500
Train loss: 2.5397
Epoch: 26/500
Train loss: 2.6874
Epoch: 27/500
Train loss: 2.3514
Epoch: 28/500
Train loss: 2.5086
Epoch: 29/500
Train loss: 2.3906
Epoch: 30/500
Train loss: 2.3286
Epoch: 31/500
Train loss: 2.4387
Epoch: 32/500
Train loss: 2.2357
Epoch: 33/500
Train loss: 2.4606
Epoch: 34/500
Train loss: 2.4407
Epoch: 35/500
Train loss: 2.2210
Epoch: 36/500
Train loss: 2.3988
Epoch: 37/500
Train loss: 2.3786
Epoch: 38/500
Train loss: 2.4120
Epoch: 39/500
Train loss: 2.2177
Epoch: 40/500
Train loss: 2.5218
Epoch: 41/500
Train loss: 2.3485
Epoch: 42/500
Train loss: 2.2682
Epoch: 43/500
Train loss: 2.3894
Epoch: 44/500
Train loss: 2.1728
Epoch: 45/500
Train loss: 2.4161
Epoch: 46/500
Train loss: 2.2487
Epoch: 47/500
Train loss: 2.4028
Epoch: 48/500
Train loss: 2.3117
Epoch: 49/500
Train loss: 2.1887
Epoch: 50/500
Train loss: 2.1966
Epoch: 51/500
Train loss: 2.1012
Epoch: 52/500
Train loss: 2.0797
Epoch: 53/500
Train loss: 2.1461
Epoch: 54/500
Train loss: 2.2022
Epoch: 55/500
Train loss: 2.2067
Epoch: 56/500
Train loss: 2.0859
Epoch: 57/500
Train loss: 2.1808
Epoch: 58/500
Train loss: 2.2787
Epoch: 59/500
Train loss: 2.0588
Epoch: 60/500
Train loss: 2.2040
Epoch: 61/500
Train loss: 2.1616
Epoch: 62/500
Train loss: 2.1853
Epoch: 63/500
Train loss: 2.1814
Epoch: 64/500
Train loss: 2.2235
Epoch: 65/500
Train loss: 2.0363
Epoch: 66/500
Train loss: 2.0937
Epoch: 67/500
Train loss: 2.2081
Epoch: 68/500
Train loss: 2.1006
Epoch: 69/500
Train loss: 2.2067
Epoch: 70/500
Train loss: 2.0341
Epoch: 71/500
Train loss: 2.1908
Epoch: 72/500
Train loss: 2.1592
Epoch: 73/500
Train loss: 2.2882
Epoch: 74/500
Train loss: 2.1278
Epoch: 75/500
Train loss: 1.9597
Epoch: 76/500
Train loss: 2.0397
Epoch: 77/500
Train loss: 2.0773
Epoch: 78/500
Train loss: 2.1663
Epoch: 79/500
Train loss: 2.2144
Epoch: 80/500
Train loss: 2.0108
Epoch: 81/500
Train loss: 2.1032
Epoch: 82/500
Train loss: 2.1255
Epoch: 83/500
Train loss: 1.9340
Epoch: 84/500
Train loss: 1.9909
Epoch: 85/500
Train loss: 2.0176
Epoch: 86/500
Train loss: 2.0797
Epoch: 87/500
Train loss: 1.9521
Epoch: 88/500
Train loss: 1.9941
Epoch: 89/500
Train loss: 2.0151
Epoch: 90/500
Train loss: 2.1147
Epoch: 91/500
Train loss: 2.0338
Epoch: 92/500
Train loss: 1.9506
Epoch: 93/500
Train loss: 2.1113
Epoch: 94/500
Train loss: 2.0837
Epoch: 95/500
Train loss: 2.0995
Epoch: 96/500
Train loss: 2.0653
Epoch: 97/500
Train loss: 2.0084
Epoch: 98/500
Train loss: 2.1438
Epoch: 99/500
Train loss: 1.9957
Epoch: 100/500
Train loss: 2.0831
Epoch: 101/500
Train loss: 2.0734
Epoch: 102/500
Train loss: 2.1767
Epoch: 103/500
Train loss: 1.9650
Epoch: 104/500
Train loss: 2.0332
Epoch: 105/500
Train loss: 1.9287
Epoch: 106/500
Train loss: 2.0838
Epoch: 107/500
Train loss: 2.0228
Epoch: 108/500
Train loss: 1.8456
Epoch: 109/500
Train loss: 1.8850
Epoch: 110/500
Train loss: 1.9859
Epoch: 111/500
Train loss: 1.9532
Epoch: 112/500
Train loss: 1.9482
Epoch: 113/500
Train loss: 1.9999
Epoch: 114/500
Train loss: 2.0221
Epoch: 115/500
Train loss: 2.0420
Epoch: 116/500
Train loss: 1.9206
Epoch: 117/500
Train loss: 2.0421
Epoch: 118/500
Train loss: 1.8821
Epoch: 119/500
Train loss: 1.9470
Epoch: 120/500
Train loss: 2.0228
Epoch: 121/500
Train loss: 2.0101
Epoch: 122/500
Train loss: 2.0073
Epoch: 123/500
Train loss: 1.8859
Epoch: 124/500
Train loss: 1.9749
Epoch: 125/500
Train loss: 1.9074
Epoch: 126/500
Train loss: 2.0132
Epoch: 127/500
Train loss: 1.9035
Epoch: 128/500
Train loss: 1.9395
Epoch: 129/500
Train loss: 1.9589
Epoch: 130/500
Train loss: 2.0421
Epoch: 131/500
Train loss: 2.1646
Epoch: 132/500
Train loss: 2.0340
Epoch: 133/500
Train loss: 1.8883
Epoch: 134/500
Train loss: 1.9426
Epoch: 135/500
Train loss: 1.9014
Epoch: 136/500
Train loss: 1.8667
Epoch: 137/500
Train loss: 1.7611
Epoch: 138/500
Train loss: 1.9306
Epoch: 139/500
Train loss: 1.8710
Epoch: 140/500
Train loss: 2.0294
Epoch: 141/500
Train loss: 1.8677
Epoch: 142/500
Train loss: 1.8924
Epoch: 143/500
Train loss: 1.8389
Epoch: 144/500
Train loss: 1.8568
Epoch: 145/500
Train loss: 1.9054
Epoch: 146/500
Train loss: 1.8417
Epoch: 147/500
Train loss: 1.9121
Epoch: 148/500
Train loss: 1.8337
Epoch: 149/500
Train loss: 1.9380
Epoch: 150/500
Train loss: 1.7638
Epoch: 151/500
Train loss: 1.9847
Epoch: 152/500
Train loss: 1.9584
Epoch: 153/500
Train loss: 1.8422
Epoch: 154/500
Train loss: 1.8942
Epoch: 155/500
Train loss: 1.8428
Epoch: 156/500
Train loss: 2.0127
Epoch: 157/500
Train loss: 1.9686
Epoch: 158/500
Train loss: 1.7748
Epoch: 159/500
Train loss: 1.7634
Epoch: 160/500
Train loss: 1.8366
Epoch: 161/500
Train loss: 1.7857
Epoch: 162/500
Train loss: 1.7532
Epoch: 163/500
Train loss: 1.8784
Epoch: 164/500
Train loss: 1.8741
Epoch: 165/500
Train loss: 1.8981
Epoch: 166/500
Train loss: 1.8460
Epoch: 167/500
Train loss: 1.7807
Epoch: 168/500
Train loss: 1.8273
Epoch: 169/500
Train loss: 1.8550
Epoch: 170/500
Train loss: 1.8850
Epoch: 171/500
Train loss: 1.8115
Epoch: 172/500
Train loss: 1.8388
Epoch: 173/500
Train loss: 1.7955
Epoch: 174/500
Train loss: 1.9940
Epoch: 175/500
Train loss: 1.8706
Epoch: 176/500
Train loss: 1.9148
Epoch: 177/500
Train loss: 1.9798
Epoch: 178/500
Train loss: 1.8340
Epoch: 179/500
Train loss: 1.8739
Epoch: 180/500
Train loss: 1.8065
Epoch: 181/500
Train loss: 1.8998
Epoch: 182/500
Train loss: 1.8293
Epoch: 183/500
Train loss: 1.8888
Epoch: 184/500
Train loss: 1.7765
Epoch: 185/500
Train loss: 1.8353
Epoch: 186/500
Train loss: 1.8778
Epoch: 187/500
Train loss: 1.6565
Epoch: 188/500
Train loss: 1.7775
Epoch: 189/500
Train loss: 1.7726
Epoch: 190/500
Train loss: 1.7668
Epoch: 191/500
Train loss: 1.8937
Epoch: 192/500
Train loss: 1.8086
Epoch: 193/500
Train loss: 1.6975
Epoch: 194/500
Train loss: 1.8030
Epoch: 195/500
Train loss: 1.9036
Epoch: 196/500
Train loss: 1.7593
Epoch: 197/500
Train loss: 1.9258
Epoch: 198/500
Train loss: 1.7415
Epoch: 199/500
Train loss: 1.9005
Epoch: 200/500
Train loss: 1.8695
Epoch: 201/500
Train loss: 1.8779
Epoch: 202/500
Train loss: 1.7733
Epoch: 203/500
Train loss: 1.8563
Epoch: 204/500
Train loss: 1.8855
Epoch: 205/500
Train loss: 1.8597
Epoch: 206/500
Train loss: 1.7718
Epoch: 207/500
Train loss: 1.8889
Epoch: 208/500
Train loss: 1.7691
Epoch: 209/500
Train loss: 1.7239
Epoch: 210/500
Train loss: 1.8045
Epoch: 211/500
Train loss: 1.8942
Epoch: 212/500
Train loss: 1.7494
Epoch: 213/500
Train loss: 1.8845
Epoch: 214/500
Train loss: 1.7758
Epoch: 215/500
Train loss: 1.8420
Epoch: 216/500
Train loss: 1.8534
Epoch: 217/500
Train loss: 1.7770
Epoch: 218/500
Train loss: 1.7066
Epoch: 219/500
Train loss: 1.7159
Epoch: 220/500
Train loss: 1.6626
Epoch: 221/500
Train loss: 1.8232
Epoch: 222/500
Train loss: 1.8582
Epoch: 223/500
Train loss: 1.7981
Epoch: 224/500
Train loss: 1.7389
Epoch: 225/500
Train loss: 1.7378
Epoch: 226/500
Train loss: 1.7861
Epoch: 227/500
Train loss: 1.7518
Epoch: 228/500
Train loss: 1.7559
Epoch: 229/500
Train loss: 1.6670
Epoch: 230/500
Train loss: 1.7117
Epoch: 231/500
Train loss: 1.8101
Epoch: 232/500
Train loss: 1.7897
Epoch: 233/500
Train loss: 1.7779
Epoch: 234/500
Train loss: 1.7496
Epoch: 235/500
Train loss: 1.7583
Epoch: 236/500
Train loss: 1.7504
Epoch: 237/500
Train loss: 1.7438
Epoch: 238/500
Train loss: 1.8546
Epoch: 239/500
Train loss: 1.7658
Epoch: 240/500
Train loss: 1.7645
Epoch: 241/500
Train loss: 1.7174
Epoch: 242/500
Train loss: 1.7445
Epoch: 243/500
Train loss: 1.6992
Epoch: 244/500
Train loss: 1.7400
Epoch: 245/500
Train loss: 1.6613
Epoch: 246/500
Train loss: 1.6724
Epoch: 247/500
Train loss: 1.6322
Epoch: 248/500
Train loss: 1.6812
Epoch: 249/500
Train loss: 1.6603
Epoch: 250/500
Train loss: 1.8497
Epoch: 251/500
Train loss: 1.7614
Epoch: 252/500
Train loss: 1.6981
Epoch: 253/500
Train loss: 1.6237
Epoch: 254/500
Train loss: 1.7247
Epoch: 255/500
Train loss: 1.6679
Epoch: 256/500
Train loss: 1.6911
Epoch: 257/500
Train loss: 1.7671
Epoch: 258/500
Train loss: 1.7197
Epoch: 259/500
Train loss: 1.8326
Epoch: 260/500
Train loss: 1.7608
Epoch: 261/500
Train loss: 1.7355
Epoch: 262/500
Train loss: 1.6832
Epoch: 263/500
Train loss: 1.7930
Epoch: 264/500
Train loss: 1.7362
Epoch: 265/500
Train loss: 1.6280
Epoch: 266/500
Train loss: 1.6429
Epoch: 267/500
Train loss: 1.7751
Epoch: 268/500
Train loss: 1.6243
Epoch: 269/500
Train loss: 1.6946
Epoch: 270/500
Train loss: 1.7357
Epoch: 271/500
Train loss: 1.7776
Epoch: 272/500
Train loss: 1.7175
Epoch: 273/500
Train loss: 1.6908
Epoch: 274/500
Train loss: 1.7600
Epoch: 275/500
Train loss: 1.7068
Epoch: 276/500
Train loss: 1.6246
Epoch: 277/500
Train loss: 1.6484
Epoch: 278/500
Train loss: 1.7911
Epoch: 279/500
Train loss: 1.7261
Epoch: 280/500
Train loss: 1.6834
Epoch: 281/500
Train loss: 1.7034
Epoch: 282/500
Train loss: 1.6417
Epoch: 283/500
Train loss: 1.6894
Epoch: 284/500
Train loss: 1.4746
Epoch: 285/500
Train loss: 1.7812
Epoch: 286/500
Train loss: 1.5511
Epoch: 287/500
Train loss: 1.7119
Epoch: 288/500
Train loss: 1.7797
Epoch: 289/500
Train loss: 1.6877
Epoch: 290/500
Train loss: 1.7354
Epoch: 291/500
Train loss: 1.6635
Epoch: 292/500
Train loss: 1.6536
Epoch: 293/500
Train loss: 1.7319
Epoch: 294/500
Train loss: 1.7433
Epoch: 295/500
Train loss: 1.7336
Epoch: 296/500
Train loss: 1.6513
Epoch: 297/500
Train loss: 1.5907
Epoch: 298/500
Train loss: 1.7746
Epoch: 299/500
Train loss: 1.7207
Epoch: 300/500
Train loss: 1.7084
Epoch: 301/500
Train loss: 1.6912
Epoch: 302/500
Train loss: 1.6131
Epoch: 303/500
Train loss: 1.7535
Epoch: 304/500
Train loss: 1.7548
Epoch: 305/500
Train loss: 1.7073
Epoch: 306/500
Train loss: 1.7287
Epoch: 307/500
Train loss: 1.7550
Epoch: 308/500
Train loss: 1.7073
Epoch: 309/500
Train loss: 1.6618
Epoch: 310/500
Train loss: 1.8478
Epoch: 311/500
Train loss: 1.6222
Epoch: 312/500
Train loss: 1.6538
Epoch: 313/500
Train loss: 1.6823
Epoch: 314/500
Train loss: 1.6920
Epoch: 315/500
Train loss: 1.6908
Epoch: 316/500
Train loss: 1.5965
Epoch: 317/500
Train loss: 1.6151
Epoch: 318/500
Train loss: 1.6080
Epoch: 319/500
Train loss: 1.6149
Epoch: 320/500
Train loss: 1.5670
Epoch: 321/500
Train loss: 1.7495
Epoch: 322/500
Train loss: 1.5838
Epoch: 323/500
Train loss: 1.6305
Epoch: 324/500
Train loss: 1.6052
Epoch: 325/500
Train loss: 1.5183
Epoch: 326/500
Train loss: 1.6356
Epoch: 327/500
Train loss: 1.6589
Epoch: 328/500
Train loss: 1.6656
Epoch: 329/500
Train loss: 1.7383
Epoch: 330/500
Train loss: 1.6480
Epoch: 331/500
Train loss: 1.6587
Epoch: 332/500
Train loss: 1.6740
Epoch: 333/500
Train loss: 1.5168
Epoch: 334/500
Train loss: 1.6499
Epoch: 335/500
Train loss: 1.6411
Epoch: 336/500
Train loss: 1.6166
Epoch: 337/500
Train loss: 1.6475
Epoch: 338/500
Train loss: 1.7281
Epoch: 339/500
Train loss: 1.6135
Epoch: 340/500
Train loss: 1.7212
Epoch: 341/500
Train loss: 1.6467
Epoch: 342/500
Train loss: 1.6572
Epoch: 343/500
Train loss: 1.7267
Epoch: 344/500
Train loss: 1.5366
Epoch: 345/500
Train loss: 1.5915
Epoch: 346/500
Train loss: 1.7223
Epoch: 347/500
Train loss: 1.6754
Epoch: 348/500
Train loss: 1.5936
Epoch: 349/500
Train loss: 1.5800
Epoch: 350/500
Train loss: 1.6016
Epoch: 351/500
Train loss: 1.6360
Epoch: 352/500
Train loss: 1.6392
Epoch: 353/500
Train loss: 1.5963
Epoch: 354/500
Train loss: 1.6947
Epoch: 355/500
Train loss: 1.6898
Epoch: 356/500
Train loss: 1.7663
Epoch: 357/500
Train loss: 1.6045
Epoch: 358/500
Train loss: 1.7163
Epoch: 359/500
Train loss: 1.5508
Epoch: 360/500
Train loss: 1.5913
Epoch: 361/500
Train loss: 1.5675
Epoch: 362/500
Train loss: 1.7041
Epoch: 363/500
Train loss: 1.5436
Epoch: 364/500
Train loss: 1.6543
Epoch: 365/500
Train loss: 1.6601
Epoch: 366/500
Train loss: 1.6604
Epoch: 367/500
Train loss: 1.5798
Epoch: 368/500
Train loss: 1.6410
Epoch: 369/500
Train loss: 1.5970
Epoch: 370/500
Train loss: 1.6163
Epoch: 371/500
Train loss: 1.5210
Epoch: 372/500
Train loss: 1.6045
Epoch: 373/500
Train loss: 1.5631
Epoch: 374/500
Train loss: 1.6161
Epoch: 375/500
Train loss: 1.5336
Epoch: 376/500
Train loss: 1.6448
Epoch: 377/500
Train loss: 1.6037
Epoch: 378/500
Train loss: 1.5467
Epoch: 379/500
Train loss: 1.6046
Epoch: 380/500
Train loss: 1.6400
Epoch: 381/500
Train loss: 1.6781
Epoch: 382/500
Train loss: 1.7092
Epoch: 383/500
Train loss: 1.5832
Epoch: 384/500
Train loss: 1.6669
Epoch: 385/500
Train loss: 1.6158
Epoch: 386/500
Train loss: 1.7016
Epoch: 387/500
Train loss: 1.5145
Epoch: 388/500
Train loss: 1.5683
Epoch: 389/500
Train loss: 1.6026
Epoch: 390/500
Train loss: 1.5192
Epoch: 391/500
Train loss: 1.6429
Epoch: 392/500
Train loss: 1.6352
Epoch: 393/500
Train loss: 1.6773
Epoch: 394/500
Train loss: 1.5963
Epoch: 395/500
Train loss: 1.6070
Epoch: 396/500
Train loss: 1.6748
Epoch: 397/500
Train loss: 1.6241
Epoch: 398/500
Train loss: 1.6462
Epoch: 399/500
Train loss: 1.6817
Epoch: 400/500
Train loss: 1.6270
Epoch: 401/500
Train loss: 1.7413
Epoch: 402/500
Train loss: 1.6415
0.7955226462501148
Model improve: 0.0000 -> 0.7955
Epoch: 403/500
Train loss: 1.6266
0.7997695829409703
Model improve: 0.7955 -> 0.7998
Epoch: 404/500
Train loss: 1.6234
0.7953716086827457
Epoch: 405/500
Train loss: 1.5626
0.8000682800944621
Model improve: 0.7998 -> 0.8001
Epoch: 406/500
Train loss: 1.6473
0.7987602117530714
Epoch: 407/500
Train loss: 1.6693
Date :05/15/2023, 00:12:10
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 192
validbs: 768
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Train loss: 46.6355
Epoch: 2/100
Train loss: 5.7512
Epoch: 3/100
Train loss: 5.2734
Epoch: 4/100
Train loss: 4.9501
Epoch: 5/100
Train loss: 4.7157
Epoch: 6/100
Train loss: 4.5157
Epoch: 7/100
Train loss: 4.3154
Epoch: 8/100
Train loss: 4.0913
Epoch: 9/100
Train loss: 4.0190
Epoch: 10/100
Train loss: 3.7586
Epoch: 11/100
Train loss: 3.7123
Epoch: 12/100
Train loss: 3.5553
Epoch: 13/100
Train loss: 3.5682
Epoch: 14/100
Train loss: 3.3877
Epoch: 15/100
Train loss: 3.4288
Epoch: 16/100
Train loss: 3.3989
Epoch: 17/100
Train loss: 3.2569
Epoch: 18/100
Train loss: 3.2741
Epoch: 19/100
Train loss: 3.2552
Epoch: 20/100
Train loss: 3.1961
Epoch: 21/100
Train loss: 3.0088
Epoch: 22/100
Train loss: 3.1207
Epoch: 23/100
Train loss: 3.1975
Epoch: 24/100
Train loss: 2.9977
Epoch: 25/100
Train loss: 2.9080
Epoch: 26/100
Train loss: 2.9387
Epoch: 27/100
Train loss: 2.8512
Epoch: 28/100
Train loss: 3.1590
Epoch: 29/100
Train loss: 2.8962
Epoch: 30/100
Train loss: 2.9572
Epoch: 31/100
Train loss: 2.8696
Epoch: 32/100
Train loss: 2.6757
Epoch: 33/100
Train loss: 2.6690
Epoch: 34/100
Train loss: 2.7081
Epoch: 35/100
Train loss: 2.7908
Epoch: 36/100
Train loss: 2.7910
Epoch: 37/100
Train loss: 2.8738
Epoch: 38/100
Train loss: 2.6747
Epoch: 39/100
Train loss: 2.7579
Epoch: 40/100
Train loss: 2.7719
Epoch: 41/100
Train loss: 2.6338
Epoch: 42/100
Train loss: 2.6418
Epoch: 43/100
Train loss: 2.6530
Epoch: 44/100
Train loss: 2.5481
Epoch: 45/100
Train loss: 2.4968
Epoch: 46/100
Train loss: 2.6255
Epoch: 47/100
Train loss: 2.6854
Epoch: 48/100
Train loss: 2.6652
Epoch: 49/100
Train loss: 2.4597
Epoch: 50/100
Train loss: 2.4708
Epoch: 51/100
Train loss: 2.6579
Epoch: 52/100
Train loss: 2.5655
0.7952824668339973
Model improve: 0.0000 -> 0.7953
Epoch: 53/100
Train loss: 2.6661
0.7962410355056115
Model improve: 0.7953 -> 0.7962
Epoch: 54/100
Train loss: 2.5597
0.7978147906146665
Model improve: 0.7962 -> 0.7978
Epoch: 55/100
Date :05/15/2023, 01:08:00
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 256
validbs: 1024
epochwarmup: 20
totalepoch: 200
learningrate: 0.0004
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 182.0102
Epoch: 2/200
Train loss: 169.0640
Epoch: 3/200
Train loss: 103.4420
Epoch: 4/200
Train loss: 20.7526
Epoch: 5/200
Train loss: 7.3675
Epoch: 6/200
Train loss: 6.2398
Epoch: 7/200
Train loss: 5.9336
Epoch: 8/200
Train loss: 5.7540
Epoch: 9/200
Train loss: 5.5962
Epoch: 10/200
Train loss: 5.4980
Epoch: 11/200
Train loss: 5.3214
Epoch: 12/200
Train loss: 5.2509
Epoch: 13/200
Train loss: 5.0450
Epoch: 14/200
Train loss: 4.9009
Epoch: 15/200
Train loss: 4.8999
Epoch: 16/200
Train loss: 4.6663
Epoch: 17/200
Train loss: 4.6396
Epoch: 18/200
Train loss: 4.4427
Epoch: 19/200
Train loss: 4.4123
Epoch: 20/200
Train loss: 4.3319
Epoch: 21/200
Train loss: 4.2875
Epoch: 22/200
Train loss: 4.0633
Epoch: 23/200
Train loss: 3.9907
Epoch: 24/200
Train loss: 4.0031
Epoch: 25/200
Train loss: 3.8959
Epoch: 26/200
Train loss: 3.8828
Epoch: 27/200
Train loss: 3.7392
Epoch: 28/200
Train loss: 3.6592
Epoch: 29/200
Train loss: 3.7571
Epoch: 30/200
Train loss: 3.6048
Epoch: 31/200
Train loss: 3.6970
Epoch: 32/200
Train loss: 3.4498
Epoch: 33/200
Train loss: 3.3919
Epoch: 34/200
Train loss: 3.4232
Epoch: 35/200
Train loss: 3.2783
Epoch: 36/200
Train loss: 3.2781
Epoch: 37/200
Train loss: 3.6063
Epoch: 38/200
Train loss: 3.3004
Epoch: 39/200
Train loss: 3.2895
Epoch: 40/200
Train loss: 3.2945
Epoch: 41/200
Train loss: 3.2571
Epoch: 42/200
Train loss: 3.0575
Epoch: 43/200
Train loss: 2.9655
Epoch: 44/200
Train loss: 3.0280
Epoch: 45/200
Train loss: 3.0430
Epoch: 46/200
Train loss: 2.9929
Epoch: 47/200
Train loss: 3.1086
Epoch: 48/200
Train loss: 3.0546
Epoch: 49/200
Train loss: 3.3552
Epoch: 50/200
Train loss: 2.7019
Epoch: 51/200
Train loss: 3.0278
Epoch: 52/200
Train loss: 2.9869
Epoch: 53/200
Train loss: 2.9960
Epoch: 54/200
Train loss: 2.9560
Epoch: 55/200
Train loss: 2.8111
Epoch: 56/200
Train loss: 2.8286
Epoch: 57/200
Train loss: 2.7606
Epoch: 58/200
Train loss: 2.9070
Epoch: 59/200
Train loss: 2.6022
Epoch: 60/200
Train loss: 2.7190
Epoch: 61/200
Train loss: 2.7343
Epoch: 62/200
Train loss: 2.8695
Epoch: 63/200
Train loss: 2.8345
Epoch: 64/200
Train loss: 2.8246
Epoch: 65/200
Train loss: 2.5600
Epoch: 66/200
Train loss: 2.6669
Epoch: 67/200
Train loss: 2.6083
Epoch: 68/200
Train loss: 2.8174
Epoch: 69/200
Train loss: 2.7030
Epoch: 70/200
Train loss: 2.7156
Epoch: 71/200
Train loss: 2.8042
Epoch: 72/200
Train loss: 2.6312
Epoch: 73/200
Train loss: 2.5384
Epoch: 74/200
Train loss: 2.6879
Epoch: 75/200
Train loss: 2.8253
Epoch: 76/200
Train loss: 2.8553
Epoch: 77/200
Train loss: 2.5943
Epoch: 78/200
Train loss: 2.4352
Epoch: 79/200
Train loss: 2.5897
Epoch: 80/200
Train loss: 2.6972
Epoch: 81/200
Train loss: 2.5888
Epoch: 82/200
Train loss: 2.5295
Epoch: 83/200
Train loss: 2.5068
Epoch: 84/200
Train loss: 2.5827
Epoch: 85/200
Train loss: 2.7354
Epoch: 86/200
Train loss: 2.5370
Epoch: 87/200
Train loss: 2.5699
Epoch: 88/200
Train loss: 2.6503
Epoch: 89/200
Train loss: 2.5894
Epoch: 90/200
Train loss: 2.6552
Epoch: 91/200
Train loss: 2.4119
Epoch: 92/200
Train loss: 2.3105
Epoch: 93/200
Train loss: 2.7221
Epoch: 94/200
Train loss: 2.4407
Epoch: 95/200
Train loss: 2.3416
Epoch: 96/200
Train loss: 2.2915
Epoch: 97/200
Train loss: 2.3662
Epoch: 98/200
Train loss: 2.2382
Epoch: 99/200
Train loss: 2.5325
Epoch: 100/200
Train loss: 2.2191
Epoch: 101/200
Train loss: 2.4567
Epoch: 102/200
Train loss: 2.3856
0.7971166928072284
Model improve: 0.0000 -> 0.7971
Epoch: 103/200
Train loss: 2.4412
0.7989993087015426
Model improve: 0.7971 -> 0.7990
Epoch: 104/200
Date :05/15/2023, 02:46:42
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 512
validbs: 2048
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Date :05/15/2023, 02:47:41
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 384
validbs: 1536
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Date :05/15/2023, 02:48:12
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 320
validbs: 1280
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 89.6583
Epoch: 2/200
Train loss: 6.6492
Epoch: 3/200
Train loss: 5.8717
Epoch: 4/200
Train loss: 5.6065
Epoch: 5/200
Train loss: 5.3561
Epoch: 6/200
Train loss: 5.1929
Epoch: 7/200
Train loss: 5.0362
Epoch: 8/200
Train loss: 4.9530
Epoch: 9/200
Train loss: 4.7859
Epoch: 10/200
Train loss: 4.7043
Epoch: 11/200
Train loss: 4.5521
Epoch: 12/200
Train loss: 4.4395
Epoch: 13/200
Train loss: 4.3295
Epoch: 14/200
Train loss: 4.1304
Epoch: 15/200
Train loss: 4.2519
Epoch: 16/200
Train loss: 4.0066
Epoch: 17/200
Train loss: 3.8540
Epoch: 18/200
Train loss: 3.9198
Epoch: 19/200
Train loss: 3.9413
Epoch: 20/200
Train loss: 3.7415
Epoch: 21/200
Train loss: 3.6673
Epoch: 22/200
Train loss: 3.6245
Epoch: 23/200
Train loss: 3.6609
Epoch: 24/200
Train loss: 3.5394
Epoch: 25/200
Train loss: 3.5645
Epoch: 26/200
Train loss: 3.5945
Epoch: 27/200
Train loss: 3.5070
Epoch: 28/200
Train loss: 3.3384
Epoch: 29/200
Train loss: 3.3942
Epoch: 30/200
Train loss: 3.4748
Epoch: 31/200
Train loss: 3.4041
Epoch: 32/200
Train loss: 3.3411
Epoch: 33/200
Train loss: 3.2132
Epoch: 34/200
Train loss: 3.2785
Epoch: 35/200
Train loss: 3.1815
Epoch: 36/200
Train loss: 3.3945
Epoch: 37/200
Train loss: 3.1011
Epoch: 38/200
Train loss: 3.3145
Epoch: 39/200
Train loss: 3.2166
Epoch: 40/200
Train loss: 3.0474
Epoch: 41/200
Train loss: 3.0418
Epoch: 42/200
Train loss: 3.0239
Epoch: 43/200
Train loss: 2.9544
Epoch: 44/200
Train loss: 2.9891
Epoch: 45/200
Train loss: 2.9326
Epoch: 46/200
Train loss: 3.2974
Epoch: 47/200
Train loss: 3.0892
Epoch: 48/200
Train loss: 2.9956
Epoch: 49/200
Train loss: 2.9661
Epoch: 50/200
Train loss: 3.0369
Epoch: 51/200
Train loss: 2.9494
Epoch: 52/200
Train loss: 2.9423
Epoch: 53/200
Train loss: 2.6620
Epoch: 54/200
Train loss: 2.7750
Epoch: 55/200
Train loss: 2.7476
Epoch: 56/200
Train loss: 2.7585
Epoch: 57/200
Train loss: 2.8606
Epoch: 58/200
Train loss: 2.7898
Epoch: 59/200
Train loss: 2.8989
Epoch: 60/200
Train loss: 2.8024
Epoch: 61/200
Train loss: 3.0822
Epoch: 62/200
Train loss: 2.7564
Epoch: 63/200
Train loss: 2.6969
Epoch: 64/200
Train loss: 2.7907
Epoch: 65/200
Train loss: 2.8032
Epoch: 66/200
Train loss: 2.8108
Epoch: 67/200
Train loss: 2.7887
Epoch: 68/200
Train loss: 2.6804
Epoch: 69/200
Train loss: 2.6778
Epoch: 70/200
Train loss: 2.7893
Epoch: 71/200
Train loss: 2.3536
Epoch: 72/200
Train loss: 2.9032
Epoch: 73/200
Train loss: 2.5721
Epoch: 74/200
Train loss: 2.5003
Epoch: 75/200
Train loss: 2.5196
Epoch: 76/200
Train loss: 2.5527
Epoch: 77/200
Train loss: 2.7636
Epoch: 78/200
Train loss: 2.7030
Epoch: 79/200
Train loss: 2.7447
Epoch: 80/200
Train loss: 2.7877
Epoch: 81/200
Train loss: 2.4082
Epoch: 82/200
Train loss: 2.5332
Epoch: 83/200
Train loss: 2.5294
Epoch: 84/200
Train loss: 2.5189
Epoch: 85/200
Train loss: 2.6257
Epoch: 86/200
Train loss: 2.6771
Epoch: 87/200
Train loss: 2.5724
Epoch: 88/200
Train loss: 2.6787
Epoch: 89/200
Train loss: 2.7393
Epoch: 90/200
Train loss: 2.5600
Epoch: 91/200
Train loss: 2.5039
Epoch: 92/200
Train loss: 2.4190
Epoch: 93/200
Train loss: 2.6662
Epoch: 94/200
Train loss: 2.7339
Epoch: 95/200
Train loss: 2.7318
Epoch: 96/200
Train loss: 2.6970
Epoch: 97/200
Train loss: 2.3686
Epoch: 98/200
Train loss: 2.3500
Epoch: 99/200
Train loss: 2.5328
Epoch: 100/200
Train loss: 2.5978
Epoch: 101/200
Train loss: 2.6433
Epoch: 102/200
Train loss: 2.6407
0.7986320048724254
Model improve: 0.0000 -> 0.7986
Epoch: 103/200
Train loss: 2.3449
0.8007689714789789
Model improve: 0.7986 -> 0.8008
Epoch: 104/200
Train loss: 2.4967
0.7994124417923966
Epoch: 105/200
Train loss: 2.4249
0.800491998653856
Epoch: 106/200
Train loss: 2.5543
0.8013800006846902
Model improve: 0.8008 -> 0.8014
Epoch: 107/200
Train loss: 2.7537
0.8010933731871415
Epoch: 108/200
Date :05/15/2023, 04:40:29
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.0
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/200
Train loss: 39.5630
Epoch: 2/200
Train loss: 5.5379
Epoch: 3/200
Train loss: 5.0930
Epoch: 4/200
Train loss: 4.8444
Epoch: 5/200
Train loss: 4.5060
Epoch: 6/200
Train loss: 4.2605
Epoch: 7/200
Train loss: 3.9608
Epoch: 8/200
Train loss: 3.9151
Epoch: 9/200
Train loss: 3.7331
Epoch: 10/200
Train loss: 3.6767
Epoch: 11/200
Train loss: 3.5726
Epoch: 12/200
Train loss: 3.4477
Epoch: 13/200
Train loss: 3.3968
Epoch: 14/200
Train loss: 3.2693
Epoch: 15/200
Train loss: 3.2904
Epoch: 16/200
Train loss: 3.2282
Epoch: 17/200
Train loss: 3.0949
Epoch: 18/200
Train loss: 2.9754
Epoch: 19/200
Train loss: 3.1760
Epoch: 20/200
Train loss: 3.0679
Epoch: 21/200
Train loss: 2.9169
Epoch: 22/200
Train loss: 2.7821
Epoch: 23/200
Train loss: 2.8214
Epoch: 24/200
Train loss: 2.8938
Epoch: 25/200
Train loss: 2.8515
Epoch: 26/200
Train loss: 2.8577
Epoch: 27/200
Train loss: 2.8070
Epoch: 28/200
Train loss: 2.6693
Epoch: 29/200
Train loss: 2.7010
Epoch: 30/200
Train loss: 2.5440
Epoch: 31/200
Train loss: 2.6862
Epoch: 32/200
Train loss: 2.7211
Epoch: 33/200
Train loss: 2.5175
Epoch: 34/200
Train loss: 2.6276
Epoch: 35/200
Train loss: 2.6095
Epoch: 36/200
Train loss: 2.6476
Epoch: 37/200
Train loss: 2.5166
Epoch: 38/200
Train loss: 2.7556
Epoch: 39/200
Train loss: 2.4345
Epoch: 40/200
Train loss: 2.5708
Epoch: 41/200
Train loss: 2.4971
Epoch: 42/200
Train loss: 2.5057
Epoch: 43/200
Train loss: 2.5905
Epoch: 44/200
Train loss: 2.5471
Epoch: 45/200
Train loss: 2.5717
Epoch: 46/200
Train loss: 2.3083
Epoch: 47/200
Train loss: 2.5468
Epoch: 48/200
Train loss: 2.2540
Epoch: 49/200
Train loss: 2.2471
Epoch: 50/200
Train loss: 2.3164
Epoch: 51/200
Train loss: 2.3748
Epoch: 52/200
Train loss: 2.3664
Epoch: 53/200
Train loss: 2.2682
Epoch: 54/200
Train loss: 2.3783
Epoch: 55/200
Train loss: 2.3569
Epoch: 56/200
Train loss: 2.3160
Epoch: 57/200
Train loss: 2.3523
Epoch: 58/200
Train loss: 2.3264
Epoch: 59/200
Train loss: 2.3332
Epoch: 60/200
Train loss: 2.3429
Epoch: 61/200
Train loss: 2.2535
Epoch: 62/200
Train loss: 2.2800
Epoch: 63/200
Train loss: 2.2858
Epoch: 64/200
Train loss: 2.3250
Epoch: 65/200
Train loss: 2.3512
Epoch: 66/200
Train loss: 2.2359
Epoch: 67/200
Train loss: 2.3140
Epoch: 68/200
Train loss: 2.2689
Epoch: 69/200
Train loss: 2.4398
Epoch: 70/200
Train loss: 2.2527
Epoch: 71/200
Train loss: 2.1033
Epoch: 72/200
Train loss: 2.1190
Epoch: 73/200
Train loss: 2.2580
Epoch: 74/200
Train loss: 2.2218
Epoch: 75/200
Train loss: 2.3107
Epoch: 76/200
Train loss: 2.2639
Epoch: 77/200
Train loss: 2.2236
Epoch: 78/200
Train loss: 2.1329
Epoch: 79/200
Train loss: 1.9913
Epoch: 80/200
Train loss: 2.1582
Epoch: 81/200
Train loss: 2.1682
Epoch: 82/200
Train loss: 2.1690
Epoch: 83/200
Train loss: 2.1147
Epoch: 84/200
Train loss: 2.0928
Epoch: 85/200
Train loss: 2.2381
Epoch: 86/200
Train loss: 2.1651
Epoch: 87/200
Train loss: 2.0312
Epoch: 88/200
Train loss: 2.2418
Epoch: 89/200
Train loss: 2.2478
Epoch: 90/200
Train loss: 2.1012
Epoch: 91/200
Train loss: 2.2235
Epoch: 92/200
Train loss: 2.1872
Epoch: 93/200
Train loss: 2.2290
Epoch: 94/200
Train loss: 2.1332
Epoch: 95/200
Train loss: 2.1791
Epoch: 96/200
Train loss: 2.2148
Epoch: 97/200
Train loss: 2.1650
Epoch: 98/200
Train loss: 2.1258
Epoch: 99/200
Train loss: 2.0770
Epoch: 100/200
Train loss: 2.1193
Epoch: 101/200
Train loss: 2.1808
Epoch: 102/200
Train loss: 1.9822
0.8013059829960681
Model improve: 0.0000 -> 0.8013
Epoch: 103/200
Train loss: 2.0262
0.8009583037345238
Epoch: 104/200
Train loss: 2.0396
0.8023182479116523
Model improve: 0.8013 -> 0.8023
Epoch: 105/200
Train loss: 2.0862
0.8011507890332122
Epoch: 106/200
Train loss: 2.0929
0.8018079754349697
Epoch: 107/200
Train loss: 2.1605
0.8002081526720314
Epoch: 108/200
Train loss: 2.1299
0.7993313615615003
Epoch: 109/200
Train loss: 2.0915
0.8009561306841422
Epoch: 110/200
Train loss: 2.1451
0.8003922762549215
Epoch: 111/200
Date :05/15/2023, 06:49:35
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 40.4845
Epoch: 2/200
Train loss: 5.5304
Epoch: 3/200
Train loss: 5.0835
Epoch: 4/200
Train loss: 4.8342
Epoch: 5/200
Date :05/15/2023, 06:53:31
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Date :05/15/2023, 06:54:06
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 39.6868
Epoch: 2/200
Train loss: 5.5482
Epoch: 3/200
Train loss: 5.1152
Epoch: 4/200
Train loss: 4.8597
Epoch: 5/200
Train loss: 4.5439
Epoch: 6/200
Train loss: 4.2936
Epoch: 7/200
Train loss: 4.0007
Epoch: 8/200
Train loss: 3.9509
Epoch: 9/200
Train loss: 3.7719
Epoch: 10/200
Train loss: 3.7108
Epoch: 11/200
Train loss: 3.6154
Epoch: 12/200
Train loss: 3.5057
Epoch: 13/200
Train loss: 3.4336
Epoch: 14/200
Train loss: 3.3100
Epoch: 15/200
Train loss: 3.3545
Epoch: 16/200
Train loss: 3.2823
Epoch: 17/200
Train loss: 3.1629
Epoch: 18/200
Train loss: 3.0410
Epoch: 19/200
Train loss: 3.2477
Epoch: 20/200
Train loss: 3.1225
Epoch: 21/200
Train loss: 2.9677
Epoch: 22/200
Train loss: 2.8416
Epoch: 23/200
Train loss: 2.8922
Epoch: 24/200
Train loss: 2.9814
Epoch: 25/200
Train loss: 2.9089
Epoch: 26/200
Train loss: 2.9180
Epoch: 27/200
Train loss: 2.8930
Epoch: 28/200
Train loss: 2.7499
Epoch: 29/200
Train loss: 2.7757
Epoch: 30/200
Train loss: 2.6093
Epoch: 31/200
Train loss: 2.7482
Epoch: 32/200
Train loss: 2.8147
Epoch: 33/200
Train loss: 2.5836
Epoch: 34/200
Train loss: 2.6868
Epoch: 35/200
Train loss: 2.6687
Epoch: 36/200
Train loss: 2.7277
Epoch: 37/200
Train loss: 2.6019
Epoch: 38/200
Train loss: 2.8205
Epoch: 39/200
Train loss: 2.5248
Epoch: 40/200
Train loss: 2.6285
Epoch: 41/200
Train loss: 2.5756
Epoch: 42/200
Train loss: 2.5650
Epoch: 43/200
Train loss: 2.6553
Epoch: 44/200
Train loss: 2.6294
Epoch: 45/200
Train loss: 2.6408
Epoch: 46/200
Train loss: 2.4050
Epoch: 47/200
Train loss: 2.6125
Epoch: 48/200
Train loss: 2.3477
Epoch: 49/200
Train loss: 2.3269
Epoch: 50/200
Train loss: 2.4008
Epoch: 51/200
Train loss: 2.4483
Epoch: 52/200
Train loss: 2.4509
Epoch: 53/200
Train loss: 2.3522
Epoch: 54/200
Train loss: 2.4577
Epoch: 55/200
Train loss: 2.4318
Epoch: 56/200
Train loss: 2.3978
Epoch: 57/200
Train loss: 2.4327
Epoch: 58/200
Train loss: 2.4166
Epoch: 59/200
Train loss: 2.4159
Epoch: 60/200
Train loss: 2.4249
Epoch: 61/200
Train loss: 2.3120
Epoch: 62/200
Train loss: 2.3348
Epoch: 63/200
Train loss: 2.3640
Epoch: 64/200
Train loss: 2.3719
Epoch: 65/200
Train loss: 2.4300
Epoch: 66/200
Train loss: 2.3018
Epoch: 67/200
Train loss: 2.3866
Epoch: 68/200
Train loss: 2.3381
Epoch: 69/200
Train loss: 2.5054
Epoch: 70/200
Train loss: 2.3194
Epoch: 71/200
Train loss: 2.1866
Epoch: 72/200
Train loss: 2.2029
Epoch: 73/200
Train loss: 2.3352
Epoch: 74/200
Train loss: 2.3054
Epoch: 75/200
Train loss: 2.3574
Epoch: 76/200
Train loss: 2.3415
Epoch: 77/200
Train loss: 2.3125
Epoch: 78/200
Train loss: 2.1894
Epoch: 79/200
Train loss: 2.0818
Epoch: 80/200
Train loss: 2.2412
Epoch: 81/200
Train loss: 2.2401
Epoch: 82/200
Train loss: 2.2444
Epoch: 83/200
Train loss: 2.1980
Epoch: 84/200
Train loss: 2.1766
Epoch: 85/200
Train loss: 2.2984
Epoch: 86/200
Train loss: 2.2372
Epoch: 87/200
Train loss: 2.1102
Epoch: 88/200
Train loss: 2.3172
Epoch: 89/200
Train loss: 2.3316
Epoch: 90/200
Train loss: 2.1759
Epoch: 91/200
Train loss: 2.3055
Epoch: 92/200
Train loss: 2.2589
Epoch: 93/200
Train loss: 2.2956
Epoch: 94/200
Train loss: 2.2184
Epoch: 95/200
Train loss: 2.2542
Epoch: 96/200
Train loss: 2.2901
Epoch: 97/200
Train loss: 2.2550
Epoch: 98/200
Train loss: 2.2265
Epoch: 99/200
Train loss: 2.1497
Epoch: 100/200
Train loss: 2.1987
Epoch: 101/200
Train loss: 2.2449
Epoch: 102/200
Train loss: 2.0381
0.8010060879170726
Model improve: 0.0000 -> 0.8010
Epoch: 103/200
Date :05/15/2023, 08:40:03
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Date :05/15/2023, 08:41:25
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 41.2904
Epoch: 2/200
Train loss: 5.6521
Epoch: 3/200
Train loss: 5.2188
Epoch: 4/200
Train loss: 4.9597
Epoch: 5/200
Train loss: 4.6410
Epoch: 6/200
Train loss: 4.3975
Epoch: 7/200
Train loss: 4.1027
Epoch: 8/200
Train loss: 4.0440
Epoch: 9/200
Train loss: 3.8737
Epoch: 10/200
Train loss: 3.7961
Epoch: 11/200
Train loss: 3.7143
Epoch: 12/200
Train loss: 3.5979
Epoch: 13/200
Train loss: 3.5356
Epoch: 14/200
Train loss: 3.4253
Epoch: 15/200
Train loss: 3.4406
Epoch: 16/200
Train loss: 3.3935
Epoch: 17/200
Train loss: 3.2467
Epoch: 18/200
Train loss: 3.1555
Epoch: 19/200
Train loss: 3.3469
Epoch: 20/200
Train loss: 3.2165
Epoch: 21/200
Train loss: 3.0727
Epoch: 22/200
Train loss: 2.9693
Epoch: 23/200
Train loss: 2.9938
Epoch: 24/200
Train loss: 3.0783
Epoch: 25/200
Train loss: 3.0332
Epoch: 26/200
Train loss: 3.0232
Epoch: 27/200
Train loss: 2.9732
Epoch: 28/200
Train loss: 2.8490
Epoch: 29/200
Train loss: 2.8728
Epoch: 30/200
Train loss: 2.7273
Epoch: 31/200
Train loss: 2.8451
Epoch: 32/200
Train loss: 2.8958
Epoch: 33/200
Train loss: 2.6947
Epoch: 34/200
Train loss: 2.8022
Epoch: 35/200
Train loss: 2.7892
Epoch: 36/200
Train loss: 2.8055
Epoch: 37/200
Train loss: 2.6917
Epoch: 38/200
Train loss: 2.9197
Epoch: 39/200
Train loss: 2.6205
Epoch: 40/200
Train loss: 2.7303
Epoch: 41/200
Train loss: 2.6658
Epoch: 42/200
Train loss: 2.6589
Epoch: 43/200
Train loss: 2.7307
Epoch: 44/200
Train loss: 2.7252
Epoch: 45/200
Train loss: 2.7373
Epoch: 46/200
Train loss: 2.4929
Epoch: 47/200
Train loss: 2.6808
Epoch: 48/200
Train loss: 2.4124
Epoch: 49/200
Train loss: 2.4276
Epoch: 50/200
Train loss: 2.4789
Epoch: 51/200
Train loss: 2.5285
Epoch: 52/200
Train loss: 2.5101
Epoch: 53/200
Train loss: 2.4477
Epoch: 54/200
Train loss: 2.5464
Epoch: 55/200
Train loss: 2.4955
Epoch: 56/200
Train loss: 2.4745
Epoch: 57/200
Train loss: 2.5037
Epoch: 58/200
Train loss: 2.4830
Epoch: 59/200
Train loss: 2.5111
Epoch: 60/200
Train loss: 2.5062
Epoch: 61/200
Train loss: 2.3928
Epoch: 62/200
Train loss: 2.4216
Epoch: 63/200
Train loss: 2.4393
Epoch: 64/200
Train loss: 2.4691
Epoch: 65/200
Train loss: 2.5067
Epoch: 66/200
Train loss: 2.3788
Epoch: 67/200
Train loss: 2.4719
Epoch: 68/200
Train loss: 2.4351
Epoch: 69/200
Train loss: 2.5906
Epoch: 70/200
Train loss: 2.4096
Epoch: 71/200
Train loss: 2.2533
Epoch: 72/200
Train loss: 2.2871
Epoch: 73/200
Train loss: 2.4162
Epoch: 74/200
Train loss: 2.3920
Epoch: 75/200
Train loss: 2.4503
Epoch: 76/200
Train loss: 2.4141
Epoch: 77/200
Train loss: 2.3906
Epoch: 78/200
Train loss: 2.2829
Epoch: 79/200
Train loss: 2.1496
Epoch: 80/200
Train loss: 2.3144
Epoch: 81/200
Train loss: 2.3168
Epoch: 82/200
Train loss: 2.3039
Epoch: 83/200
Train loss: 2.2784
Epoch: 84/200
Train loss: 2.2490
Epoch: 85/200
Train loss: 2.3748
Epoch: 86/200
Train loss: 2.3210
Epoch: 87/200
Train loss: 2.1896
Epoch: 88/200
Train loss: 2.3883
Epoch: 89/200
Train loss: 2.4116
Epoch: 90/200
Train loss: 2.2503
Epoch: 91/200
Train loss: 2.3676
Epoch: 92/200
Train loss: 2.3534
Epoch: 93/200
Train loss: 2.3723
Epoch: 94/200
Train loss: 2.2764
Epoch: 95/200
Train loss: 2.3412
Epoch: 96/200
Train loss: 2.3615
Epoch: 97/200
Train loss: 2.3173
Epoch: 98/200
Train loss: 2.2906
Epoch: 99/200
Train loss: 2.2281
Epoch: 100/200
Train loss: 2.2797
Epoch: 101/200
Train loss: 2.3289
Epoch: 102/200
Train loss: 2.1205
0.8112132109456557
Epoch: 103/200
Train loss: 2.1729
0.8115267200398166
Epoch: 104/200
Train loss: 2.1766
0.8125238298193489
Epoch: 105/200
Train loss: 2.2248
0.813995923912728
Epoch: 106/200
Train loss: 2.2423
0.8119485512054291
Epoch: 107/200
Train loss: 2.3114
0.8127790586729418
Epoch: 108/200
Train loss: 2.2897
0.8116991882843431
Epoch: 109/200
Train loss: 2.2402
0.8126840817543279
Epoch: 110/200
Train loss: 2.2947
0.8133992851732208
Epoch: 111/200
Train loss: 2.1058
0.8139758847727446
Epoch: 112/200
Train loss: 2.2114
0.8104686687229578
Epoch: 113/200
Train loss: 2.2075
0.813450363311479
Epoch: 114/200
Date :05/15/2023, 10:58:39
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 41.3006
Epoch: 2/200
Train loss: 5.6525
Epoch: 3/200
Train loss: 5.2195
Epoch: 4/200
Train loss: 4.9605
Epoch: 5/200
Train loss: 4.6417
Epoch: 6/200
Train loss: 4.3979
Epoch: 7/200
Train loss: 4.1033
Epoch: 8/200
Train loss: 4.0436
Epoch: 9/200
Train loss: 3.8736
Epoch: 10/200
Train loss: 3.7957
Epoch: 11/200
Train loss: 3.7146
Epoch: 12/200
Train loss: 3.5978
Epoch: 13/200
Train loss: 3.5356
Epoch: 14/200
Train loss: 3.4253
Epoch: 15/200
Train loss: 3.4404
Epoch: 16/200
Train loss: 3.3931
Epoch: 17/200
Train loss: 3.2461
Epoch: 18/200
Date :05/15/2023, 11:15:25
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Date :05/15/2023, 11:15:40
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 41.2934
Epoch: 2/200
Train loss: 5.6518
Epoch: 3/200
Train loss: 5.2186
Epoch: 4/200
Train loss: 4.9592
Epoch: 5/200
Train loss: 4.6404
Epoch: 6/200
Train loss: 4.3969
Epoch: 7/200
Train loss: 4.1027
Epoch: 8/200
Train loss: 4.0437
Epoch: 9/200
Train loss: 3.8734
Epoch: 10/200
Train loss: 3.7953
Epoch: 11/200
Train loss: 3.7148
Epoch: 12/200
Train loss: 3.5977
Epoch: 13/200
Train loss: 3.5354
Epoch: 14/200
Train loss: 3.4257
Epoch: 15/200
Train loss: 3.4404
Epoch: 16/200
Train loss: 3.3930
Epoch: 17/200
Train loss: 3.2468
Epoch: 18/200
Train loss: 3.1560
Epoch: 19/200
Train loss: 3.3469
Epoch: 20/200
Train loss: 3.2158
Epoch: 21/200
Train loss: 3.0730
Epoch: 22/200
Train loss: 2.9697
Epoch: 23/200
Train loss: 2.9948
Epoch: 24/200
Train loss: 3.0780
Epoch: 25/200
Train loss: 3.0323
Epoch: 26/200
Train loss: 3.0233
Epoch: 27/200
Train loss: 2.9735
Epoch: 28/200
Train loss: 2.8489
Epoch: 29/200
Train loss: 2.8731
Epoch: 30/200
Train loss: 2.7269
Epoch: 31/200
Train loss: 2.8447
Epoch: 32/200
Train loss: 2.8961
Epoch: 33/200
Train loss: 2.6943
Epoch: 34/200
Train loss: 2.8023
Epoch: 35/200
Train loss: 2.7887
Epoch: 36/200
Train loss: 2.8052
Epoch: 37/200
Train loss: 2.6912
Epoch: 38/200
Train loss: 2.9192
Epoch: 39/200
Train loss: 2.6205
Epoch: 40/200
Train loss: 2.7305
Epoch: 41/200
Train loss: 2.6660
Epoch: 42/200
Train loss: 2.6586
Epoch: 43/200
Train loss: 2.7314
Epoch: 44/200
Train loss: 2.7257
Epoch: 45/200
Train loss: 2.7377
Epoch: 46/200
Train loss: 2.4930
Epoch: 47/200
Train loss: 2.6808
Epoch: 48/200
Train loss: 2.4127
Epoch: 49/200
Train loss: 2.4280
Epoch: 50/200
Train loss: 2.4783
Epoch: 51/200
Train loss: 2.5288
Epoch: 52/200
Train loss: 2.5099
Epoch: 53/200
Train loss: 2.4474
Epoch: 54/200
Train loss: 2.5465
Epoch: 55/200
Train loss: 2.4957
Epoch: 56/200
Train loss: 2.4744
Epoch: 57/200
Train loss: 2.5024
Epoch: 58/200
Train loss: 2.4831
Epoch: 59/200
Train loss: 2.5108
Epoch: 60/200
Train loss: 2.5054
Epoch: 61/200
Train loss: 2.3933
Epoch: 62/200
Train loss: 2.4213
Epoch: 63/200
Train loss: 2.4394
Epoch: 64/200
Train loss: 2.4694
Epoch: 65/200
Train loss: 2.5062
Epoch: 66/200
Train loss: 2.3795
Epoch: 67/200
Train loss: 2.4717
Epoch: 68/200
Train loss: 2.4355
Epoch: 69/200
Train loss: 2.5906
Epoch: 70/200
Train loss: 2.4090
Epoch: 71/200
Train loss: 2.2526
Epoch: 72/200
Train loss: 2.2863
Epoch: 73/200
Train loss: 2.4163
Epoch: 74/200
Train loss: 2.3918
Epoch: 75/200
Train loss: 2.4504
Epoch: 76/200
Train loss: 2.4135
Epoch: 77/200
Train loss: 2.3914
Epoch: 78/200
Train loss: 2.2834
Epoch: 79/200
Train loss: 2.1500
Epoch: 80/200
Train loss: 2.3143
Epoch: 81/200
Train loss: 2.3169
Epoch: 82/200
Train loss: 2.3041
Epoch: 83/200
Train loss: 2.2781
Epoch: 84/200
Train loss: 2.2489
Epoch: 85/200
Train loss: 2.3744
Epoch: 86/200
Train loss: 2.3216
Epoch: 87/200
Train loss: 2.1895
Epoch: 88/200
Train loss: 2.3880
Epoch: 89/200
Train loss: 2.4118
Epoch: 90/200
Train loss: 2.2499
Epoch: 91/200
Train loss: 2.3675
Epoch: 92/200
Train loss: 2.3532
Epoch: 93/200
Train loss: 2.3732
Epoch: 94/200
Train loss: 2.2765
Epoch: 95/200
Train loss: 2.3406
Epoch: 96/200
Train loss: 2.3609
Epoch: 97/200
Train loss: 2.3174
Epoch: 98/200
Train loss: 2.2909
Epoch: 99/200
Train loss: 2.2279
Epoch: 100/200
Train loss: 2.2802
Epoch: 101/200
Train loss: 2.3303
Epoch: 102/200
Train loss: 2.1200
Epoch: 103/200
Train loss: 2.1733
Epoch: 104/200
Train loss: 2.1702
Epoch: 105/200
Train loss: 2.2363
Epoch: 106/200
Train loss: 2.2407
Epoch: 107/200
Train loss: 2.2940
Epoch: 108/200
Train loss: 2.2901
Epoch: 109/200
Train loss: 2.2561
Epoch: 110/200
Train loss: 2.3240
Epoch: 111/200
Train loss: 2.0923
Epoch: 112/200
Train loss: 2.2052
Epoch: 113/200
Train loss: 2.1957
Epoch: 114/200
Train loss: 2.3069
Epoch: 115/200
Train loss: 2.2043
Epoch: 116/200
Train loss: 2.2569
Epoch: 117/200
Train loss: 2.2032
Epoch: 118/200
Train loss: 2.1683
Epoch: 119/200
Train loss: 2.2767
Epoch: 120/200
Train loss: 2.1734
Epoch: 121/200
Train loss: 2.1521
Epoch: 122/200
Train loss: 2.2450
Epoch: 123/200
Train loss: 2.3680
Epoch: 124/200
Train loss: 2.3696
Epoch: 125/200
Train loss: 2.3045
Epoch: 126/200
Train loss: 2.1431
Epoch: 127/200
Train loss: 2.2280
Epoch: 128/200
Train loss: 2.0789
Epoch: 129/200
Train loss: 2.1481
Epoch: 130/200
Train loss: 2.0777
Epoch: 131/200
Train loss: 2.1594
Epoch: 132/200
Train loss: 2.3078
Epoch: 133/200
Train loss: 2.0938
Epoch: 134/200
Train loss: 2.2113
Epoch: 135/200
Train loss: 2.0785
Epoch: 136/200
Train loss: 2.1656
Epoch: 137/200
Train loss: 2.1972
Epoch: 138/200
Train loss: 2.0528
Epoch: 139/200
Train loss: 2.2111
Epoch: 140/200
Train loss: 2.1303
Epoch: 141/200
Train loss: 2.1713
Epoch: 142/200
Train loss: 2.0999
0.8124798295864842
Model improve: 0.0000 -> 0.8125
Epoch: 143/200
Train loss: 2.2601
0.8121698889956653
Epoch: 144/200
Train loss: 2.2148
0.8138722232803806
Model improve: 0.8125 -> 0.8139
Epoch: 145/200
Train loss: 2.0875
0.8135957642712012
Epoch: 146/200
Train loss: 2.2353
0.8121835986629743
Epoch: 147/200
Train loss: 2.2244
0.8130521015396915
Epoch: 148/200
Train loss: 2.2754
0.8124406877898259
Epoch: 149/200
Train loss: 2.1359
0.8125330977867536
Epoch: 150/200
Train loss: 1.9647
0.8134170894847796
Epoch: 151/200
Train loss: 2.1602
0.812821363742659
Epoch: 152/200
Train loss: 2.0428
0.8130095786270579
Epoch: 153/200
Train loss: 2.0302
0.8130089385123743
Epoch: 154/200
Train loss: 2.1445
0.8126439856071047
Epoch: 155/200
Train loss: 2.2197
0.8102632445723466
Epoch: 156/200
Train loss: 2.1692
0.8124296730116927
Epoch: 157/200
Train loss: 2.1513
0.81197043044248
Epoch: 158/200
Train loss: 1.9675
0.8145178653340388
Model improve: 0.8139 -> 0.8145
Epoch: 159/200
Train loss: 2.2149
0.8126358790047049
Epoch: 160/200
Train loss: 2.0202
0.8142589627782654
Epoch: 161/200
Train loss: 2.2518
0.8134337525940918
Epoch: 162/200
Train loss: 2.1125
0.8123476816812962
Epoch: 163/200
Train loss: 2.1060
0.813199704360488
Epoch: 164/200
Train loss: 2.2418
0.8119667552456885
Epoch: 165/200
Train loss: 2.2638
0.8126025388928065
Epoch: 166/200
Train loss: 2.2092
0.8142489294947689
Epoch: 167/200
Train loss: 2.2353
0.8126601591552972
Epoch: 168/200
Train loss: 2.1449
0.8133997268705426
Epoch: 169/200
Train loss: 2.2158
0.8131276362659887
Epoch: 170/200
Train loss: 2.0901
0.8126752466956502
Epoch: 171/200
Train loss: 2.2289
0.8113205065563764
Epoch: 172/200
Train loss: 2.0797
0.812592385104737
Epoch: 173/200
Train loss: 2.2152
0.8128977561206584
Epoch: 174/200
Train loss: 2.0739
0.8129473922513498
Epoch: 175/200
Train loss: 2.1965
0.810510309031078
Epoch: 176/200
Train loss: 2.0005
0.814976406464041
Model improve: 0.8145 -> 0.8150
Epoch: 177/200
Train loss: 2.0815
0.8126962633314135
Epoch: 178/200
Train loss: 2.0985
0.8145582069543722
Epoch: 179/200
Train loss: 2.0485
0.8136722011310571
Epoch: 180/200
Train loss: 2.2278
0.8114519617348896
Epoch: 181/200
Train loss: 2.1285
0.8137273283834776
Epoch: 182/200
Train loss: 2.0250
0.8122056145962354
Epoch: 183/200
Train loss: 2.0738
0.8143379826689842
Epoch: 184/200
Train loss: 2.2249
0.8124040469399793
Epoch: 185/200
Train loss: 2.1073
0.8143996038231752
Epoch: 186/200
Train loss: 2.2510
0.8125343148847118
Epoch: 187/200
Train loss: 2.0426
0.8140195635185432
Epoch: 188/200
Train loss: 2.2642
0.8127002124082968
Epoch: 189/200
Train loss: 2.2250
0.8115898243389106
Epoch: 190/200
Train loss: 2.1037
0.8146491391620889
Epoch: 191/200
Train loss: 2.2146
0.8120201279420076
Epoch: 192/200
Train loss: 2.1586
0.8135093084517321
Epoch: 193/200
Train loss: 2.1964
0.811743829323876
Epoch: 194/200
Train loss: 2.1992
0.8132599055671348
Epoch: 195/200
Train loss: 2.1849
0.8133933498474102
Epoch: 196/200
Train loss: 2.2024
0.8119810769126067
Epoch: 197/200
Train loss: 2.1561
0.8129090839282764
Epoch: 198/200
Train loss: 2.0840
0.8123407836537533
Epoch: 199/200
Train loss: 2.2109
0.8114443891199586
Epoch: 200/200
Train loss: 2.1365
0.8131741890184364
Date :05/15/2023, 17:19:45
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 4
13830
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 41.2604
Epoch: 2/200
Train loss: 5.6434
Epoch: 3/200
Train loss: 5.1918
Epoch: 4/200
Train loss: 4.9205
Epoch: 5/200
Train loss: 4.6220
Epoch: 6/200
Date :05/15/2023, 17:25:08
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 4
13830
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 41.2660
Epoch: 2/200
Train loss: 5.6432
Epoch: 3/200
Train loss: 5.1917
Epoch: 4/200
Train loss: 4.9204
Epoch: 5/200
Train loss: 4.6224
Epoch: 6/200
Train loss: 4.3827
Epoch: 7/200
Train loss: 4.0802
Epoch: 8/200
Train loss: 4.0279
Epoch: 9/200
Train loss: 3.8618
Epoch: 10/200
Train loss: 3.7734
Epoch: 11/200
Train loss: 3.6987
Epoch: 12/200
Train loss: 3.5862
Epoch: 13/200
Train loss: 3.5450
Epoch: 14/200
Train loss: 3.4059
Epoch: 15/200
Train loss: 3.4331
Epoch: 16/200
Train loss: 3.3739
Epoch: 17/200
Train loss: 3.2254
Epoch: 18/200
Train loss: 3.1404
Epoch: 19/200
Train loss: 3.3486
Epoch: 20/200
Train loss: 3.2252
Epoch: 21/200
Train loss: 3.0685
Epoch: 22/200
Train loss: 2.9322
Epoch: 23/200
Train loss: 2.9928
Epoch: 24/200
Train loss: 3.0691
Epoch: 25/200
Train loss: 3.0144
Epoch: 26/200
Train loss: 2.9975
Epoch: 27/200
Train loss: 2.9682
Epoch: 28/200
Train loss: 2.8389
Epoch: 29/200
Train loss: 2.8638
Epoch: 30/200
Train loss: 2.7059
Epoch: 31/200
Train loss: 2.8595
Epoch: 32/200
Train loss: 2.8900
Epoch: 33/200
Train loss: 2.6907
Epoch: 34/200
Train loss: 2.7829
Epoch: 35/200
Train loss: 2.7886
Epoch: 36/200
Train loss: 2.7988
Epoch: 37/200
Train loss: 2.6862
Epoch: 38/200
Train loss: 2.9220
Epoch: 39/200
Train loss: 2.6008
Epoch: 40/200
Train loss: 2.7318
Epoch: 41/200
Train loss: 2.6657
Epoch: 42/200
Train loss: 2.6471
Epoch: 43/200
Train loss: 2.7512
Epoch: 44/200
Train loss: 2.7132
Epoch: 45/200
Train loss: 2.7291
Epoch: 46/200
Train loss: 2.4762
Epoch: 47/200
Train loss: 2.6620
Epoch: 48/200
Train loss: 2.4102
Epoch: 49/200
Train loss: 2.4144
Epoch: 50/200
Train loss: 2.4804
Epoch: 51/200
Train loss: 2.5321
Epoch: 52/200
Train loss: 2.5286
Epoch: 53/200
Train loss: 2.4387
Epoch: 54/200
Train loss: 2.5432
Epoch: 55/200
Train loss: 2.5030
Epoch: 56/200
Train loss: 2.4810
Epoch: 57/200
Train loss: 2.4983
Epoch: 58/200
Train loss: 2.4995
Epoch: 59/200
Train loss: 2.4839
Epoch: 60/200
Train loss: 2.4997
Epoch: 61/200
Train loss: 2.3961
Epoch: 62/200
Train loss: 2.4137
Epoch: 63/200
Train loss: 2.4292
Epoch: 64/200
Train loss: 2.4367
Epoch: 65/200
Train loss: 2.4962
Epoch: 66/200
Train loss: 2.3775
Epoch: 67/200
Train loss: 2.4707
Epoch: 68/200
Train loss: 2.4133
Epoch: 69/200
Train loss: 2.6160
Epoch: 70/200
Train loss: 2.3827
Epoch: 71/200
Train loss: 2.2684
Epoch: 72/200
Train loss: 2.2838
Epoch: 73/200
Train loss: 2.3957
Epoch: 74/200
Train loss: 2.3924
Epoch: 75/200
Train loss: 2.4408
Epoch: 76/200
Train loss: 2.4146
Epoch: 77/200
Train loss: 2.3716
Epoch: 78/200
Train loss: 2.2817
Epoch: 79/200
Train loss: 2.1303
Epoch: 80/200
Train loss: 2.3096
Epoch: 81/200
Train loss: 2.3202
Epoch: 82/200
Train loss: 2.2995
Epoch: 83/200
Train loss: 2.2649
Epoch: 84/200
Train loss: 2.2585
Epoch: 85/200
Train loss: 2.3770
Epoch: 86/200
Train loss: 2.3075
Epoch: 87/200
Train loss: 2.1797
Epoch: 88/200
Train loss: 2.3920
Epoch: 89/200
Train loss: 2.3969
Epoch: 90/200
Train loss: 2.2605
Epoch: 91/200
Train loss: 2.3542
Epoch: 92/200
Train loss: 2.3443
Epoch: 93/200
Train loss: 2.3676
Epoch: 94/200
Train loss: 2.2749
Epoch: 95/200
Train loss: 2.3241
Epoch: 96/200
Train loss: 2.3774
Epoch: 97/200
Train loss: 2.3025
Epoch: 98/200
Train loss: 2.2778
Epoch: 99/200
Train loss: 2.2288
Epoch: 100/200
Train loss: 2.2651
Epoch: 101/200
Train loss: 2.3078
Epoch: 102/200
Train loss: 2.1060
0.801594046410119
Model improve: 0.000000 -> 0.801594
Epoch: 103/200
Train loss: 2.1514
0.7995648184436899
Epoch: 104/200
Train loss: 2.1748
0.8020834744106786
Model improve: 0.801594 -> 0.802083
Epoch: 105/200
Train loss: 2.2253
0.8028216943095935
Model improve: 0.802083 -> 0.802822
Epoch: 106/200
Train loss: 2.2511
0.8003909205420657
Epoch: 107/200
Train loss: 2.2886
0.799423804043778
Epoch: 108/200
Train loss: 2.2811
0.7990336006577495
Epoch: 109/200
Train loss: 2.2615
0.8013728005196163
Epoch: 110/200
Train loss: 2.2905
0.7995105027518473
Epoch: 111/200
Train loss: 2.1027
0.8014109153751722
Epoch: 112/200
Train loss: 2.2091
0.8002682253427739
Epoch: 113/200
Train loss: 2.2072
0.8004045411247611
Epoch: 114/200
Train loss: 2.3200
0.79985921917311
Epoch: 115/200
Train loss: 2.2004
0.8023274699666377
Epoch: 116/200
Train loss: 2.2218
0.8036184254948902
Model improve: 0.802822 -> 0.803618
Epoch: 117/200
Train loss: 2.2111
0.8022323574340097
Epoch: 118/200
Train loss: 2.1586
0.8010887111035764
Epoch: 119/200
Train loss: 2.2806
0.8000991091831605
Epoch: 120/200
Train loss: 2.1545
0.8013051671814564
Epoch: 121/200
Train loss: 2.1583
0.8015261868358584
Epoch: 122/200
Train loss: 2.2585
0.8033085592410691
Epoch: 123/200
Train loss: 2.3400
0.8012697689664694
Epoch: 124/200
Train loss: 2.3622
0.8021462167045805
Epoch: 125/200
Train loss: 2.2823
0.8035407423078751
Epoch: 126/200
Train loss: 2.1305
0.80544629297251
Model improve: 0.803618 -> 0.805446
Epoch: 127/200
Train loss: 2.2056
0.8033699791035516
Epoch: 128/200
Train loss: 2.0950
0.8062123555433799
Model improve: 0.805446 -> 0.806212
Epoch: 129/200
Train loss: 2.1629
0.805130790849596
Epoch: 130/200
Train loss: 2.0615
0.8018285684146786
Epoch: 131/200
Train loss: 2.1629
0.804582920379707
Epoch: 132/200
Train loss: 2.2917
0.8019471146197732
Epoch: 133/200
Train loss: 2.0997
0.8059640022450847
Epoch: 134/200
Train loss: 2.2173
0.8045892752536795
Epoch: 135/200
Train loss: 2.0771
0.8055277783824731
Epoch: 136/200
Train loss: 2.1445
0.805801865316109
Epoch: 137/200
Train loss: 2.1881
0.8037790590668633
Epoch: 138/200
Train loss: 2.0619
0.805867405112457
Epoch: 139/200
Train loss: 2.1841
0.8041051320505159
Epoch: 140/200
Train loss: 2.1334
0.8040387788180209
Epoch: 141/200
Train loss: 2.1411
0.8048150249401056
Epoch: 142/200
Train loss: 2.0937
0.8042946165723477
Epoch: 143/200
Train loss: 2.2466
0.804739999778379
Epoch: 144/200
Train loss: 2.2026
0.8057047246298252
Epoch: 145/200
Train loss: 2.0585
0.8064057816921386
Model improve: 0.806212 -> 0.806406
Epoch: 146/200
Train loss: 2.2222
0.8055391177037471
Epoch: 147/200
Train loss: 2.2063
0.8048425527609472
Epoch: 148/200
Train loss: 2.2657
0.8044899442675485
Epoch: 149/200
Train loss: 2.1286
0.8040710096174927
Epoch: 150/200
Train loss: 1.9392
0.8044132333330152
Epoch: 151/200
Train loss: 2.1347
0.8046364176666938
Epoch: 152/200
Train loss: 2.0385
0.8052117722165469
Epoch: 153/200
Train loss: 2.0181
0.8043728923010037
Epoch: 154/200
Train loss: 2.1474
0.8051153704141719
Epoch: 155/200
Train loss: 2.2092
0.8020901678235394
Epoch: 156/200
Train loss: 2.1536
0.8047706517055622
Epoch: 157/200
Train loss: 2.1449
0.8039353078170862
Epoch: 158/200
Train loss: 1.9743
0.8076017379680399
Model improve: 0.806406 -> 0.807602
Epoch: 159/200
Train loss: 2.2029
0.8040529602587788
Epoch: 160/200
Train loss: 2.0013
0.8063753061537422
Epoch: 161/200
Train loss: 2.2656
0.8054959572298176
Epoch: 162/200
Train loss: 2.1270
0.8040134724186683
Epoch: 163/200
Train loss: 2.0927
0.8043997831931928
Epoch: 164/200
Train loss: 2.2350
0.8027809201766942
Epoch: 165/200
Train loss: 2.2329
0.8035763200021275
Epoch: 166/200
Train loss: 2.1822
0.8053146962542146
Epoch: 167/200
Train loss: 2.2444
0.8046251955565484
Epoch: 168/200
Train loss: 2.1557
0.805688286937107
Epoch: 169/200
Train loss: 2.2137
0.8047665051802709
Epoch: 170/200
Train loss: 2.0894
0.8044539966656824
Epoch: 171/200
Train loss: 2.2150
0.8038684647931938
Epoch: 172/200
Train loss: 2.0804
0.8048851031687955
Epoch: 173/200
Train loss: 2.2018
0.8042655940913802
Epoch: 174/200
Train loss: 2.0774
0.8042386345214672
Epoch: 175/200
Train loss: 2.2119
0.8015103008090113
Epoch: 176/200
Train loss: 1.9946
0.8082032643742317
Model improve: 0.807602 -> 0.808203
Epoch: 177/200
Train loss: 2.0644
0.8045975409218421
Epoch: 178/200
Train loss: 2.0914
0.8069670131235166
Epoch: 179/200
Train loss: 2.0450
0.8058169386827658
Epoch: 180/200
Train loss: 2.2281
0.8032077086269989
Epoch: 181/200
Train loss: 2.1204
0.8065444065158232
Epoch: 182/200
Train loss: 2.0432
0.8031695570950246
Epoch: 183/200
Train loss: 2.0808
0.8069709617323821
Epoch: 184/200
Train loss: 2.2143
0.8043560139565329
Epoch: 185/200
Train loss: 2.0939
0.806955244538089
Epoch: 186/200
Train loss: 2.2606
0.8048942492932883
Epoch: 187/200
Train loss: 2.0390
0.8064787167772116
Epoch: 188/200
Train loss: 2.2504
0.8045293114611548
Epoch: 189/200
Train loss: 2.1989
0.8040060355740698
Epoch: 190/200
Train loss: 2.1102
0.8075837393054426
Epoch: 191/200
Train loss: 2.2128
0.8039405151694996
Epoch: 192/200
Train loss: 2.1590
0.8056699715366005
Epoch: 193/200
Train loss: 2.1909
0.803417688949416
Epoch: 194/200
Train loss: 2.1992
0.8049073396382272
Epoch: 195/200
Train loss: 2.1688
0.8049207984177212
Epoch: 196/200
Train loss: 2.1788
0.804369795910348
Epoch: 197/200
Train loss: 2.1350
0.8051497330588422
Epoch: 198/200
Train loss: 2.0874
0.8043219403852842
Epoch: 199/200
Train loss: 2.1978
0.8040261239089022
Epoch: 200/200
Train loss: 2.1174
0.8053843449392403
Date :05/16/2023, 00:59:56
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 41.7124
Epoch: 2/200
Train loss: 5.6844
Epoch: 3/200
Train loss: 5.2457
Epoch: 4/200
Train loss: 4.9816
Epoch: 5/200
Train loss: 4.6718
Epoch: 6/200
Train loss: 4.4268
Epoch: 7/200
Train loss: 4.1372
Epoch: 8/200
Train loss: 4.0741
Epoch: 9/200
Train loss: 3.9048
Epoch: 10/200
Train loss: 3.8264
Epoch: 11/200
Train loss: 3.7439
Epoch: 12/200
Train loss: 3.6265
Epoch: 13/200
Train loss: 3.5666
Epoch: 14/200
Train loss: 3.4552
Epoch: 15/200
Train loss: 3.4694
Epoch: 16/200
Train loss: 3.4219
Epoch: 17/200
Train loss: 3.2771
Epoch: 18/200
Train loss: 3.1851
Epoch: 19/200
Train loss: 3.3779
Epoch: 20/200
Train loss: 3.2471
Epoch: 21/200
Train loss: 3.1018
Epoch: 22/200
Train loss: 3.0015
Epoch: 23/200
Train loss: 3.0259
Epoch: 24/200
Train loss: 3.1080
Epoch: 25/200
Train loss: 3.0630
Epoch: 26/200
Train loss: 3.0523
Epoch: 27/200
Train loss: 3.0022
Epoch: 28/200
Train loss: 2.8788
Epoch: 29/200
Train loss: 2.9021
Epoch: 30/200
Train loss: 2.7577
Epoch: 31/200
Train loss: 2.8748
Epoch: 32/200
Train loss: 2.9257
Epoch: 33/200
Train loss: 2.7226
Epoch: 34/200
Train loss: 2.8304
Epoch: 35/200
Train loss: 2.8175
Epoch: 36/200
Train loss: 2.8311
Epoch: 37/200
Train loss: 2.7203
Epoch: 38/200
Train loss: 2.9470
Epoch: 39/200
Train loss: 2.6499
Epoch: 40/200
Train loss: 2.7554
Epoch: 41/200
Train loss: 2.6922
Epoch: 42/200
Train loss: 2.6838
Epoch: 43/200
Train loss: 2.7568
Epoch: 44/200
Train loss: 2.7533
Epoch: 45/200
Train loss: 2.7647
Epoch: 46/200
Train loss: 2.5219
Epoch: 47/200
Train loss: 2.7077
Epoch: 48/200
Train loss: 2.4405
Epoch: 49/200
Train loss: 2.4559
Epoch: 50/200
Train loss: 2.5063
Epoch: 51/200
Train loss: 2.5549
Epoch: 52/200
Train loss: 2.5353
Epoch: 53/200
Train loss: 2.4748
Epoch: 54/200
Train loss: 2.5720
Epoch: 55/200
Train loss: 2.5208
Epoch: 56/200
Train loss: 2.5018
Epoch: 57/200
Train loss: 2.5298
Epoch: 58/200
Train loss: 2.5065
Epoch: 59/200
Train loss: 2.5353
Epoch: 60/200
Train loss: 2.5320
Epoch: 61/200
Train loss: 2.4179
Epoch: 62/200
Train loss: 2.4482
Epoch: 63/200
Train loss: 2.4610
Epoch: 64/200
Train loss: 2.4934
Epoch: 65/200
Train loss: 2.5300
Epoch: 66/200
Train loss: 2.4035
Epoch: 67/200
Train loss: 2.4970
Epoch: 68/200
Train loss: 2.4614
Epoch: 69/200
Train loss: 2.6122
Epoch: 70/200
Train loss: 2.4364
Epoch: 71/200
Train loss: 2.2766
Epoch: 72/200
Train loss: 2.3099
Epoch: 73/200
Train loss: 2.4404
Epoch: 74/200
Train loss: 2.4167
Epoch: 75/200
Train loss: 2.4714
Epoch: 76/200
Train loss: 2.4378
Epoch: 77/200
Train loss: 2.4175
Epoch: 78/200
Train loss: 2.3073
Epoch: 79/200
Train loss: 2.1745
Epoch: 80/200
Train loss: 2.3393
Epoch: 81/200
Train loss: 2.3416
Epoch: 82/200
Train loss: 2.3281
Epoch: 83/200
Train loss: 2.3028
Epoch: 84/200
Train loss: 2.2743
Epoch: 85/200
Train loss: 2.3976
Epoch: 86/200
Train loss: 2.3450
Epoch: 87/200
Train loss: 2.2108
Epoch: 88/200
Train loss: 2.4131
Epoch: 89/200
Train loss: 2.4365
Epoch: 90/200
Train loss: 2.2741
Epoch: 91/200
Train loss: 2.3905
Epoch: 92/200
Train loss: 2.3780
Epoch: 93/200
Train loss: 2.3944
Epoch: 94/200
Train loss: 2.3000
Epoch: 95/200
Train loss: 2.3633
Epoch: 96/200
Train loss: 2.3851
Epoch: 97/200
Train loss: 2.3408
Epoch: 98/200
Train loss: 2.3158
Epoch: 99/200
Train loss: 2.2506
Epoch: 100/200
Train loss: 2.3028
Epoch: 101/200
Train loss: 2.3532
Epoch: 102/200
Train loss: 2.1426
0.8122229749801834
Model improve: 0.000000 -> 0.812223
Epoch: 103/200
Train loss: 2.1952
0.812231252957616
Model improve: 0.812223 -> 0.812231
Epoch: 104/200
Train loss: 2.1979
0.8127161738303089
Model improve: 0.812231 -> 0.812716
Epoch: 105/200
Train loss: 2.2473
0.814483631122839
Model improve: 0.812716 -> 0.814484
Epoch: 106/200
# 0.3 0.35
Date :05/16/2023, 02:51:29
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Date :05/16/2023, 02:51:38
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.25
drop_path_rate: 0.4
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 42.1100
Epoch: 2/200
Train loss: 5.7185
Epoch: 3/200
Train loss: 5.2748
Epoch: 4/200
Train loss: 5.0089
Epoch: 5/200
Train loss: 4.6974
Epoch: 6/200
Train loss: 4.4568
Epoch: 7/200
Train loss: 4.1692
Epoch: 8/200
Train loss: 4.1041
Epoch: 9/200
Train loss: 3.9429
Epoch: 10/200
Train loss: 3.8597
Epoch: 11/200
Train loss: 3.7761
Epoch: 12/200
Train loss: 3.6582
Epoch: 13/200
Train loss: 3.6000
Epoch: 14/200
Train loss: 3.4906
Epoch: 15/200
Train loss: 3.5012
Epoch: 16/200
Train loss: 3.4565
Epoch: 17/200
Train loss: 3.3112
Epoch: 18/200
Train loss: 3.2172
Epoch: 19/200
Train loss: 3.4092
Epoch: 20/200
Train loss: 3.2792
Epoch: 21/200
Train loss: 3.1335
Epoch: 22/200
Train loss: 3.0334
Epoch: 23/200
Train loss: 3.0593
Epoch: 24/200
Train loss: 3.1409
Epoch: 25/200
Train loss: 3.0925
Epoch: 26/200
Train loss: 3.0826
Epoch: 27/200
Train loss: 3.0337
Epoch: 28/200
Train loss: 2.9114
Epoch: 29/200
Train loss: 2.9360
Epoch: 30/200
Train loss: 2.7885
Epoch: 31/200
Train loss: 2.9044
Epoch: 32/200
Train loss: 2.9569
Epoch: 33/200
Train loss: 2.7537
Epoch: 34/200
Train loss: 2.8621
Epoch: 35/200
Train loss: 2.8478
Epoch: 36/200
Train loss: 2.8617
Epoch: 37/200
Train loss: 2.7541
Epoch: 38/200
Train loss: 2.9741
Epoch: 39/200
Train loss: 2.6808
Epoch: 40/200
Train loss: 2.7833
Epoch: 41/200
Train loss: 2.7205
Epoch: 42/200
Train loss: 2.7112
Epoch: 43/200
Train loss: 2.7864
Epoch: 44/200
Train loss: 2.7826
Epoch: 45/200
Train loss: 2.7935
Epoch: 46/200
Train loss: 2.5530
Epoch: 47/200
Train loss: 2.7373
Epoch: 48/200
Train loss: 2.4700
Epoch: 49/200
Train loss: 2.4856
Epoch: 50/200
Train loss: 2.5347
Epoch: 51/200
Train loss: 2.5828
Epoch: 52/200
Train loss: 2.5623
Epoch: 53/200
Train loss: 2.5006
Epoch: 54/200
Train loss: 2.5986
Epoch: 55/200
Train loss: 2.5462
Epoch: 56/200
Train loss: 2.5287
Epoch: 57/200
Train loss: 2.5564
Epoch: 58/200
Train loss: 2.5344
Epoch: 59/200
Train loss: 2.5595
Epoch: 60/200
Train loss: 2.5574
Epoch: 61/200
Train loss: 2.4433
Epoch: 62/200
Train loss: 2.4736
Epoch: 63/200
Train loss: 2.4857
Epoch: 64/200
Train loss: 2.5188
Epoch: 65/200
Train loss: 2.5567
Epoch: 66/200
Train loss: 2.4319
Epoch: 67/200
Train loss: 2.5237
Epoch: 68/200
Train loss: 2.4849
Epoch: 69/200
Train loss: 2.6361
Epoch: 70/200
Train loss: 2.4627
Epoch: 71/200
Train loss: 2.2993
Epoch: 72/200
Train loss: 2.3360
Epoch: 73/200
Train loss: 2.4662
Epoch: 74/200
Train loss: 2.4412
Epoch: 75/200
Train loss: 2.4960
Epoch: 76/200
Train loss: 2.4625
Epoch: 77/200
Train loss: 2.4413
Epoch: 78/200
Train loss: 2.3321
Epoch: 79/200
Train loss: 2.1998
Epoch: 80/200
Train loss: 2.3652
Epoch: 81/200
Train loss: 2.3664
Epoch: 82/200
Train loss: 2.3527
Epoch: 83/200
Train loss: 2.3247
Epoch: 84/200
Train loss: 2.3010
Epoch: 85/200
Train loss: 2.4222
Epoch: 86/200
Train loss: 2.3706
Epoch: 87/200
Train loss: 2.2338
Epoch: 88/200
Train loss: 2.4365
Epoch: 89/200
Train loss: 2.4584
Epoch: 90/200
Train loss: 2.2982
Epoch: 91/200
Train loss: 2.4144
Epoch: 92/200
Train loss: 2.4014
Epoch: 93/200
Train loss: 2.4179
Epoch: 94/200
Train loss: 2.3255
Epoch: 95/200
Train loss: 2.3860
Epoch: 96/200
Train loss: 2.4098
Epoch: 97/200
Train loss: 2.3644
Epoch: 98/200
Train loss: 2.3400
Epoch: 99/200
Train loss: 2.2750
Epoch: 100/200
Train loss: 2.3281
Epoch: 101/200
Train loss: 2.3773
Epoch: 102/200
Train loss: 2.1658
0.8123619175588187
Model improve: 0.000000 -> 0.812362
Epoch: 103/200
Train loss: 2.2193
0.8123667912200305
Model improve: 0.812362 -> 0.812367
Epoch: 104/200
Train loss: 2.2213
0.8130235694704968
Model improve: 0.812367 -> 0.813024
Epoch: 105/200
Train loss: 2.2710
0.8143317092950938
Model improve: 0.813024 -> 0.814332
Epoch: 106/200
Date :05/16/2023, 04:40:45
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.15
drop_path_rate: 0.35
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Date :05/16/2023, 04:40:52
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.15
drop_path_rate: 0.35
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 42.1190
Epoch: 2/200
Train loss: 5.7189
Epoch: 3/200
Train loss: 5.2755
Epoch: 4/200
Train loss: 5.0100
Epoch: 5/200
Train loss: 4.6981
Epoch: 6/200
Train loss: 4.4571
Epoch: 7/200
Train loss: 4.1691
Epoch: 8/200
Train loss: 4.1039
Epoch: 9/200
Train loss: 3.9423
Epoch: 10/200
Train loss: 3.8585
Epoch: 11/200
Train loss: 3.7760
Epoch: 12/200
Train loss: 3.6579
Epoch: 13/200
Train loss: 3.5995
Epoch: 14/200
Train loss: 3.4900
Epoch: 15/200
Train loss: 3.5008
Epoch: 16/200
Train loss: 3.4556
Epoch: 17/200
Train loss: 3.3115
Epoch: 18/200
Train loss: 3.2173
Epoch: 19/200
Train loss: 3.4088
Epoch: 20/200
Train loss: 3.2798
Epoch: 21/200
Train loss: 3.1332
Epoch: 22/200
Train loss: 3.0336
Epoch: 23/200
Train loss: 3.0589
Epoch: 24/200
Train loss: 3.1404
Epoch: 25/200
Train loss: 3.0929
Epoch: 26/200
Train loss: 3.0827
Epoch: 27/200
Train loss: 3.0329
Epoch: 28/200
Train loss: 2.9117
Epoch: 29/200
Train loss: 2.9357
Epoch: 30/200
Train loss: 2.7884
Epoch: 31/200
Train loss: 2.9044
Epoch: 32/200
Train loss: 2.9566
Epoch: 33/200
Train loss: 2.7535
Epoch: 34/200
Train loss: 2.8618
Epoch: 35/200
Train loss: 2.8474
Epoch: 36/200
Train loss: 2.8617
Epoch: 37/200
Train loss: 2.7537
Epoch: 38/200
Train loss: 2.9741
Epoch: 39/200
Train loss: 2.6808
Epoch: 40/200
Train loss: 2.7833
Epoch: 41/200
Train loss: 2.7212
Epoch: 42/200
Train loss: 2.7116
Epoch: 43/200
Train loss: 2.7862
Epoch: 44/200
Train loss: 2.7815
Epoch: 45/200
Train loss: 2.7937
Epoch: 46/200
Train loss: 2.5528
Epoch: 47/200
Train loss: 2.7379
Epoch: 48/200
Train loss: 2.4695
Epoch: 49/200
Train loss: 2.4852
Epoch: 50/200
Train loss: 2.5349
Epoch: 51/200
Train loss: 2.5831
Epoch: 52/200
Train loss: 2.5622
Epoch: 53/200
Train loss: 2.5007
Epoch: 54/200
Train loss: 2.5979
Epoch: 55/200
Train loss: 2.5457
Epoch: 56/200
Train loss: 2.5287
Epoch: 57/200
Train loss: 2.5565
Epoch: 58/200
Train loss: 2.5339
Epoch: 59/200
Train loss: 2.5599
Epoch: 60/200
Train loss: 2.5579
Epoch: 61/200
Train loss: 2.4431
Epoch: 62/200
Train loss: 2.4731
Epoch: 63/200
Train loss: 2.4860
Epoch: 64/200
Train loss: 2.5191
Epoch: 65/200
Train loss: 2.5559
Epoch: 66/200
Train loss: 2.4315
Epoch: 67/200
Train loss: 2.5233
Epoch: 68/200
Train loss: 2.4850
Epoch: 69/200
Train loss: 2.6363
Epoch: 70/200
Train loss: 2.4622
Epoch: 71/200
Train loss: 2.2988
Epoch: 72/200
Train loss: 2.3364
Epoch: 73/200
Train loss: 2.4665
Epoch: 74/200
Train loss: 2.4409
Epoch: 75/200
Train loss: 2.4955
Epoch: 76/200
Train loss: 2.4625
Epoch: 77/200
Train loss: 2.4408
Epoch: 78/200
Train loss: 2.3322
Epoch: 79/200
Train loss: 2.1997
Epoch: 80/200
Train loss: 2.3643
Epoch: 81/200
Train loss: 2.3660
Epoch: 82/200
Train loss: 2.3521
Epoch: 83/200
Train loss: 2.3250
Epoch: 84/200
Train loss: 2.3003
Epoch: 85/200
Train loss: 2.4223
Epoch: 86/200
Train loss: 2.3695
Epoch: 87/200
Train loss: 2.2342
Epoch: 88/200
Train loss: 2.4365
Epoch: 89/200
Train loss: 2.4578
Epoch: 90/200
Train loss: 2.2988
Epoch: 91/200
Train loss: 2.4143
Epoch: 92/200
Train loss: 2.4008
Epoch: 93/200
Train loss: 2.4176
Epoch: 94/200
Train loss: 2.3252
Epoch: 95/200
Train loss: 2.3864
Epoch: 96/200
Train loss: 2.4092
Epoch: 97/200
Train loss: 2.3641
Epoch: 98/200
Train loss: 2.3398
Epoch: 99/200
Train loss: 2.2745
Epoch: 100/200
Train loss: 2.3283
Epoch: 101/200
Train loss: 2.3773
Epoch: 102/200
Train loss: 2.1672
0.8123003434565864
Model improve: 0.000000 -> 0.812300
Epoch: 103/200
Train loss: 2.2198
0.8123838320323559
Model improve: 0.812300 -> 0.812384
Epoch: 104/200
Train loss: 2.2209
0.8132723267573947
Model improve: 0.812384 -> 0.813272
Epoch: 105/200
Train loss: 2.2703
0.8148012427459932
Model improve: 0.813272 -> 0.814801
Epoch: 106/200
Train loss: 2.2904
0.8120618251489403
Epoch: 107/200
Train loss: 2.3581
0.8122248025563913
Epoch: 108/200
Date :05/16/2023, 06:37:43
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.1
drop_path_rate: 0.3
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 42.1091
Epoch: 2/200
Train loss: 5.7193
Epoch: 3/200
Train loss: 5.2760
Epoch: 4/200
Train loss: 5.0105
Epoch: 5/200
Train loss: 4.6984
Epoch: 6/200
Train loss: 4.4570
Epoch: 7/200
Train loss: 4.1693
Epoch: 8/200
Train loss: 4.1038
Epoch: 9/200
Train loss: 3.9422
Epoch: 10/200
Train loss: 3.8586
Epoch: 11/200
Train loss: 3.7758
Epoch: 12/200
Train loss: 3.6578
Epoch: 13/200
Train loss: 3.5990
Epoch: 14/200
Train loss: 3.4898
Epoch: 15/200
Train loss: 3.5008
Epoch: 16/200
Train loss: 3.4559
Epoch: 17/200
Train loss: 3.3109
Epoch: 18/200
Train loss: 3.2168
Epoch: 19/200
Train loss: 3.4085
Epoch: 20/200
Train loss: 3.2786
Epoch: 21/200
Train loss: 3.1332
Epoch: 22/200
Train loss: 3.0331
Epoch: 23/200
Train loss: 3.0591
Epoch: 24/200
Train loss: 3.1400
Epoch: 25/200
Train loss: 3.0926
Epoch: 26/200
Train loss: 3.0823
Epoch: 27/200
Train loss: 3.0333
Epoch: 28/200
Train loss: 2.9113
Epoch: 29/200
Train loss: 2.9355
Epoch: 30/200
Train loss: 2.7876
Epoch: 31/200
Train loss: 2.9046
Epoch: 32/200
Train loss: 2.9568
Epoch: 33/200
Train loss: 2.7542
Epoch: 34/200
Train loss: 2.8619
Epoch: 35/200
Train loss: 2.8478
Epoch: 36/200
Train loss: 2.8622
Epoch: 37/200
Train loss: 2.7528
Epoch: 38/200
Train loss: 2.9742
Epoch: 39/200
Train loss: 2.6802
Epoch: 40/200
Train loss: 2.7835
Epoch: 41/200
Train loss: 2.7202
Epoch: 42/200
Train loss: 2.7110
Epoch: 43/200
Train loss: 2.7858
Epoch: 44/200
Train loss: 2.7822
Epoch: 45/200
Train loss: 2.7936
Epoch: 46/200
Train loss: 2.5523
Epoch: 47/200
Train loss: 2.7369
Epoch: 48/200
Train loss: 2.4693
Epoch: 49/200
Train loss: 2.4856
Epoch: 50/200
Train loss: 2.5345
Epoch: 51/200
Train loss: 2.5829
Epoch: 52/200
Train loss: 2.5622
Epoch: 53/200
Train loss: 2.5008
Epoch: 54/200
Train loss: 2.5980
Epoch: 55/200
Train loss: 2.5459
Epoch: 56/200
Train loss: 2.5287
Epoch: 57/200
Train loss: 2.5563
Epoch: 58/200
Train loss: 2.5336
Epoch: 59/200
Train loss: 2.5597
Epoch: 60/200
Train loss: 2.5583
Epoch: 61/200
Train loss: 2.4428
Epoch: 62/200
Train loss: 2.4739
Epoch: 63/200
Train loss: 2.4860
Epoch: 64/200
Train loss: 2.5187
Epoch: 65/200
Train loss: 2.5563
Epoch: 66/200
Train loss: 2.4318
Epoch: 67/200
Train loss: 2.5239
Epoch: 68/200
Train loss: 2.4862
Epoch: 69/200
Train loss: 2.6363
Epoch: 70/200
Train loss: 2.4630
Epoch: 71/200
Train loss: 2.2989
Epoch: 72/200
Train loss: 2.3362
Epoch: 73/200
Train loss: 2.4666
Epoch: 74/200
Train loss: 2.4411
Epoch: 75/200
Train loss: 2.4950
Epoch: 76/200
Train loss: 2.4626
Epoch: 77/200
Train loss: 2.4410
Epoch: 78/200
Train loss: 2.3314
Epoch: 79/200
Train loss: 2.1998
Epoch: 80/200
Train loss: 2.3645
Epoch: 81/200
Train loss: 2.3651
Epoch: 82/200
Train loss: 2.3519
Epoch: 83/200
Train loss: 2.3248
Epoch: 84/200
Train loss: 2.3004
Epoch: 85/200
Train loss: 2.4232
Epoch: 86/200
Train loss: 2.3699
Epoch: 87/200
Train loss: 2.2344
Epoch: 88/200
Train loss: 2.4361
Epoch: 89/200
Train loss: 2.4573
Epoch: 90/200
Train loss: 2.2996
Epoch: 91/200
Train loss: 2.4148
Epoch: 92/200
Train loss: 2.4007
Epoch: 93/200
Train loss: 2.4166
Epoch: 94/200
Train loss: 2.3262
Epoch: 95/200
Train loss: 2.3856
Epoch: 96/200
Train loss: 2.4099
Epoch: 97/200
Train loss: 2.3641
Epoch: 98/200
Train loss: 2.3390
Epoch: 99/200
Train loss: 2.2749
Epoch: 100/200
Train loss: 2.3276
Epoch: 101/200
Train loss: 2.3778
Epoch: 102/200
Train loss: 2.1666
0.8125438886245036
Model improve: 0.000000 -> 0.812544
Epoch: 103/200
Train loss: 2.2194
0.8124080435843392
Epoch: 104/200
Train loss: 2.2214
0.8132355823244176
Model improve: 0.812544 -> 0.813236
Epoch: 105/200
Train loss: 2.2715
0.8146790465868743
Model improve: 0.813236 -> 0.814679
Epoch: 106/200
Train loss: 2.2909
0.8119206370433618
Epoch: 107/200
Date :05/16/2023, 08:31:29
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.15
drop_path_rate: 0.4
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 42.1187
Epoch: 2/200
Train loss: 5.7188
Epoch: 3/200
Train loss: 5.2760
Epoch: 4/200
Train loss: 5.0102
Epoch: 5/200
Train loss: 4.6987
Epoch: 6/200
Train loss: 4.4580
Epoch: 7/200
Train loss: 4.1701
Epoch: 8/200
Train loss: 4.1051
Epoch: 9/200
Train loss: 3.9432
Epoch: 10/200
Train loss: 3.8596
Epoch: 11/200
Train loss: 3.7769
Epoch: 12/200
Train loss: 3.6583
Epoch: 13/200
Train loss: 3.6003
Epoch: 14/200
Train loss: 3.4909
Epoch: 15/200
Train loss: 3.5011
Epoch: 16/200
Train loss: 3.4562
Epoch: 17/200
Train loss: 3.3110
Epoch: 18/200
Train loss: 3.2176
Epoch: 19/200
Train loss: 3.4085
Epoch: 20/200
Train loss: 3.2796
Epoch: 21/200
Train loss: 3.1334
Epoch: 22/200
Train loss: 3.0334
Epoch: 23/200
Train loss: 3.0593
Epoch: 24/200
Train loss: 3.1411
Epoch: 25/200
Train loss: 3.0925
Epoch: 26/200
Train loss: 3.0826
Epoch: 27/200
Train loss: 3.0329
Epoch: 28/200
Train loss: 2.9111
Epoch: 29/200
Train loss: 2.9355
Epoch: 30/200
Train loss: 2.7887
Epoch: 31/200
Train loss: 2.9050
Epoch: 32/200
Train loss: 2.9561
Epoch: 33/200
Train loss: 2.7545
Epoch: 34/200
Train loss: 2.8625
Epoch: 35/200
Train loss: 2.8481
Epoch: 36/200
Train loss: 2.8618
Epoch: 37/200
Train loss: 2.7532
Epoch: 38/200
Train loss: 2.9746
Epoch: 39/200
Train loss: 2.6804
Epoch: 40/200
Train loss: 2.7837
Epoch: 41/200
Train loss: 2.7212
Epoch: 42/200
Train loss: 2.7112
Epoch: 43/200
Train loss: 2.7862
Epoch: 44/200
Train loss: 2.7821
Epoch: 45/200
Train loss: 2.7937
Epoch: 46/200
Train loss: 2.5532
Epoch: 47/200
Train loss: 2.7369
Epoch: 48/200
Train loss: 2.4689
Epoch: 49/200
Train loss: 2.4860
Epoch: 50/200
Train loss: 2.5344
Epoch: 51/200
Train loss: 2.5834
Epoch: 52/200
Train loss: 2.5622
Epoch: 53/200
Train loss: 2.5008
Epoch: 54/200
Train loss: 2.5986
Epoch: 55/200
Train loss: 2.5459
Epoch: 56/200
Train loss: 2.5283
Epoch: 57/200
Train loss: 2.5568
Epoch: 58/200
Train loss: 2.5342
Epoch: 59/200
Train loss: 2.5594
Epoch: 60/200
Train loss: 2.5583
Epoch: 61/200
Train loss: 2.4437
Epoch: 62/200
Train loss: 2.4743
Epoch: 63/200
Train loss: 2.4861
Epoch: 64/200
Train loss: 2.5189
Epoch: 65/200
Train loss: 2.5558
Epoch: 66/200
Train loss: 2.4315
Epoch: 67/200
Train loss: 2.5237
Epoch: 68/200
Train loss: 2.4851
Epoch: 69/200
Train loss: 2.6369
Epoch: 70/200
Train loss: 2.4628
Epoch: 71/200
Train loss: 2.2991
Epoch: 72/200
Train loss: 2.3354
Epoch: 73/200
Train loss: 2.4661
Epoch: 74/200
Train loss: 2.4411
Epoch: 75/200
Train loss: 2.4955
Epoch: 76/200
Train loss: 2.4627
Epoch: 77/200
Train loss: 2.4406
Epoch: 78/200
Train loss: 2.3321
Epoch: 79/200
Train loss: 2.1994
Epoch: 80/200
Train loss: 2.3644
Epoch: 81/200
Train loss: 2.3666
Epoch: 82/200
Train loss: 2.3523
Epoch: 83/200
Train loss: 2.3252
Epoch: 84/200
Train loss: 2.3008
Epoch: 85/200
Train loss: 2.4225
Epoch: 86/200
Train loss: 2.3709
Epoch: 87/200
Train loss: 2.2342
Epoch: 88/200
Train loss: 2.4364
Epoch: 89/200
Train loss: 2.4580
Epoch: 90/200
Train loss: 2.2993
Epoch: 91/200
Train loss: 2.4148
Epoch: 92/200
Train loss: 2.4009
Epoch: 93/200
Train loss: 2.4178
Epoch: 94/200
Train loss: 2.3255
Epoch: 95/200
Train loss: 2.3856
Epoch: 96/200
Train loss: 2.4102
Epoch: 97/200
Train loss: 2.3644
Epoch: 98/200
Train loss: 2.3395
Epoch: 99/200
Train loss: 2.2753
Epoch: 100/200
Train loss: 2.3274
Epoch: 101/200
Train loss: 2.3775
Epoch: 102/200
Train loss: 2.1664
0.8125167118777719
Model improve: 0.000000 -> 0.812517
Epoch: 103/200
Train loss: 2.2190
0.8125602883456449
Model improve: 0.812517 -> 0.812560
Epoch: 104/200
Train loss: 2.2203
0.8132146100841384
Model improve: 0.812560 -> 0.813215
Epoch: 105/200
Train loss: 2.2704
0.8145199014110694
Model improve: 0.813215 -> 0.814520
Epoch: 106/200
Train loss: 2.2902
0.8119334033739769
Epoch: 107/200
Train loss: 2.3583
0.8124874097366817
Epoch: 108/200
Train loss: 2.3365
0.8120315617316557
Epoch: 109/200
Train loss: 2.2881
0.8134242557225706
Epoch: 110/200
Train loss: 2.3402
0.8142426958535719
Epoch: 111/200
Train loss: 2.1515
0.8151238474900411
Model improve: 0.814520 -> 0.815124
Epoch: 112/200
Train loss: 2.2577
0.8125762012178632
Epoch: 113/200
Train loss: 2.2563
0.8145796424941794
Epoch: 114/200
Train loss: 2.3543
0.8124710881536594
Epoch: 115/200
Train loss: 2.2504
0.8138707387209617
Epoch: 116/200
Train loss: 2.2908
0.8156890914255452
Model improve: 0.815124 -> 0.815689
Epoch: 117/200
Train loss: 2.2461
0.8131154154983765
Epoch: 118/200
Train loss: 2.2183
0.811155381780733
Epoch: 119/200
Train loss: 2.3157
0.8132352812478489
Epoch: 120/200
Train loss: 2.2049
0.8136914685591526
Epoch: 121/200
Train loss: 2.2102
0.8124423021339201
Epoch: 122/200
Train loss: 2.3038
0.814395645763719
Epoch: 123/200
Train loss: 2.3986
0.8126612186889322
Epoch: 124/200
Train loss: 2.4056
0.8144645141125502
Epoch: 125/200
Train loss: 2.3651
0.8150925715390217
Epoch: 126/200
Train loss: 2.1813
0.8167289829446454
Model improve: 0.815689 -> 0.816729
Epoch: 127/200
Train loss: 2.2660
0.813463766829599
Epoch: 128/200
Train loss: 2.1583
0.8158537480474491
Epoch: 129/200
Train loss: 2.2025
0.8152340671612583
Epoch: 130/200
Train loss: 2.1177
0.8123682615954422
Epoch: 131/200
Train loss: 2.2137
0.81515015126393
Epoch: 132/200
Train loss: 2.3505
0.8121683192015385
Epoch: 133/200
Train loss: 2.1448
0.8147946298192896
Epoch: 134/200
Train loss: 2.2794
0.8147368948886854
Epoch: 135/200
Train loss: 2.1298
0.8149265541755043
Epoch: 136/200
Train loss: 2.1961
0.8149461989521942
Epoch: 137/200
Train loss: 2.2315
0.8138389925864212
Epoch: 138/200
Train loss: 2.1085
0.81526776415267
Epoch: 139/200
Train loss: 2.2450
0.8146270069096483
Epoch: 140/200
Train loss: 2.1657
0.8144690599187688
Epoch: 141/200
Train loss: 2.1855
0.8150077225170861
Epoch: 142/200
Train loss: 2.1586
0.8147941249943385
Epoch: 143/200
Train loss: 2.2967
0.8148304976126435
Epoch: 144/200
Train loss: 2.2302
0.8156960469899946
Epoch: 145/200
Train loss: 2.1203
0.8158516908812685
Epoch: 146/200
Train loss: 2.2748
0.8154443426787306
Epoch: 147/200
Train loss: 2.2494
0.8152325660643396
Epoch: 148/200
Train loss: 2.3115
0.8151378569334177
Epoch: 149/200
Train loss: 2.1894
0.8145886785975431
Epoch: 150/200
Train loss: 2.0027
0.815492591554441
Epoch: 151/200
Train loss: 2.2062
0.8154542123727534
Epoch: 152/200
Train loss: 2.1251
0.815757249607004
Epoch: 153/200
Train loss: 2.0865
0.8153601282284147
Epoch: 154/200
Train loss: 2.1728
0.8150987550889758
Epoch: 155/200
Train loss: 2.2566
0.8127204002810263
Epoch: 156/200
Train loss: 2.2008
0.8159544547600698
Epoch: 157/200
Train loss: 2.1738
0.8150956133500645
Epoch: 158/200
Train loss: 2.0288
0.8179003744570694
Model improve: 0.816729 -> 0.817900
Epoch: 159/200
Train loss: 2.2660
0.8157977368993132
Epoch: 160/200
Train loss: 2.0540
0.8170941346045343
Epoch: 161/200
Train loss: 2.3125
0.8161125333001197
Epoch: 162/200
Train loss: 2.1729
0.8144081591869271
Epoch: 163/200
Train loss: 2.1505
0.815871753516221
Epoch: 164/200
Train loss: 2.2791
0.8137933770119288
Epoch: 165/200
Train loss: 2.3003
0.8140697661815451
Epoch: 166/200
Train loss: 2.2348
0.815684659965403
Epoch: 167/200
Train loss: 2.2956
0.8144955604131059
Epoch: 168/200
Train loss: 2.2132
0.8163856618400558
Epoch: 169/200
Train loss: 2.2546
0.8157478721821759
Epoch: 170/200
Train loss: 2.1313
0.8150684796524714
Epoch: 171/200
Train loss: 2.2627
0.81426479847205
Epoch: 172/200
Train loss: 2.1247
0.8158815696858983
Epoch: 173/200
Train loss: 2.2676
0.8151836879141665
Epoch: 174/200
Train loss: 2.1242
0.8154904172031241
Epoch: 175/200
Train loss: 2.2320
0.8132244045685264
Epoch: 176/200
Train loss: 2.0263
0.8183171013959332
Model improve: 0.817900 -> 0.818317
Epoch: 177/200
Train loss: 2.1308
0.8154958592327384
Epoch: 178/200
Train loss: 2.1419
0.8173100306468015
Epoch: 179/200
Train loss: 2.0894
0.8167192792698922
Epoch: 180/200
Train loss: 2.3004
0.8143905391682782
Epoch: 181/200
Train loss: 2.1830
0.8162717242347123
Epoch: 182/200
Train loss: 2.0853
0.8142037290740093
Epoch: 183/200
Train loss: 2.1305
0.8173485571043615
Epoch: 184/200
Train loss: 2.2803
0.8151589171764432
Epoch: 185/200
Train loss: 2.1482
0.8173813614974992
Epoch: 186/200
Train loss: 2.3063
0.8158502102150113
Epoch: 187/200
Train loss: 2.0869
0.8167601985931217
Epoch: 188/200
Train loss: 2.2916
0.8155313088921459
Epoch: 189/200
Train loss: 2.2536
0.8142830336603151
Epoch: 190/200
Train loss: 2.1534
0.8178582483235933
Epoch: 191/200
Train loss: 2.2649
0.8142790927171335
Epoch: 192/200
Train loss: 2.2021
0.8161795946150836
Epoch: 193/200
Train loss: 2.2345
0.8139975072347316
Epoch: 194/200
Train loss: 2.2361
0.815910263388293
Epoch: 195/200
Train loss: 2.2187
0.8158662784582589
Epoch: 196/200
Train loss: 2.2252
0.8153583603071446
Epoch: 197/200
Train loss: 2.1862
0.8160354460302514
Epoch: 198/200
Train loss: 2.1502
0.8153575486761184
Epoch: 199/200
Train loss: 2.2568
0.8143810708140612
Epoch: 200/200
Train loss: 2.1937
0.8159984775008228
Date :05/16/2023, 16:00:09
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.1
drop_path_rate: 0.4
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 42.1181
Epoch: 2/200
Train loss: 5.7192
Epoch: 3/200
Train loss: 5.2757
Epoch: 4/200
Train loss: 5.0098
Epoch: 5/200
Train loss: 4.6977
Epoch: 6/200
Train loss: 4.4572
Epoch: 7/200
Train loss: 4.1694
Epoch: 8/200
Train loss: 4.1040
Epoch: 9/200
Train loss: 3.9419
Epoch: 10/200
Train loss: 3.8587
Epoch: 11/200
Train loss: 3.7758
Epoch: 12/200
Train loss: 3.6579
Epoch: 13/200
Train loss: 3.5992
Epoch: 14/200
Train loss: 3.4900
Epoch: 15/200
Train loss: 3.5004
Epoch: 16/200
Train loss: 3.4559
Epoch: 17/200
Train loss: 3.3109
Epoch: 18/200
Train loss: 3.2165
Epoch: 19/200
Train loss: 3.4088
Epoch: 20/200
Train loss: 3.2797
Epoch: 21/200
Train loss: 3.1331
Epoch: 22/200
Train loss: 3.0339
Epoch: 23/200
Train loss: 3.0585
Epoch: 24/200
Train loss: 3.1402
Epoch: 25/200
Train loss: 3.0927
Epoch: 26/200
Train loss: 3.0825
Epoch: 27/200
Train loss: 3.0330
Epoch: 28/200
Train loss: 2.9117
Epoch: 29/200
Train loss: 2.9353
Epoch: 30/200
Train loss: 2.7882
Epoch: 31/200
Train loss: 2.9041
Epoch: 32/200
Train loss: 2.9566
Epoch: 33/200
Train loss: 2.7542
Epoch: 34/200
Train loss: 2.8618
Epoch: 35/200
Train loss: 2.8475
Epoch: 36/200
Train loss: 2.8615
Epoch: 37/200
Train loss: 2.7533
Epoch: 38/200
Train loss: 2.9737
Epoch: 39/200
Train loss: 2.6809
Epoch: 40/200
Train loss: 2.7835
Epoch: 41/200
Train loss: 2.7209
Epoch: 42/200
Train loss: 2.7116
Epoch: 43/200
Train loss: 2.7870
Epoch: 44/200
Train loss: 2.7821
Epoch: 45/200
Train loss: 2.7933
Epoch: 46/200
Train loss: 2.5529
Epoch: 47/200
Train loss: 2.7372
Epoch: 48/200
Train loss: 2.4691
Epoch: 49/200
Train loss: 2.4856
Epoch: 50/200
Train loss: 2.5344
Epoch: 51/200
Train loss: 2.5829
Epoch: 52/200
Train loss: 2.5623
Epoch: 53/200
Train loss: 2.5005
Epoch: 54/200
Train loss: 2.5980
Epoch: 55/200
Train loss: 2.5460
Epoch: 56/200
Train loss: 2.5287
Epoch: 57/200
Train loss: 2.5570
Epoch: 58/200
Train loss: 2.5337
Epoch: 59/200
Train loss: 2.5594
Epoch: 60/200
Train loss: 2.5578
Epoch: 61/200
Train loss: 2.4432
Epoch: 62/200
Train loss: 2.4737
Epoch: 63/200
Train loss: 2.4860
Epoch: 64/200
Train loss: 2.5180
Epoch: 65/200
Train loss: 2.5561
Epoch: 66/200
Train loss: 2.4314
Epoch: 67/200
Train loss: 2.5237
Epoch: 68/200
Train loss: 2.4861
Epoch: 69/200
Train loss: 2.6367
Epoch: 70/200
Train loss: 2.4625
Epoch: 71/200
Train loss: 2.2987
Epoch: 72/200
Train loss: 2.3350
Epoch: 73/200
Train loss: 2.4661
Epoch: 74/200
Train loss: 2.4412
Epoch: 75/200
Train loss: 2.4948
Epoch: 76/200
Train loss: 2.4624
Epoch: 77/200
Train loss: 2.4408
Epoch: 78/200
Train loss: 2.3325
Epoch: 79/200
Train loss: 2.2005
Epoch: 80/200
Train loss: 2.3648
Epoch: 81/200
Train loss: 2.3661
Epoch: 82/200
Train loss: 2.3520
Epoch: 83/200
Train loss: 2.3244
Epoch: 84/200
Train loss: 2.3008
Epoch: 85/200
Train loss: 2.4226
Epoch: 86/200
Train loss: 2.3703
Epoch: 87/200
Train loss: 2.2334
Epoch: 88/200
Train loss: 2.4373
Epoch: 89/200
Train loss: 2.4574
Epoch: 90/200
Train loss: 2.2991
Epoch: 91/200
Train loss: 2.4138
Epoch: 92/200
Train loss: 2.4013
Epoch: 93/200
Train loss: 2.4177
Epoch: 94/200
Train loss: 2.3257
Epoch: 95/200
Train loss: 2.3863
Epoch: 96/200
Train loss: 2.4101
Epoch: 97/200
Train loss: 2.3649
Epoch: 98/200
Train loss: 2.3396
Epoch: 99/200
Train loss: 2.2746
Epoch: 100/200
Train loss: 2.3279
Epoch: 101/200
Train loss: 2.3776
Epoch: 102/200
Train loss: 2.1671
0.8125584143582301
Model improve: 0.000000 -> 0.812558
Epoch: 103/200
Train loss: 2.2201
0.8123339765058684
Epoch: 104/200
Train loss: 2.2210
0.8132966671595531
Model improve: 0.812558 -> 0.813297
Epoch: 105/200
Train loss: 2.2708
0.8144042586318667
Model improve: 0.813297 -> 0.814404
Epoch: 106/200
Train loss: 2.2910
0.8120030581743937
Epoch: 107/200
Train loss: 2.3586
0.8122136157444557
Epoch: 108/200
Train loss: 2.3351
0.8121362387101887
Epoch: 109/200
Train loss: 2.2881
0.813110662214117
Epoch: 110/200
Train loss: 2.3400
0.8140773807775996
Epoch: 111/200
Train loss: 2.1513
0.8147795076218625
Model improve: 0.814404 -> 0.814780
Epoch: 112/200
Train loss: 2.2578
0.8122825027379212
Epoch: 113/200
Train loss: 2.2563
0.814473670058823
Epoch: 114/200
Train loss: 2.3537
0.8122446863454583
Epoch: 115/200
Train loss: 2.2509
Date :05/16/2023, 18:23:48
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b2
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.15
drop_path_rate: 0.4
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 41.8346
Epoch: 2/200
Train loss: 5.7064
Epoch: 3/200
Train loss: 5.2158
Epoch: 4/200
Train loss: 4.9573
Epoch: 5/200
Train loss: 4.6584
Epoch: 6/200
Train loss: 4.4169
Epoch: 7/200
Train loss: 4.1120
Epoch: 8/200
Train loss: 4.0754
Epoch: 9/200
Train loss: 3.9088
Epoch: 10/200
Train loss: 3.8337
Epoch: 11/200
Train loss: 3.7210
Epoch: 12/200
Train loss: 3.6205
Epoch: 13/200
Train loss: 3.5881
Epoch: 14/200
Train loss: 3.4588
Epoch: 15/200
Train loss: 3.4746
Epoch: 16/200
Train loss: 3.4097
Epoch: 17/200
Train loss: 3.2680
Epoch: 18/200
Train loss: 3.1621
Epoch: 19/200
Train loss: 3.3738
Epoch: 20/200
Train loss: 3.2480
Epoch: 21/200
Train loss: 3.1067
Epoch: 22/200
Train loss: 2.9792
Epoch: 23/200
Train loss: 3.0268
Epoch: 24/200
Train loss: 3.0952
Epoch: 25/200
Train loss: 3.0531
Epoch: 26/200
Train loss: 3.0283
Epoch: 27/200
Train loss: 3.0340
Epoch: 28/200
Train loss: 2.8689
Epoch: 29/200
Train loss: 2.8905
Epoch: 30/200
Train loss: 2.7191
Epoch: 31/200
Train loss: 2.8701
Epoch: 32/200
Train loss: 2.9239
Epoch: 33/200
Train loss: 2.7098
Epoch: 34/200
Train loss: 2.8055
Epoch: 35/200
Train loss: 2.7902
Epoch: 36/200
Train loss: 2.8277
Epoch: 37/200
Train loss: 2.6892
Epoch: 38/200
Train loss: 2.9331
Epoch: 39/200
Train loss: 2.6138
Epoch: 40/200
Train loss: 2.7341
Epoch: 41/200
Train loss: 2.6753
Epoch: 42/200
Train loss: 2.6693
Epoch: 43/200
Train loss: 2.7694
Epoch: 44/200
Train loss: 2.7327
Epoch: 45/200
Train loss: 2.7434
Epoch: 46/200
Train loss: 2.4976
Epoch: 47/200
Train loss: 2.6919
Epoch: 48/200
Train loss: 2.4409
Epoch: 49/200
Train loss: 2.4283
Epoch: 50/200
Train loss: 2.4963
Epoch: 51/200
Train loss: 2.5464
Epoch: 52/200
Train loss: 2.5208
Epoch: 53/200
Train loss: 2.4678
Epoch: 54/200
Train loss: 2.5683
Epoch: 55/200
Train loss: 2.5268
Epoch: 56/200
Train loss: 2.4775
Epoch: 57/200
Train loss: 2.5041
Epoch: 58/200
Train loss: 2.4979
Epoch: 59/200
Train loss: 2.4946
Epoch: 60/200
Train loss: 2.5269
Epoch: 61/200
Train loss: 2.4029
Epoch: 62/200
Train loss: 2.4203
Epoch: 63/200
Train loss: 2.4625
Epoch: 64/200
Train loss: 2.4494
Epoch: 65/200
Train loss: 2.4887
Epoch: 66/200
Train loss: 2.4009
Epoch: 67/200
Train loss: 2.4682
Epoch: 68/200
Train loss: 2.4539
Epoch: 69/200
Train loss: 2.6024
Epoch: 70/200
Train loss: 2.3916
Epoch: 71/200
Train loss: 2.2777
Epoch: 72/200
Train loss: 2.2834
Epoch: 73/200
Train loss: 2.4232
Epoch: 74/200
Train loss: 2.4095
Epoch: 75/200
Train loss: 2.4483
Epoch: 76/200
Train loss: 2.4167
Epoch: 77/200
Train loss: 2.3887
Epoch: 78/200
Train loss: 2.3019
Epoch: 79/200
Train loss: 2.1390
Epoch: 80/200
Train loss: 2.2936
Epoch: 81/200
Train loss: 2.3227
Epoch: 82/200
Train loss: 2.3292
Epoch: 83/200
Train loss: 2.2701
Epoch: 84/200
Train loss: 2.2564
Epoch: 85/200
Train loss: 2.3753
Epoch: 86/200
Train loss: 2.3195
Epoch: 87/200
Train loss: 2.1974
Epoch: 88/200
Train loss: 2.3882
Epoch: 89/200
Train loss: 2.4041
Epoch: 90/200
Train loss: 2.2645
Epoch: 91/200
Train loss: 2.3741
Epoch: 92/200
Train loss: 2.3411
Epoch: 93/200
Train loss: 2.3815
Epoch: 94/200
Train loss: 2.2790
Epoch: 95/200
Train loss: 2.3278
Epoch: 96/200
Train loss: 2.3922
Epoch: 97/200
Train loss: 2.3252
Epoch: 98/200
Train loss: 2.2998
Epoch: 99/200
Train loss: 2.2186
Epoch: 100/200
Train loss: 2.2775
Epoch: 101/200
Train loss: 2.3281
Epoch: 102/200
Train loss: 2.1149
0.8130193544695101
Model improve: 0.000000 -> 0.813019
Epoch: 103/200
Train loss: 2.1773
0.8110551748633198
Epoch: 104/200
Train loss: 2.1915
0.8127297901068693
Epoch: 105/200
Train loss: 2.2353
0.8122365135359804
Epoch: 106/200
Train loss: 2.2327
0.8122143068531142
Epoch: 107/200
Train loss: 2.2959
0.8114322687077646
Epoch: 108/200
Train loss: 2.2839
0.8116186557063451
Epoch: 109/200
Train loss: 2.2714
0.8117210107580147
Epoch: 110/200
Train loss: 2.3204
0.8111119124383682
Epoch: 111/200
Train loss: 2.1044
0.8131373780103991
Model improve: 0.813019 -> 0.813137
Epoch: 112/200
Train loss: 2.2228
0.8100347013721786
Epoch: 113/200
Train loss: 2.2004
0.8125536922092748
Epoch: 114/200
Train loss: 2.3189
0.8112248459274821
Epoch: 115/200
Train loss: 2.1891
0.8125671396417689
Epoch: 116/200
Train loss: 2.2509
0.8124690554792655
Epoch: 117/200
Train loss: 2.2019
0.8124900361903331
Epoch: 118/200
Train loss: 2.1742
0.8123787310740074
Epoch: 119/200
Train loss: 2.2944
0.8125658677200703
Epoch: 120/200
Train loss: 2.1793
0.8132897699444
Model improve: 0.813137 -> 0.813290
Epoch: 121/200
Train loss: 2.1853
0.8134919331065262
Model improve: 0.813290 -> 0.813492
Epoch: 122/200
Train loss: 2.2602
0.8146546747596407
Model improve: 0.813492 -> 0.814655
Epoch: 123/200
Train loss: 2.3595
0.8132947011047992
Epoch: 124/200
Train loss: 2.3595
0.8130676843003949
Epoch: 125/200
Train loss: 2.3077
0.8143867677865098
Epoch: 126/200
Train loss: 2.1521
0.8133922113760161
Epoch: 127/200
Train loss: 2.2298
0.8125672171349019
Epoch: 128/200
Train loss: 2.1138
0.8139466729050582
Epoch: 129/200
Train loss: 2.1446
0.8146517801504972
Epoch: 130/200
Train loss: 2.0698
0.8130353983935574
Epoch: 131/200
Train loss: 2.1641
0.814939904145606
Model improve: 0.814655 -> 0.814940
Epoch: 132/200
Train loss: 2.2836
0.8139097423460344
Epoch: 133/200
Train loss: 2.0943
0.8153344843131625
Model improve: 0.814940 -> 0.815334
Epoch: 134/200
Train loss: 2.2248
0.8145207447511028
Epoch: 135/200
Train loss: 2.0687
0.8150092201627133
Epoch: 136/200
Train loss: 2.1597
0.8142542314982024
Epoch: 137/200
Train loss: 2.1840
0.812679708999841
Epoch: 138/200
Train loss: 2.0818
0.8150134735712985
Epoch: 139/200
Train loss: 2.2017
0.813307912630761
Epoch: 140/200
Train loss: 2.1479
0.813623676300976
Epoch: 141/200
Train loss: 2.1449
0.8143220005716052
Epoch: 142/200
Train loss: 2.0987
0.8142765314023213
Epoch: 143/200
Train loss: 2.2501
0.8145954264184586
Epoch: 144/200
Train loss: 2.2121
0.8148173674141849
Epoch: 145/200
Train loss: 2.0724
0.8145771771522041
Epoch: 146/200
Train loss: 2.2296
0.814401180292405
Epoch: 147/200
Train loss: 2.2204
0.8140854422809288
Epoch: 148/200
Train loss: 2.2625
0.8136202420062042
Epoch: 149/200
Train loss: 2.1324
0.8139122376023173
Epoch: 150/200
Train loss: 1.9767
0.8139348393681263
Epoch: 151/200
Train loss: 2.1604
0.8140598825773326
Epoch: 152/200
Train loss: 2.0747
0.8140402976195948
Epoch: 153/200
Train loss: 2.0259
0.8135213618750711
Epoch: 154/200
Train loss: 2.1522
0.8140356065713608
Epoch: 155/200
Train loss: 2.2186
Date :05/16/2023, 23:08:40
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b2
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.15
drop_path_rate: 0.4
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 41.8221
Epoch: 2/200
Train loss: 5.7049
Epoch: 3/200
Train loss: 5.2142
Epoch: 4/200
Train loss: 4.9568
Epoch: 5/200
Train loss: 4.6579
Epoch: 6/200
Train loss: 4.4165
Epoch: 7/200
Train loss: 4.1117
Epoch: 8/200
Train loss: 4.0756
Epoch: 9/200
Train loss: 3.9093
Epoch: 10/200
Train loss: 3.8339
Epoch: 11/200
Train loss: 3.7210
Epoch: 12/200
Train loss: 3.6206
Epoch: 13/200
Train loss: 3.5883
Epoch: 14/200
Train loss: 3.4586
Epoch: 15/200
Train loss: 3.4747
Epoch: 16/200
Train loss: 3.4091
Epoch: 17/200
Train loss: 3.2680
Epoch: 18/200
Train loss: 3.1617
Epoch: 19/200
Train loss: 3.3737
Epoch: 20/200
Train loss: 3.2480
Epoch: 21/200
Train loss: 3.1062
Epoch: 22/200
Train loss: 2.9786
Epoch: 23/200
Train loss: 3.0262
Epoch: 24/200
Train loss: 3.0960
Epoch: 25/200
Train loss: 3.0531
Epoch: 26/200
Train loss: 3.0286
Epoch: 27/200
Train loss: 3.0338
Epoch: 28/200
Train loss: 2.8684
Epoch: 29/200
Train loss: 2.8908
Epoch: 30/200
Train loss: 2.7189
Epoch: 31/200
Train loss: 2.8706
Epoch: 32/200
Train loss: 2.9236
Epoch: 33/200
Train loss: 2.7087
Epoch: 34/200
Train loss: 2.8052
Epoch: 35/200
Train loss: 2.7906
Epoch: 36/200
Train loss: 2.8279
Epoch: 37/200
Train loss: 2.6892
Epoch: 38/200
Train loss: 2.9338
Epoch: 39/200
Train loss: 2.6133
Epoch: 40/200
Train loss: 2.7337
Epoch: 41/200
Train loss: 2.6755
Epoch: 42/200
Train loss: 2.6695
Epoch: 43/200
Train loss: 2.7696
Epoch: 44/200
Train loss: 2.7324
Epoch: 45/200
Train loss: 2.7431
Epoch: 46/200
Train loss: 2.4983
Epoch: 47/200
Train loss: 2.6921
Epoch: 48/200
Train loss: 2.4403
Epoch: 49/200
Train loss: 2.4285
Epoch: 50/200
Train loss: 2.4965
Epoch: 51/200
Train loss: 2.5466
Epoch: 52/200
Train loss: 2.5202
Epoch: 53/200
Train loss: 2.4678
Epoch: 54/200
Train loss: 2.5678
Epoch: 55/200
Train loss: 2.5271
Epoch: 56/200
Train loss: 2.4769
Epoch: 57/200
Train loss: 2.5038
Epoch: 58/200
Train loss: 2.4977
Epoch: 59/200
Train loss: 2.4943
Epoch: 60/200
Train loss: 2.5265
Epoch: 61/200
Train loss: 2.4038
Epoch: 62/200
Train loss: 2.4200
Epoch: 63/200
Train loss: 2.4621
Epoch: 64/200
Train loss: 2.4486
Epoch: 65/200
Train loss: 2.4900
Epoch: 66/200
Train loss: 2.4003
Epoch: 67/200
Train loss: 2.4689
Epoch: 68/200
Train loss: 2.4535
Epoch: 69/200
Train loss: 2.6024
Epoch: 70/200
Train loss: 2.3915
Epoch: 71/200
Train loss: 2.2771
Epoch: 72/200
Train loss: 2.2839
Epoch: 73/200
Train loss: 2.4229
Epoch: 74/200
Train loss: 2.4094
Epoch: 75/200
Train loss: 2.4483
Epoch: 76/200
Train loss: 2.4159
Epoch: 77/200
Train loss: 2.3892
Epoch: 78/200
Train loss: 2.3020
Epoch: 79/200
Train loss: 2.1394
Epoch: 80/200
Train loss: 2.2938
Epoch: 81/200
Train loss: 2.3231
Epoch: 82/200
Train loss: 2.3294
Epoch: 83/200
Train loss: 2.2693
Epoch: 84/200
Train loss: 2.2570
Epoch: 85/200
Train loss: 2.3760
Epoch: 86/200
Train loss: 2.3198
Epoch: 87/200
Train loss: 2.1976
Epoch: 88/200
Train loss: 2.3881
Epoch: 89/200
Train loss: 2.4044
Epoch: 90/200
Train loss: 2.2645
Epoch: 91/200
Train loss: 2.3743
Epoch: 92/200
Train loss: 2.3405
Epoch: 93/200
Train loss: 2.3812
Epoch: 94/200
Train loss: 2.2784
Epoch: 95/200
Train loss: 2.3278
Epoch: 96/200
Train loss: 2.3918
Epoch: 97/200
Train loss: 2.3257
Epoch: 98/200
Train loss: 2.2995
Epoch: 99/200
Train loss: 2.2184
Epoch: 100/200
Train loss: 2.2776
Epoch: 101/200
Train loss: 2.3292
Epoch: 102/200
Train loss: 2.1150
0.8131242403707917
Model improve: 0.000000 -> 0.813124
Epoch: 103/200
Train loss: 2.1775
0.81106520164957
Epoch: 104/200
Train loss: 2.1915
0.8126996265175158
Epoch: 105/200
Train loss: 2.2357
0.8121924245224534
Epoch: 106/200
Train loss: 2.2327
0.8121580295485841
Epoch: 107/200
Train loss: 2.2954
0.8113295642007143
Epoch: 108/200
Train loss: 2.2844
0.8115722195198516
Epoch: 109/200
Train loss: 2.2710
0.811715425030893
Epoch: 110/200
Train loss: 2.3208
0.8110246352227172
Epoch: 111/200
Date :05/17/2023, 01:14:17
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b2
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.3
drop_path_rate: 0.45
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 42.9619
Epoch: 2/200
Train loss: 5.7413
Epoch: 3/200
Train loss: 5.2534
Epoch: 4/200
Train loss: 4.9981
Epoch: 5/200
Train loss: 4.6934
Epoch: 6/200
Train loss: 4.4551
Epoch: 7/200
Train loss: 4.1466
Epoch: 8/200
Train loss: 4.1106
Epoch: 9/200
Train loss: 3.9523
Epoch: 10/200
Train loss: 3.8690
Epoch: 11/200
Train loss: 3.7582
Epoch: 12/200
Train loss: 3.6568
Epoch: 13/200
Train loss: 3.6205
Epoch: 14/200
Train loss: 3.4914
Epoch: 15/200
Train loss: 3.5093
Epoch: 16/200
Train loss: 3.4420
Epoch: 17/200
Train loss: 3.2988
Epoch: 18/200
Train loss: 3.1966
Epoch: 19/200
Train loss: 3.4044
Epoch: 20/200
Train loss: 3.2813
Epoch: 21/200
Train loss: 3.1378
Epoch: 22/200
Train loss: 3.0129
Epoch: 23/200
Train loss: 3.0577
Epoch: 24/200
Train loss: 3.1279
Epoch: 25/200
Train loss: 3.0838
Epoch: 26/200
Train loss: 3.0604
Epoch: 27/200
Train loss: 3.0608
Epoch: 28/200
Train loss: 2.8998
Epoch: 29/200
Train loss: 2.9217
Epoch: 30/200
Train loss: 2.7530
Epoch: 31/200
Train loss: 2.9002
Epoch: 32/200
Train loss: 2.9527
Epoch: 33/200
Train loss: 2.7408
Epoch: 34/200
Train loss: 2.8361
Epoch: 35/200
Train loss: 2.8216
Epoch: 36/200
Train loss: 2.8555
Epoch: 37/200
Train loss: 2.7181
Epoch: 38/200
Train loss: 2.9602
Epoch: 39/200
Train loss: 2.6419
Epoch: 40/200
Train loss: 2.7659
Epoch: 41/200
Train loss: 2.7039
Epoch: 42/200
Train loss: 2.6974
Epoch: 43/200
Train loss: 2.7950
Epoch: 44/200
Train loss: 2.7607
Epoch: 45/200
Train loss: 2.7701
Epoch: 46/200
Train loss: 2.5250
Epoch: 47/200
Train loss: 2.7176
Epoch: 48/200
Train loss: 2.4690
Epoch: 49/200
Train loss: 2.4560
Epoch: 50/200
Train loss: 2.5230
Epoch: 51/200
Train loss: 2.5743
Epoch: 52/200
Train loss: 2.5469
Epoch: 53/200
Train loss: 2.4953
Epoch: 54/200
Train loss: 2.5941
Epoch: 55/200
Train loss: 2.5549
Epoch: 56/200
Train loss: 2.5031
Epoch: 57/200
Train loss: 2.5295
Epoch: 58/200
Train loss: 2.5226
Epoch: 59/200
Train loss: 2.5175
Epoch: 60/200
Train loss: 2.5522
Epoch: 61/200
Train loss: 2.4301
Epoch: 62/200
Train loss: 2.4488
Epoch: 63/200
Train loss: 2.4846
Epoch: 64/200
Train loss: 2.4736
Epoch: 65/200
Train loss: 2.5142
Epoch: 66/200
Train loss: 2.4249
Epoch: 67/200
Train loss: 2.4955
Epoch: 68/200
Train loss: 2.4753
Epoch: 69/200
Train loss: 2.6276
Epoch: 70/200
Train loss: 2.4159
Epoch: 71/200
Train loss: 2.3015
Epoch: 72/200
Train loss: 2.3089
Epoch: 73/200
Train loss: 2.4475
Epoch: 74/200
Train loss: 2.4347
Epoch: 75/200
Train loss: 2.4695
Epoch: 76/200
Train loss: 2.4389
Epoch: 77/200
Train loss: 2.4127
Epoch: 78/200
Train loss: 2.3268
Epoch: 79/200
Train loss: 2.1643
Epoch: 80/200
Train loss: 2.3173
Epoch: 81/200
Train loss: 2.3459
Epoch: 82/200
Train loss: 2.3527
Epoch: 83/200
Train loss: 2.2955
Epoch: 84/200
Train loss: 2.2793
Epoch: 85/200
Train loss: 2.3981
Epoch: 86/200
Train loss: 2.3414
Epoch: 87/200
Train loss: 2.2186
Epoch: 88/200
Train loss: 2.4122
Epoch: 89/200
Train loss: 2.4275
Epoch: 90/200
Train loss: 2.2856
Epoch: 91/200
Train loss: 2.3968
Epoch: 92/200
Train loss: 2.3664
Epoch: 93/200
Train loss: 2.4044
Epoch: 94/200
Train loss: 2.3024
Epoch: 95/200
Train loss: 2.3495
Epoch: 96/200
Train loss: 2.4132
Epoch: 97/200
Train loss: 2.3491
Epoch: 98/200
Train loss: 2.3231
Epoch: 99/200
Train loss: 2.2412
Epoch: 100/200
Train loss: 2.3024
Epoch: 101/200
Train loss: 2.3506
Epoch: 102/200
Train loss: 2.1368
0.8148941164889827
Model improve: 0.000000 -> 0.814894
Epoch: 103/200
Train loss: 2.1998
0.8125008502808777
Epoch: 104/200
Train loss: 2.2137
0.8142188544020733
Epoch: 105/200
Train loss: 2.2577
0.8137944311534574
Epoch: 106/200
Train loss: 2.2541
Date :05/17/2023, 03:05:55
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.3
drop_path_rate: 0.4
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 42.1143
Epoch: 2/200
Train loss: 5.7188
Epoch: 3/200
Train loss: 5.2756
Epoch: 4/200
Train loss: 5.0100
Epoch: 5/200
Train loss: 4.6978
Epoch: 6/200
Train loss: 4.4566
Epoch: 7/200
Train loss: 4.1691
Epoch: 8/200
Train loss: 4.1040
Epoch: 9/200
Train loss: 3.9423
Epoch: 10/200
Train loss: 3.8591
Epoch: 11/200
Train loss: 3.7760
Epoch: 12/200
Train loss: 3.6582
Epoch: 13/200
Train loss: 3.5992
Epoch: 14/200
Train loss: 3.4903
Epoch: 15/200
Train loss: 3.5006
Epoch: 16/200
Train loss: 3.4559
Epoch: 17/200
Train loss: 3.3111
Epoch: 18/200
Train loss: 3.2172
Epoch: 19/200
Train loss: 3.4085
Epoch: 20/200
Train loss: 3.2797
Epoch: 21/200
Train loss: 3.1334
Epoch: 22/200
Train loss: 3.0334
Epoch: 23/200
Train loss: 3.0593
Epoch: 24/200
Train loss: 3.1399
Epoch: 25/200
Train loss: 3.0931
Epoch: 26/200
Train loss: 3.0825
Epoch: 27/200
Train loss: 3.0326
Epoch: 28/200
Train loss: 2.9119
Epoch: 29/200
Train loss: 2.9355
Epoch: 30/200
Train loss: 2.7882
Epoch: 31/200
Train loss: 2.9040
Epoch: 32/200
Train loss: 2.9567
Epoch: 33/200
Train loss: 2.7532
Epoch: 34/200
Train loss: 2.8622
Epoch: 35/200
Train loss: 2.8477
Epoch: 36/200
Train loss: 2.8610
Epoch: 37/200
Train loss: 2.7542
Epoch: 38/200
Train loss: 2.9740
Epoch: 39/200
Train loss: 2.6801
Epoch: 40/200
Train loss: 2.7835
Epoch: 41/200
Train loss: 2.7204
Epoch: 42/200
Train loss: 2.7112
Epoch: 43/200
Train loss: 2.7857
Epoch: 44/200
Train loss: 2.7818
Epoch: 45/200
Train loss: 2.7930
Epoch: 46/200
Train loss: 2.5532
Epoch: 47/200
Train loss: 2.7371
Epoch: 48/200
Train loss: 2.4692
Epoch: 49/200
Train loss: 2.4858
Epoch: 50/200
Train loss: 2.5347
Epoch: 51/200
Train loss: 2.5825
Epoch: 52/200
Train loss: 2.5625
Epoch: 53/200
Train loss: 2.5002
Epoch: 54/200
Train loss: 2.5985
Epoch: 55/200
Train loss: 2.5457
Epoch: 56/200
Train loss: 2.5297
Epoch: 57/200
Train loss: 2.5563
Epoch: 58/200
Train loss: 2.5338
Epoch: 59/200
Train loss: 2.5600
Epoch: 60/200
Train loss: 2.5580
Epoch: 61/200
Train loss: 2.4434
Epoch: 62/200
Train loss: 2.4733
Epoch: 63/200
Train loss: 2.4858
Epoch: 64/200
Train loss: 2.5182
Epoch: 65/200
Train loss: 2.5559
Epoch: 66/200
Train loss: 2.4311
Epoch: 67/200
Train loss: 2.5231
Epoch: 68/200
Train loss: 2.4856
Epoch: 69/200
Train loss: 2.6362
Epoch: 70/200
Train loss: 2.4625
Epoch: 71/200
Train loss: 2.2992
Epoch: 72/200
Train loss: 2.3357
Epoch: 73/200
Train loss: 2.4659
Epoch: 74/200
Train loss: 2.4409
Epoch: 75/200
Train loss: 2.4953
Epoch: 76/200
Train loss: 2.4624
Epoch: 77/200
Train loss: 2.4405
Epoch: 78/200
Train loss: 2.3321
Epoch: 79/200
Train loss: 2.1997
Epoch: 80/200
Train loss: 2.3649
Epoch: 81/200
Train loss: 2.3665
Epoch: 82/200
Train loss: 2.3523
Epoch: 83/200
Train loss: 2.3248
Epoch: 84/200
Train loss: 2.3009
Epoch: 85/200
Train loss: 2.4224
Epoch: 86/200
Train loss: 2.3705
Epoch: 87/200
Train loss: 2.2342
Epoch: 88/200
Train loss: 2.4363
Epoch: 89/200
Train loss: 2.4577
Epoch: 90/200
Train loss: 2.2994
Epoch: 91/200
Train loss: 2.4139
Epoch: 92/200
Train loss: 2.4006
Epoch: 93/200
Train loss: 2.4175
Epoch: 94/200
Train loss: 2.3250
Epoch: 95/200
Train loss: 2.3863
Epoch: 96/200
Train loss: 2.4096
Epoch: 97/200
Train loss: 2.3651
Epoch: 98/200
Train loss: 2.3399
Epoch: 99/200
Train loss: 2.2749
Epoch: 100/200
Train loss: 2.3276
Epoch: 101/200
Train loss: 2.3771
Epoch: 102/200
Train loss: 2.1670
0.8123106324496299
Model improve: 0.000000 -> 0.812311
Epoch: 103/200
Train loss: 2.2200
0.8123536222156187
Model improve: 0.812311 -> 0.812354
Epoch: 104/200
Train loss: 2.2211
0.8131262212724322
Model improve: 0.812354 -> 0.813126
Epoch: 105/200
Train loss: 2.2718
Date :05/17/2023, 04:52:17
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.2
drop_path_rate: 0.4
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 42.1168
Epoch: 2/200
Train loss: 5.7184
Epoch: 3/200
Train loss: 5.2749
Epoch: 4/200
Train loss: 5.0090
Epoch: 5/200
Train loss: 4.6971
Epoch: 6/200
Train loss: 4.4565
Epoch: 7/200
Train loss: 4.1689
Epoch: 8/200
Train loss: 4.1038
Epoch: 9/200
Train loss: 3.9426
Epoch: 10/200
Train loss: 3.8591
Epoch: 11/200
Train loss: 3.7762
Epoch: 12/200
Train loss: 3.6579
Epoch: 13/200
Train loss: 3.5997
Epoch: 14/200
Train loss: 3.4901
Epoch: 15/200
Train loss: 3.5009
Epoch: 16/200
Train loss: 3.4560
Epoch: 17/200
Train loss: 3.3109
Epoch: 18/200
Train loss: 3.2167
Epoch: 19/200
Train loss: 3.4089
Epoch: 20/200
Train loss: 3.2792
Epoch: 21/200
Train loss: 3.1335
Epoch: 22/200
Train loss: 3.0332
Epoch: 23/200
Train loss: 3.0591
Epoch: 24/200
Train loss: 3.1406
Epoch: 25/200
Train loss: 3.0929
Epoch: 26/200
Train loss: 3.0825
Epoch: 27/200
Train loss: 3.0331
Epoch: 28/200
Train loss: 2.9115
Epoch: 29/200
Train loss: 2.9358
Epoch: 30/200
Train loss: 2.7880
Epoch: 31/200
Train loss: 2.9051
Epoch: 32/200
Train loss: 2.9570
Epoch: 33/200
Train loss: 2.7538
Epoch: 34/200
Train loss: 2.8617
Epoch: 35/200
Train loss: 2.8473
Epoch: 36/200
Train loss: 2.8610
Epoch: 37/200
Train loss: 2.7539
Epoch: 38/200
Train loss: 2.9744
Epoch: 39/200
Train loss: 2.6803
Epoch: 40/200
Train loss: 2.7834
Epoch: 41/200
Train loss: 2.7211
Epoch: 42/200
Train loss: 2.7112
Epoch: 43/200
Train loss: 2.7857
Epoch: 44/200
Train loss: 2.7823
Epoch: 45/200
Train loss: 2.7939
Epoch: 46/200
Train loss: 2.5525
Epoch: 47/200
Train loss: 2.7368
Epoch: 48/200
Train loss: 2.4689
Epoch: 49/200
Train loss: 2.4852
Epoch: 50/200
Train loss: 2.5351
Epoch: 51/200
Train loss: 2.5829
Epoch: 52/200
Train loss: 2.5625
Epoch: 53/200
Train loss: 2.5013
Epoch: 54/200
Train loss: 2.5982
Epoch: 55/200
Train loss: 2.5465
Epoch: 56/200
Train loss: 2.5293
Epoch: 57/200
Train loss: 2.5562
Epoch: 58/200
Train loss: 2.5335
Epoch: 59/200
Train loss: 2.5598
Epoch: 60/200
Train loss: 2.5578
Epoch: 61/200
Train loss: 2.4430
Epoch: 62/200
Train loss: 2.4740
Epoch: 63/200
Train loss: 2.4861
Epoch: 64/200
Train loss: 2.5186
Epoch: 65/200
Train loss: 2.5563
Epoch: 66/200
Train loss: 2.4317
Epoch: 67/200
Train loss: 2.5236
Epoch: 68/200
Train loss: 2.4850
Epoch: 69/200
Train loss: 2.6367
Epoch: 70/200
Train loss: 2.4626
Epoch: 71/200
Train loss: 2.2989
Epoch: 72/200
Train loss: 2.3355
Epoch: 73/200
Train loss: 2.4664
Epoch: 74/200
Train loss: 2.4413
Epoch: 75/200
Train loss: 2.4957
Epoch: 76/200
Train loss: 2.4626
Epoch: 77/200
Train loss: 2.4406
Epoch: 78/200
Train loss: 2.3324
Epoch: 79/200
Train loss: 2.1995
Epoch: 80/200
Train loss: 2.3654
Epoch: 81/200
Train loss: 2.3659
Epoch: 82/200
Train loss: 2.3524
Epoch: 83/200
Train loss: 2.3251
Epoch: 84/200
Train loss: 2.3009
Epoch: 85/200
Train loss: 2.4219
Epoch: 86/200
Train loss: 2.3707
Epoch: 87/200
Train loss: 2.2342
Epoch: 88/200
Train loss: 2.4372
Epoch: 89/200
Train loss: 2.4578
Epoch: 90/200
Train loss: 2.2988
Epoch: 91/200
Train loss: 2.4143
Epoch: 92/200
Train loss: 2.4009
Epoch: 93/200
Train loss: 2.4170
Epoch: 94/200
Train loss: 2.3249
Epoch: 95/200
Train loss: 2.3858
Epoch: 96/200
Train loss: 2.4097
Epoch: 97/200
Train loss: 2.3641
Epoch: 98/200
Train loss: 2.3396
Epoch: 99/200
Train loss: 2.2752
Epoch: 100/200
Train loss: 2.3281
Epoch: 101/200
Train loss: 2.3774
Epoch: 102/200
Train loss: 2.1668
0.8122928169303931
Model improve: 0.000000 -> 0.812293
Epoch: 103/200
Train loss: 2.2197
0.81240636008268
Model improve: 0.812293 -> 0.812406
Epoch: 104/200
Train loss: 2.2209
Date :05/17/2023, 06:36:02
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.25
drop_path_rate: 0.4
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 42.0701
Epoch: 2/200
Train loss: 5.7084
Epoch: 3/200
Train loss: 5.2596
Epoch: 4/200
Train loss: 5.0087
Epoch: 5/200
Train loss: 4.6939
Epoch: 6/200
Train loss: 4.4554
Epoch: 7/200
Train loss: 4.1683
Epoch: 8/200
Date :05/17/2023, 06:43:17
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.25
drop_path_rate: 0.4
76407
Fold: 4
13830
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 42.0817
Epoch: 2/200
Train loss: 5.7104
Epoch: 3/200
Train loss: 5.2569
Epoch: 4/200
Train loss: 4.9834
Epoch: 5/200
Train loss: 4.6878
Epoch: 6/200
Train loss: 4.4528
Epoch: 7/200
Train loss: 4.1471
Epoch: 8/200
Train loss: 4.0961
Epoch: 9/200
Train loss: 3.9296
Epoch: 10/200
Train loss: 3.8375
Epoch: 11/200
Train loss: 3.7585
Epoch: 12/200
Train loss: 3.6443
Epoch: 13/200
Train loss: 3.6083
Epoch: 14/200
Train loss: 3.4677
Epoch: 15/200
Train loss: 3.4930
Epoch: 16/200
Train loss: 3.4320
Epoch: 17/200
Train loss: 3.2880
Epoch: 18/200
Train loss: 3.2006
Epoch: 19/200
Train loss: 3.4092
Epoch: 20/200
Train loss: 3.2835
Epoch: 21/200
Train loss: 3.1298
Epoch: 22/200
Train loss: 2.9917
Epoch: 23/200
Train loss: 3.0532
Epoch: 24/200
Train loss: 3.1295
Epoch: 25/200
Train loss: 3.0739
Epoch: 26/200
Train loss: 3.0568
Epoch: 27/200
Train loss: 3.0271
Epoch: 28/200
Train loss: 2.8977
Epoch: 29/200
Train loss: 2.9242
Epoch: 30/200
Train loss: 2.7660
Epoch: 31/200
Train loss: 2.9158
Epoch: 32/200
Train loss: 2.9454
Epoch: 33/200
Train loss: 2.7486
Epoch: 34/200
Train loss: 2.8388
Epoch: 35/200
Train loss: 2.8467
Epoch: 36/200
Train loss: 2.8548
Epoch: 37/200
Train loss: 2.7445
Epoch: 38/200
Train loss: 2.9759
Epoch: 39/200
Train loss: 2.6593
Epoch: 40/200
Train loss: 2.7863
Epoch: 41/200
Train loss: 2.7222
Epoch: 42/200
Train loss: 2.7000
Epoch: 43/200
Train loss: 2.8063
Epoch: 44/200
Train loss: 2.7662
Epoch: 45/200
Train loss: 2.7825
Epoch: 46/200
Train loss: 2.5338
Epoch: 47/200
Train loss: 2.7141
Epoch: 48/200
Train loss: 2.4688
Epoch: 49/200
Train loss: 2.4681
Epoch: 50/200
Train loss: 2.5333
Epoch: 51/200
Train loss: 2.5842
Epoch: 52/200
Train loss: 2.5832
Epoch: 53/200
Train loss: 2.4919
Epoch: 54/200
Train loss: 2.5983
Epoch: 55/200
Train loss: 2.5587
Epoch: 56/200
Train loss: 2.5328
Epoch: 57/200
Train loss: 2.5498
Epoch: 58/200
Train loss: 2.5491
Epoch: 59/200
Train loss: 2.5369
Epoch: 60/200
Train loss: 2.5487
Epoch: 61/200
Train loss: 2.4449
Epoch: 62/200
Train loss: 2.4634
Epoch: 63/200
Train loss: 2.4837
Epoch: 64/200
Train loss: 2.4861
Epoch: 65/200
Train loss: 2.5480
Epoch: 66/200
Train loss: 2.4278
Epoch: 67/200
Train loss: 2.5217
Epoch: 68/200
Train loss: 2.4656
Epoch: 69/200
Train loss: 2.6643
Epoch: 70/200
Train loss: 2.4306
Epoch: 71/200
Train loss: 2.3158
Epoch: 72/200
Train loss: 2.3323
Epoch: 73/200
Train loss: 2.4429
Epoch: 74/200
Train loss: 2.4418
Epoch: 75/200
Train loss: 2.4885
Epoch: 76/200
Train loss: 2.4611
Epoch: 77/200
Train loss: 2.4209
Epoch: 78/200
Train loss: 2.3306
Epoch: 79/200
Train loss: 2.1806
Epoch: 80/200
Train loss: 2.3597
Epoch: 81/200
Train loss: 2.3636
Epoch: 82/200
Train loss: 2.3461
Epoch: 83/200
Train loss: 2.3157
Epoch: 84/200
Train loss: 2.3061
Epoch: 85/200
Train loss: 2.4239
Epoch: 86/200
Train loss: 2.3559
Epoch: 87/200
Train loss: 2.2267
Epoch: 88/200
Train loss: 2.4382
Epoch: 89/200
Train loss: 2.4419
Epoch: 90/200
Train loss: 2.3068
Epoch: 91/200
Train loss: 2.4034
Epoch: 92/200
Train loss: 2.3938
Epoch: 93/200
Train loss: 2.4146
Epoch: 94/200
Train loss: 2.3231
Epoch: 95/200
Train loss: 2.3702
Epoch: 96/200
Train loss: 2.4251
Epoch: 97/200
Train loss: 2.3493
Epoch: 98/200
Train loss: 2.3227
Epoch: 99/200
Train loss: 2.2725
Epoch: 100/200
Train loss: 2.3110
Epoch: 101/200
Train loss: 2.3556
Epoch: 102/200
Train loss: 2.1514
0.804056564021784
Model improve: 0.000000 -> 0.804057
Epoch: 103/200
Train loss: 2.1969
0.8020508081905898
Epoch: 104/200
Train loss: 2.2211
0.8042790944329193
Model improve: 0.804057 -> 0.804279
Epoch: 105/200
Train loss: 2.2679
0.8052603789009551
Model improve: 0.804279 -> 0.805260
Epoch: 106/200
Train loss: 2.2956
0.8031745658805498
Epoch: 107/200
Train loss: 2.3353
0.8018161286274854
Epoch: 108/200
Date :05/17/2023, 08:41:11
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.25
drop_path_rate: 0.4
76407
Fold: 4
13830
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 42.0854
Epoch: 2/200
Train loss: 5.7097
Epoch: 3/200
Train loss: 5.2560
Epoch: 4/200
Train loss: 4.9830
Epoch: 5/200
Train loss: 4.6876
Epoch: 6/200
Train loss: 4.4523
Epoch: 7/200
Train loss: 4.1468
Epoch: 8/200
Train loss: 4.0955
Epoch: 9/200
Train loss: 3.9289
Epoch: 10/200
Train loss: 3.8372
Epoch: 11/200
Train loss: 3.7580
Epoch: 12/200
Train loss: 3.6441
Epoch: 13/200
Train loss: 3.6085
Epoch: 14/200
Train loss: 3.4677
Epoch: 15/200
Train loss: 3.4926
Epoch: 16/200
Train loss: 3.4320
Epoch: 17/200
Train loss: 3.2872
Epoch: 18/200
Train loss: 3.2002
Epoch: 19/200
Train loss: 3.4085
Epoch: 20/200
Train loss: 3.2832
Epoch: 21/200
Train loss: 3.1293
Epoch: 22/200
Train loss: 2.9926
Epoch: 23/200
Train loss: 3.0531
Epoch: 24/200
Train loss: 3.1288
Epoch: 25/200
Train loss: 3.0736
Epoch: 26/200
Train loss: 3.0566
Epoch: 27/200
Train loss: 3.0268
Epoch: 28/200
Train loss: 2.8975
Epoch: 29/200
Train loss: 2.9240
Epoch: 30/200
Train loss: 2.7663
Epoch: 31/200
Train loss: 2.9155
Epoch: 32/200
Train loss: 2.9451
Epoch: 33/200
Train loss: 2.7488
Epoch: 34/200
Train loss: 2.8379
Epoch: 35/200
Train loss: 2.8463
Epoch: 36/200
Train loss: 2.8545
Epoch: 37/200
Train loss: 2.7443
Epoch: 38/200
Train loss: 2.9750
Epoch: 39/200
Train loss: 2.6592
Epoch: 40/200
Train loss: 2.7862
Epoch: 41/200
Train loss: 2.7222
Epoch: 42/200
Train loss: 2.7001
Epoch: 43/200
Train loss: 2.8064
Epoch: 44/200
Train loss: 2.7657
Epoch: 45/200
Train loss: 2.7825
Epoch: 46/200
Train loss: 2.5343
Epoch: 47/200
Train loss: 2.7143
Epoch: 48/200
Train loss: 2.4683
Epoch: 49/200
Train loss: 2.4685
Epoch: 50/200
Train loss: 2.5325
Epoch: 51/200
Train loss: 2.5842
Epoch: 52/200
Train loss: 2.5837
Epoch: 53/200
Train loss: 2.4919
Epoch: 54/200
Train loss: 2.5984
Epoch: 55/200
Train loss: 2.5591
Epoch: 56/200
Train loss: 2.5323
Epoch: 57/200
Train loss: 2.5501
Epoch: 58/200
Train loss: 2.5485
Epoch: 59/200
Train loss: 2.5362
Epoch: 60/200
Train loss: 2.5485
Epoch: 61/200
Train loss: 2.4450
Epoch: 62/200
Train loss: 2.4623
Epoch: 63/200
Train loss: 2.4838
Epoch: 64/200
Train loss: 2.4850
Epoch: 65/200
Train loss: 2.5482
Epoch: 66/200
Train loss: 2.4270
Epoch: 67/200
Train loss: 2.5216
Epoch: 68/200
Train loss: 2.4652
Epoch: 69/200
Train loss: 2.6639
Epoch: 70/200
Train loss: 2.4307
Epoch: 71/200
Train loss: 2.3154
Epoch: 72/200
Train loss: 2.3317
Epoch: 73/200
Train loss: 2.4429
Epoch: 74/200
Train loss: 2.4421
Epoch: 75/200
Train loss: 2.4889
Epoch: 76/200
Train loss: 2.4609
Epoch: 77/200
Train loss: 2.4208
Epoch: 78/200
Train loss: 2.3300
Epoch: 79/200
Train loss: 2.1799
Epoch: 80/200
Train loss: 2.3605
Epoch: 81/200
Train loss: 2.3640
Epoch: 82/200
Train loss: 2.3461
Epoch: 83/200
Train loss: 2.3163
Epoch: 84/200
Train loss: 2.3068
Epoch: 85/200
Train loss: 2.4225
Epoch: 86/200
Train loss: 2.3558
Epoch: 87/200
Train loss: 2.2268
Epoch: 88/200
Train loss: 2.4379
Epoch: 89/200
Train loss: 2.4414
Epoch: 90/200
Train loss: 2.3068
Epoch: 91/200
Train loss: 2.4040
Epoch: 92/200
Train loss: 2.3934
Epoch: 93/200
Train loss: 2.4155
Epoch: 94/200
Train loss: 2.3221
Epoch: 95/200
Train loss: 2.3707
Epoch: 96/200
Train loss: 2.4244
Epoch: 97/200
Train loss: 2.3497
Epoch: 98/200
Train loss: 2.3231
Epoch: 99/200
Train loss: 2.2730
Epoch: 100/200
Train loss: 2.3114
Epoch: 101/200
Train loss: 2.3554
Epoch: 102/200
Train loss: 2.1505
0.8039624072035313
Model improve: 0.000000 -> 0.803962
Epoch: 103/200
Train loss: 2.1967
0.8017798526064629
Epoch: 104/200
Train loss: 2.2222
0.804205996261645
Model improve: 0.803962 -> 0.804206
Epoch: 105/200
Train loss: 2.2683
0.8049058978591251
Model improve: 0.804206 -> 0.804906
Epoch: 106/200
Train loss: 2.2963
0.8031021869062988
Epoch: 107/200
Train loss: 2.3355
0.8016743542913055
Epoch: 108/200
Train loss: 2.3299
0.8006439591546493
Epoch: 109/200
Train loss: 2.3068
0.8024192160155521
Epoch: 110/200
Train loss: 2.3396
0.801362568828642
Epoch: 111/200
Train loss: 2.1489
0.8042149984912791
Epoch: 112/200
Train loss: 2.2516
0.8024661721859025
Epoch: 113/200
Train loss: 2.2544
0.8027095818936956
Epoch: 114/200
Train loss: 2.3642
0.8025675829499761
Epoch: 115/200
Train loss: 2.2442
0.8048998671617407
Epoch: 116/200
Train loss: 2.2696
0.8052849669037595
Model improve: 0.804906 -> 0.805285
Epoch: 117/200
Train loss: 2.2569
0.8044625756202516
Epoch: 118/200
Train loss: 2.2042
0.803401658053729
Epoch: 119/200
Train loss: 2.3255
0.8023007557543839
Epoch: 120/200
Train loss: 2.2002
0.8033194255734267
Epoch: 121/200
Train loss: 2.2019
0.8035580014846363
Epoch: 122/200
Train loss: 2.3035
0.804929457096938
Epoch: 123/200
Train loss: 2.3869
0.8028123089450805
Epoch: 124/200
Train loss: 2.4054
0.8040943634978053
Epoch: 125/200
Train loss: 2.3297
0.8055536023572578
Model improve: 0.805285 -> 0.805554
Epoch: 126/200
Train loss: 2.1773
0.806345442083022
Model improve: 0.805554 -> 0.806345
Epoch: 127/200
Train loss: 2.2503
0.804886553730962
Epoch: 128/200
Train loss: 2.1385
0.8079275820309992
Model improve: 0.806345 -> 0.807928
Epoch: 129/200
Train loss: 2.2079
0.8070655208502544
Epoch: 130/200
Train loss: 2.1068
0.8035262742129949
Epoch: 131/200
Train loss: 2.2082
0.8065471866794875
Epoch: 132/200
Train loss: 2.3351
0.8034891009201285
Epoch: 133/200
Train loss: 2.1448
0.8081176051085059
Model improve: 0.807928 -> 0.808118
Epoch: 134/200
Train loss: 2.2621
0.806200056716401
Epoch: 135/200
Train loss: 2.1219
0.8072989337182439
Epoch: 136/200
Train loss: 2.1904
0.8080344834288303
Epoch: 137/200
Train loss: 2.2319
0.8058508533659396
Epoch: 138/200
Train loss: 2.1066
0.808105337723444
Epoch: 139/200
Train loss: 2.2307
0.805623827391362
Epoch: 140/200
Train loss: 2.1800
0.8060666282602381
Epoch: 141/200
Train loss: 2.1828
0.8063165812514812
Epoch: 142/200
Train loss: 2.1391
0.8059269679425277
Epoch: 143/200
Train loss: 2.2931
0.8062237040441321
Epoch: 144/200
Train loss: 2.2476
0.8068562232815593
Epoch: 145/200
Train loss: 2.1036
0.8077333527530414
Epoch: 146/200
Train loss: 2.2668
0.8072035412221968
Epoch: 147/200
Train loss: 2.2473
0.8071663507120599
Epoch: 148/200
Train loss: 2.3125
0.8062358112848965
Epoch: 149/200
Train loss: 2.1712
0.8060481535026177
Epoch: 150/200
Train loss: 1.9838
0.8066883951823233
Epoch: 151/200
Train loss: 2.1781
0.806299418787709
Epoch: 152/200
Train loss: 2.0841
0.8068441860799347
Epoch: 153/200
Train loss: 2.0652
0.8062017103696847
Epoch: 154/200
Train loss: 2.1917
0.8071418499082941
Epoch: 155/200
Train loss: 2.2537
0.8038978433938174
Epoch: 156/200
Train loss: 2.1997
0.806883194774448
Epoch: 157/200
Train loss: 2.1916
0.8057176411785923
Epoch: 158/200
Train loss: 2.0200
0.8093884430854559
Model improve: 0.808118 -> 0.809388
Epoch: 159/200
Train loss: 2.2477
0.805858438923188
Epoch: 160/200
Train loss: 2.0464
0.8082714485125463
Epoch: 161/200
Train loss: 2.3092
0.8072112839915633
Epoch: 162/200
Train loss: 2.1733
0.8052173622127295
Epoch: 163/200
Train loss: 2.1413
0.8064336305955463
Epoch: 164/200
Train loss: 2.2840
0.8039663698902225
Epoch: 165/200
Train loss: 2.2801
0.8044961143944562
Epoch: 166/200
Train loss: 2.2263
0.807157902628359
Epoch: 167/200
Train loss: 2.2879
0.8059989981283039
Epoch: 168/200
Train loss: 2.2007
0.8074811957871314
Epoch: 169/200
Train loss: 2.2588
0.8066127170509368
Epoch: 170/200
Train loss: 2.1341
0.8064324798363838
Epoch: 171/200
Train loss: 2.2626
0.8058171410797809
Epoch: 172/200
Train loss: 2.1253
0.8066479162750931
Epoch: 173/200
Train loss: 2.2493
0.8063200873490962
Epoch: 174/200
Train loss: 2.1223
0.8061668558093923
Epoch: 175/200
Train loss: 2.2577
0.8032092481437055
Epoch: 176/200
Train loss: 2.0412
0.810505406331178
Model improve: 0.809388 -> 0.810505
Epoch: 177/200
Train loss: 2.1071
0.806075330514493
Epoch: 178/200
Train loss: 2.1364
0.8092760787090568
Epoch: 179/200
Train loss: 2.0916
0.8078175110499894
Epoch: 180/200
Train loss: 2.2713
0.8046413516875237
Epoch: 181/200
Train loss: 2.1653
0.8084481063663366
Epoch: 182/200
Train loss: 2.0888
0.8047375697908656
Epoch: 183/200
Train loss: 2.1252
0.8089754413195976
Epoch: 184/200
Train loss: 2.2581
0.8057789094364226
Epoch: 185/200
Train loss: 2.1366
0.8089050183841486
Epoch: 186/200
Train loss: 2.3031
0.8064845882333348
Epoch: 187/200
Train loss: 2.0853
Date :05/17/2023, 15:17:17
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b2
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.25
drop_path_rate: 0.4
76407
Fold: 4
13830
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 41.7221
Epoch: 2/200
Train loss: 5.6979
Epoch: 3/200
Train loss: 5.2247
Epoch: 4/200
Train loss: 4.9660
Epoch: 5/200
Train loss: 4.6479
Epoch: 6/200
Train loss: 4.4088
Epoch: 7/200
Train loss: 4.1104
Epoch: 8/200
Train loss: 4.0622
Epoch: 9/200
Train loss: 3.8843
Epoch: 10/200
Train loss: 3.8112
Epoch: 11/200
Train loss: 3.7167
Epoch: 12/200
Train loss: 3.6137
Epoch: 13/200
Train loss: 3.5523
Epoch: 14/200
Train loss: 3.4369
Epoch: 15/200
Train loss: 3.4753
Epoch: 16/200
Train loss: 3.3879
Epoch: 17/200
Train loss: 3.2717
Epoch: 18/200
Train loss: 3.1577
Epoch: 19/200
Train loss: 3.3646
Epoch: 20/200
Train loss: 3.2277
Epoch: 21/200
Train loss: 3.0971
Epoch: 22/200
Train loss: 2.9717
Epoch: 23/200
Train loss: 3.0046
Epoch: 24/200
Train loss: 3.0835
Epoch: 25/200
Train loss: 3.0298
Epoch: 26/200
Train loss: 3.0461
Epoch: 27/200
Train loss: 3.0148
Epoch: 28/200
Train loss: 2.8501
Epoch: 29/200
Train loss: 2.8984
Epoch: 30/200
Train loss: 2.7465
Epoch: 31/200
Train loss: 2.8331
Epoch: 32/200
Train loss: 2.8991
Epoch: 33/200
Train loss: 2.7117
Epoch: 34/200
Train loss: 2.7757
Epoch: 35/200
Train loss: 2.7855
Epoch: 36/200
Train loss: 2.8350
Epoch: 37/200
Train loss: 2.6897
Epoch: 38/200
Train loss: 2.9381
Epoch: 39/200
Train loss: 2.6050
Epoch: 40/200
Train loss: 2.7461
Epoch: 41/200
Train loss: 2.6613
Epoch: 42/200
Train loss: 2.6691
Epoch: 43/200
Train loss: 2.7525
Epoch: 44/200
Train loss: 2.6939
Epoch: 45/200
Train loss: 2.7189
Epoch: 46/200
Train loss: 2.4817
Epoch: 47/200
Train loss: 2.6723
Epoch: 48/200
Train loss: 2.4283
Epoch: 49/200
Train loss: 2.4215
Epoch: 50/200
Train loss: 2.4955
Epoch: 51/200
Train loss: 2.5237
Epoch: 52/200
Train loss: 2.5152
Epoch: 53/200
Train loss: 2.4236
Epoch: 54/200
Train loss: 2.5387
Epoch: 55/200
Train loss: 2.5061
Epoch: 56/200
Train loss: 2.4904
Epoch: 57/200
Train loss: 2.5021
Epoch: 58/200
Train loss: 2.4989
Epoch: 59/200
Train loss: 2.4869
Epoch: 60/200
Train loss: 2.4998
Epoch: 61/200
Train loss: 2.4023
Epoch: 62/200
Train loss: 2.4399
Epoch: 63/200
Train loss: 2.4497
Epoch: 64/200
Train loss: 2.4544
Epoch: 65/200
Train loss: 2.4938
Epoch: 66/200
Train loss: 2.3790
Epoch: 67/200
Train loss: 2.4599
Epoch: 68/200
Train loss: 2.4277
Epoch: 69/200
Train loss: 2.6004
Epoch: 70/200
Train loss: 2.3989
Epoch: 71/200
Train loss: 2.2705
Epoch: 72/200
Train loss: 2.2927
Epoch: 73/200
Train loss: 2.4240
Epoch: 74/200
Train loss: 2.3942
Epoch: 75/200
Train loss: 2.4574
Epoch: 76/200
Train loss: 2.4075
Epoch: 77/200
Train loss: 2.3707
Epoch: 78/200
Train loss: 2.2834
Epoch: 79/200
Train loss: 2.1325
Epoch: 80/200
Train loss: 2.3362
Epoch: 81/200
Train loss: 2.3318
Epoch: 82/200
Train loss: 2.3127
Epoch: 83/200
Train loss: 2.2751
Epoch: 84/200
Train loss: 2.2497
Epoch: 85/200
Train loss: 2.3863
Epoch: 86/200
Train loss: 2.3082
Epoch: 87/200
Train loss: 2.1870
Epoch: 88/200
Train loss: 2.3917
Epoch: 89/200
Train loss: 2.3999
Epoch: 90/200
Train loss: 2.2644
Epoch: 91/200
Train loss: 2.3616
Epoch: 92/200
Train loss: 2.3456
Epoch: 93/200
Train loss: 2.3655
Epoch: 94/200
Train loss: 2.2958
Epoch: 95/200
Train loss: 2.3151
Epoch: 96/200
Train loss: 2.3681
Epoch: 97/200
Train loss: 2.3128
Epoch: 98/200
Train loss: 2.2851
Epoch: 99/200
Train loss: 2.2309
Epoch: 100/200
Train loss: 2.2773
Epoch: 101/200
Train loss: 2.3209
Epoch: 102/200
Train loss: 2.1206
0.8074549146679212
Model improve: 0.000000 -> 0.807455
Epoch: 103/200
Train loss: 2.1710
0.8053414919288135
Epoch: 104/200
Train loss: 2.1694
0.8053457440153865
Epoch: 105/200
Train loss: 2.2247
0.8066679951873142
Epoch: 106/200
Train loss: 2.2435
0.8066174426730739
Epoch: 107/200
Train loss: 2.3078
0.8055019029221561
Epoch: 108/200
Train loss: 2.3023
0.804483520734321
Epoch: 109/200
Train loss: 2.2488
0.8047739255730979
Epoch: 110/200
Train loss: 2.3072
0.8056349258740882
Epoch: 111/200
Train loss: 2.1098
0.8071996488619088
Epoch: 112/200
Train loss: 2.2111
0.804247672608233
Epoch: 113/200
Train loss: 2.2186
0.806235364357464
Epoch: 114/200
Train loss: 2.3150
0.806859166362022
Epoch: 115/200
Train loss: 2.1948
0.8076653229000925
Model improve: 0.807455 -> 0.807665
Epoch: 116/200
Train loss: 2.2522
0.8083159523614002
Model improve: 0.807665 -> 0.808316
Epoch: 117/200
Train loss: 2.2048
0.806664922754409
Epoch: 118/200
Train loss: 2.1815
0.8044243319001411
Epoch: 119/200
Train loss: 2.2927
0.8058221423654379
Epoch: 120/200
Train loss: 2.1629
0.806416223361952
Epoch: 121/200
Train loss: 2.1683
0.8064648223410019
Epoch: 122/200
Train loss: 2.2586
0.8075522987184967
Epoch: 123/200
Train loss: 2.3590
0.8059422355206011
Epoch: 124/200
Train loss: 2.3554
0.8063812866021446
Epoch: 125/200
Train loss: 2.2928
0.8082334513640024
Epoch: 126/200
Train loss: 2.1450
0.8081225358502186
Epoch: 127/200
Train loss: 2.2175
0.8052973334747714
Epoch: 128/200
Train loss: 2.1128
0.8082830130681203
Epoch: 129/200
Train loss: 2.1585
0.8086960016322771
Model improve: 0.808316 -> 0.808696
Epoch: 130/200
Train loss: 2.0573
0.8058728622650477
Epoch: 131/200
Train loss: 2.1582
0.8077302391923192
Epoch: 132/200
Train loss: 2.2919
0.8054719913187096
Epoch: 133/200
Train loss: 2.0812
0.8079162228568895
Epoch: 134/200
Train loss: 2.2326
0.8073192418872802
Epoch: 135/200
Train loss: 2.0750
0.8082775515098094
Epoch: 136/200
Train loss: 2.1555
0.8089407919568867
Model improve: 0.808696 -> 0.808941
Epoch: 137/200
Train loss: 2.1928
0.8074032486481691
Epoch: 138/200
Train loss: 2.0702
0.8086719001796517
Epoch: 139/200
Train loss: 2.2015
0.8073509650981351
Epoch: 140/200
Train loss: 2.1351
0.8068437651900323
Epoch: 141/200
Train loss: 2.1611
0.8077432557579017
Epoch: 142/200
Train loss: 2.0928
0.8081514384503327
Epoch: 143/200
Train loss: 2.2384
0.8080255094178362
Epoch: 144/200
Train loss: 2.2175
0.8090269012935868
Model improve: 0.808941 -> 0.809027
Epoch: 145/200
Train loss: 2.0710
0.8100329117031656
Model improve: 0.809027 -> 0.810033
Epoch: 146/200
Train loss: 2.2208
0.8080253323135986
Epoch: 147/200
Train loss: 2.2220
0.8092830984122675
Epoch: 148/200
Train loss: 2.2684
0.8087934882932097
Epoch: 149/200
Train loss: 2.1242
0.8086911075954235
Epoch: 150/200
Train loss: 1.9687
0.8094580673824225
Epoch: 151/200
Train loss: 2.1550
0.8091740925466179
Epoch: 152/200
Train loss: 2.0585
0.8089589156511808
Epoch: 153/200
Train loss: 2.0402
0.8092603369304813
Epoch: 154/200
Train loss: 2.1376
0.8096931577813796
Epoch: 155/200
Train loss: 2.1919
0.8073329117313353
Epoch: 156/200
Train loss: 2.1711
0.8088160243300959
Epoch: 157/200
Train loss: 2.1404
0.8081253645924972
Epoch: 158/200
Train loss: 1.9806
0.8103567646943927
Model improve: 0.810033 -> 0.810357
Epoch: 159/200
Train loss: 2.1881
0.8082563563222711
Epoch: 160/200
Train loss: 2.0167
0.8092794823432934
Epoch: 161/200
Train loss: 2.2542
0.8088104253083107
Epoch: 162/200
Train loss: 2.1299
0.8076066614260629
Epoch: 163/200
Train loss: 2.1130
0.8080635140636024
Epoch: 164/200
Train loss: 2.2327
0.8069722936776746
Epoch: 165/200
Train loss: 2.2537
0.8069201434477614
Epoch: 166/200
Train loss: 2.1885
0.8088957590649567
Epoch: 167/200
Train loss: 2.2478
0.8079087623761296
Epoch: 168/200
Train loss: 2.1590
0.8093353982264928
Epoch: 169/200
Train loss: 2.2019
0.8085348874249295
Epoch: 170/200
Train loss: 2.0647
0.8080684607346296
Epoch: 171/200
Train loss: 2.2255
0.8045243591015694
Epoch: 172/200
Train loss: 2.0923
0.8089712213667881
Epoch: 173/200
Train loss: 2.2056
0.8083816357317355
Epoch: 174/200
Train loss: 2.0804
0.8085958936150615
Epoch: 175/200
Train loss: 2.2003
0.8063673940331323
Epoch: 176/200
Train loss: 1.9962
0.8103829057457745
Model improve: 0.810357 -> 0.810383
Epoch: 177/200
Train loss: 2.0822
0.8075644075329591
Epoch: 178/200
Train loss: 2.1039
0.8093967341677831
Epoch: 179/200
Train loss: 2.0461
0.809388531442009
Epoch: 180/200
Train loss: 2.2269
0.8070718960016665
Epoch: 181/200
Train loss: 2.1109
0.8096211040465624
Epoch: 182/200
Train loss: 2.0253
0.807828753206891
Epoch: 183/200
Train loss: 2.0708
0.8100285090098275
Epoch: 184/200
Train loss: 2.2226
0.8080415640729939
Epoch: 185/200
Train loss: 2.0925
0.809620467349111
Epoch: 186/200
Train loss: 2.2565
0.8081053182503409
Epoch: 187/200
Train loss: 2.0335
0.8095948572080658
Epoch: 188/200
Train loss: 2.2533
0.8086709531756651
Epoch: 189/200
Train loss: 2.2096
0.8069744789877635
Epoch: 190/200
Train loss: 2.0938
0.8103469616755713
Epoch: 191/200
Train loss: 2.2332
0.8078213902349708
Epoch: 192/200
Train loss: 2.1469
0.8088888734848378
Epoch: 193/200
Train loss: 2.1892
0.8071537871111659
Epoch: 194/200
Train loss: 2.1992
0.8082824467034657
Epoch: 195/200
Train loss: 2.1665
0.8086413377628896
Epoch: 196/200
Train loss: 2.1677
0.8069688650873563
Epoch: 197/200
Train loss: 2.1443
0.8087216644894433
Epoch: 198/200
Train loss: 2.0898
0.8083370614283665
Epoch: 199/200
Train loss: 2.2194
0.8060574152270831
Epoch: 200/200
Train loss: 2.1196
0.8086257070172869
Date :05/17/2023, 22:59:44
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b2
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.25
drop_path_rate: 0.4
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 41.8278
Epoch: 2/200
Train loss: 5.7056
Epoch: 3/200
Train loss: 5.2151
Epoch: 4/200
Train loss: 4.9569
Epoch: 5/200
Train loss: 4.6586
Epoch: 6/200
Train loss: 4.4172
Epoch: 7/200
Train loss: 4.1123
Epoch: 8/200
Train loss: 4.0756
Epoch: 9/200
Train loss: 3.9097
Epoch: 10/200
Train loss: 3.8338
Epoch: 11/200
Train loss: 3.7214
Epoch: 12/200
Train loss: 3.6211
Epoch: 13/200
Train loss: 3.5881
Epoch: 14/200
Train loss: 3.4591
Epoch: 15/200
Train loss: 3.4749
Epoch: 16/200
Train loss: 3.4101
Epoch: 17/200
Train loss: 3.2684
Epoch: 18/200
Train loss: 3.1626
Epoch: 19/200
Train loss: 3.3740
Epoch: 20/200
Train loss: 3.2482
Epoch: 21/200
Train loss: 3.1061
Epoch: 22/200
Train loss: 2.9785
Epoch: 23/200
Train loss: 3.0267
Epoch: 24/200
Train loss: 3.0962
Epoch: 25/200
Train loss: 3.0535
Epoch: 26/200
Train loss: 3.0290
Epoch: 27/200
Train loss: 3.0339
Epoch: 28/200
Train loss: 2.8685
Epoch: 29/200
Train loss: 2.8905
Epoch: 30/200
Train loss: 2.7194
Epoch: 31/200
Train loss: 2.8707
Epoch: 32/200
Train loss: 2.9238
Epoch: 33/200
Train loss: 2.7096
Epoch: 34/200
Train loss: 2.8053
Epoch: 35/200
Train loss: 2.7901
Epoch: 36/200
Train loss: 2.8280
Epoch: 37/200
Train loss: 2.6890
Epoch: 38/200
Train loss: 2.9342
Epoch: 39/200
Train loss: 2.6134
Epoch: 40/200
Train loss: 2.7337
Epoch: 41/200
Train loss: 2.6754
Epoch: 42/200
Train loss: 2.6691
Epoch: 43/200
Train loss: 2.7691
Epoch: 44/200
Train loss: 2.7325
Epoch: 45/200
Train loss: 2.7433
Epoch: 46/200
Train loss: 2.4980
Epoch: 47/200
Train loss: 2.6921
Epoch: 48/200
Train loss: 2.4408
Epoch: 49/200
Train loss: 2.4286
Epoch: 50/200
Train loss: 2.4970
Epoch: 51/200
Train loss: 2.5465
Epoch: 52/200
Train loss: 2.5202
Epoch: 53/200
Train loss: 2.4679
Epoch: 54/200
Train loss: 2.5683
Epoch: 55/200
Train loss: 2.5269
Epoch: 56/200
Train loss: 2.4771
Epoch: 57/200
Train loss: 2.5046
Epoch: 58/200
Train loss: 2.4981
Epoch: 59/200
Train loss: 2.4943
Epoch: 60/200
Train loss: 2.5267
Epoch: 61/200
Train loss: 2.4026
Epoch: 62/200
Train loss: 2.4200
Epoch: 63/200
Train loss: 2.4618
Epoch: 64/200
Train loss: 2.4486
Epoch: 65/200
Train loss: 2.4892
Epoch: 66/200
Train loss: 2.4004
Epoch: 67/200
Train loss: 2.4682
Epoch: 68/200
Train loss: 2.4538
Epoch: 69/200
Train loss: 2.6027
Epoch: 70/200
Train loss: 2.3915
Epoch: 71/200
Train loss: 2.2768
Epoch: 72/200
Train loss: 2.2831
Epoch: 73/200
Train loss: 2.4235
Epoch: 74/200
Train loss: 2.4089
Epoch: 75/200
Train loss: 2.4492
Epoch: 76/200
Train loss: 2.4162
Epoch: 77/200
Train loss: 2.3886
Epoch: 78/200
Train loss: 2.3022
Epoch: 79/200
Train loss: 2.1397
Epoch: 80/200
Train loss: 2.2933
Epoch: 81/200
Train loss: 2.3228
Epoch: 82/200
Train loss: 2.3291
Epoch: 83/200
Train loss: 2.2690
Epoch: 84/200
Train loss: 2.2560
Epoch: 85/200
Train loss: 2.3752
Epoch: 86/200
Train loss: 2.3198
Epoch: 87/200
Train loss: 2.1972
Epoch: 88/200
Train loss: 2.3876
Epoch: 89/200
Train loss: 2.4041
Epoch: 90/200
Train loss: 2.2648
Epoch: 91/200
Train loss: 2.3737
Epoch: 92/200
Train loss: 2.3400
Epoch: 93/200
Train loss: 2.3813
Epoch: 94/200
Train loss: 2.2784
Epoch: 95/200
Train loss: 2.3269
Epoch: 96/200
Train loss: 2.3917
Epoch: 97/200
Train loss: 2.3256
Epoch: 98/200
Train loss: 2.2994
Epoch: 99/200
Train loss: 2.2188
Epoch: 100/200
Train loss: 2.2781
Epoch: 101/200
Train loss: 2.3280
Epoch: 102/200
Train loss: 2.1149
0.8132825212577541
Model improve: 0.000000 -> 0.813283
Epoch: 103/200
Train loss: 2.1781
0.8113232654823426
Epoch: 104/200
Train loss: 2.1913
0.8128982277819775
Epoch: 105/200
Train loss: 2.2351
0.8123150121940589
Epoch: 106/200
Train loss: 2.2328
0.812245306343352
Epoch: 107/200
Train loss: 2.2960
0.8111572801591584
Epoch: 108/200
Train loss: 2.2844
0.8116954514552668
Epoch: 109/200
Train loss: 2.2712
0.8117429029278616
Epoch: 110/200
Train loss: 2.3208
0.8110011758134242
Epoch: 111/200
Train loss: 2.1042
0.8128705227671194
Epoch: 112/200
Train loss: 2.2223
0.8100306144718653
Epoch: 113/200
Train loss: 2.2002
0.812831161554218
Epoch: 114/200
Train loss: 2.3187
0.8111818753341153
Epoch: 115/200
Train loss: 2.1896
0.8125344845789276
Epoch: 116/200
Date :05/18/2023, 01:22:44
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b2
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.3
drop_path_rate: 0.45
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 42.3523
Epoch: 2/200
Train loss: 5.7348
Epoch: 3/200
Train loss: 5.2493
Epoch: 4/200
Train loss: 4.9953
Epoch: 5/200
Train loss: 4.6922
Epoch: 6/200
Train loss: 4.4539
Epoch: 7/200
Train loss: 4.1446
Epoch: 8/200
Train loss: 4.1087
Epoch: 9/200
Train loss: 3.9489
Epoch: 10/200
Train loss: 3.8666
Epoch: 11/200
Train loss: 3.7558
Epoch: 12/200
Train loss: 3.6558
Epoch: 13/200
Train loss: 3.6193
Epoch: 14/200
Train loss: 3.4903
Epoch: 15/200
Train loss: 3.5082
Epoch: 16/200
Train loss: 3.4417
Epoch: 17/200
Train loss: 3.2977
Epoch: 18/200
Train loss: 3.1953
Epoch: 19/200
Train loss: 3.4039
Epoch: 20/200
Train loss: 3.2801
Epoch: 21/200
Train loss: 3.1367
Epoch: 22/200
Train loss: 3.0124
Epoch: 23/200
Train loss: 3.0568
Epoch: 24/200
Train loss: 3.1282
Epoch: 25/200
Train loss: 3.0829
Epoch: 26/200
Train loss: 3.0600
Epoch: 27/200
Train loss: 3.0599
Epoch: 28/200
Train loss: 2.8989
Epoch: 29/200
Train loss: 2.9212
Epoch: 30/200
Train loss: 2.7523
Epoch: 31/200
Train loss: 2.9002
Epoch: 32/200
Train loss: 2.9523
Epoch: 33/200
Train loss: 2.7410
Epoch: 34/200
Train loss: 2.8357
Epoch: 35/200
Train loss: 2.8216
Epoch: 36/200
Train loss: 2.8560
Epoch: 37/200
Train loss: 2.7184
Epoch: 38/200
Train loss: 2.9606
Epoch: 39/200
Train loss: 2.6416
Epoch: 40/200
Train loss: 2.7659
Epoch: 41/200
Train loss: 2.7047
Epoch: 42/200
Train loss: 2.6976
Epoch: 43/200
Train loss: 2.7955
Epoch: 44/200
Train loss: 2.7610
Epoch: 45/200
Train loss: 2.7706
Epoch: 46/200
Train loss: 2.5251
Epoch: 47/200
Train loss: 2.7184
Epoch: 48/200
Train loss: 2.4685
Epoch: 49/200
Train loss: 2.4567
Epoch: 50/200
Train loss: 2.5229
Epoch: 51/200
Train loss: 2.5741
Epoch: 52/200
Train loss: 2.5474
Epoch: 53/200
Train loss: 2.4961
Epoch: 54/200
Train loss: 2.5935
Epoch: 55/200
Train loss: 2.5549
Epoch: 56/200
Train loss: 2.5033
Epoch: 57/200
Train loss: 2.5302
Epoch: 58/200
Train loss: 2.5238
Epoch: 59/200
Train loss: 2.5169
Epoch: 60/200
Train loss: 2.5515
Epoch: 61/200
Train loss: 2.4313
Epoch: 62/200
Train loss: 2.4484
Epoch: 63/200
Train loss: 2.4855
Epoch: 64/200
Train loss: 2.4731
Epoch: 65/200
Train loss: 2.5153
Epoch: 66/200
Train loss: 2.4251
Epoch: 67/200
Train loss: 2.4950
Epoch: 68/200
Train loss: 2.4749
Epoch: 69/200
Train loss: 2.6276
Epoch: 70/200
Train loss: 2.4169
Epoch: 71/200
Train loss: 2.3031
Epoch: 72/200
Train loss: 2.3087
Epoch: 73/200
Train loss: 2.4478
Epoch: 74/200
Train loss: 2.4361
Epoch: 75/200
Train loss: 2.4699
Epoch: 76/200
Train loss: 2.4392
Epoch: 77/200
Train loss: 2.4133
Epoch: 78/200
Train loss: 2.3265
Epoch: 79/200
Train loss: 2.1646
Epoch: 80/200
Train loss: 2.3177
Epoch: 81/200
Train loss: 2.3459
Epoch: 82/200
Train loss: 2.3531
Epoch: 83/200
Train loss: 2.2959
Epoch: 84/200
Train loss: 2.2806
Epoch: 85/200
Train loss: 2.3986
Epoch: 86/200
Train loss: 2.3424
Epoch: 87/200
Train loss: 2.2190
Epoch: 88/200
Train loss: 2.4129
Epoch: 89/200
Train loss: 2.4284
Epoch: 90/200
Train loss: 2.2862
Epoch: 91/200
Train loss: 2.3970
Epoch: 92/200
Train loss: 2.3666
Epoch: 93/200
Train loss: 2.4048
Epoch: 94/200
Train loss: 2.3032
Epoch: 95/200
Train loss: 2.3499
Epoch: 96/200
Train loss: 2.4132
Epoch: 97/200
Train loss: 2.3498
Epoch: 98/200
Train loss: 2.3229
Epoch: 99/200
Train loss: 2.2403
Epoch: 100/200
Train loss: 2.3021
Epoch: 101/200
Train loss: 2.3506
Epoch: 102/200
Train loss: 2.1379
0.8147945584705388
Model improve: 0.000000 -> 0.814795
Epoch: 103/200
Train loss: 2.1996
0.8128508258604219
Epoch: 104/200
Train loss: 2.2127
0.8143894977293815
Epoch: 105/200
Train loss: 2.2580
0.8137064542012329
Epoch: 106/200
Train loss: 2.2548
0.8139860672280634
Epoch: 107/200
Train loss: 2.3187
0.8129275501539788
Epoch: 108/200
Train loss: 2.3083
0.8133478972093929
Epoch: 109/200
Train loss: 2.2938
0.8134753698433375
Epoch: 110/200
Train loss: 2.3442
0.8127943113044686
Epoch: 111/200
Train loss: 2.1273
0.8147669348320176
Epoch: 112/200
Train loss: 2.2425
0.8120305791080712
Epoch: 113/200
Train loss: 2.2241
0.8147131921196406
Epoch: 114/200
Train loss: 2.3424
0.8136044193858553
Epoch: 115/200
Train loss: 2.2120
0.8150943442371844
Model improve: 0.814795 -> 0.815094
Epoch: 116/200
Train loss: 2.2734
0.8141517634866768
Epoch: 117/200
Train loss: 2.2232
0.8143619057659527
Epoch: 118/200
Train loss: 2.1966
0.8140897295344529
Epoch: 119/200
Train loss: 2.3172
0.8148242039414549
Epoch: 120/200
Train loss: 2.2000
0.8156618679967301
Model improve: 0.815094 -> 0.815662
Epoch: 121/200
Train loss: 2.2075
0.8165829685864578
Model improve: 0.815662 -> 0.816583
Epoch: 122/200
Train loss: 2.2826
0.8165782542901826
Epoch: 123/200
Train loss: 2.3814
0.815205556723585
Epoch: 124/200
Train loss: 2.3825
0.814899214582757
Epoch: 125/200
Train loss: 2.3298
0.8162051204708103
Epoch: 126/200
Train loss: 2.1753
0.8156556280503224
Epoch: 127/200
Train loss: 2.2509
0.8152122244034188
Epoch: 128/200
Train loss: 2.1367
0.8164909470151492
Epoch: 129/200
Train loss: 2.1686
0.8171189317805545
Model improve: 0.816583 -> 0.817119
Epoch: 130/200
Train loss: 2.0914
0.815519599470873
Epoch: 131/200
Train loss: 2.1858
0.8171289940016958
Model improve: 0.817119 -> 0.817129
Epoch: 132/200
Train loss: 2.3076
0.8160630303566112
Epoch: 133/200
Train loss: 2.1170
0.8178369894400999
Model improve: 0.817129 -> 0.817837
Epoch: 134/200
Train loss: 2.2469
0.8165453235092637
Epoch: 135/200
Train loss: 2.0905
0.8171149957448236
Epoch: 136/200
Train loss: 2.1830
0.8167543905397963
Epoch: 137/200
Train loss: 2.2065
0.8152991038559162
Epoch: 138/200
Train loss: 2.1039
0.81761924644265
Epoch: 139/200
Train loss: 2.2247
0.8157896702485254
Epoch: 140/200
Train loss: 2.1706
0.8159463149562209
Epoch: 141/200
Train loss: 2.1655
0.8165180435227548
Epoch: 142/200
Train loss: 2.1236
0.816417380557185
Epoch: 143/200
Train loss: 2.2728
0.8165906584603798
Epoch: 144/200
Train loss: 2.2323
0.8172683820742083
Epoch: 145/200
Train loss: 2.0943
0.8169174862224954
Epoch: 146/200
Train loss: 2.2511
0.8167043993683676
Epoch: 147/200
Train loss: 2.2435
0.8166772565528241
Epoch: 148/200
Train loss: 2.2853
0.8165584984817592
Epoch: 149/200
Train loss: 2.1559
0.8167177053991362
Epoch: 150/200
Train loss: 1.9970
0.8165266609057833
Epoch: 151/200
Train loss: 2.1828
0.816875270888667
Epoch: 152/200
Train loss: 2.0978
0.8165963497487339
Epoch: 153/200
Train loss: 2.0477
0.816114646821158
Epoch: 154/200
Train loss: 2.1722
0.8168913639232698
Epoch: 155/200
Train loss: 2.2418
0.8152196095539047
Epoch: 156/200
Train loss: 2.1900
0.8164748958520054
Epoch: 157/200
Train loss: 2.1819
0.8165184210617008
Epoch: 158/200
Train loss: 2.0007
0.8179074810297179
Model improve: 0.817837 -> 0.817907
Epoch: 159/200
Train loss: 2.2287
0.8173302956761785
Epoch: 160/200
Train loss: 2.0523
0.8178220865977355
Epoch: 161/200
Train loss: 2.2689
0.8176325593325934
Epoch: 162/200
Train loss: 2.1334
0.8161377824890781
Epoch: 163/200
Train loss: 2.1312
0.8167252418461326
Epoch: 164/200
Train loss: 2.2563
0.8160389253605604
Epoch: 165/200
Train loss: 2.2633
0.8161352214043971
Epoch: 166/200
Train loss: 2.2437
0.8175917621158086
Epoch: 167/200
Train loss: 2.2728
0.8164889652108701
Epoch: 168/200
Train loss: 2.1783
0.8179531439625631
Model improve: 0.817907 -> 0.817953
Epoch: 169/200
Train loss: 2.2536
0.8171398981810066
Epoch: 170/200
Train loss: 2.1134
0.8168641434566927
Epoch: 171/200
Train loss: 2.2442
0.8168097120146992
Epoch: 172/200
Train loss: 2.1045
0.8174939189492644
Epoch: 173/200
Train loss: 2.2232
0.8168325540987004
Epoch: 174/200
Train loss: 2.1108
0.8167634518654641
Epoch: 175/200
Train loss: 2.2310
0.8149745865098432
Epoch: 176/200
Train loss: 2.0120
0.8188624010351261
Model improve: 0.817953 -> 0.818862
Epoch: 177/200
Train loss: 2.1145
0.8172785641078513
Epoch: 178/200
Train loss: 2.1054
0.8172627658006314
Epoch: 179/200
Train loss: 2.0867
0.8176597618826431
Epoch: 180/200
Train loss: 2.2695
0.8158234979738207
Epoch: 181/200
Train loss: 2.1392
0.8177906551515527
Epoch: 182/200
Train loss: 2.0514
0.8160553858271812
Epoch: 183/200
Train loss: 2.1099
0.8182836845555013
Epoch: 184/200
Train loss: 2.2387
0.816869312005387
Epoch: 185/200
Train loss: 2.1325
0.8181449530457713
Epoch: 186/200
Train loss: 2.2768
0.816827204178372
Epoch: 187/200
Train loss: 2.0609
0.8179054114499626
Epoch: 188/200
Train loss: 2.2607
0.8165989162087092
Epoch: 189/200
Train loss: 2.2443
0.8163407979277403
Epoch: 190/200
Train loss: 2.1325
0.8184666648434067
Epoch: 191/200
Train loss: 2.2439
Date :05/18/2023, 08:14:26
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_s
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.3
drop_path_rate: 0.45
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Date :05/18/2023, 08:14:55
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_s
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.3
drop_path_rate: 0.45
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 30.0384
Epoch: 2/200
Train loss: 5.4408
Epoch: 3/200
Train loss: 4.9764
Epoch: 4/200
Train loss: 4.7628
Epoch: 5/200
Train loss: 4.4662
Epoch: 6/200
Train loss: 4.2591
Epoch: 7/200
Train loss: 3.9967
Epoch: 8/200
Train loss: 3.9582
Epoch: 9/200
Train loss: 3.8058
Epoch: 10/200
Train loss: 3.7588
Epoch: 11/200
Train loss: 3.6914
Epoch: 12/200
Train loss: 3.5757
Epoch: 13/200
Train loss: 3.5389
Epoch: 14/200
Train loss: 3.4245
Epoch: 15/200
Train loss: 3.4357
Epoch: 16/200
Train loss: 3.3946
Epoch: 17/200
Train loss: 3.2546
Epoch: 18/200
Train loss: 3.1572
Epoch: 19/200
Train loss: 3.3391
Epoch: 20/200
Train loss: 3.2187
Epoch: 21/200
Train loss: 3.0899
Epoch: 22/200
Train loss: 2.9397
Epoch: 23/200
Train loss: 2.9968
Epoch: 24/200
Train loss: 3.0841
Epoch: 25/200
Train loss: 3.0326
Epoch: 26/200
Train loss: 3.0247
Epoch: 27/200
Train loss: 2.9941
Epoch: 28/200
Train loss: 2.8252
Epoch: 29/200
Train loss: 2.8821
Epoch: 30/200
Train loss: 2.7323
Epoch: 31/200
Train loss: 2.8821
Epoch: 32/200
Train loss: 2.9135
Epoch: 33/200
Train loss: 2.6883
Epoch: 34/200
Train loss: 2.7827
Epoch: 35/200
Train loss: 2.7809
Epoch: 36/200
Train loss: 2.8159
Epoch: 37/200
Train loss: 2.6934
Epoch: 38/200
Train loss: 2.9293
Epoch: 39/200
Train loss: 2.6076
Epoch: 40/200
Train loss: 2.7495
Epoch: 41/200
Train loss: 2.6678
Epoch: 42/200
Train loss: 2.6423
Epoch: 43/200
Train loss: 2.7510
Epoch: 44/200
Train loss: 2.7172
Epoch: 45/200
Train loss: 2.7130
Epoch: 46/200
Train loss: 2.4616
Epoch: 47/200
Train loss: 2.6694
Epoch: 48/200
Train loss: 2.4156
Epoch: 49/200
Train loss: 2.4083
Epoch: 50/200
Train loss: 2.4765
Epoch: 51/200
Train loss: 2.5188
Epoch: 52/200
Train loss: 2.5040
Epoch: 53/200
Train loss: 2.4201
Epoch: 54/200
Train loss: 2.5358
Epoch: 55/200
Train loss: 2.5031
Epoch: 56/200
Train loss: 2.4615
Epoch: 57/200
Train loss: 2.4881
Epoch: 58/200
Train loss: 2.4830
Epoch: 59/200
Train loss: 2.4856
Epoch: 60/200
Train loss: 2.4868
Epoch: 61/200
Train loss: 2.3933
Epoch: 62/200
Train loss: 2.4066
Epoch: 63/200
Train loss: 2.4158
Epoch: 64/200
Train loss: 2.4180
Epoch: 65/200
Train loss: 2.4827
Epoch: 66/200
Train loss: 2.3627
Epoch: 67/200
Train loss: 2.4625
Epoch: 68/200
Train loss: 2.3934
Epoch: 69/200
Train loss: 2.5918
Epoch: 70/200
Train loss: 2.3827
Epoch: 71/200
Train loss: 2.2526
Epoch: 72/200
Train loss: 2.2626
Epoch: 73/200
Train loss: 2.3967
Epoch: 74/200
Train loss: 2.3696
Epoch: 75/200
Train loss: 2.4098
Epoch: 76/200
Train loss: 2.3901
Epoch: 77/200
Train loss: 2.3617
Epoch: 78/200
Train loss: 2.2629
Epoch: 79/200
Train loss: 2.1118
Epoch: 80/200
Train loss: 2.2872
Epoch: 81/200
Train loss: 2.2869
Epoch: 82/200
Train loss: 2.2744
Epoch: 83/200
Train loss: 2.2345
Epoch: 84/200
Train loss: 2.2253
Epoch: 85/200
Train loss: 2.3694
Epoch: 86/200
Train loss: 2.2919
Epoch: 87/200
Train loss: 2.1551
Epoch: 88/200
Train loss: 2.3583
Epoch: 89/200
Train loss: 2.3954
Epoch: 90/200
Train loss: 2.2154
Epoch: 91/200
Train loss: 2.3325
Epoch: 92/200
Train loss: 2.3312
Epoch: 93/200
Train loss: 2.3586
Epoch: 94/200
Train loss: 2.2621
Epoch: 95/200
Train loss: 2.3130
Epoch: 96/200
Train loss: 2.3522
Epoch: 97/200
Train loss: 2.2932
Epoch: 98/200
Train loss: 2.2597
Epoch: 99/200
Train loss: 2.2009
Epoch: 100/200
Train loss: 2.2481
Epoch: 101/200
Train loss: 2.3064
Epoch: 102/200
Train loss: 2.0908
0.8050496087506173
Model improve: 0.000000 -> 0.805050
Epoch: 103/200
Train loss: 2.1371
0.8020357644658308
Epoch: 104/200
Train loss: 2.1416
Date :05/18/2023, 10:29:55
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_s
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.25
drop_path_rate: 0.4
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Date :05/18/2023, 10:30:40
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_s
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.25
drop_path_rate: 0.4
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Date :05/18/2023, 10:31:20
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_s
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.25
drop_path_rate: 0.4
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Date :05/18/2023, 10:32:39
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_s
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.25
drop_path_rate: 0.4
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/25
Train loss: 12.2805
Val cmap: 0.682542
Valid loss: 4.050240
Epoch: 2/25
Train loss: 4.5134
Val cmap: 0.791043
Valid loss: 2.864267
Epoch: 3/25
Date :05/18/2023, 10:36:57
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.25
drop_path_rate: 0.4
76407
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/25
Train loss: 10.6591
Val cmap: 0.769803
Valid loss: 3.621824
Epoch: 2/25
Train loss: 4.5045
Val cmap: 0.838349
Valid loss: 2.475292
Epoch: 3/25
Date :05/18/2023, 10:43:25
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_s
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.5
drop_path_rate: 0.1
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :05/18/2023, 10:44:46
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 100
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_s
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.5
drop_path_rate: 0.1
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :05/18/2023, 10:44:54
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 100
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_s
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.5
drop_path_rate: 0.2
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 29.4942
Val cmap: 0.520337
Valid loss: 5.544775
Epoch: 2/100
Train loss: 5.4382
Val cmap: 0.606529
Valid loss: 4.540014
Epoch: 3/100
Train loss: 4.9233
Val cmap: 0.683312
Valid loss: 3.844635
Epoch: 4/100
Train loss: 4.6738
Date :05/18/2023, 10:52:04
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_s
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.5
drop_path_rate: 0.2
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 29.4995
Epoch: 2/200
Train loss: 5.4392
Epoch: 3/200
Train loss: 4.9150
Epoch: 4/200
Train loss: 4.6699
Epoch: 5/200
Train loss: 4.3393
Epoch: 6/200
Train loss: 4.1062
Epoch: 7/200
Train loss: 3.8251
Epoch: 8/200
Train loss: 3.7906
Epoch: 9/200
Train loss: 3.6263
Epoch: 10/200
Train loss: 3.5777
Epoch: 11/200
Train loss: 3.5099
Epoch: 12/200
Train loss: 3.3916
Epoch: 13/200
Train loss: 3.3461
Epoch: 14/200
Train loss: 3.2198
Epoch: 15/200
Train loss: 3.2441
Epoch: 16/200
Train loss: 3.1966
Epoch: 17/200
Train loss: 3.0497
Epoch: 18/200
Train loss: 2.9543
Epoch: 19/200
Train loss: 3.1492
Epoch: 20/200
Train loss: 3.0235
Epoch: 21/200
Train loss: 2.8886
Epoch: 22/200
Train loss: 2.7439
Epoch: 23/200
Train loss: 2.7983
Epoch: 24/200
Train loss: 2.8963
Epoch: 25/200
Train loss: 2.8461
Epoch: 26/200
Train loss: 2.8403
Epoch: 27/200
Train loss: 2.8074
Epoch: 28/200
Train loss: 2.6351
Epoch: 29/200
Train loss: 2.6950
Epoch: 30/200
Train loss: 2.5451
Epoch: 31/200
Train loss: 2.7032
Epoch: 32/200
Train loss: 2.7403
Epoch: 33/200
Train loss: 2.5048
Epoch: 34/200
Train loss: 2.6124
Epoch: 35/200
Train loss: 2.6086
Epoch: 36/200
Train loss: 2.6488
Epoch: 37/200
Train loss: 2.5276
Epoch: 38/200
Train loss: 2.7678
Epoch: 39/200
Train loss: 2.4424
Epoch: 40/200
Train loss: 2.5924
Epoch: 41/200
Train loss: 2.5053
Epoch: 42/200
Train loss: 2.4899
Epoch: 43/200
Train loss: 2.5890
Epoch: 44/200
Train loss: 2.5640
Epoch: 45/200
Train loss: 2.5596
Epoch: 46/200
Train loss: 2.3086
Epoch: 47/200
Train loss: 2.5220
Epoch: 48/200
Train loss: 2.2647
Epoch: 49/200
Train loss: 2.2584
Epoch: 50/200
Train loss: 2.3286
Epoch: 51/200
Train loss: 2.3722
Epoch: 52/200
Train loss: 2.3641
Epoch: 53/200
Train loss: 2.2762
Epoch: 54/200
Train loss: 2.3947
Epoch: 55/200
Train loss: 2.3642
Epoch: 56/200
Train loss: 2.3264
Epoch: 57/200
Train loss: 2.3515
Epoch: 58/200
Train loss: 2.3398
Epoch: 59/200
Train loss: 2.3468
Epoch: 60/200
Train loss: 2.3522
Epoch: 61/200
Train loss: 2.2595
Epoch: 62/200
Train loss: 2.2726
Epoch: 63/200
Train loss: 2.2812
Epoch: 64/200
Train loss: 2.2858
Epoch: 65/200
Train loss: 2.3516
Epoch: 66/200
Train loss: 2.2317
Epoch: 67/200
Train loss: 2.3328
Epoch: 68/200
Train loss: 2.2669
Epoch: 69/200
Train loss: 2.4610
Epoch: 70/200
Train loss: 2.2512
Epoch: 71/200
Train loss: 2.1236
Epoch: 72/200
Train loss: 2.1315
Epoch: 73/200
Train loss: 2.2722
Epoch: 74/200
Train loss: 2.2377
Epoch: 75/200
Train loss: 2.2747
Epoch: 76/200
Train loss: 2.2642
Epoch: 77/200
Train loss: 2.2345
Epoch: 78/200
Train loss: 2.1335
Epoch: 79/200
Train loss: 1.9871
Epoch: 80/200
Train loss: 2.1629
Epoch: 81/200
Train loss: 2.1679
Epoch: 82/200
Train loss: 2.1506
Epoch: 83/200
Train loss: 2.1083
Epoch: 84/200
Train loss: 2.0975
Epoch: 85/200
Train loss: 2.2457
Epoch: 86/200
Train loss: 2.1620
Epoch: 87/200
Train loss: 2.0350
Epoch: 88/200
Train loss: 2.2339
Epoch: 89/200
Train loss: 2.2713
Epoch: 90/200
Train loss: 2.0882
Epoch: 91/200
Train loss: 2.2086
Epoch: 92/200
Train loss: 2.2142
Epoch: 93/200
Train loss: 2.2370
Epoch: 94/200
Train loss: 2.1401
Epoch: 95/200
Train loss: 2.1818
Epoch: 96/200
Train loss: 2.2264
Epoch: 97/200
Train loss: 2.1666
Epoch: 98/200
Train loss: 2.1387
Epoch: 99/200
Train loss: 2.0764
Epoch: 100/200
Train loss: 2.1202
Epoch: 101/200
Train loss: 2.1897
Epoch: 102/200
Train loss: 1.9706
Epoch: 103/200
Train loss: 2.0280
Epoch: 104/200
Train loss: 2.0300
Epoch: 105/200
Train loss: 2.0853
Epoch: 106/200
Train loss: 2.0726
Epoch: 107/200
Train loss: 2.1456
Epoch: 108/200
Train loss: 2.1291
Epoch: 109/200
Train loss: 2.1048
Epoch: 110/200
Train loss: 2.1673
Epoch: 111/200
Train loss: 1.9620
Epoch: 112/200
Train loss: 2.0730
Epoch: 113/200
Train loss: 2.0512
Epoch: 114/200
Train loss: 2.1555
Epoch: 115/200
Train loss: 2.0474
Epoch: 116/200
Train loss: 2.0808
Epoch: 117/200
Train loss: 2.0421
Epoch: 118/200
Train loss: 2.0123
Epoch: 119/200
Train loss: 2.1343
Epoch: 120/200
Train loss: 2.0176
Epoch: 121/200
Train loss: 2.0147
Epoch: 122/200
Train loss: 2.0854
Epoch: 123/200
Train loss: 2.1971
Epoch: 124/200
Train loss: 2.1967
Epoch: 125/200
Train loss: 2.1588
Epoch: 126/200
Train loss: 1.9924
Epoch: 127/200
Train loss: 2.0702
Epoch: 128/200
Train loss: 1.9617
Epoch: 129/200
Train loss: 1.9877
Epoch: 130/200
Train loss: 1.8873
Epoch: 131/200
Train loss: 2.0000
Epoch: 132/200
Train loss: 2.1527
Epoch: 133/200
Train loss: 1.9334
Epoch: 134/200
Train loss: 2.0510
Epoch: 135/200
Train loss: 1.9232
Epoch: 136/200
Train loss: 2.0050
Epoch: 137/200
Train loss: 2.0335
Epoch: 138/200
Train loss: 1.9042
Epoch: 139/200
Train loss: 2.0434
Epoch: 140/200
Train loss: 1.9829
Epoch: 141/200
Train loss: 2.0057
Epoch: 142/200
Train loss: 1.9179
Epoch: 143/200
Train loss: 2.0717
Epoch: 144/200
Train loss: 2.0448
Epoch: 145/200
Train loss: 1.9077
Epoch: 146/200
Train loss: 2.0716
Epoch: 147/200
Train loss: 2.0427
Epoch: 148/200
Train loss: 2.0980
Epoch: 149/200
Train loss: 1.9739
Epoch: 150/200
Train loss: 1.8037
Epoch: 151/200
Train loss: 1.9822
Epoch: 152/200
Train loss: 1.9099
Epoch: 153/200
Train loss: 1.8727
Epoch: 154/200
Train loss: 1.9655
Epoch: 155/200
Train loss: 2.0350
Epoch: 156/200
Train loss: 2.0071
Epoch: 157/200
Train loss: 1.9793
Epoch: 158/200
Train loss: 1.8234
0.8019375582821916
Model improve: 0.000000 -> 0.801938
Epoch: 159/200
Date :05/18/2023, 14:10:41
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_s
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.3
drop_path_rate: 0.45
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 31.2203
Epoch: 2/200
Date :05/18/2023, 14:13:18
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_s
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.4
drop_path_rate: 0.45
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 31.2269
Epoch: 2/200
Train loss: 5.5324
Epoch: 3/200
Train loss: 5.0385
Epoch: 4/200
Train loss: 4.7941
Epoch: 5/200
Train loss: 4.4780
Epoch: 6/200
Train loss: 4.2524
Epoch: 7/200
Train loss: 3.9834
Epoch: 8/200
Train loss: 3.9383
Epoch: 9/200
Train loss: 3.7795
Epoch: 10/200
Train loss: 3.7337
Epoch: 11/200
Train loss: 3.6642
Epoch: 12/200
Train loss: 3.5465
Epoch: 13/200
Train loss: 3.5088
Epoch: 14/200
Train loss: 3.3921
Epoch: 15/200
Train loss: 3.4051
Epoch: 16/200
Train loss: 3.3636
Epoch: 17/200
Train loss: 3.2269
Epoch: 18/200
Train loss: 3.1284
Epoch: 19/200
Train loss: 3.3136
Epoch: 20/200
Train loss: 3.1915
Epoch: 21/200
Train loss: 3.0586
Epoch: 22/200
Train loss: 2.9136
Epoch: 23/200
Train loss: 2.9704
Epoch: 24/200
Train loss: 3.0577
Epoch: 25/200
Train loss: 3.0075
Epoch: 26/200
Train loss: 3.0024
Epoch: 27/200
Train loss: 2.9714
Epoch: 28/200
Train loss: 2.8011
Epoch: 29/200
Train loss: 2.8628
Epoch: 30/200
Train loss: 2.7115
Epoch: 31/200
Train loss: 2.8621
Epoch: 32/200
Train loss: 2.8931
Epoch: 33/200
Train loss: 2.6676
Epoch: 34/200
Train loss: 2.7670
Epoch: 35/200
Train loss: 2.7599
Epoch: 36/200
Train loss: 2.7952
Epoch: 37/200
Train loss: 2.6770
Epoch: 38/200
Train loss: 2.9088
Epoch: 39/200
Train loss: 2.5893
Epoch: 40/200
Train loss: 2.7334
Epoch: 41/200
Train loss: 2.6519
Epoch: 42/200
Train loss: 2.6276
Epoch: 43/200
Train loss: 2.7303
Epoch: 44/200
Train loss: 2.7027
Epoch: 45/200
Train loss: 2.6939
Epoch: 46/200
Train loss: 2.4444
Epoch: 47/200
Train loss: 2.6531
Epoch: 48/200
Train loss: 2.3978
Epoch: 49/200
Train loss: 2.3921
Epoch: 50/200
Train loss: 2.4612
Epoch: 51/200
Train loss: 2.5030
Epoch: 52/200
Train loss: 2.4874
Epoch: 53/200
Train loss: 2.4065
Epoch: 54/200
Train loss: 2.5221
Epoch: 55/200
Train loss: 2.4876
Epoch: 56/200
Train loss: 2.4481
Epoch: 57/200
Train loss: 2.4734
Epoch: 58/200
Train loss: 2.4694
Epoch: 59/200
Train loss: 2.4704
Epoch: 60/200
Train loss: 2.4761
Epoch: 61/200
Train loss: 2.3797
Epoch: 62/200
Train loss: 2.3947
Epoch: 63/200
Train loss: 2.4022
Epoch: 64/200
Train loss: 2.4071
Epoch: 65/200
Train loss: 2.4690
Epoch: 66/200
Train loss: 2.3488
Epoch: 67/200
Train loss: 2.4506
Epoch: 68/200
Train loss: 2.3785
Epoch: 69/200
Train loss: 2.5796
Epoch: 70/200
Train loss: 2.3691
Epoch: 71/200
Train loss: 2.2373
Epoch: 72/200
Train loss: 2.2530
Epoch: 73/200
Train loss: 2.3822
Epoch: 74/200
Train loss: 2.3592
Epoch: 75/200
Train loss: 2.3937
Epoch: 76/200
Train loss: 2.3775
Epoch: 77/200
Train loss: 2.3497
Epoch: 78/200
Train loss: 2.2500
Epoch: 79/200
Train loss: 2.1001
Epoch: 80/200
Train loss: 2.2733
Epoch: 81/200
Train loss: 2.2750
Epoch: 82/200
Train loss: 2.2645
Epoch: 83/200
Train loss: 2.2209
Epoch: 84/200
Train loss: 2.2113
Epoch: 85/200
Train loss: 2.3597
Epoch: 86/200
Train loss: 2.2772
Epoch: 87/200
Train loss: 2.1431
Epoch: 88/200
Train loss: 2.3489
Epoch: 89/200
Train loss: 2.3831
Epoch: 90/200
Train loss: 2.2022
Epoch: 91/200
Train loss: 2.3215
Epoch: 92/200
Train loss: 2.3209
Epoch: 93/200
Train loss: 2.3463
Epoch: 94/200
Train loss: 2.2509
Epoch: 95/200
Train loss: 2.3011
Epoch: 96/200
Train loss: 2.3425
Epoch: 97/200
Train loss: 2.2784
Epoch: 98/200
Train loss: 2.2469
Epoch: 99/200
Train loss: 2.1880
Epoch: 100/200
Train loss: 2.2382
Epoch: 101/200
Train loss: 2.2983
Epoch: 102/200
Train loss: 2.0807
Epoch: 103/200
Train loss: 2.1376
Epoch: 104/200
Train loss: 2.1390
Epoch: 105/200
Train loss: 2.1940
Epoch: 106/200
Train loss: 2.1779
Epoch: 107/200
Train loss: 2.2585
Epoch: 108/200
Train loss: 2.2407
Epoch: 109/200
Train loss: 2.2156
Epoch: 110/200
Train loss: 2.2747
Epoch: 111/200
Train loss: 2.0694
Epoch: 112/200
Train loss: 2.1723
Epoch: 113/200
Train loss: 2.1571
Epoch: 114/200
Train loss: 2.2634
Epoch: 115/200
Train loss: 2.1502
Epoch: 116/200
Train loss: 2.1946
Epoch: 117/200
Train loss: 2.1504
Epoch: 118/200
Train loss: 2.1171
Epoch: 119/200
Train loss: 2.2488
Epoch: 120/200
Train loss: 2.1260
Epoch: 121/200
Train loss: 2.1222
Epoch: 122/200
Train loss: 2.1926
Epoch: 123/200
Train loss: 2.3082
Epoch: 124/200
Train loss: 2.3043
Epoch: 125/200
Train loss: 2.2671
Epoch: 126/200
Train loss: 2.1052
Epoch: 127/200
Train loss: 2.1714
Epoch: 128/200
Train loss: 2.0711
Epoch: 129/200
Train loss: 2.0943
Epoch: 130/200
Train loss: 1.9981
Epoch: 131/200
Train loss: 2.1061
Epoch: 132/200
Train loss: 2.2558
Epoch: 133/200
Train loss: 2.0426
Epoch: 134/200
Train loss: 2.1581
Epoch: 135/200
Train loss: 2.0284
Epoch: 136/200
Train loss: 2.1113
Epoch: 137/200
Train loss: 2.1371
Epoch: 138/200
Train loss: 2.0128
Epoch: 139/200
Train loss: 2.1567
Epoch: 140/200
Train loss: 2.0879
Epoch: 141/200
Train loss: 2.1037
Epoch: 142/200
Train loss: 2.0274
Epoch: 143/200
Train loss: 2.1813
Epoch: 144/200
Train loss: 2.1506
Epoch: 145/200
Train loss: 2.0157
Epoch: 146/200
Train loss: 2.1813
Epoch: 147/200
Train loss: 2.1545
Epoch: 148/200
Train loss: 2.2122
Epoch: 149/200
Train loss: 2.0841
Epoch: 150/200
Train loss: 1.9066
Epoch: 151/200
Train loss: 2.0901
Epoch: 152/200
Train loss: 2.0173
Epoch: 153/200
Train loss: 1.9801
Epoch: 154/200
Train loss: 2.0717
Epoch: 155/200
Train loss: 2.1471
Epoch: 156/200
Train loss: 2.1156
Epoch: 157/200
Train loss: 2.0876
Epoch: 158/200
Train loss: 1.9278
0.8086057760755718
Model improve: 0.000000 -> 0.808606
Epoch: 159/200
Train loss: 2.1495
0.8057837161582838
Epoch: 160/200
Train loss: 1.9534
0.807720239498942
Epoch: 161/200
Train loss: 2.2152
0.8065990648053318
Epoch: 162/200
Train loss: 2.0529
0.8053906606001036
Epoch: 163/200
Train loss: 2.0499
0.8067679198012992
Epoch: 164/200
Train loss: 2.1619
0.8047715511031738
Epoch: 165/200
Train loss: 2.1916
0.8051434339935911
Epoch: 166/200
Train loss: 2.1244
0.8069574948350604
Epoch: 167/200
Train loss: 2.1925
0.8055455345656732
Epoch: 168/200
Train loss: 2.0969
0.8082539780230155
Epoch: 169/200
Train loss: 2.1563
0.8072402995215078
Epoch: 170/200
Train loss: 2.0384
0.805525255139652
Epoch: 171/200
Train loss: 2.1540
0.80386286206539
Epoch: 172/200
Train loss: 2.0429
0.8070392567844161
Epoch: 173/200
Train loss: 2.1658
0.8068863343811103
Epoch: 174/200
Train loss: 2.0202
0.8060145382305333
Epoch: 175/200
Train loss: 2.1510
0.8039848257716267
Epoch: 176/200
Train loss: 1.9238
0.8092483201169629
Model improve: 0.808606 -> 0.809248
Epoch: 177/200
Train loss: 2.0169
0.8060857924129072
Epoch: 178/200
Train loss: 2.0389
0.8081072684421036
Epoch: 179/200
Train loss: 1.9992
0.8078925233664239
Epoch: 180/200
Train loss: 2.1912
0.8044529776821226
Epoch: 181/200
Train loss: 2.0639
0.8070735228170886
Epoch: 182/200
Train loss: 1.9630
0.806022121627817
Epoch: 183/200
Train loss: 2.0150
0.8081943527704769
Epoch: 184/200
Train loss: 2.1749
0.8063939837919888
Epoch: 185/200
Train loss: 2.0399
0.808215106130808
Epoch: 186/200
Train loss: 2.1887
0.8060438487582554
Epoch: 187/200
Train loss: 2.0013
0.807563991820136
Epoch: 188/200
Train loss: 2.1920
0.8058370736334257
Epoch: 189/200
Train loss: 2.1546
0.8047290173250284
Epoch: 190/200
Date :05/18/2023, 19:40:40
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_s
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.25
drop_path_rate: 0.4
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Date :05/18/2023, 19:41:28
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_s
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.25
drop_path_rate: 0.4
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Date :05/18/2023, 19:41:47
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_s
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.25
drop_path_rate: 0.4
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 29.3701
Epoch: 2/200
Train loss: 5.5137
Epoch: 3/200
Train loss: 5.0223
Epoch: 4/200
Train loss: 4.6296
Epoch: 5/200
Train loss: 4.4485
Epoch: 6/200
Train loss: 4.2131
Epoch: 7/200
Train loss: 4.1526
Epoch: 8/200
Train loss: 3.8991
Epoch: 9/200
Train loss: 3.8107
Epoch: 10/200
Train loss: 3.7033
Epoch: 11/200
Train loss: 3.7493
Epoch: 12/200
Train loss: 3.5093
Epoch: 13/200
Train loss: 3.4333
Epoch: 14/200
Train loss: 3.5030
Epoch: 15/200
Train loss: 3.3730
Epoch: 16/200
Train loss: 3.3828
Epoch: 17/200
Train loss: 3.1058
Epoch: 18/200
Train loss: 3.2367
Epoch: 19/200
Train loss: 3.1826
Epoch: 20/200
Train loss: 3.0552
Epoch: 21/200
Train loss: 3.0649
Epoch: 22/200
Train loss: 2.9424
Epoch: 23/200
Train loss: 3.0441
Epoch: 24/200
Train loss: 2.8872
Epoch: 25/200
Train loss: 2.9222
Epoch: 26/200
Train loss: 2.8778
Epoch: 27/200
Train loss: 2.9525
Epoch: 28/200
Train loss: 2.8692
Epoch: 29/200
Train loss: 2.8403
Epoch: 30/200
Train loss: 2.7364
Epoch: 31/200
Train loss: 2.7000
Epoch: 32/200
Train loss: 2.8757
Epoch: 33/200
Train loss: 2.6675
Epoch: 34/200
Train loss: 2.6853
Epoch: 35/200
Train loss: 2.6865
Epoch: 36/200
Train loss: 2.6377
Epoch: 37/200
Train loss: 2.7023
Epoch: 38/200
Train loss: 2.6035
Epoch: 39/200
Train loss: 2.7040
Epoch: 40/200
Train loss: 2.6845
Epoch: 41/200
Train loss: 2.6546
Epoch: 42/200
Train loss: 2.5252
Epoch: 43/200
Train loss: 2.5034
Epoch: 44/200
Train loss: 2.6684
Epoch: 45/200
Train loss: 2.5435
Epoch: 46/200
Train loss: 2.4448
Epoch: 47/200
Train loss: 2.5890
Epoch: 48/200
Train loss: 2.5263
Epoch: 49/200
Train loss: 2.4878
Epoch: 50/200
Train loss: 2.5764
Epoch: 51/200
Train loss: 2.4836
Epoch: 52/200
Train loss: 2.5459
Epoch: 53/200
Train loss: 2.5725
Epoch: 54/200
Train loss: 2.4247
Epoch: 55/200
Train loss: 2.3996
Epoch: 56/200
Train loss: 2.4593
Epoch: 57/200
Train loss: 2.4723
Epoch: 58/200
Train loss: 2.4876
Epoch: 59/200
Train loss: 2.4247
Epoch: 60/200
Train loss: 2.3918
Epoch: 61/200
Train loss: 2.4111
Epoch: 62/200
Train loss: 2.3174
Epoch: 63/200
Train loss: 2.3904
Epoch: 64/200
Train loss: 2.4634
Epoch: 65/200
Train loss: 2.4602
Epoch: 66/200
Train loss: 2.3655
Epoch: 67/200
Train loss: 2.3052
Epoch: 68/200
Train loss: 2.2972
Epoch: 69/200
Train loss: 2.4064
Epoch: 70/200
Train loss: 2.3631
Epoch: 71/200
Train loss: 2.4597
Epoch: 72/200
Train loss: 2.3647
Epoch: 73/200
Train loss: 2.2420
Epoch: 74/200
Train loss: 2.4459
Epoch: 75/200
Train loss: 2.2345
Epoch: 76/200
Train loss: 2.3737
Epoch: 77/200
Train loss: 2.3195
Epoch: 78/200
Train loss: 2.3796
Epoch: 79/200
Train loss: 2.3824
Epoch: 80/200
Train loss: 2.3402
Epoch: 81/200
Train loss: 2.3300
Epoch: 82/200
Train loss: 2.4072
Epoch: 83/200
Train loss: 2.1763
Epoch: 84/200
Train loss: 2.3056
Epoch: 85/200
Train loss: 2.3124
Epoch: 86/200
Train loss: 2.3097
Epoch: 87/200
Train loss: 2.3298
Epoch: 88/200
Train loss: 2.2264
Epoch: 89/200
Train loss: 2.2470
Epoch: 90/200
Train loss: 2.2626
Epoch: 91/200
Train loss: 2.2599
Epoch: 92/200
Train loss: 2.1300
Epoch: 93/200
Train loss: 2.2913
Epoch: 94/200
Train loss: 2.2701
Epoch: 95/200
Train loss: 2.2087
Epoch: 96/200
Train loss: 2.0910
Epoch: 97/200
Train loss: 2.1949
Epoch: 98/200
Train loss: 2.0474
Epoch: 99/200
Train loss: 2.2843
Epoch: 100/200
Train loss: 2.1948
Epoch: 101/200
Train loss: 0.4383
Epoch: 102/200
Train loss: 0.4035
0.7984994636586668
Model improve: 0.000000 -> 0.798499
Epoch: 103/200
Train loss: 0.3868
0.7992416915562356
Model improve: 0.798499 -> 0.799242
Epoch: 104/200
Train loss: 0.3915
0.7981842889969941
Epoch: 105/200
Train loss: 0.3815
0.7980920401857663
Epoch: 106/200
Train loss: 0.3741
0.7967695064908077
Epoch: 107/200
Train loss: 0.3524
0.7979932170280694
Epoch: 108/200
Train loss: 0.3609
0.7967803993049088
Epoch: 109/200
Train loss: 0.3440
0.7965337797928684
Epoch: 110/200
Train loss: 0.3343
0.7970411976787407
Epoch: 111/200
Train loss: 0.3220
0.7963031575428537
Epoch: 112/200
Train loss: 0.3350
0.7951764107198406
Epoch: 113/200
Train loss: 0.3162
0.7960859331374276
Epoch: 114/200
Train loss: 0.3278
0.7932919042785338
Epoch: 115/200
Train loss: 0.3234
0.7942636290620276
Epoch: 116/200
Train loss: 0.3066
0.7950592412945386
Epoch: 117/200
Train loss: 0.3044
0.7939243450491427
Epoch: 118/200
Train loss: 0.2869
0.7941202682254033
Epoch: 119/200
Train loss: 0.2972
0.7940144352170121
Epoch: 120/200
Train loss: 0.2952
0.7932423541395359
Epoch: 121/200
Train loss: 0.2863
0.7939092703685229
Epoch: 122/200
Train loss: 0.2928
Date :05/18/2023, 23:13:54
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_s
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.0
drop_path_rate: 0.0
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 28.7395
Epoch: 2/200
Date :05/18/2023, 23:16:59
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_s
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.3
drop_path_rate: 0.45
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 31.2210
Epoch: 2/200
Train loss: 5.5298
Epoch: 3/200
Train loss: 5.0367
Epoch: 4/200
Train loss: 4.7928
Epoch: 5/200
Train loss: 4.4769
Epoch: 6/200
Train loss: 4.2513
Epoch: 7/200
Train loss: 3.9828
Epoch: 8/200
Train loss: 3.9374
Epoch: 9/200
Train loss: 3.7787
Epoch: 10/200
Train loss: 3.7329
Epoch: 11/200
Train loss: 3.6635
Epoch: 12/200
Train loss: 3.5458
Epoch: 13/200
Train loss: 3.5080
Epoch: 14/200
Train loss: 3.3912
Epoch: 15/200
Train loss: 3.4048
Epoch: 16/200
Train loss: 3.3627
Epoch: 17/200
Train loss: 3.2258
Epoch: 18/200
Train loss: 3.1271
Epoch: 19/200
Train loss: 3.3133
Epoch: 20/200
Train loss: 3.1913
Epoch: 21/200
Train loss: 3.0578
Epoch: 22/200
Train loss: 2.9122
Epoch: 23/200
Train loss: 2.9693
Epoch: 24/200
Train loss: 3.0571
Epoch: 25/200
Train loss: 3.0072
Epoch: 26/200
Train loss: 3.0008
Epoch: 27/200
Train loss: 2.9706
Epoch: 28/200
Train loss: 2.7999
Epoch: 29/200
Train loss: 2.8610
Epoch: 30/200
Train loss: 2.7101
Epoch: 31/200
Train loss: 2.8615
Epoch: 32/200
Train loss: 2.8930
Epoch: 33/200
Train loss: 2.6663
Epoch: 34/200
Train loss: 2.7659
Epoch: 35/200
Train loss: 2.7592
Epoch: 36/200
Train loss: 2.7946
Epoch: 37/200
Train loss: 2.6768
Epoch: 38/200
Train loss: 2.9086
Epoch: 39/200
Train loss: 2.5889
Epoch: 40/200
Train loss: 2.7325
Epoch: 41/200
Train loss: 2.6505
Epoch: 42/200
Train loss: 2.6275
Epoch: 43/200
Train loss: 2.7303
Epoch: 44/200
Train loss: 2.7018
Epoch: 45/200
Train loss: 2.6936
Epoch: 46/200
Train loss: 2.4436
Epoch: 47/200
Train loss: 2.6528
Epoch: 48/200
Train loss: 2.3968
Epoch: 49/200
Train loss: 2.3912
Epoch: 50/200
Train loss: 2.4613
Epoch: 51/200
Train loss: 2.5026
Epoch: 52/200
Train loss: 2.4882
Epoch: 53/200
Train loss: 2.4064
Epoch: 54/200
Train loss: 2.5219
Epoch: 55/200
Train loss: 2.4875
Epoch: 56/200
Train loss: 2.4481
Epoch: 57/200
Train loss: 2.4732
Epoch: 58/200
Train loss: 2.4685
Epoch: 59/200
Train loss: 2.4702
Epoch: 60/200
Train loss: 2.4760
Epoch: 61/200
Train loss: 2.3792
Epoch: 62/200
Train loss: 2.3952
Epoch: 63/200
Train loss: 2.4016
Epoch: 64/200
Train loss: 2.4066
Epoch: 65/200
Train loss: 2.4686
Epoch: 66/200
Train loss: 2.3493
Epoch: 67/200
Train loss: 2.4516
Epoch: 68/200
Train loss: 2.3782
Epoch: 69/200
Train loss: 2.5778
Epoch: 70/200
Train loss: 2.3698
Epoch: 71/200
Train loss: 2.2375
Epoch: 72/200
Train loss: 2.2526
Epoch: 73/200
Train loss: 2.3808
Epoch: 74/200
Train loss: 2.3588
Epoch: 75/200
Train loss: 2.3934
Epoch: 76/200
Train loss: 2.3781
Epoch: 77/200
Train loss: 2.3486
Epoch: 78/200
Train loss: 2.2492
Epoch: 79/200
Train loss: 2.0994
Epoch: 80/200
Train loss: 2.2740
Epoch: 81/200
Train loss: 2.2748
Epoch: 82/200
Train loss: 2.2630
Epoch: 83/200
Train loss: 2.2200
Epoch: 84/200
Train loss: 2.2106
Epoch: 85/200
Train loss: 2.3594
Epoch: 86/200
Train loss: 2.2770
Epoch: 87/200
Train loss: 2.1435
Epoch: 88/200
Train loss: 2.3487
Epoch: 89/200
Train loss: 2.3831
Epoch: 90/200
Train loss: 2.2027
Epoch: 91/200
Train loss: 2.3210
Epoch: 92/200
Train loss: 2.3208
Epoch: 93/200
Train loss: 2.3454
Epoch: 94/200
Train loss: 2.2501
Epoch: 95/200
Train loss: 2.3020
Epoch: 96/200
Train loss: 2.3417
Epoch: 97/200
Train loss: 2.2771
Epoch: 98/200
Train loss: 2.2472
Epoch: 99/200
Train loss: 2.1875
Epoch: 100/200
Train loss: 2.2370
Epoch: 101/200
Train loss: 2.2370
Epoch: 102/200
Train loss: 2.2370
0.8038973960125048
Model improve: 0.000000 -> 0.803897
Epoch: 103/200
Train loss: 2.2370
0.8038973960125048
Epoch: 104/200
Train loss: 2.2370
0.8038973960125048
Epoch: 105/200
Train loss: 2.2370
0.8038973960125048
Epoch: 106/200
Train loss: 2.2370
Date :05/19/2023, 01:35:03
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b2
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.2
drop_path_rate: 0.35
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 41.3836
Epoch: 2/200
Train loss: 5.6780
Epoch: 3/200
Train loss: 5.1900
Epoch: 4/200
Train loss: 4.9309
Epoch: 5/200
Train loss: 4.6306
Epoch: 6/200
Train loss: 4.3866
Epoch: 7/200
Train loss: 4.0805
Epoch: 8/200
Train loss: 4.0455
Epoch: 9/200
Train loss: 3.8758
Epoch: 10/200
Train loss: 3.8034
Epoch: 11/200
Train loss: 3.6928
Epoch: 12/200
Train loss: 3.5905
Epoch: 13/200
Train loss: 3.5586
Epoch: 14/200
Train loss: 3.4301
Epoch: 15/200
Train loss: 3.4469
Epoch: 16/200
Train loss: 3.3811
Epoch: 17/200
Train loss: 3.2369
Epoch: 18/200
Train loss: 3.1332
Epoch: 19/200
Train loss: 3.3457
Epoch: 20/200
Train loss: 3.2198
Epoch: 21/200
Train loss: 3.0760
Epoch: 22/200
Train loss: 2.9497
Epoch: 23/200
Train loss: 2.9974
Epoch: 24/200
Train loss: 3.0662
Epoch: 25/200
Train loss: 3.0219
Epoch: 26/200
Train loss: 2.9996
Epoch: 27/200
Train loss: 3.0058
Epoch: 28/200
Train loss: 2.8379
Epoch: 29/200
Train loss: 2.8622
Epoch: 30/200
Train loss: 2.6890
Epoch: 31/200
Train loss: 2.8405
Epoch: 32/200
Train loss: 2.8948
Epoch: 33/200
Train loss: 2.6791
Epoch: 34/200
Train loss: 2.7746
Epoch: 35/200
Train loss: 2.7610
Epoch: 36/200
Train loss: 2.7972
Epoch: 37/200
Train loss: 2.6616
Epoch: 38/200
Train loss: 2.9046
Epoch: 39/200
Train loss: 2.5848
Epoch: 40/200
Train loss: 2.7051
Epoch: 41/200
Train loss: 2.6482
Epoch: 42/200
Train loss: 2.6422
Epoch: 43/200
Train loss: 2.7413
Epoch: 44/200
Train loss: 2.7054
Epoch: 45/200
Train loss: 2.7155
Epoch: 46/200
Train loss: 2.4699
Epoch: 47/200
Train loss: 2.6681
Epoch: 48/200
Train loss: 2.4137
Epoch: 49/200
Train loss: 2.3994
Epoch: 50/200
Train loss: 2.4704
Epoch: 51/200
Train loss: 2.5204
Epoch: 52/200
Train loss: 2.4935
Epoch: 53/200
Train loss: 2.4407
Epoch: 54/200
Train loss: 2.5420
Epoch: 55/200
Train loss: 2.5007
Epoch: 56/200
Train loss: 2.4502
Epoch: 57/200
Train loss: 2.4802
Epoch: 58/200
Train loss: 2.4707
Epoch: 59/200
Train loss: 2.4697
Epoch: 60/200
Train loss: 2.4995
Epoch: 61/200
Train loss: 2.3786
Epoch: 62/200
Train loss: 2.3940
Epoch: 63/200
Train loss: 2.4383
Epoch: 64/200
Train loss: 2.4247
Epoch: 65/200
Train loss: 2.4645
Epoch: 66/200
Train loss: 2.3759
Epoch: 67/200
Train loss: 2.4408
Epoch: 68/200
Train loss: 2.4281
Epoch: 69/200
Train loss: 2.5795
Epoch: 70/200
Train loss: 2.3675
Epoch: 71/200
Train loss: 2.2520
Epoch: 72/200
Train loss: 2.2588
Epoch: 73/200
Train loss: 2.4027
Epoch: 74/200
Train loss: 2.3842
Epoch: 75/200
Train loss: 2.4236
Epoch: 76/200
Train loss: 2.3924
Epoch: 77/200
Train loss: 2.3640
Epoch: 78/200
Train loss: 2.2801
Epoch: 79/200
Train loss: 2.1156
Epoch: 80/200
Train loss: 2.2692
Epoch: 81/200
Train loss: 2.2996
Epoch: 82/200
Train loss: 2.3035
Epoch: 83/200
Train loss: 2.2464
Epoch: 84/200
Train loss: 2.2314
Epoch: 85/200
Train loss: 2.3521
Epoch: 86/200
Train loss: 2.2950
Epoch: 87/200
Train loss: 2.1736
Epoch: 88/200
Train loss: 2.3638
Epoch: 89/200
Train loss: 2.3824
Epoch: 90/200
Train loss: 2.2412
Epoch: 91/200
Train loss: 2.3518
Epoch: 92/200
Train loss: 2.3178
Epoch: 93/200
Train loss: 2.3588
Epoch: 94/200
Train loss: 2.2531
Epoch: 95/200
Train loss: 2.3023
Epoch: 96/200
Train loss: 2.3694
Epoch: 97/200
Train loss: 2.3020
Epoch: 98/200
Train loss: 2.2776
Epoch: 99/200
Train loss: 2.1965
Epoch: 100/200
Train loss: 2.2537
Epoch: 101/200
Train loss: 2.3053
Epoch: 102/200
Train loss: 2.0916
0.8111812501836134
Model improve: 0.000000 -> 0.811181
Epoch: 103/200
Train loss: 2.1539
0.8097720530989788
Epoch: 104/200
Train loss: 2.1689
0.8111725299897278
Epoch: 105/200
Date :05/19/2023, 03:19:32
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b2
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.5
drop_path_rate: 0.4
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 41.8387
Epoch: 2/200
Train loss: 5.7038
Epoch: 3/200
Train loss: 5.2128
Epoch: 4/200
Train loss: 4.9559
Epoch: 5/200
Train loss: 4.6573
Epoch: 6/200
Train loss: 4.4162
Epoch: 7/200
Train loss: 4.1114
Epoch: 8/200
Train loss: 4.0752
Epoch: 9/200
Train loss: 3.9097
Epoch: 10/200
Train loss: 3.8342
Epoch: 11/200
Train loss: 3.7213
Epoch: 12/200
Train loss: 3.6210
Epoch: 13/200
Train loss: 3.5884
Epoch: 14/200
Train loss: 3.4591
Epoch: 15/200
Train loss: 3.4751
Epoch: 16/200
Train loss: 3.4092
Epoch: 17/200
Train loss: 3.2680
Epoch: 18/200
Train loss: 3.1623
Epoch: 19/200
Train loss: 3.3741
Epoch: 20/200
Train loss: 3.2483
Epoch: 21/200
Train loss: 3.1065
Epoch: 22/200
Train loss: 2.9787
Epoch: 23/200
Train loss: 3.0270
Epoch: 24/200
Train loss: 3.0963
Epoch: 25/200
Train loss: 3.0535
Epoch: 26/200
Train loss: 3.0287
Epoch: 27/200
Train loss: 3.0342
Epoch: 28/200
Train loss: 2.8684
Epoch: 29/200
Train loss: 2.8907
Epoch: 30/200
Train loss: 2.7189
Epoch: 31/200
Train loss: 2.8702
Epoch: 32/200
Train loss: 2.9231
Epoch: 33/200
Train loss: 2.7088
Epoch: 34/200
Train loss: 2.8057
Epoch: 35/200
Train loss: 2.7908
Epoch: 36/200
Train loss: 2.8277
Epoch: 37/200
Train loss: 2.6896
Epoch: 38/200
Train loss: 2.9334
Epoch: 39/200
Train loss: 2.6130
Epoch: 40/200
Train loss: 2.7333
Epoch: 41/200
Train loss: 2.6756
Epoch: 42/200
Train loss: 2.6691
Epoch: 43/200
Train loss: 2.7688
Epoch: 44/200
Train loss: 2.7320
Epoch: 45/200
Train loss: 2.7431
Epoch: 46/200
Train loss: 2.4982
Epoch: 47/200
Train loss: 2.6927
Epoch: 48/200
Train loss: 2.4404
Epoch: 49/200
Train loss: 2.4285
Epoch: 50/200
Train loss: 2.4963
Epoch: 51/200
Train loss: 2.5471
Epoch: 52/200
Train loss: 2.5207
Epoch: 53/200
Train loss: 2.4676
Epoch: 54/200
Train loss: 2.5682
Epoch: 55/200
Train loss: 2.5271
Epoch: 56/200
Train loss: 2.4771
Epoch: 57/200
Train loss: 2.5043
Epoch: 58/200
Train loss: 2.4970
Epoch: 59/200
Train loss: 2.4936
Epoch: 60/200
Train loss: 2.5272
Epoch: 61/200
Train loss: 2.4035
Epoch: 62/200
Train loss: 2.4200
Epoch: 63/200
Train loss: 2.4621
Epoch: 64/200
Train loss: 2.4488
Epoch: 65/200
Train loss: 2.4897
Epoch: 66/200
Train loss: 2.4003
Epoch: 67/200
Train loss: 2.4685
Epoch: 68/200
Train loss: 2.4538
Epoch: 69/200
Train loss: 2.6032
Epoch: 70/200
Train loss: 2.3912
Epoch: 71/200
Train loss: 2.2773
Epoch: 72/200
Train loss: 2.2834
Epoch: 73/200
Train loss: 2.4229
Epoch: 74/200
Train loss: 2.4096
Epoch: 75/200
Train loss: 2.4486
Epoch: 76/200
Train loss: 2.4169
Epoch: 77/200
Train loss: 2.3890
Epoch: 78/200
Train loss: 2.3016
Epoch: 79/200
Train loss: 2.1395
Epoch: 80/200
Train loss: 2.2937
Epoch: 81/200
Train loss: 2.3228
Epoch: 82/200
Train loss: 2.3291
Epoch: 83/200
Train loss: 2.2697
Epoch: 84/200
Train loss: 2.2566
Epoch: 85/200
Train loss: 2.3754
Epoch: 86/200
Train loss: 2.3199
Epoch: 87/200
Train loss: 2.1976
Epoch: 88/200
Train loss: 2.3880
Epoch: 89/200
Train loss: 2.4046
Epoch: 90/200
Train loss: 2.2644
Epoch: 91/200
Train loss: 2.3747
Epoch: 92/200
Train loss: 2.3403
Epoch: 93/200
Train loss: 2.3812
Epoch: 94/200
Train loss: 2.2785
Epoch: 95/200
Train loss: 2.3276
Epoch: 96/200
Train loss: 2.3915
Epoch: 97/200
Train loss: 2.3253
Epoch: 98/200
Train loss: 2.3000
Epoch: 99/200
Train loss: 2.2181
Epoch: 100/200
Train loss: 2.2779
Epoch: 101/200
Train loss: 2.3288
Epoch: 102/200
Train loss: 2.1147
0.8133531499678407
Model improve: 0.000000 -> 0.813353
Epoch: 103/200
Train loss: 2.1776
0.8110645178370797
Epoch: 104/200
Train loss: 2.1906
0.8127238073995626
Epoch: 105/200
Train loss: 2.2351
0.8120475271269303
Epoch: 106/200
Train loss: 2.2323
0.8119607309769581
Epoch: 107/200
Train loss: 2.2957
0.8110433052544104
Epoch: 108/200
Train loss: 2.2841
0.8114979530993417
Epoch: 109/200
Train loss: 2.2715
0.8116240333729655
Epoch: 110/200
Train loss: 2.3205
0.8108273301492002
Epoch: 111/200
Train loss: 2.1046
0.8129491336195527
Epoch: 112/200
Train loss: 2.2224
0.810085591590667
Epoch: 113/200
Train loss: 2.2005
0.8125668270697689
Epoch: 114/200
Train loss: 2.3183
0.8112417011958132
Epoch: 115/200
Train loss: 2.1900
0.8123900800728212
Epoch: 116/200
Train loss: 2.2499
0.8124389217871806
Epoch: 117/200
Train loss: 2.2018
0.8123661524920226
Epoch: 118/200
Train loss: 2.1747
0.8121938612631615
Epoch: 119/200
Train loss: 2.2951
0.812561207321602
Epoch: 120/200
Train loss: 2.1792
0.8132461705044005
Epoch: 121/200
Train loss: 2.1854
0.813405690531157
Model improve: 0.813353 -> 0.813406
Epoch: 122/200
Train loss: 2.2617
0.8147608783835206
Model improve: 0.813406 -> 0.814761
Epoch: 123/200
Train loss: 2.3595
0.8132733462208597
Epoch: 124/200
Train loss: 2.3593
0.813057378055427
Epoch: 125/200
Train loss: 2.3064
0.8143058870639598
Epoch: 126/200
Train loss: 2.1518
0.8133160269619661
Epoch: 127/200
Train loss: 2.2303
0.8123323163595907
Epoch: 128/200
Train loss: 2.1147
0.8137740966211323
Epoch: 129/200
Train loss: 2.1450
0.8146228175276338
Epoch: 130/200
Train loss: 2.0696
0.8130207778223754
Epoch: 131/200
Date :05/19/2023, 06:51:54
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.25
drop_path_rate: 0.4
76407
Fold: 1
13839
SAM (
Parameter Group 0
    adaptive: False
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: False
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    rho: 0.05
    weight_decay: 0.01
)
Epoch: 1/200
Date :05/19/2023, 06:52:18
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.25
drop_path_rate: 0.4
76407
Fold: 1
13839
SAM (
Parameter Group 0
    adaptive: False
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: False
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    rho: 0.05
    weight_decay: 0.01
)
Epoch: 1/200
Date :05/19/2023, 06:52:48
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.25
drop_path_rate: 0.4
76407
Fold: 1
13839
SAM (
Parameter Group 0
    adaptive: False
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: False
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    rho: 0.05
    weight_decay: 0.01
)
Epoch: 1/200
Date :05/19/2023, 07:54:45
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.25
drop_path_rate: 0.4
76407
Fold: 1
13839
SAM (
Parameter Group 0
    adaptive: False
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: False
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    rho: 0.05
    weight_decay: 0.01
)
Epoch: 1/200
Date :05/19/2023, 07:55:45
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.25
drop_path_rate: 0.4
76407
Fold: 1
13839
SAM (
Parameter Group 0
    adaptive: False
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: False
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    rho: 0.05
    weight_decay: 0.01
)
Epoch: 1/200
Date :05/19/2023, 07:57:02
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.25
drop_path_rate: 0.4
76407
Fold: 1
13839
SAM (
Parameter Group 0
    adaptive: False
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: False
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    rho: 0.05
    weight_decay: 0.01
)
Epoch: 1/200
Date :05/19/2023, 07:57:30
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.25
drop_path_rate: 0.4
76407
Fold: 1
13839
SAM (
Parameter Group 0
    adaptive: False
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: False
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    rho: 0.05
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 42.5597
Epoch: 2/200
Date :05/19/2023, 07:59:40
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.25
drop_path_rate: 0.4
76407
Fold: 1
13839
SAM (
Parameter Group 0
    adaptive: False
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: False
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    rho: 0.05
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 42.8170
Epoch: 2/200
Train loss: 6.0849
Epoch: 3/200
Train loss: 5.7564
Epoch: 4/200
Train loss: 5.5443
Epoch: 5/200
Train loss: 5.3349
Epoch: 6/200
Train loss: 5.1728
Epoch: 7/200
Train loss: 5.0591
Epoch: 8/200
Train loss: 4.9754
Epoch: 9/200
Train loss: 4.8803
Epoch: 10/200
Train loss: 4.8120
Epoch: 11/200
Train loss: 4.8512
Epoch: 12/200
Train loss: 4.7734
Epoch: 13/200
Train loss: 4.8802
Epoch: 14/200
Train loss: 4.8361
Epoch: 15/200
Train loss: 4.7622
Epoch: 16/200
Train loss: 4.6369
Epoch: 17/200
Train loss: 4.6751
Epoch: 18/200
Train loss: 4.7830
Epoch: 19/200
Train loss: 4.7964
Epoch: 20/200
Train loss: 4.7013
Epoch: 21/200
Date :05/19/2023, 08:22:44
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.25
drop_path_rate: 0.4
76407
Fold: 1
13839
SAM (
Parameter Group 0
    adaptive: False
    dampening: 0
    differentiable: False
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    momentum: 0
    nesterov: False
    rho: 0.05
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 174.8428
Epoch: 2/200
Date :05/19/2023, 08:24:05
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.25
drop_path_rate: 0.4
76407
Fold: 1
13839
SAM (
Parameter Group 0
    adaptive: False
    dampening: 0
    differentiable: False
    foreach: None
    initial_lr: 0.1
    lr: 0.1
    maximize: False
    momentum: 0
    nesterov: False
    rho: 0.05
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 9.2782
Epoch: 2/200
Train loss: 5.4095
Epoch: 3/200
Train loss: 5.1479
Epoch: 4/200
Train loss: 5.0140
Epoch: 5/200
Date :05/19/2023, 08:30:37
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 300
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 0
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.25
drop_path_rate: 0.4
76407
Fold: 1
13553
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/300
Train loss: 42.9448
Epoch: 2/300
Train loss: 5.6694
Epoch: 3/300
Train loss: 5.1966
Epoch: 4/300
Train loss: 4.9234
Epoch: 5/300
Train loss: 4.6291
Epoch: 6/300
Train loss: 4.3949
Epoch: 7/300
Train loss: 4.1524
Epoch: 8/300
Train loss: 4.0764
Epoch: 9/300
Train loss: 3.8676
Epoch: 10/300
Train loss: 3.8095
Epoch: 11/300
Train loss: 3.7419
Epoch: 12/300
Train loss: 3.6458
Epoch: 13/300
Train loss: 3.5753
Epoch: 14/300
Train loss: 3.4974
Epoch: 15/300
Train loss: 3.4618
Epoch: 16/300
Train loss: 3.4636
Epoch: 17/300
Train loss: 3.3059
Epoch: 18/300
Train loss: 3.2500
Epoch: 19/300
Train loss: 3.3727
Epoch: 20/300
Train loss: 3.2531
Epoch: 21/300
Train loss: 3.2181
Epoch: 22/300
Train loss: 3.0452
Epoch: 23/300
Train loss: 3.0353
Epoch: 24/300
Train loss: 3.0660
Epoch: 25/300
Train loss: 3.1976
Epoch: 26/300
Train loss: 2.9403
Epoch: 27/300
Train loss: 3.0504
Epoch: 28/300
Train loss: 2.9883
Epoch: 29/300
Train loss: 2.8987
Epoch: 30/300
Train loss: 2.8873
Epoch: 31/300
Train loss: 2.7751
Epoch: 32/300
Train loss: 2.9420
Epoch: 33/300
Train loss: 2.9178
Epoch: 34/300
Train loss: 2.7074
Epoch: 35/300
Train loss: 2.8764
Epoch: 36/300
Train loss: 2.8037
Epoch: 37/300
Train loss: 2.8604
Epoch: 38/300
Train loss: 2.7380
Epoch: 39/300
Train loss: 2.9451
Epoch: 40/300
Train loss: 2.6430
Epoch: 41/300
Train loss: 2.7842
Epoch: 42/300
Train loss: 2.7706
Epoch: 43/300
Train loss: 2.6456
Epoch: 44/300
Train loss: 2.7896
Epoch: 45/300
Train loss: 2.7602
Epoch: 46/300
Train loss: 2.7525
Epoch: 47/300
Train loss: 2.6175
Epoch: 48/300
Train loss: 2.6827
Epoch: 49/300
Train loss: 2.5621
Epoch: 50/300
Train loss: 2.4064
Epoch: 51/300
Train loss: 2.5649
Epoch: 52/300
Train loss: 2.5099
Epoch: 53/300
Train loss: 2.5544
Epoch: 54/300
Train loss: 2.4467
Epoch: 55/300
Train loss: 2.6513
Epoch: 56/300
Train loss: 2.5593
Epoch: 57/300
Train loss: 2.3923
Epoch: 58/300
Train loss: 2.6401
Epoch: 59/300
Train loss: 2.4523
Epoch: 60/300
Train loss: 2.6196
Epoch: 61/300
Train loss: 2.5139
Epoch: 62/300
Train loss: 2.4980
Epoch: 63/300
Train loss: 2.4635
Epoch: 64/300
Train loss: 2.3958
Epoch: 65/300
Train loss: 2.5122
Epoch: 66/300
Train loss: 2.4765
Epoch: 67/300
Train loss: 2.5468
Epoch: 68/300
Train loss: 2.3413
Epoch: 69/300
Train loss: 2.5157
Epoch: 70/300
Train loss: 2.4825
Epoch: 71/300
Train loss: 2.5981
Epoch: 72/300
Train loss: 2.4613
Epoch: 73/300
Train loss: 2.2956
Epoch: 74/300
Train loss: 2.3383
Epoch: 75/300
Train loss: 2.4361
Epoch: 76/300
Train loss: 2.4550
Epoch: 77/300
Train loss: 2.5295
Epoch: 78/300
Train loss: 2.3147
Epoch: 79/300
Train loss: 2.4305
Epoch: 80/300
Train loss: 2.3776
Epoch: 81/300
Train loss: 2.2388
Epoch: 82/300
Train loss: 2.2374
Epoch: 83/300
Train loss: 2.3759
Epoch: 84/300
Train loss: 2.3146
Epoch: 85/300
Train loss: 2.2718
Epoch: 86/300
Train loss: 2.2601
Epoch: 87/300
Train loss: 2.3859
Epoch: 88/300
Train loss: 2.4558
Epoch: 89/300
Train loss: 2.1587
Epoch: 90/300
Train loss: 2.3751
Epoch: 91/300
Train loss: 2.3735
Epoch: 92/300
Train loss: 2.4481
Epoch: 93/300
Train loss: 2.3313
Epoch: 94/300
Train loss: 2.2977
Epoch: 95/300
Train loss: 2.4461
Epoch: 96/300
Train loss: 2.3410
Epoch: 97/300
Train loss: 2.2736
Epoch: 98/300
Train loss: 2.3602
Epoch: 99/300
Train loss: 2.4674
Epoch: 100/300
Train loss: 2.2985
Epoch: 101/300
Train loss: 2.2773
Epoch: 102/300
Train loss: 2.2398
Epoch: 103/300
Train loss: 2.3712
Epoch: 104/300
Train loss: 2.3200
Epoch: 105/300
Train loss: 2.1351
Epoch: 106/300
Train loss: 2.1749
Epoch: 107/300
Train loss: 2.1962
Epoch: 108/300
Train loss: 2.2654
Epoch: 109/300
Train loss: 2.2437
Epoch: 110/300
Train loss: 2.3014
Epoch: 111/300
Train loss: 2.2811
Epoch: 112/300
Train loss: 2.2862
Epoch: 113/300
Train loss: 2.3250
Epoch: 114/300
Train loss: 2.1681
Epoch: 115/300
Train loss: 2.1355
Epoch: 116/300
Train loss: 2.2796
Epoch: 117/300
Train loss: 2.3208
Epoch: 118/300
Train loss: 2.2480
Epoch: 119/300
Train loss: 2.2469
Epoch: 120/300
Train loss: 2.2572
Epoch: 121/300
Train loss: 2.1582
Epoch: 122/300
Train loss: 2.2214
Epoch: 123/300
Train loss: 2.2388
Epoch: 124/300
Train loss: 2.1704
Epoch: 125/300
Train loss: 2.2418
Epoch: 126/300
Train loss: 2.3245
Epoch: 127/300
Train loss: 2.4432
Epoch: 128/300
Train loss: 2.3043
Epoch: 129/300
Train loss: 2.1962
Epoch: 130/300
Train loss: 2.2655
Epoch: 131/300
Train loss: 2.1566
Epoch: 132/300
Train loss: 2.1747
Epoch: 133/300
Train loss: 2.0194
Epoch: 134/300
Train loss: 2.2007
Epoch: 135/300
Train loss: 2.1530
Epoch: 136/300
Train loss: 2.2806
Epoch: 137/300
Train loss: 2.1437
Epoch: 138/300
Train loss: 2.1476
Epoch: 139/300
Train loss: 2.1272
Epoch: 140/300
Train loss: 2.1169
Epoch: 141/300
Train loss: 2.1816
Epoch: 142/300
Train loss: 2.0445
Epoch: 143/300
Train loss: 2.2579
Epoch: 144/300
Train loss: 2.1193
Epoch: 145/300
Train loss: 2.1701
Epoch: 146/300
Train loss: 2.0710
Epoch: 147/300
Train loss: 2.2775
Epoch: 148/300
Train loss: 2.2191
Epoch: 149/300
Train loss: 2.0746
Epoch: 150/300
Train loss: 2.1990
Epoch: 151/300
Train loss: 2.2175
Epoch: 152/300
Train loss: 2.2877
Epoch: 153/300
Train loss: 2.1048
Epoch: 154/300
Train loss: 1.9545
Epoch: 155/300
Train loss: 2.1707
Epoch: 156/300
Train loss: 2.0397
Epoch: 157/300
Train loss: 2.0424
Epoch: 158/300
Train loss: 2.1966
Epoch: 159/300
Train loss: 2.0769
Epoch: 160/300
Train loss: 2.1858
Epoch: 161/300
Train loss: 2.1759
Epoch: 162/300
Train loss: 2.0280
Epoch: 163/300
Train loss: 2.0061
Epoch: 164/300
Train loss: 2.1920
Epoch: 165/300
Train loss: 2.1443
Epoch: 166/300
Train loss: 2.1047
Epoch: 167/300
Train loss: 2.1775
Epoch: 168/300
Train loss: 2.0396
Epoch: 169/300
Train loss: 2.2986
Epoch: 170/300
Train loss: 2.1489
Epoch: 171/300
Train loss: 2.2097
Epoch: 172/300
Train loss: 2.2526
Epoch: 173/300
Train loss: 2.1038
Epoch: 174/300
Train loss: 2.1587
Epoch: 175/300
Train loss: 2.0920
Epoch: 176/300
Train loss: 2.2140
Epoch: 177/300
Train loss: 2.0584
Epoch: 178/300
Train loss: 2.1487
Epoch: 179/300
Train loss: 2.0515
Epoch: 180/300
Train loss: 2.1739
Epoch: 181/300
Train loss: 1.9862
Epoch: 182/300
Train loss: 2.0276
Epoch: 183/300
Train loss: 2.0844
Epoch: 184/300
Train loss: 2.0073
Epoch: 185/300
Train loss: 2.1626
Epoch: 186/300
Train loss: 2.1355
Epoch: 187/300
Train loss: 1.9390
Epoch: 188/300
Train loss: 2.0548
Epoch: 189/300
Train loss: 2.1177
Epoch: 190/300
Train loss: 2.1674
Epoch: 191/300
Train loss: 2.1648
Epoch: 192/300
Train loss: 2.0737
Epoch: 193/300
Train loss: 2.1391
Epoch: 194/300
Train loss: 2.0953
Epoch: 195/300
Train loss: 2.2243
Epoch: 196/300
Train loss: 2.0916
Epoch: 197/300
Train loss: 2.1118
Epoch: 198/300
Train loss: 2.1729
Epoch: 199/300
Train loss: 2.1677
Epoch: 200/300
Train loss: 2.0483
Epoch: 201/300
Train loss: 2.2203
Epoch: 202/300
Train loss: 2.0411
0.8135789206061038
Model improve: 0.000000 -> 0.813579
Epoch: 203/300
Train loss: 1.9988
0.814607409674915
Model improve: 0.813579 -> 0.814607
Epoch: 204/300
Train loss: 2.1113
0.8130557932936848
Epoch: 205/300
Train loss: 2.2102
0.8109390294791565
Epoch: 206/300
Train loss: 2.0307
0.8116522122398973
Epoch: 207/300
Train loss: 2.1918
0.8109957712132061
Epoch: 208/300
Train loss: 1.9898
0.8160446573477632
Model improve: 0.814607 -> 0.816045
Epoch: 209/300
Train loss: 2.1623
0.8135922471262953
Epoch: 210/300
Train loss: 2.1371
0.8136786697547268
Epoch: 211/300
Train loss: 2.0899
0.812301919238263
Epoch: 212/300
Train loss: 1.9929
0.8140441541310997
Epoch: 213/300
Train loss: 1.9627
0.8148950516078804
Epoch: 214/300
Train loss: 1.9444
0.8133253591796663
Epoch: 215/300
Train loss: 2.1638
0.8124294943713414
Epoch: 216/300
Train loss: 2.1689
0.811882195179193
Epoch: 217/300
Date :05/19/2023, 12:24:33
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0001
weightdecay: 0.01
thrupsample: 0
model_name: tf_efficientnetv2_b1
mix_up: 0.5
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.25
drop_path_rate: 0.4
76407
Date :05/19/2023, 12:25:05
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0001
weightdecay: 0.01
thrupsample: 0
model_name: tf_efficientnetv2_b1
mix_up: 0.5
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.25
drop_path_rate: 0.4
76407
Fold: 1
12741
Date :05/19/2023, 12:25:27
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0001
weightdecay: 0.01
thrupsample: 0
model_name: tf_efficientnetv2_b1
mix_up: 0.5
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.25
drop_path_rate: 0.4
76407
Fold: 1
12741
AdamP (
Parameter Group 0
    betas: (0.9, 0.999)
    delta: 0.1
    eps: 1e-08
    initial_lr: 0.0001
    lr: 0.0001
    nesterov: False
    wd_ratio: 0.1
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 153.6853
Epoch: 2/200
Train loss: 50.3784
Epoch: 3/200
Train loss: 10.8136
Epoch: 4/200
Date :05/19/2023, 12:28:42
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0001
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.5
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.25
drop_path_rate: 0.4
76407
Fold: 1
13027
AdamP (
Parameter Group 0
    betas: (0.9, 0.999)
    delta: 0.1
    eps: 1e-08
    initial_lr: 0.0001
    lr: 0.0001
    nesterov: False
    wd_ratio: 0.1
    weight_decay: 0.01
)
Epoch: 1/200
Date :05/19/2023, 12:28:59
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.5
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.25
drop_path_rate: 0.4
76407
Fold: 1
13027
AdamP (
Parameter Group 0
    betas: (0.9, 0.999)
    delta: 0.1
    eps: 1e-08
    initial_lr: 0.0003
    lr: 0.0003
    nesterov: False
    wd_ratio: 0.1
    weight_decay: 0.01
)
Epoch: 1/200
Date :05/19/2023, 12:32:12
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 600
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.5
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.25
drop_path_rate: 0.4
76407
Fold: 1
13839
AdamP (
Parameter Group 0
    betas: (0.9, 0.999)
    delta: 0.1
    eps: 1e-08
    initial_lr: 0.0003
    lr: 0.0003
    nesterov: False
    wd_ratio: 0.1
    weight_decay: 0.01
)
Epoch: 1/600
Date :05/19/2023, 12:32:32
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 600
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.25
drop_path_rate: 0.4
76407
Fold: 1
13839
AdamP (
Parameter Group 0
    betas: (0.9, 0.999)
    delta: 0.1
    eps: 1e-08
    initial_lr: 0.0003
    lr: 0.0003
    nesterov: False
    wd_ratio: 0.1
    weight_decay: 0.01
)
Epoch: 1/600
Date :05/19/2023, 12:33:24
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 600
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 0
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.25
drop_path_rate: 0.4
76407
Fold: 1
13553
AdamP (
Parameter Group 0
    betas: (0.9, 0.999)
    delta: 0.1
    eps: 1e-08
    initial_lr: 0.0003
    lr: 0.0003
    nesterov: False
    wd_ratio: 0.1
    weight_decay: 0.01
)
Epoch: 1/600
Train loss: 80.9306
Epoch: 2/600
Train loss: 6.5200
Epoch: 3/600
Train loss: 5.8219
Epoch: 4/600
Train loss: 5.5519
Epoch: 5/600
Date :05/19/2023, 12:37:01
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 600
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 0
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.25
drop_path_rate: 0.4
76407
Fold: 1
13553
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: False
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/600
Train loss: 88.3003
Epoch: 2/600
Date :05/19/2023, 12:38:16
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 600
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 0
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.25
drop_path_rate: 0.4
76407
Fold: 1
13553
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/600
Train loss: 80.9543
Epoch: 2/600
Train loss: 6.5212
Epoch: 3/600
Train loss: 5.8217
Epoch: 4/600
Train loss: 5.5517
Epoch: 5/600
Train loss: 5.3223
Epoch: 6/600
Train loss: 5.1234
Epoch: 7/600
Date :05/19/2023, 12:44:16
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 30
totalepoch: 600
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 0
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.25
drop_path_rate: 0.4
76407
Fold: 1
13553
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/600
Train loss: 182.7007
Epoch: 2/600
Train loss: 178.1760
Epoch: 3/600
Train loss: 160.8906
Epoch: 4/600
Train loss: 109.6715
Epoch: 5/600
Train loss: 43.2261
Epoch: 6/600
Train loss: 14.7590
Epoch: 7/600
Train loss: 8.5700
Epoch: 8/600
Train loss: 6.9367
Epoch: 9/600
Train loss: 6.3962
Epoch: 10/600
Train loss: 6.0998
Epoch: 11/600
Train loss: 5.8926
Epoch: 12/600
Train loss: 5.7644
Epoch: 13/600
Train loss: 5.6572
Epoch: 14/600
Train loss: 5.5441
Epoch: 15/600
Train loss: 5.4703
Epoch: 16/600
Train loss: 5.3907
Epoch: 17/600
Train loss: 5.2804
Epoch: 18/600
Train loss: 5.1828
Epoch: 19/600
Train loss: 5.1552
Epoch: 20/600
Train loss: 5.0399
Epoch: 21/600
Date :05/19/2023, 13:01:52
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 30
totalepoch: 600
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 0
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.25
drop_path_rate: 0.4
76407
Fold: 1
13553
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/600
Train loss: 182.7012
Epoch: 2/600
Train loss: 178.1753
Epoch: 3/600
Train loss: 160.8915
Epoch: 4/600
Train loss: 109.6683
Epoch: 5/600
Train loss: 43.2117
Epoch: 6/600
Train loss: 14.7549
Epoch: 7/600
Train loss: 8.5441
Epoch: 8/600
Train loss: 6.9226
Epoch: 9/600
Train loss: 6.3833
Epoch: 10/600
Train loss: 6.0946
Epoch: 11/600
Train loss: 5.8886
Epoch: 12/600
Train loss: 5.7620
Epoch: 13/600
Train loss: 5.6556
Epoch: 14/600
Train loss: 5.5419
Epoch: 15/600
Train loss: 5.4695
Epoch: 16/600
Train loss: 5.3899
Epoch: 17/600
Train loss: 5.2794
Epoch: 18/600
Train loss: 5.1819
Epoch: 19/600
Train loss: 5.1546
Epoch: 20/600
Train loss: 5.0396
Epoch: 21/600
Train loss: 4.9608
Epoch: 22/600
Train loss: 4.8292
Epoch: 23/600
Train loss: 4.7402
Epoch: 24/600
Train loss: 4.7109
Epoch: 25/600
Train loss: 4.7290
Epoch: 26/600
Train loss: 4.5450
Epoch: 27/600
Train loss: 4.5335
Epoch: 28/600
Train loss: 4.4450
Epoch: 29/600
Train loss: 4.3381
Epoch: 30/600
Train loss: 4.2926
Epoch: 31/600
Train loss: 4.1628
Epoch: 32/600
Train loss: 4.2269
Epoch: 33/600
Train loss: 4.1574
Epoch: 34/600
Train loss: 3.9693
Epoch: 35/600
Train loss: 4.0626
Epoch: 36/600
Train loss: 3.9697
Epoch: 37/600
Train loss: 3.9840
Epoch: 38/600
Train loss: 3.8696
Epoch: 39/600
Train loss: 3.9910
Epoch: 40/600
Train loss: 3.7338
Epoch: 41/600
Train loss: 3.8146
Epoch: 42/600
Train loss: 3.7730
Epoch: 43/600
Train loss: 3.6562
Epoch: 44/600
Train loss: 3.7597
Epoch: 45/600
Train loss: 3.7182
Epoch: 46/600
Train loss: 3.6970
Epoch: 47/600
Train loss: 3.5639
Epoch: 48/600
Train loss: 3.5894
Epoch: 49/600
Train loss: 3.4828
Epoch: 50/600
Train loss: 3.3306
Epoch: 51/600
Train loss: 3.4559
Epoch: 52/600
Train loss: 3.3696
Epoch: 53/600
Train loss: 3.4247
Epoch: 54/600
Train loss: 3.3104
Epoch: 55/600
Train loss: 3.4744
Epoch: 56/600
Train loss: 3.3899
Epoch: 57/600
Train loss: 3.2167
Epoch: 58/600
Train loss: 3.4135
Epoch: 59/600
Train loss: 3.2490
Epoch: 60/600
Train loss: 3.3713
Epoch: 61/600
Train loss: 3.2677
Epoch: 62/600
Train loss: 3.2499
Epoch: 63/600
Train loss: 3.2053
Epoch: 64/600
Train loss: 3.1399
Epoch: 65/600
Train loss: 3.2333
Epoch: 66/600
Train loss: 3.1876
Epoch: 67/600
Train loss: 3.2606
Epoch: 68/600
Train loss: 3.0637
Epoch: 69/600
Train loss: 3.2162
Epoch: 70/600
Train loss: 3.1649
Epoch: 71/600
Train loss: 3.2554
Epoch: 72/600
Train loss: 3.1439
Epoch: 73/600
Train loss: 2.9622
Epoch: 74/600
Train loss: 2.9964
Epoch: 75/600
Train loss: 3.0843
Epoch: 76/600
Train loss: 3.0936
Epoch: 77/600
Train loss: 3.1491
Epoch: 78/600
Train loss: 2.9447
Epoch: 79/600
Train loss: 3.0480
Epoch: 80/600
Train loss: 2.9770
Epoch: 81/600
Train loss: 2.8568
Epoch: 82/600
Train loss: 2.8505
Epoch: 83/600
Train loss: 2.9724
Epoch: 84/600
Train loss: 2.9016
Epoch: 85/600
Train loss: 2.8565
Epoch: 86/600
Train loss: 2.8457
Epoch: 87/600
Train loss: 2.9530
Epoch: 88/600
Train loss: 3.0134
Epoch: 89/600
Train loss: 2.7300
Epoch: 90/600
Train loss: 2.9321
Epoch: 91/600
Train loss: 2.9236
Epoch: 92/600
Train loss: 2.9958
Epoch: 93/600
Train loss: 2.8605
Epoch: 94/600
Train loss: 2.8331
Epoch: 95/600
Train loss: 2.9683
Epoch: 96/600
Train loss: 2.8565
Epoch: 97/600
Train loss: 2.7918
Epoch: 98/600
Train loss: 2.8800
Epoch: 99/600
Train loss: 2.9685
Epoch: 100/600
Train loss: 2.8047
Epoch: 101/600
Train loss: 2.7746
Epoch: 102/600
Train loss: 2.7436
Epoch: 103/600
Train loss: 2.8443
Epoch: 104/600
Train loss: 2.8049
Epoch: 105/600
Train loss: 2.6184
Epoch: 106/600
Train loss: 2.6538
Epoch: 107/600
Train loss: 2.6761
Epoch: 108/600
Train loss: 2.7362
Epoch: 109/600
Train loss: 2.7158
Epoch: 110/600
Train loss: 2.7674
Epoch: 111/600
Train loss: 2.7475
Epoch: 112/600
Train loss: 2.7502
Epoch: 113/600
Train loss: 2.7695
Epoch: 114/600
Train loss: 2.6226
Epoch: 115/600
Train loss: 2.5779
Epoch: 116/600
Train loss: 2.7179
Epoch: 117/600
Train loss: 2.7539
Epoch: 118/600
Train loss: 2.6751
Epoch: 119/600
Train loss: 2.6884
Epoch: 120/600
Train loss: 2.6913
Epoch: 121/600
Train loss: 2.5788
Epoch: 122/600
Train loss: 2.6438
Epoch: 123/600
Train loss: 2.6640
Epoch: 124/600
Train loss: 2.5919
Epoch: 125/600
Train loss: 2.6616
Epoch: 126/600
Train loss: 2.7405
Epoch: 127/600
Train loss: 2.8455
Epoch: 128/600
Train loss: 2.7181
Epoch: 129/600
Train loss: 2.6085
Epoch: 130/600
Train loss: 2.6765
Epoch: 131/600
Train loss: 2.5487
Epoch: 132/600
Train loss: 2.5763
Epoch: 133/600
Train loss: 2.4202
Epoch: 134/600
Train loss: 2.6001
Epoch: 135/600
Train loss: 2.5387
Epoch: 136/600
Train loss: 2.6644
Epoch: 137/600
Train loss: 2.5262
Epoch: 138/600
Train loss: 2.5278
Epoch: 139/600
Train loss: 2.5000
Epoch: 140/600
Train loss: 2.5046
Epoch: 141/600
Train loss: 2.5557
Epoch: 142/600
Train loss: 2.4153
Epoch: 143/600
Train loss: 2.6277
Epoch: 144/600
Train loss: 2.4822
Epoch: 145/600
Train loss: 2.5395
Epoch: 146/600
Train loss: 2.4503
Epoch: 147/600
Train loss: 2.6443
Epoch: 148/600
Train loss: 2.5821
Epoch: 149/600
Train loss: 2.4350
Epoch: 150/600
Train loss: 2.5687
Epoch: 151/600
Train loss: 2.5706
Epoch: 152/600
Train loss: 2.6350
Epoch: 153/600
Train loss: 2.4513
Epoch: 154/600
Train loss: 2.3028
Epoch: 155/600
Train loss: 2.5151
Epoch: 156/600
Train loss: 2.3867
Epoch: 157/600
Train loss: 2.3924
Epoch: 158/600
Train loss: 2.5424
Epoch: 159/600
Train loss: 2.4221
Epoch: 160/600
Train loss: 2.5286
Epoch: 161/600
Train loss: 2.5046
Epoch: 162/600
Train loss: 2.3622
Epoch: 163/600
Train loss: 2.3338
Epoch: 164/600
Train loss: 2.5265
Epoch: 165/600
Train loss: 2.4726
Epoch: 166/600
Train loss: 2.4292
Epoch: 167/600
Train loss: 2.5037
Epoch: 168/600
Train loss: 2.3703
Epoch: 169/600
Train loss: 2.6184
Epoch: 170/600
Train loss: 2.4773
Epoch: 171/600
Train loss: 2.5306
Epoch: 172/600
Train loss: 2.5720
Epoch: 173/600
Train loss: 2.4169
Epoch: 174/600
Train loss: 2.4696
Epoch: 175/600
Train loss: 2.4087
Epoch: 176/600
Train loss: 2.5225
Epoch: 177/600
Train loss: 2.3744
Epoch: 178/600
Train loss: 2.4512
Epoch: 179/600
Train loss: 2.3515
Epoch: 180/600
Train loss: 2.4756
Epoch: 181/600
Train loss: 2.2899
Epoch: 182/600
Train loss: 2.3341
Epoch: 183/600
Train loss: 2.3841
Epoch: 184/600
Train loss: 2.3047
Epoch: 185/600
Train loss: 2.4539
Epoch: 186/600
Train loss: 2.4249
Epoch: 187/600
Train loss: 2.2350
Epoch: 188/600
Train loss: 2.3541
Epoch: 189/600
Train loss: 2.4037
Epoch: 190/600
Train loss: 2.4551
Epoch: 191/600
Train loss: 2.4532
Epoch: 192/600
Train loss: 2.3594
Epoch: 193/600
Train loss: 2.4232
Epoch: 194/600
Train loss: 2.3804
Epoch: 195/600
Train loss: 2.5093
Epoch: 196/600
Train loss: 2.3733
Epoch: 197/600
Train loss: 2.3899
Epoch: 198/600
Train loss: 2.4546
Epoch: 199/600
Train loss: 2.4506
Epoch: 200/600
Train loss: 2.3237
Epoch: 201/600
Train loss: 2.5009
Epoch: 202/600
Train loss: 2.3168
Epoch: 203/600
Train loss: 2.2767
Epoch: 204/600
Train loss: 2.3892
Epoch: 205/600
Train loss: 2.4916
Epoch: 206/600
Train loss: 2.2940
Epoch: 207/600
Train loss: 2.4404
Epoch: 208/600
Train loss: 2.2572
Epoch: 209/600
Train loss: 2.4320
Epoch: 210/600
Train loss: 2.4047
Epoch: 211/600
Train loss: 2.3640
Epoch: 212/600
Train loss: 2.2449
Epoch: 213/600
Train loss: 2.2181
Epoch: 214/600
Train loss: 2.1945
Epoch: 215/600
Train loss: 2.4200
Epoch: 216/600
Train loss: 2.4329
Epoch: 217/600
Train loss: 2.3290
Epoch: 218/600
Train loss: 2.2683
Epoch: 219/600
Train loss: 2.2909
Epoch: 220/600
Train loss: 2.3164
Epoch: 221/600
Train loss: 2.2860
Epoch: 222/600
Train loss: 2.2554
Epoch: 223/600
Train loss: 2.3055
Epoch: 224/600
Train loss: 2.3416
Epoch: 225/600
Train loss: 2.3160
Epoch: 226/600
Train loss: 2.3312
Epoch: 227/600
Train loss: 2.3165
Epoch: 228/600
Train loss: 2.2408
Epoch: 229/600
Train loss: 2.2743
Epoch: 230/600
Train loss: 2.4309
Epoch: 231/600
Train loss: 2.3284
Epoch: 232/600
Train loss: 2.3193
Epoch: 233/600
Train loss: 2.2652
Epoch: 234/600
Train loss: 2.2860
Epoch: 235/600
Train loss: 2.2818
Epoch: 236/600
Train loss: 2.2367
Epoch: 237/600
Train loss: 2.2728
Epoch: 238/600
Train loss: 2.2211
Epoch: 239/600
Train loss: 2.1810
Epoch: 240/600
Train loss: 2.2061
Epoch: 241/600
Train loss: 2.1841
Epoch: 242/600
Train loss: 2.2534
Epoch: 243/600
Train loss: 2.3899
Epoch: 244/600
Train loss: 2.3329
Epoch: 245/600
Train loss: 2.1804
Epoch: 246/600
Train loss: 2.1618
Epoch: 247/600
Train loss: 2.2206
Epoch: 248/600
Train loss: 2.1791
Epoch: 249/600
Train loss: 2.3181
Epoch: 250/600
Train loss: 2.2186
Epoch: 251/600
Train loss: 2.3265
Epoch: 252/600
Train loss: 2.3860
Epoch: 253/600
Train loss: 2.3018
Epoch: 254/600
Train loss: 2.1630
Epoch: 255/600
Train loss: 2.2841
Epoch: 256/600
Train loss: 2.3164
Epoch: 257/600
Train loss: 2.1516
Epoch: 258/600
Train loss: 2.3169
Epoch: 259/600
Train loss: 2.1863
Epoch: 260/600
Train loss: 2.2661
Epoch: 261/600
Train loss: 2.2161
Epoch: 262/600
Train loss: 2.2041
Epoch: 263/600
Train loss: 2.3201
Epoch: 264/600
Train loss: 2.2567
Epoch: 265/600
Train loss: 2.1883
Epoch: 266/600
Train loss: 2.2825
Epoch: 267/600
Train loss: 2.3223
Epoch: 268/600
Train loss: 2.1317
Epoch: 269/600
Train loss: 2.2010
Epoch: 270/600
Train loss: 2.3573
Epoch: 271/600
Train loss: 2.2298
Epoch: 272/600
Train loss: 2.1950
Epoch: 273/600
Train loss: 2.2671
Epoch: 274/600
Train loss: 2.1623
Epoch: 275/600
Train loss: 2.1972
Epoch: 276/600
Train loss: 1.9483
Epoch: 277/600
Train loss: 2.3051
Epoch: 278/600
Train loss: 2.0670
Epoch: 279/600
Train loss: 2.3201
Epoch: 280/600
Train loss: 2.3396
Epoch: 281/600
Train loss: 2.2964
Epoch: 282/600
Train loss: 2.1320
Epoch: 283/600
Train loss: 2.2216
Epoch: 284/600
Train loss: 2.2660
Epoch: 285/600
Train loss: 2.2330
Epoch: 286/600
Train loss: 2.2733
Epoch: 287/600
Train loss: 2.2328
Epoch: 288/600
Train loss: 2.1267
Epoch: 289/600
Train loss: 2.1368
Epoch: 290/600
Train loss: 2.3449
Epoch: 291/600
Train loss: 2.2409
Epoch: 292/600
Train loss: 2.1879
Epoch: 293/600
Train loss: 2.2551
Epoch: 294/600
Train loss: 2.1671
Epoch: 295/600
Train loss: 2.3439
Epoch: 296/600
Train loss: 2.2628
Epoch: 297/600
Train loss: 2.2088
Epoch: 298/600
Train loss: 2.2887
Epoch: 299/600
Train loss: 2.2389
Epoch: 300/600
Train loss: 2.2619
Epoch: 301/600
Train loss: 2.3152
Epoch: 302/600
Train loss: 2.2158
0.8127882085245025
Model improve: 0.000000 -> 0.812788
Epoch: 303/600
Train loss: 2.2073
0.8113759141762776
Epoch: 304/600
Train loss: 2.1829
0.8133354027829758
Model improve: 0.812788 -> 0.813335
Epoch: 305/600
Train loss: 2.1794
0.8128451829414347
Epoch: 306/600
Train loss: 2.2116
Epoch: 307/600
Train loss: 2.1155
0.8131996024199586
Epoch: 308/600
Train loss: 2.1854
Epoch: 309/600
Train loss: 2.0860
0.812605880023046
Epoch: 310/600
Train loss: 2.0482
0.8142853243172279
Model improve: 0.813335 -> 0.814285
Epoch: 311/600
Train loss: 2.1238
Epoch: 312/600
Train loss: 2.2546
Epoch: 313/600
Train loss: 2.1132
Epoch: 314/600
Train loss: 2.1454
Epoch: 315/600
Train loss: 2.0969
Epoch: 316/600
Train loss: 2.0169
0.8137268784812605
Epoch: 317/600
Train loss: 2.1377
Epoch: 318/600
Train loss: 2.1932
Epoch: 319/600
Train loss: 2.2964
Epoch: 320/600
Train loss: 2.2441
Epoch: 321/600
Train loss: 2.1104
Epoch: 322/600
Train loss: 2.2370
Epoch: 323/600
Train loss: 2.0444
Epoch: 324/600
Train loss: 2.0691
Epoch: 325/600
Train loss: 2.1919
Epoch: 326/600
Train loss: 2.1416
Epoch: 327/600
Train loss: 2.1946
Epoch: 328/600
Train loss: 2.2786
Epoch: 329/600
Train loss: 2.1537
Epoch: 330/600
Train loss: 2.1576
Epoch: 331/600
Train loss: 2.2754
Epoch: 332/600
Train loss: 2.1432
Epoch: 333/600
Train loss: 2.2761
Epoch: 334/600
Train loss: 2.1108
Epoch: 335/600
Train loss: 2.1003
Epoch: 336/600
Train loss: 2.2152
Epoch: 337/600
Train loss: 2.1773
Epoch: 338/600
Train loss: 2.1123
Epoch: 339/600
Train loss: 2.0897
Epoch: 340/600
Train loss: 2.1089
Epoch: 341/600
Train loss: 2.1508
Epoch: 342/600
Train loss: 2.1731
Epoch: 343/600
Train loss: 2.1264
Epoch: 344/600
Train loss: 2.1939
Epoch: 345/600
Train loss: 2.2258
Epoch: 346/600
Train loss: 2.3266
Epoch: 347/600
Train loss: 2.0488
Epoch: 348/600
Train loss: 2.3030
Epoch: 349/600
Train loss: 2.0617
Epoch: 350/600
Train loss: 2.0462
Epoch: 351/600
Train loss: 2.1273
Epoch: 352/600
Train loss: 2.1592
Epoch: 353/600
Train loss: 2.0886
Epoch: 354/600
Train loss: 2.1564
Epoch: 355/600
Train loss: 2.0883
Epoch: 356/600
Train loss: 2.1868
Epoch: 357/600
Train loss: 2.1365
Epoch: 358/600
Train loss: 2.1463
Epoch: 359/600
Train loss: 2.1184
Epoch: 360/600
Train loss: 2.0088
0.8144769626920209
Model improve: 0.814285 -> 0.814477
Epoch: 361/600
Train loss: 2.2046
Epoch: 362/600
Train loss: 2.0549
Epoch: 363/600
Train loss: 2.1091
Epoch: 364/600
Train loss: 2.0802
Epoch: 365/600
Train loss: 2.0927
Epoch: 366/600
Train loss: 2.1780
Epoch: 367/600
Train loss: 2.0108
Epoch: 368/600
Train loss: 2.0861
Epoch: 369/600
Train loss: 2.2159
Epoch: 370/600
Train loss: 2.1185
Epoch: 371/600
Train loss: 2.3271
Epoch: 372/600
Train loss: 2.1064
Epoch: 373/600
Train loss: 2.1616
Epoch: 374/600
Train loss: 2.1585
Epoch: 375/600
Train loss: 2.2281
Epoch: 376/600
Train loss: 1.9755
0.8168426871419836
Model improve: 0.814477 -> 0.816843
Epoch: 377/600
Train loss: 2.0919
Epoch: 378/600
Train loss: 2.0666
Epoch: 379/600
Train loss: 2.0620
Epoch: 380/600
Train loss: 2.1687
Epoch: 381/600
Train loss: 2.1758
Epoch: 382/600
Train loss: 2.1860
Epoch: 383/600
Train loss: 2.0342
Epoch: 384/600
Train loss: 2.2376
Epoch: 385/600
Train loss: 2.1438
Epoch: 386/600
Train loss: 2.1555
Epoch: 387/600
Train loss: 2.2600
Epoch: 388/600
Train loss: 2.2691
Epoch: 389/600
Train loss: 2.1571
Epoch: 390/600
Train loss: 2.1700
Epoch: 391/600
Train loss: 2.1532
Epoch: 392/600
Train loss: 2.1590
Epoch: 393/600
Train loss: 2.0593
Epoch: 394/600
Train loss: 2.2286
Epoch: 395/600
Train loss: 2.1303
Epoch: 396/600
Train loss: 2.0002
Epoch: 397/600
Train loss: 2.2238
Epoch: 398/600
Train loss: 1.9551
0.8125214122871433
Epoch: 399/600
Train loss: 2.1501
Epoch: 400/600
Train loss: 2.0746
Epoch: 401/600
Train loss: 2.0976
Epoch: 402/600
Train loss: 2.1338
Epoch: 403/600
Train loss: 1.9585
Epoch: 404/600
Train loss: 2.0682
Epoch: 405/600
Train loss: 2.1075
Epoch: 406/600
Train loss: 2.0186
Epoch: 407/600
Train loss: 2.0473
Epoch: 408/600
Train loss: 2.0430
Epoch: 409/600
Train loss: 2.1492
Epoch: 410/600
Train loss: 2.1594
Epoch: 411/600
Train loss: 2.0362
Epoch: 412/600
Train loss: 2.1584
Epoch: 413/600
Train loss: 2.1890
Epoch: 414/600
Train loss: 2.1396
Epoch: 415/600
Train loss: 2.1868
Epoch: 416/600
Train loss: 2.0776
Epoch: 417/600
Train loss: 2.0520
Epoch: 418/600
Train loss: 2.0321
Epoch: 419/600
Train loss: 1.9470
0.8148600426903754
Epoch: 420/600
Train loss: 2.1670
Epoch: 421/600
Train loss: 2.0543
Epoch: 422/600
Train loss: 1.9285
0.815558651223211
Epoch: 423/600
Train loss: 1.9900
Epoch: 424/600
Train loss: 2.1365
Epoch: 425/600
Train loss: 2.1542
Epoch: 426/600
Train loss: 2.0793
Epoch: 427/600
Train loss: 2.0972
Epoch: 428/600
Train loss: 2.0262
Epoch: 429/600
Train loss: 2.0145
Epoch: 430/600
Train loss: 2.1005
Epoch: 431/600
Train loss: 2.1287
Epoch: 432/600
Train loss: 2.1329
Epoch: 433/600
Train loss: 2.0671
Epoch: 434/600
Train loss: 2.1768
Epoch: 435/600
Train loss: 2.1349
Epoch: 436/600
Train loss: 2.1416
Epoch: 437/600
Train loss: 2.0776
Epoch: 438/600
Train loss: 2.1634
Epoch: 439/600
Train loss: 2.1336
Epoch: 440/600
Train loss: 2.1573
Epoch: 441/600
Train loss: 2.1706
Epoch: 442/600
Train loss: 2.0645
Epoch: 443/600
Train loss: 2.0590
Epoch: 444/600
Train loss: 2.0513
Epoch: 445/600
Train loss: 2.0165
Epoch: 446/600
Train loss: 2.1014
Epoch: 447/600
Train loss: 2.1487
Epoch: 448/600
Train loss: 2.0793
Epoch: 449/600
Train loss: 2.1090
Epoch: 450/600
Train loss: 2.0942
Epoch: 451/600
Train loss: 1.9740
Epoch: 452/600
Train loss: 2.1249
Epoch: 453/600
Train loss: 1.9537
Epoch: 454/600
Train loss: 2.0833
Epoch: 455/600
Train loss: 2.1097
Epoch: 456/600
Train loss: 2.1461
Epoch: 457/600
Train loss: 2.0664
Epoch: 458/600
Train loss: 2.0946
Epoch: 459/600
Train loss: 2.1636
Epoch: 460/600
Train loss: 2.0744
Epoch: 461/600
Train loss: 2.0342
Epoch: 462/600
Train loss: 2.1104
Epoch: 463/600
Train loss: 2.1308
Epoch: 464/600
Train loss: 2.0941
Epoch: 465/600
Train loss: 2.1092
Epoch: 466/600
Train loss: 2.0805
Epoch: 467/600
Train loss: 2.0273
Epoch: 468/600
Train loss: 2.1072
Epoch: 469/600
Train loss: 2.0259
Epoch: 470/600
Train loss: 2.0824
Epoch: 471/600
Train loss: 2.0785
Epoch: 472/600
Train loss: 2.1261
Epoch: 473/600
Train loss: 2.0683
Epoch: 474/600
Train loss: 2.1444
Epoch: 475/600
Train loss: 1.9702
Epoch: 476/600
Train loss: 2.1113
Epoch: 477/600
Train loss: 2.0487
Epoch: 478/600
Train loss: 1.9775
Epoch: 479/600
Train loss: 2.1537
Epoch: 480/600
Train loss: 2.1923
Epoch: 481/600
Train loss: 2.0650
Epoch: 482/600
Train loss: 2.1506
Epoch: 483/600
Train loss: 2.0784
Epoch: 484/600
Train loss: 2.0262
Epoch: 485/600
Train loss: 2.1682
Epoch: 486/600
Train loss: 2.0965
Epoch: 487/600
Train loss: 2.1524
Epoch: 488/600
Train loss: 2.0936
Epoch: 489/600
Train loss: 2.0944
Epoch: 490/600
Train loss: 2.0209
Epoch: 491/600
Train loss: 2.0602
Epoch: 492/600
Train loss: 2.1709
Epoch: 493/600
Train loss: 2.1591
Epoch: 494/600
Train loss: 2.0103
Epoch: 495/600
Train loss: 2.0931
Epoch: 496/600
Train loss: 1.8808
0.8153383520158889
Epoch: 497/600
Train loss: 2.0913
Epoch: 498/600
Train loss: 2.0385
Epoch: 499/600
Train loss: 2.0245
Epoch: 500/600
Train loss: 1.9351
Epoch: 501/600
Train loss: 2.1272
Epoch: 502/600
Train loss: 2.0061
Epoch: 503/600
Train loss: 2.0044
Epoch: 504/600
Train loss: 2.0831
Epoch: 505/600
Train loss: 2.1678
Epoch: 506/600
Train loss: 2.0621
Epoch: 507/600
Train loss: 2.1552
Epoch: 508/600
Train loss: 2.0043
Epoch: 509/600
Train loss: 2.1444
Epoch: 510/600
Train loss: 2.0724
Epoch: 511/600
Train loss: 2.0936
Epoch: 512/600
Train loss: 2.0523
Epoch: 513/600
Train loss: 2.1742
Epoch: 514/600
Train loss: 1.9526
Epoch: 515/600
Train loss: 2.1779
Epoch: 516/600
Train loss: 1.9638
Epoch: 517/600
Train loss: 2.1050
Epoch: 518/600
Train loss: 2.1080
Epoch: 519/600
Train loss: 1.9934
Epoch: 520/600
Train loss: 2.1211
Epoch: 521/600
Train loss: 2.0822
Epoch: 522/600
Train loss: 2.1301
Epoch: 523/600
Train loss: 2.0405
Epoch: 524/600
Train loss: 1.9300
Epoch: 525/600
Train loss: 2.0185
Epoch: 526/600
Train loss: 2.1068
Epoch: 527/600
Train loss: 2.0615
Epoch: 528/600
Train loss: 1.9447
Epoch: 529/600
Train loss: 2.1389
Epoch: 530/600
Train loss: 2.0740
Epoch: 531/600
Train loss: 1.9813
Epoch: 532/600
Train loss: 2.0563
Epoch: 533/600
Train loss: 2.0069
Epoch: 534/600
Train loss: 2.0646
Epoch: 535/600
Train loss: 2.1884
Epoch: 536/600
Train loss: 2.0240
Epoch: 537/600
Train loss: 2.1059
Epoch: 538/600
Train loss: 2.0303
Epoch: 539/600
Train loss: 2.0114
Epoch: 540/600
Train loss: 2.0181
Epoch: 541/600
Train loss: 2.2043
Epoch: 542/600
Train loss: 2.0201
Epoch: 543/600
Train loss: 2.1593
Epoch: 544/600
Train loss: 2.0254
Epoch: 545/600
Train loss: 2.0878
Epoch: 546/600
Train loss: 2.0917
Epoch: 547/600
Train loss: 2.1318
Epoch: 548/600
Train loss: 1.8894
0.8163076606354946
Epoch: 549/600
Train loss: 1.9933
Epoch: 550/600
Train loss: 2.0914
Epoch: 551/600
Train loss: 2.1450
Epoch: 552/600
Train loss: 2.0901
Epoch: 553/600
Train loss: 2.0148
Epoch: 554/600
Train loss: 1.9561
Epoch: 555/600
Train loss: 2.2087
Epoch: 556/600
Train loss: 1.9749
Epoch: 557/600
Train loss: 2.0858
Epoch: 558/600
Train loss: 2.1214
Epoch: 559/600
Train loss: 2.0843
Epoch: 560/600
Train loss: 1.9798
Epoch: 561/600
Train loss: 2.0971
Epoch: 562/600
Train loss: 2.0797
Epoch: 563/600
Train loss: 2.0837
Epoch: 564/600
Train loss: 2.2014
Epoch: 565/600
Train loss: 1.9623
Epoch: 566/600
Train loss: 1.9771
Epoch: 567/600
Train loss: 2.1598
Epoch: 568/600
Train loss: 1.9367
Epoch: 569/600
Train loss: 2.0261
Epoch: 570/600
Train loss: 2.0580
Epoch: 571/600
Train loss: 2.0711
Epoch: 572/600
Train loss: 2.0132
Epoch: 573/600
Train loss: 2.0437
Epoch: 574/600
Train loss: 2.0405
Epoch: 575/600
Train loss: 2.0509
Epoch: 576/600
Train loss: 2.0009
Epoch: 577/600
Train loss: 2.1112
Epoch: 578/600
Train loss: 2.0755
Epoch: 579/600
Train loss: 2.1294
Epoch: 580/600
Train loss: 2.0164
Epoch: 581/600
Train loss: 2.1231
Epoch: 582/600
Train loss: 2.0462
Epoch: 583/600
Train loss: 2.0643
Epoch: 584/600
Train loss: 2.0501
Epoch: 585/600
Train loss: 2.0513
Epoch: 586/600
Train loss: 2.0812
Epoch: 587/600
Train loss: 2.0633
Epoch: 588/600
Train loss: 2.0658
Epoch: 589/600
Train loss: 2.0676
Epoch: 590/600
Train loss: 2.0924
Epoch: 591/600
Train loss: 2.1279
Epoch: 592/600
Train loss: 1.9815
Epoch: 593/600
Train loss: 2.1792
Epoch: 594/600
Train loss: 2.0057
Epoch: 595/600
Train loss: 1.9874
Epoch: 596/600
Train loss: 2.1467
Epoch: 597/600
Train loss: 2.0635
Epoch: 598/600
Train loss: 2.0210
Epoch: 599/600
Train loss: 2.0004
Epoch: 600/600
Train loss: 2.0321
Date :05/19/2023, 22:48:51
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 192
validbs: 768
epochwarmup: 30
totalepoch: 600
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 0
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.25
drop_path_rate: 0.4
76407
Fold: 1
13553
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/600
Date :05/19/2023, 22:49:17
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 30
totalepoch: 600
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 0
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.25
drop_path_rate: 0.4
76407
Fold: 1
13553
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/600
Train loss: 182.7011
Epoch: 2/600
Train loss: 178.1761
Epoch: 3/600
Train loss: 160.8946
Epoch: 4/600
Date :05/19/2023, 22:53:56
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 800
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 0
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.25
drop_path_rate: 0.4
76407
Fold: 1
13553
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/800
Train loss: 80.9501
Epoch: 2/800
Train loss: 6.5220
Epoch: 3/800
Train loss: 5.8214
Epoch: 4/800
Train loss: 5.5521
Epoch: 5/800
Train loss: 5.3224
Epoch: 6/800
Train loss: 5.1243
Epoch: 7/800
Train loss: 4.9180
Epoch: 8/800
Train loss: 4.8165
Epoch: 9/800
Train loss: 4.6326
Epoch: 10/800
Train loss: 4.5599
Epoch: 11/800
Train loss: 4.4604
Epoch: 12/800
Train loss: 4.3443
Epoch: 13/800
Train loss: 4.2702
Epoch: 14/800
Train loss: 4.1737
Epoch: 15/800
Train loss: 4.1287
Epoch: 16/800
Train loss: 4.1053
Epoch: 17/800
Train loss: 3.9476
Epoch: 18/800
Train loss: 3.8832
Epoch: 19/800
Train loss: 3.9586
Epoch: 20/800
Train loss: 3.8423
Epoch: 21/800
Train loss: 3.7857
Epoch: 22/800
Train loss: 3.6234
Epoch: 23/800
Train loss: 3.5967
Epoch: 24/800
Train loss: 3.6086
Epoch: 25/800
Train loss: 3.7172
Epoch: 26/800
Train loss: 3.4894
Epoch: 27/800
Train loss: 3.5704
Epoch: 28/800
Train loss: 3.5048
Epoch: 29/800
Train loss: 3.4099
Epoch: 30/800
Train loss: 3.4041
Epoch: 31/800
Train loss: 3.2913
Epoch: 32/800
Train loss: 3.4312
Epoch: 33/800
Train loss: 3.3985
Epoch: 34/800
Train loss: 3.1986
Epoch: 35/800
Train loss: 3.3456
Epoch: 36/800
Train loss: 3.2740
Epoch: 37/800
Train loss: 3.3191
Epoch: 38/800
Train loss: 3.2073
Epoch: 39/800
Train loss: 3.3820
Epoch: 40/800
Train loss: 3.1048
Epoch: 41/800
Train loss: 3.2271
Epoch: 42/800
Train loss: 3.2025
Epoch: 43/800
Train loss: 3.0847
Epoch: 44/800
Train loss: 3.2174
Epoch: 45/800
Train loss: 3.1848
Epoch: 46/800
Train loss: 3.1804
Epoch: 47/800
Train loss: 3.0483
Epoch: 48/800
Train loss: 3.1007
Epoch: 49/800
Train loss: 2.9863
Epoch: 50/800
Train loss: 2.8341
Epoch: 51/800
Train loss: 2.9788
Epoch: 52/800
Train loss: 2.9115
Epoch: 53/800
Train loss: 2.9669
Epoch: 54/800
Train loss: 2.8609
Epoch: 55/800
Train loss: 3.0450
Epoch: 56/800
Train loss: 2.9622
Epoch: 57/800
Train loss: 2.7976
Epoch: 58/800
Train loss: 3.0205
Epoch: 59/800
Train loss: 2.8476
Epoch: 60/800
Train loss: 2.9926
Epoch: 61/800
Train loss: 2.8880
Epoch: 62/800
Train loss: 2.8772
Epoch: 63/800
Train loss: 2.8352
Epoch: 64/800
Train loss: 2.7736
Epoch: 65/800
Train loss: 2.8818
Epoch: 66/800
Train loss: 2.8399
Epoch: 67/800
Train loss: 2.9169
Epoch: 68/800
Train loss: 2.7179
Epoch: 69/800
Train loss: 2.8872
Epoch: 70/800
Train loss: 2.8365
Epoch: 71/800
Train loss: 2.9443
Epoch: 72/800
Train loss: 2.8257
Epoch: 73/800
Train loss: 2.6491
Epoch: 74/800
Train loss: 2.6884
Epoch: 75/800
Train loss: 2.7852
Epoch: 76/800
Train loss: 2.7968
Epoch: 77/800
Train loss: 2.8622
Epoch: 78/800
Train loss: 2.6560
Epoch: 79/800
Train loss: 2.7665
Epoch: 80/800
Train loss: 2.7072
Epoch: 81/800
Train loss: 2.5789
Epoch: 82/800
Train loss: 2.5718
Epoch: 83/800
Train loss: 2.7112
Epoch: 84/800
Train loss: 2.6399
Epoch: 85/800
Train loss: 2.6011
Epoch: 86/800
Train loss: 2.5900
Epoch: 87/800
Train loss: 2.7075
Epoch: 88/800
Train loss: 2.7711
Epoch: 89/800
Train loss: 2.4805
Epoch: 90/800
Train loss: 2.6963
Epoch: 91/800
Train loss: 2.6882
Epoch: 92/800
Train loss: 2.7609
Epoch: 93/800
Train loss: 2.6373
Epoch: 94/800
Train loss: 2.6073
Epoch: 95/800
Train loss: 2.7530
Epoch: 96/800
Train loss: 2.6404
Epoch: 97/800
Train loss: 2.5773
Epoch: 98/800
Train loss: 2.6632
Epoch: 99/800
Train loss: 2.7638
Epoch: 100/800
Train loss: 2.5987
Epoch: 101/800
Train loss: 2.5686
Epoch: 102/800
Train loss: 2.5369
Epoch: 103/800
Train loss: 2.6508
Epoch: 104/800
Train loss: 2.6105
Epoch: 105/800
Train loss: 2.4249
Epoch: 106/800
Train loss: 2.4633
Epoch: 107/800
Train loss: 2.4869
Epoch: 108/800
Train loss: 2.5531
Epoch: 109/800
Train loss: 2.5309
Epoch: 110/800
Train loss: 2.5880
Epoch: 111/800
Train loss: 2.5681
Epoch: 112/800
Train loss: 2.5719
Epoch: 113/800
Train loss: 2.5972
Epoch: 114/800
Train loss: 2.4475
Epoch: 115/800
Train loss: 2.4079
Epoch: 116/800
Train loss: 2.5513
Epoch: 117/800
Train loss: 2.5889
Epoch: 118/800
Train loss: 2.5139
Epoch: 119/800
Train loss: 2.5214
Epoch: 120/800
Train loss: 2.5326
Epoch: 121/800
Train loss: 2.4185
Epoch: 122/800
Train loss: 2.4873
Epoch: 123/800
Train loss: 2.5068
Epoch: 124/800
Train loss: 2.4360
Epoch: 125/800
Train loss: 2.5101
Epoch: 126/800
Train loss: 2.5905
Epoch: 127/800
Train loss: 2.7024
Epoch: 128/800
Train loss: 2.5723
Epoch: 129/800
Train loss: 2.4611
Epoch: 130/800
Train loss: 2.5309
Epoch: 131/800
Train loss: 2.4074
Epoch: 132/800
Train loss: 2.4332
Epoch: 133/800
Train loss: 2.2781
Epoch: 134/800
Train loss: 2.4590
Epoch: 135/800
Train loss: 2.4024
Epoch: 136/800
Train loss: 2.5283
Epoch: 137/800
Train loss: 2.3917
Epoch: 138/800
Train loss: 2.3958
Epoch: 139/800
Train loss: 2.3724
Epoch: 140/800
Train loss: 2.3707
Epoch: 141/800
Train loss: 2.4292
Epoch: 142/800
Train loss: 2.2875
Epoch: 143/800
Train loss: 2.5017
Epoch: 144/800
Train loss: 2.3596
Epoch: 145/800
Train loss: 2.4146
Epoch: 146/800
Train loss: 2.3217
Epoch: 147/800
Train loss: 2.5206
Epoch: 148/800
Train loss: 2.4593
Epoch: 149/800
Train loss: 2.3138
Epoch: 150/800
Train loss: 2.4460
Epoch: 151/800
Train loss: 2.4536
Epoch: 152/800
Train loss: 2.5215
Epoch: 153/800
Train loss: 2.3370
Epoch: 154/800
Train loss: 2.1893
Epoch: 155/800
Train loss: 2.4017
Epoch: 156/800
Train loss: 2.2699
Epoch: 157/800
Train loss: 2.2769
Epoch: 158/800
Train loss: 2.4353
Epoch: 159/800
Train loss: 2.3141
Epoch: 160/800
Train loss: 2.4209
Epoch: 161/800
Train loss: 2.3999
Epoch: 162/800
Train loss: 2.2525
Epoch: 163/800
Train loss: 2.2265
Epoch: 164/800
Train loss: 2.4252
Epoch: 165/800
Train loss: 2.3709
Epoch: 166/800
Train loss: 2.3270
Epoch: 167/800
Train loss: 2.4017
Epoch: 168/800
Train loss: 2.2650
Epoch: 169/800
Train loss: 2.5194
Epoch: 170/800
Train loss: 2.3731
Epoch: 171/800
Train loss: 2.4285
Epoch: 172/800
Train loss: 2.4744
Epoch: 173/800
Train loss: 2.3218
Epoch: 174/800
Train loss: 2.3768
Epoch: 175/800
Train loss: 2.3139
Epoch: 176/800
Train loss: 2.4264
Epoch: 177/800
Train loss: 2.2740
Epoch: 178/800
Train loss: 2.3599
Epoch: 179/800
Train loss: 2.2590
Epoch: 180/800
Train loss: 2.3837
Epoch: 181/800
Train loss: 2.1979
Epoch: 182/800
Train loss: 2.2422
Epoch: 183/800
Train loss: 2.2930
Epoch: 184/800
Train loss: 2.2166
Epoch: 185/800
Train loss: 2.3677
Epoch: 186/800
Train loss: 2.3377
Epoch: 187/800
Train loss: 2.1453
Epoch: 188/800
Train loss: 2.2660
Epoch: 189/800
Train loss: 2.3183
Epoch: 190/800
Train loss: 2.3690
Epoch: 191/800
Train loss: 2.3687
Epoch: 192/800
Train loss: 2.2759
Epoch: 193/800
Train loss: 2.3368
Epoch: 194/800
Train loss: 2.2944
Epoch: 195/800
Train loss: 2.4268
Epoch: 196/800
Train loss: 2.2911
Epoch: 197/800
Train loss: 2.3087
Epoch: 198/800
Train loss: 2.3721
Epoch: 199/800
Train loss: 2.3684
Epoch: 200/800
Train loss: 2.2445
Epoch: 201/800
Train loss: 2.4196
Epoch: 202/800
Train loss: 2.2368
Epoch: 203/800
Train loss: 2.1992
Epoch: 204/800
Train loss: 2.3095
Epoch: 205/800
Train loss: 2.4138
Epoch: 206/800
Train loss: 2.2167
Epoch: 207/800
Train loss: 2.3681
Epoch: 208/800
Train loss: 2.1853
Epoch: 209/800
Train loss: 2.3559
Epoch: 210/800
Train loss: 2.3294
Epoch: 211/800
Train loss: 2.2912
Epoch: 212/800
Train loss: 2.1718
Epoch: 213/800
Train loss: 2.1449
Epoch: 214/800
Train loss: 2.1219
Epoch: 215/800
Train loss: 2.3453
Epoch: 216/800
Train loss: 2.3571
Epoch: 217/800
Train loss: 2.2549
Epoch: 218/800
Train loss: 2.1906
Epoch: 219/800
Train loss: 2.2172
Epoch: 220/800
Train loss: 2.2450
Epoch: 221/800
Train loss: 2.2164
Epoch: 222/800
Train loss: 2.1868
Epoch: 223/800
Train loss: 2.2353
Epoch: 224/800
Train loss: 2.2734
Epoch: 225/800
Train loss: 2.2458
Epoch: 226/800
Train loss: 2.2635
Epoch: 227/800
Train loss: 2.2452
Epoch: 228/800
Train loss: 2.1772
Epoch: 229/800
Train loss: 2.2044
Epoch: 230/800
Train loss: 2.3586
Epoch: 231/800
Train loss: 2.2618
Epoch: 232/800
Train loss: 2.2466
Epoch: 233/800
Train loss: 2.1972
Epoch: 234/800
Train loss: 2.2183
Epoch: 235/800
Train loss: 2.2148
Epoch: 236/800
Train loss: 2.1709
Epoch: 237/800
Train loss: 2.2076
Epoch: 238/800
Train loss: 2.1535
Epoch: 239/800
Train loss: 2.1158
Epoch: 240/800
Train loss: 2.1414
Epoch: 241/800
Train loss: 2.1191
Epoch: 242/800
Train loss: 2.1882
Epoch: 243/800
Train loss: 2.3249
Epoch: 244/800
Train loss: 2.2678
Epoch: 245/800
Train loss: 2.1144
Epoch: 246/800
Train loss: 2.1007
Epoch: 247/800
Train loss: 2.1584
Epoch: 248/800
Train loss: 2.1190
Epoch: 249/800
Train loss: 2.2551
Epoch: 250/800
Train loss: 2.1551
Epoch: 251/800
Train loss: 2.2673
Epoch: 252/800
Train loss: 2.3291
Epoch: 253/800
Train loss: 2.2397
Epoch: 254/800
Train loss: 2.1048
Epoch: 255/800
Train loss: 2.2249
Epoch: 256/800
Train loss: 2.2557
Epoch: 257/800
Train loss: 2.0904
Epoch: 258/800
Train loss: 2.2577
Epoch: 259/800
Train loss: 2.1263
Epoch: 260/800
Train loss: 2.2066
Epoch: 261/800
Train loss: 2.1564
Epoch: 262/800
Train loss: 2.1459
Epoch: 263/800
Train loss: 2.2588
Epoch: 264/800
Train loss: 2.1981
Epoch: 265/800
Train loss: 2.1291
Epoch: 266/800
Train loss: 2.2279
Epoch: 267/800
Train loss: 2.2623
Epoch: 268/800
Train loss: 2.0714
Epoch: 269/800
Train loss: 2.1415
Epoch: 270/800
Train loss: 2.2978
Epoch: 271/800
Train loss: 2.1730
Epoch: 272/800
Train loss: 2.1369
Epoch: 273/800
Train loss: 2.2118
Epoch: 274/800
Train loss: 2.1049
Epoch: 275/800
Train loss: 2.1392
Epoch: 276/800
Train loss: 1.8867
Epoch: 277/800
Train loss: 2.2461
Epoch: 278/800
Train loss: 2.0104
Epoch: 279/800
Train loss: 2.2611
Epoch: 280/800
Train loss: 2.2803
Epoch: 281/800
Train loss: 2.2392
Epoch: 282/800
Train loss: 2.0741
Epoch: 283/800
Train loss: 2.1687
Epoch: 284/800
Train loss: 2.2139
Epoch: 285/800
Train loss: 2.1772
Epoch: 286/800
Train loss: 2.2193
Epoch: 287/800
Train loss: 2.1785
Epoch: 288/800
Train loss: 2.0753
Epoch: 289/800
Train loss: 2.0829
Epoch: 290/800
Train loss: 2.2913
Epoch: 291/800
Train loss: 2.1870
Epoch: 292/800
Train loss: 2.1330
Epoch: 293/800
Train loss: 2.2006
Epoch: 294/800
Train loss: 2.1162
Epoch: 295/800
Train loss: 2.2839
Epoch: 296/800
Train loss: 2.2088
Epoch: 297/800
Train loss: 2.1603
Epoch: 298/800
Train loss: 2.2358
Epoch: 299/800
Train loss: 2.1880
Epoch: 300/800
Train loss: 2.2138
Epoch: 301/800
Train loss: 2.2626
Epoch: 302/800
Train loss: 2.1625
Epoch: 303/800
Train loss: 2.1658
Epoch: 304/800
Train loss: 2.1352
Epoch: 305/800
Train loss: 2.1357
Epoch: 306/800
Train loss: 2.1747
Epoch: 307/800
Train loss: 2.0639
Epoch: 308/800
Train loss: 2.1070
Epoch: 309/800
Train loss: 2.0505
Epoch: 310/800
Train loss: 1.9954
Epoch: 311/800
Train loss: 2.0677
Epoch: 312/800
Train loss: 2.2037
Epoch: 313/800
Train loss: 2.0584
Epoch: 314/800
Train loss: 2.0852
Epoch: 315/800
Train loss: 2.0525
Epoch: 316/800
Train loss: 1.9567
Epoch: 317/800
Train loss: 2.0962
Epoch: 318/800
Train loss: 2.1362
Epoch: 319/800
Train loss: 2.2512
Epoch: 320/800
Train loss: 2.1838
Epoch: 321/800
Train loss: 2.0564
Epoch: 322/800
Train loss: 2.1954
Epoch: 323/800
Train loss: 1.9967
Epoch: 324/800
Train loss: 2.0034
Epoch: 325/800
Train loss: 2.1269
Epoch: 326/800
Train loss: 2.0993
Epoch: 327/800
Train loss: 2.1474
Epoch: 328/800
Train loss: 2.2174
Epoch: 329/800
Train loss: 2.1090
Epoch: 330/800
Train loss: 2.1019
Epoch: 331/800
Train loss: 2.2152
Epoch: 332/800
Train loss: 2.0796
Epoch: 333/800
Train loss: 2.1980
Epoch: 334/800
Train loss: 2.0705
Epoch: 335/800
Train loss: 2.0394
Epoch: 336/800
Train loss: 2.1740
Epoch: 337/800
Train loss: 2.1264
Epoch: 338/800
Train loss: 2.0593
Epoch: 339/800
Train loss: 2.0369
Epoch: 340/800
Train loss: 2.0535
Epoch: 341/800
Train loss: 2.1089
Epoch: 342/800
Train loss: 2.1414
Epoch: 343/800
Train loss: 2.0640
Epoch: 344/800
Train loss: 2.1472
Epoch: 345/800
Train loss: 2.1715
Epoch: 346/800
Train loss: 2.2683
Epoch: 347/800
Train loss: 2.0065
Epoch: 348/800
Train loss: 2.2574
Epoch: 349/800
Train loss: 2.0169
Epoch: 350/800
Train loss: 2.0014
Epoch: 351/800
Train loss: 2.0962
Epoch: 352/800
Train loss: 2.1088
Epoch: 353/800
Train loss: 2.0634
Epoch: 354/800
Train loss: 2.1242
Epoch: 355/800
Train loss: 2.0104
Epoch: 356/800
Train loss: 2.1485
Epoch: 357/800
Train loss: 2.1108
Epoch: 358/800
Train loss: 2.0890
Epoch: 359/800
Train loss: 2.0759
Epoch: 360/800
Train loss: 1.9594
Epoch: 361/800
Train loss: 2.1561
Epoch: 362/800
Train loss: 1.9933
Epoch: 363/800
Train loss: 2.0414
Epoch: 364/800
Train loss: 2.0533
Epoch: 365/800
Train loss: 2.0411
Epoch: 366/800
Train loss: 2.1295
Epoch: 367/800
Train loss: 1.9648
Epoch: 368/800
Train loss: 2.0174
Epoch: 369/800
Train loss: 2.1839
Epoch: 370/800
Train loss: 2.0704
Epoch: 371/800
Train loss: 2.2608
Epoch: 372/800
Train loss: 2.0388
Epoch: 373/800
Train loss: 2.1056
Epoch: 374/800
Train loss: 2.1158
Epoch: 375/800
Train loss: 2.1773
Epoch: 376/800
Train loss: 1.9241
Epoch: 377/800
Train loss: 2.0603
Epoch: 378/800
Train loss: 2.0210
Epoch: 379/800
Train loss: 2.0173
Epoch: 380/800
Train loss: 2.1147
Epoch: 381/800
Train loss: 2.1177
Epoch: 382/800
Train loss: 2.1562
Epoch: 383/800
Train loss: 1.9905
Epoch: 384/800
Train loss: 2.1800
Epoch: 385/800
Train loss: 2.0986
Epoch: 386/800
Train loss: 2.1299
Epoch: 387/800
Train loss: 2.1856
Epoch: 388/800
Train loss: 2.2251
Epoch: 389/800
Train loss: 2.1065
Epoch: 390/800
Train loss: 2.1118
Epoch: 391/800
Train loss: 2.0995
Epoch: 392/800
Train loss: 2.0932
Epoch: 393/800
Train loss: 2.0209
Epoch: 394/800
Train loss: 2.1607
Epoch: 395/800
Train loss: 2.0779
Epoch: 396/800
Train loss: 1.9563
Epoch: 397/800
Train loss: 2.1938
Epoch: 398/800
Train loss: 1.9062
Epoch: 399/800
Train loss: 2.0934
Epoch: 400/800
Train loss: 2.0311
Epoch: 401/800
Train loss: 2.0379
Epoch: 402/800
Train loss: 2.0810
0.8081147836015081
Model improve: 0.000000 -> 0.808115
Epoch: 403/800
Train loss: 1.9162
0.8133263819253713
Model improve: 0.808115 -> 0.813326
Epoch: 404/800
Train loss: 2.0191
0.8134005391727087
Model improve: 0.813326 -> 0.813401
Epoch: 405/800
Train loss: 2.0494
0.8131811181167284
Epoch: 406/800
Train loss: 1.9606
0.8123145282243396
Epoch: 407/800
Train loss: 1.9949
0.8115933633538064
Epoch: 408/800
Train loss: 1.9886
0.8124025781135774
Epoch: 409/800
Train loss: 2.0966
0.8120942144627221
Epoch: 410/800
Train loss: 2.1185
Epoch: 411/800
Train loss: 1.9849
0.8144564101293699
Model improve: 0.813401 -> 0.814456
Epoch: 412/800
Train loss: 2.1241
Epoch: 413/800
Train loss: 2.1484
Epoch: 414/800
Train loss: 2.0796
0.8152008983925908
Model improve: 0.814456 -> 0.815201
Epoch: 415/800
Train loss: 2.1375
Epoch: 416/800
Train loss: 2.0311
0.8116070387149766
Epoch: 417/800
Train loss: 2.0014
0.8123716256408396
Epoch: 418/800
Train loss: 1.9786
0.8109694555728375
Epoch: 419/800
Train loss: 1.8912
0.8118928892790698
Epoch: 420/800
Train loss: 2.0953
0.8132753159750298
Epoch: 421/800
Train loss: 1.9940
0.815809048225993
Model improve: 0.815201 -> 0.815809
Epoch: 422/800
Train loss: 1.8764
0.8116951515475086
Epoch: 423/800
Train loss: 1.9418
0.8142355464290514
Epoch: 424/800
Train loss: 2.0824
0.8146442004969018
Epoch: 425/800
Train loss: 2.0903
0.8107340857797862
Epoch: 426/800
Train loss: 2.0366
0.8132222123276371
Epoch: 427/800
Train loss: 2.0415
0.8132145743364865
Epoch: 428/800
Train loss: 1.9665
0.8144376661645315
Epoch: 429/800
Train loss: 1.9536
0.8104582955184132
Epoch: 430/800
Train loss: 2.0559
0.8110557494919788
Epoch: 431/800
Train loss: 2.0765
0.8124690762798557
Epoch: 432/800
Train loss: 2.0653
0.8112240800507293
Epoch: 433/800
Train loss: 2.0329
0.811755160663023
Epoch: 434/800
Train loss: 2.1257
Epoch: 435/800
Train loss: 2.0990
0.8109986612690302
Epoch: 436/800
Train loss: 2.0764
0.8114633679263997
Epoch: 437/800
Train loss: 2.0234
0.8114769872711782
Epoch: 438/800
Train loss: 2.1110
Epoch: 439/800
Train loss: 2.0602
0.8126632876380044
Epoch: 440/800
Train loss: 2.1085
Epoch: 441/800
Train loss: 2.1051
Epoch: 442/800
Train loss: 1.9878
0.8148131997743039
Epoch: 443/800
Train loss: 1.9988
0.8113736394555705
Epoch: 444/800
Train loss: 1.9918
0.8128018223926096
Epoch: 445/800
Train loss: 1.9347
0.8114227256376809
Epoch: 446/800
Train loss: 2.0459
0.8093983509253898
Epoch: 447/800
Train loss: 2.0946
0.8102024520288462
Epoch: 448/800
Train loss: 2.0202
0.8105264684506412
Epoch: 449/800
Train loss: 2.0477
0.8096194658956509
Epoch: 450/800
Train loss: 2.0322
0.8090401347358573
Epoch: 451/800
Train loss: 1.8999
0.812390778282812
Epoch: 452/800
Train loss: 2.0814
0.8112946677121591
Epoch: 453/800
Train loss: 1.9077
0.8128380917604551
Epoch: 454/800
Train loss: 2.0230
0.812811553836519
Epoch: 455/800
Train loss: 2.0474
0.8133107323740079
Epoch: 456/800
Train loss: 2.0858
0.8139892887579202
Epoch: 457/800
Train loss: 2.0151
0.8095662231850033
Epoch: 458/800
Train loss: 2.0528
0.8101948346858312
Epoch: 459/800
Train loss: 2.0943
0.8102300890431706
Epoch: 460/800
Train loss: 2.0239
0.8125017530358086
Epoch: 461/800
Train loss: 1.9589
0.8133382309642003
Epoch: 462/800
Train loss: 2.0561
0.8117564283178569
Epoch: 463/800
Train loss: 2.0699
0.8108606686504929
Epoch: 464/800
Train loss: 2.0254
0.8130640063195252
Epoch: 465/800
Train loss: 2.0536
Date :05/20/2023, 07:48:10
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 500
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 0
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.25
drop_path_rate: 0.4
76407
Fold: 1
13553
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/500
Date :05/20/2023, 07:48:23
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 0
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.25
drop_path_rate: 0.4
76407
Fold: 1
13553
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :05/20/2023, 07:49:17
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 0
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.25
drop_path_rate: 0.4
76407
Fold: 1
13553
AdamP (
Parameter Group 0
    betas: (0.9, 0.999)
    delta: 0.1
    eps: 1e-08
    initial_lr: 0.0003
    lr: 0.0003
    nesterov: False
    wd_ratio: 0.1
    weight_decay: 0.01
)
Epoch: 1/200
Date :05/20/2023, 07:49:29
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.25
drop_path_rate: 0.4
76407
Fold: 1
13839
AdamP (
Parameter Group 0
    betas: (0.9, 0.999)
    delta: 0.1
    eps: 1e-08
    initial_lr: 0.0003
    lr: 0.0003
    nesterov: False
    wd_ratio: 0.1
    weight_decay: 0.01
)
Epoch: 1/200
Date :05/20/2023, 07:49:37
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.25
drop_path_rate: 0.4
76407
Fold: 1
13839
AdamP (
Parameter Group 0
    betas: (0.9, 0.999)
    delta: 0.1
    eps: 1e-08
    initial_lr: 0.0003
    lr: 0.0003
    nesterov: False
    wd_ratio: 0.1
    weight_decay: 0.01
)
Epoch: 1/100
Date :05/20/2023, 07:50:42
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.25
drop_path_rate: 0.4
76407
Fold: 1
13839
AdamP (
Parameter Group 0
    betas: (0.9, 0.999)
    delta: 0.1
    eps: 1e-08
    initial_lr: 0.0003
    lr: 0.0003
    nesterov: False
    wd_ratio: 0.1
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 79.0208
Epoch: 2/200
Date :05/20/2023, 07:51:47
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.25
drop_path_rate: 0.4
76407
Fold: 1
13839
AdamP (
Parameter Group 0
    betas: (0.9, 0.999)
    delta: 0.1
    eps: 1e-08
    initial_lr: 0.0003
    lr: 0.0003
    nesterov: False
    wd_ratio: 0.1
    weight_decay: 0.01
)
Epoch: 1/200
Date :05/20/2023, 07:52:24
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.25
drop_path_rate: 0.4
76407
Fold: 1
13839
AdamP (
Parameter Group 0
    betas: (0.9, 0.999)
    delta: 0.1
    eps: 1e-08
    initial_lr: 0.0003
    lr: 0.0003
    nesterov: False
    wd_ratio: 0.1
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 24.6197
Epoch: 2/100
Train loss: 5.2727
Epoch: 3/100
Train loss: 4.7438
Epoch: 4/100
Train loss: 4.3274
Epoch: 5/100
Train loss: 4.0623
Epoch: 6/100
Train loss: 3.8768
Epoch: 7/100
Train loss: 3.7215
Epoch: 8/100
Train loss: 3.6484
Epoch: 9/100
Train loss: 3.4311
Epoch: 10/100
Train loss: 3.5093
Epoch: 11/100
Train loss: 3.2506
Epoch: 12/100
Train loss: 3.2586
Epoch: 13/100
Train loss: 3.2550
Epoch: 14/100
Train loss: 3.1353
Epoch: 15/100
Train loss: 3.0280
Epoch: 16/100
Train loss: 3.1042
Epoch: 17/100
Train loss: 2.9593
Epoch: 18/100
Train loss: 3.0263
Epoch: 19/100
Train loss: 3.0238
Epoch: 20/100
Train loss: 2.8838
Epoch: 21/100
Train loss: 2.8674
Epoch: 22/100
Train loss: 2.9388
Epoch: 23/100
Train loss: 2.8188
Epoch: 24/100
Train loss: 2.7719
Epoch: 25/100
Train loss: 2.6622
Epoch: 26/100
Train loss: 2.7310
Epoch: 27/100
Train loss: 2.7188
Epoch: 28/100
Train loss: 2.7140
Epoch: 29/100
Train loss: 2.7233
Epoch: 30/100
Train loss: 2.6995
Epoch: 31/100
Train loss: 2.5943
Epoch: 32/100
Train loss: 2.6633
Epoch: 33/100
Train loss: 2.6443
Epoch: 34/100
Train loss: 2.6571
Epoch: 35/100
Train loss: 2.7104
Epoch: 36/100
Train loss: 2.4872
Epoch: 37/100
Train loss: 2.6169
Epoch: 38/100
Train loss: 2.6395
Epoch: 39/100
Train loss: 2.5139
Epoch: 40/100
Train loss: 2.4245
Epoch: 41/100
Train loss: 2.5204
Epoch: 42/100
Train loss: 2.4568
Epoch: 43/100
Train loss: 2.5327
Epoch: 44/100
Train loss: 2.4653
Epoch: 45/100
Train loss: 2.5336
Epoch: 46/100
Train loss: 2.5605
Epoch: 47/100
Train loss: 2.5069
Epoch: 48/100
Train loss: 2.5309
Epoch: 49/100
Train loss: 2.4861
Epoch: 50/100
Train loss: 2.4492
Epoch: 51/100
Train loss: 2.4075
Epoch: 52/100
Train loss: 2.3729
Epoch: 53/100
Train loss: 2.4049
Epoch: 54/100
Train loss: 2.4878
Epoch: 55/100
Train loss: 2.4657
Epoch: 56/100
Train loss: 2.3335
Epoch: 57/100
Train loss: 2.4520
Epoch: 58/100
Train loss: 2.4108
Epoch: 59/100
Train loss: 2.3574
Epoch: 60/100
Train loss: 2.4055
Epoch: 61/100
Train loss: 2.4092
Epoch: 62/100
Train loss: 2.5455
0.8111130506626184
Model improve: 0.000000 -> 0.811113
Epoch: 63/100
Train loss: 2.4083
0.8104872008216604
Epoch: 64/100
Train loss: 2.3333
0.8122649536917067
Model improve: 0.811113 -> 0.812265
Epoch: 65/100
Train loss: 2.2797
0.8102431165186
Epoch: 66/100
Train loss: 2.4046
Date :05/20/2023, 09:24:38
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.25
drop_path_rate: 0.4
76407
Fold: 0
13830
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :05/20/2023, 09:24:53
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.25
drop_path_rate: 0.4
76407
Fold: 0
13830
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 41.9235
Epoch: 2/200
Train loss: 5.7246
Epoch: 3/200
Date :05/20/2023, 09:27:23
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.25
drop_path_rate: 0.4
76407
Fold: 0
13830
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 41.9097
Epoch: 2/200
Train loss: 5.7240
Epoch: 3/200
Train loss: 5.2722
Epoch: 4/200
Train loss: 5.0186
Epoch: 5/200
Train loss: 4.7161
Epoch: 6/200
Train loss: 4.4680
Epoch: 7/200
Train loss: 4.1686
Epoch: 8/200
Train loss: 4.1175
Epoch: 9/200
Date :05/20/2023, 09:35:49
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b2
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.2
drop_path_rate: 0.35
76407
Fold: 4
13830
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 41.2391
Epoch: 2/200
Train loss: 5.6639
Epoch: 3/200
Train loss: 5.1903
Epoch: 4/200
Train loss: 4.9343
Epoch: 5/200
Train loss: 4.6154
Epoch: 6/200
Train loss: 4.3747
Epoch: 7/200
Train loss: 4.0733
Epoch: 8/200
Train loss: 4.0302
Epoch: 9/200
Train loss: 3.8518
Epoch: 10/200
Train loss: 3.7788
Epoch: 11/200
Train loss: 3.6855
Epoch: 12/200
Train loss: 3.5835
Epoch: 13/200
Train loss: 3.5215
Epoch: 14/200
Train loss: 3.4067
Epoch: 15/200
Train loss: 3.4439
Epoch: 16/200
Train loss: 3.3583
Epoch: 17/200
Train loss: 3.2418
Epoch: 18/200
Train loss: 3.1274
Epoch: 19/200
Train loss: 3.3348
Epoch: 20/200
Train loss: 3.1995
Epoch: 21/200
Train loss: 3.0661
Epoch: 22/200
Train loss: 2.9416
Epoch: 23/200
Train loss: 2.9751
Epoch: 24/200
Train loss: 3.0545
Epoch: 25/200
Train loss: 3.0012
Epoch: 26/200
Train loss: 3.0165
Epoch: 27/200
Train loss: 2.9859
Epoch: 28/200
Train loss: 2.8223
Epoch: 29/200
Train loss: 2.8690
Epoch: 30/200
Train loss: 2.7145
Epoch: 31/200
Train loss: 2.8030
Epoch: 32/200
Train loss: 2.8730
Epoch: 33/200
Train loss: 2.6815
Epoch: 34/200
Train loss: 2.7486
Epoch: 35/200
Train loss: 2.7568
Epoch: 36/200
Train loss: 2.8084
Epoch: 37/200
Train loss: 2.6616
Epoch: 38/200
Train loss: 2.9155
Epoch: 39/200
Train loss: 2.5757
Epoch: 40/200
Train loss: 2.7181
Epoch: 41/200
Train loss: 2.6355
Epoch: 42/200
Train loss: 2.6408
Epoch: 43/200
Train loss: 2.7256
Epoch: 44/200
Train loss: 2.6689
Epoch: 45/200
Train loss: 2.6927
Epoch: 46/200
Train loss: 2.4539
Epoch: 47/200
Train loss: 2.6481
Epoch: 48/200
Train loss: 2.4011
Epoch: 49/200
Train loss: 2.3916
Epoch: 50/200
Train loss: 2.4686
Epoch: 51/200
Train loss: 2.4956
Epoch: 52/200
Train loss: 2.4892
Epoch: 53/200
Train loss: 2.3971
Epoch: 54/200
Train loss: 2.5107
Epoch: 55/200
Train loss: 2.4803
Epoch: 56/200
Train loss: 2.4641
Epoch: 57/200
Train loss: 2.4751
Epoch: 58/200
Train loss: 2.4720
Epoch: 59/200
Train loss: 2.4614
Epoch: 60/200
Train loss: 2.4725
Epoch: 61/200
Train loss: 2.3751
Epoch: 62/200
Train loss: 2.4154
Epoch: 63/200
Train loss: 2.4267
Epoch: 64/200
Train loss: 2.4295
Epoch: 65/200
Train loss: 2.4696
Epoch: 66/200
Train loss: 2.3551
Epoch: 67/200
Train loss: 2.4353
Epoch: 68/200
Train loss: 2.4036
Epoch: 69/200
Train loss: 2.5770
Epoch: 70/200
Train loss: 2.3756
Epoch: 71/200
Train loss: 2.2450
Epoch: 72/200
Train loss: 2.2690
Epoch: 73/200
Train loss: 2.3969
Epoch: 74/200
Train loss: 2.3712
Epoch: 75/200
Train loss: 2.4344
Epoch: 76/200
Train loss: 2.3852
Epoch: 77/200
Train loss: 2.3471
Epoch: 78/200
Train loss: 2.2593
Epoch: 79/200
Train loss: 2.1086
Epoch: 80/200
Train loss: 2.3107
Epoch: 81/200
Train loss: 2.3074
Epoch: 82/200
Train loss: 2.2890
Epoch: 83/200
Train loss: 2.2499
Epoch: 84/200
Train loss: 2.2244
Epoch: 85/200
Train loss: 2.3615
Epoch: 86/200
Train loss: 2.2821
Epoch: 87/200
Train loss: 2.1660
Epoch: 88/200
Train loss: 2.3684
Epoch: 89/200
Train loss: 2.3777
Epoch: 90/200
Train loss: 2.2412
Epoch: 91/200
Train loss: 2.3368
Epoch: 92/200
Train loss: 2.3209
Epoch: 93/200
Train loss: 2.3437
Epoch: 94/200
Train loss: 2.2727
Epoch: 95/200
Train loss: 2.2937
Epoch: 96/200
Train loss: 2.3458
Epoch: 97/200
Train loss: 2.2876
Epoch: 98/200
Train loss: 2.2606
Epoch: 99/200
Train loss: 2.2086
Epoch: 100/200
Train loss: 2.2551
Epoch: 101/200
Train loss: 2.3010
Epoch: 102/200
Train loss: 2.0962
0.807098965615276
Model improve: 0.000000 -> 0.807099
Epoch: 103/200
Date :05/20/2023, 11:13:37
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b2
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.3
drop_path_rate: 0.45
76407
Fold: 4
13830
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 42.5775
Epoch: 2/200
Train loss: 5.7252
Epoch: 3/200
Train loss: 5.2535
Epoch: 4/200
Train loss: 4.9971
Epoch: 5/200
Train loss: 4.6824
Epoch: 6/200
Train loss: 4.4466
Epoch: 7/200
Train loss: 4.1483
Epoch: 8/200
Train loss: 4.1009
Epoch: 9/200
Train loss: 3.9267
Epoch: 10/200
Train loss: 3.8443
Epoch: 11/200
Train loss: 3.7510
Epoch: 12/200
Train loss: 3.6478
Epoch: 13/200
Train loss: 3.5866
Epoch: 14/200
Train loss: 3.4696
Epoch: 15/200
Train loss: 3.5087
Epoch: 16/200
Train loss: 3.4206
Epoch: 17/200
Train loss: 3.3037
Epoch: 18/200
Train loss: 3.1909
Epoch: 19/200
Train loss: 3.3967
Epoch: 20/200
Train loss: 3.2579
Epoch: 21/200
Train loss: 3.1279
Epoch: 22/200
Train loss: 3.0035
Epoch: 23/200
Train loss: 3.0366
Epoch: 24/200
Train loss: 3.1151
Epoch: 25/200
Train loss: 3.0603
Epoch: 26/200
Train loss: 3.0765
Epoch: 27/200
Train loss: 3.0458
Epoch: 28/200
Train loss: 2.8803
Epoch: 29/200
Train loss: 2.9292
Epoch: 30/200
Train loss: 2.7783
Epoch: 31/200
Train loss: 2.8614
Epoch: 32/200
Train loss: 2.9288
Epoch: 33/200
Train loss: 2.7418
Epoch: 34/200
Train loss: 2.8043
Epoch: 35/200
Train loss: 2.8136
Epoch: 36/200
Train loss: 2.8609
Epoch: 37/200
Train loss: 2.7203
Epoch: 38/200
Train loss: 2.9652
Epoch: 39/200
Train loss: 2.6347
Epoch: 40/200
Train loss: 2.7736
Epoch: 41/200
Train loss: 2.6901
Epoch: 42/200
Train loss: 2.6954
Epoch: 43/200
Train loss: 2.7820
Epoch: 44/200
Train loss: 2.7235
Epoch: 45/200
Train loss: 2.7435
Epoch: 46/200
Train loss: 2.5111
Epoch: 47/200
Train loss: 2.7003
Epoch: 48/200
Train loss: 2.4563
Epoch: 49/200
Train loss: 2.4476
Epoch: 50/200
Train loss: 2.5233
Epoch: 51/200
Train loss: 2.5496
Epoch: 52/200
Train loss: 2.5433
Epoch: 53/200
Train loss: 2.4503
Epoch: 54/200
Train loss: 2.5658
Epoch: 55/200
Train loss: 2.5323
Epoch: 56/200
Train loss: 2.5156
Epoch: 57/200
Train loss: 2.5294
Epoch: 58/200
Train loss: 2.5234
Epoch: 59/200
Train loss: 2.5117
Epoch: 60/200
Train loss: 2.5282
Epoch: 61/200
Train loss: 2.4258
Epoch: 62/200
Train loss: 2.4665
Epoch: 63/200
Train loss: 2.4773
Epoch: 64/200
Train loss: 2.4788
Epoch: 65/200
Train loss: 2.5176
Epoch: 66/200
Train loss: 2.4030
Epoch: 67/200
Train loss: 2.4826
Epoch: 68/200
Train loss: 2.4518
Epoch: 69/200
Train loss: 2.6239
Epoch: 70/200
Train loss: 2.4230
Epoch: 71/200
Train loss: 2.2935
Epoch: 72/200
Train loss: 2.3184
Epoch: 73/200
Train loss: 2.4498
Epoch: 74/200
Train loss: 2.4186
Epoch: 75/200
Train loss: 2.4812
Epoch: 76/200
Train loss: 2.4303
Epoch: 77/200
Train loss: 2.3943
Epoch: 78/200
Train loss: 2.3067
Epoch: 79/200
Train loss: 2.1577
Epoch: 80/200
Train loss: 2.3599
Epoch: 81/200
Train loss: 2.3576
Epoch: 82/200
Train loss: 2.3371
Epoch: 83/200
Train loss: 2.2979
Epoch: 84/200
Train loss: 2.2733
Epoch: 85/200
Train loss: 2.4103
Epoch: 86/200
Train loss: 2.3298
Epoch: 87/200
Train loss: 2.2096
Epoch: 88/200
Train loss: 2.4140
Epoch: 89/200
Train loss: 2.4218
Epoch: 90/200
Train loss: 2.2866
Epoch: 91/200
Train loss: 2.3825
Epoch: 92/200
Train loss: 2.3681
Epoch: 93/200
Train loss: 2.3889
Epoch: 94/200
Train loss: 2.3192
Epoch: 95/200
Train loss: 2.3397
Epoch: 96/200
Train loss: 2.3920
Epoch: 97/200
Train loss: 2.3371
Epoch: 98/200
Train loss: 2.3102
Epoch: 99/200
Train loss: 2.2534
Epoch: 100/200
Train loss: 2.2994
Epoch: 101/200
Train loss: 2.3436
Epoch: 102/200
Train loss: 2.1429
0.8083941788014699
Model improve: 0.000000 -> 0.808394
Epoch: 103/200
Train loss: 2.1935
0.80559743541958
Epoch: 104/200
Train loss: 2.1926
0.8067383796193388
Epoch: 105/200
Train loss: 2.2488
0.8076785324923237
Epoch: 106/200
Train loss: 2.2673
0.8083330302449439
Epoch: 107/200
Train loss: 2.3293
0.8066598811370579
Epoch: 108/200
Train loss: 2.3246
0.8058445986199709
Epoch: 109/200
Train loss: 2.2740
0.8062260560475452
Epoch: 110/200
Train loss: 2.3304
0.8070159013471764
Epoch: 111/200
Train loss: 2.1326
0.8085734929667925
Model improve: 0.808394 -> 0.808573
Epoch: 112/200
Train loss: 2.2349
0.8059898756748646
Epoch: 113/200
Train loss: 2.2421
0.8079141555520367
Epoch: 114/200
Train loss: 2.3388
0.8079902544218739
Epoch: 115/200
Train loss: 2.2199
0.808713119571505
Model improve: 0.808573 -> 0.808713
Epoch: 116/200
Train loss: 2.2747
0.8092434582504963
Model improve: 0.808713 -> 0.809243
Epoch: 117/200
Train loss: 2.2264
0.8076442496114211
Epoch: 118/200
Train loss: 2.2023
0.8056234706311679
Epoch: 119/200
Train loss: 2.3141
0.8066012615198975
Epoch: 120/200
Train loss: 2.1834
0.8070724798115662
Epoch: 121/200
Train loss: 2.1888
0.8073699409438153
Epoch: 122/200
Train loss: 2.2795
0.8084327533552693
Epoch: 123/200
Train loss: 2.3800
0.8065613552414637
Epoch: 124/200
Train loss: 2.3773
0.8074137063321822
Epoch: 125/200
Train loss: 2.3166
0.8090491688233514
Epoch: 126/200
Train loss: 2.1668
0.8089008997585146
Epoch: 127/200
Train loss: 2.2382
0.8062993573182741
Epoch: 128/200
Train loss: 2.1351
0.8090355670394639
Epoch: 129/200
Train loss: 2.1778
0.8091023258245218
Epoch: 130/200
Train loss: 2.0781
0.8069986790867448
Epoch: 131/200
Train loss: 2.1801
0.8086897269982103
Epoch: 132/200
Train loss: 2.3156
0.8057643634959686
Epoch: 133/200
Train loss: 2.1030
0.8090889100517179
Epoch: 134/200
Train loss: 2.2557
0.8085595533290597
Epoch: 135/200
Train loss: 2.0970
0.8091140145432153
Epoch: 136/200
Train loss: 2.1786
0.8098104269382351
Model improve: 0.809243 -> 0.809810
Epoch: 137/200
Train loss: 2.2158
0.8083734225498082
Epoch: 138/200
Train loss: 2.0906
0.8098263002663046
Model improve: 0.809810 -> 0.809826
Epoch: 139/200
Train loss: 2.2236
0.8084226682970811
Epoch: 140/200
Train loss: 2.1577
0.8077407777397672
Epoch: 141/200
Train loss: 2.1823
0.8085724627925859
Epoch: 142/200
Train loss: 2.1136
0.8089954411994698
Epoch: 143/200
Train loss: 2.2605
0.8092551534298843
Epoch: 144/200
Train loss: 2.2400
0.8101604713672463
Model improve: 0.809826 -> 0.810160
Epoch: 145/200
Train loss: 2.0934
0.8111801332273602
Model improve: 0.810160 -> 0.811180
Epoch: 146/200
Train loss: 2.2430
0.8090780417694476
Epoch: 147/200
Train loss: 2.2442
0.8105354972692431
Epoch: 148/200
Train loss: 2.2872
0.8098411363097582
Epoch: 149/200
Train loss: 2.1458
0.8096200572635912
Epoch: 150/200
Train loss: 1.9930
0.810443389811402
Epoch: 151/200
Train loss: 2.1755
0.8102516819493597
Epoch: 152/200
Train loss: 2.0801
0.8102073049796618
Epoch: 153/200
Train loss: 2.0610
0.810444434526982
Epoch: 154/200
Train loss: 2.1605
0.810390611253156
Epoch: 155/200
Train loss: 2.2124
0.8081398688887681
Epoch: 156/200
Train loss: 2.1923
0.8097579286086727
Epoch: 157/200
Train loss: 2.1625
0.8090997285068441
Epoch: 158/200
Train loss: 2.0029
0.8113862182490187
Model improve: 0.811180 -> 0.811386
Epoch: 159/200
Train loss: 2.2109
0.809367158253276
Epoch: 160/200
Train loss: 2.0390
0.8105764389256063
Epoch: 161/200
Train loss: 2.2758
0.8100150451509122
Epoch: 162/200
Train loss: 2.1513
0.8086106342408951
Epoch: 163/200
Train loss: 2.1346
0.8094396683883236
Epoch: 164/200
Train loss: 2.2543
0.8078934871600112
Epoch: 165/200
Train loss: 2.2756
0.8078147418832041
Epoch: 166/200
Train loss: 2.2107
0.8101142156270802
Epoch: 167/200
Train loss: 2.2695
0.8089873305883474
Epoch: 168/200
Train loss: 2.1803
0.8101373798893005
Epoch: 169/200
Train loss: 2.2244
0.809741204527717
Epoch: 170/200
Train loss: 2.0884
0.8093195886240905
Epoch: 171/200
Train loss: 2.2484
0.8057435868930879
Epoch: 172/200
Train loss: 2.1145
0.8098703523208657
Epoch: 173/200
Train loss: 2.2267
0.8097230480002988
Epoch: 174/200
Train loss: 2.1044
0.8095605168845544
Epoch: 175/200
Train loss: 2.2224
0.8076043226755156
Epoch: 176/200
Train loss: 2.0175
0.8117726646530716
Model improve: 0.811386 -> 0.811773
Epoch: 177/200
Train loss: 2.1068
0.8084125977362108
Epoch: 178/200
Train loss: 2.1268
0.8108163655168906
Epoch: 179/200
Train loss: 2.0684
0.8105364146750218
Epoch: 180/200
Train loss: 2.2467
0.8080441937694366
Epoch: 181/200
Train loss: 2.1323
0.8109513664749112
Epoch: 182/200
Train loss: 2.0470
0.8088335353683539
Epoch: 183/200
Train loss: 2.0934
0.8111882984902617
Epoch: 184/200
Train loss: 2.2445
0.8089868919750396
Epoch: 185/200
Train loss: 2.1143
0.8107225392004651
Epoch: 186/200
Train loss: 2.2789
0.808846100225911
Epoch: 187/200
Train loss: 2.0587
0.8104289025353485
Epoch: 188/200
Train loss: 2.2768
0.8094531563971916
Epoch: 189/200
Train loss: 2.2332
0.807922671897912
Epoch: 190/200
Train loss: 2.1173
0.8114268223179782
Epoch: 191/200
Train loss: 2.2554
0.8087535546140137
Epoch: 192/200
Train loss: 2.1671
0.8097210733079745
Epoch: 193/200
Train loss: 2.2111
0.8082976004354658
Epoch: 194/200
Train loss: 2.2215
0.8093571107842504
Epoch: 195/200
Train loss: 2.1882
0.8094081098897302
Epoch: 196/200
Train loss: 2.1887
0.8077080135951458
Epoch: 197/200
Train loss: 2.1668
0.8096357850107732
Epoch: 198/200
Train loss: 2.1101
0.8094129237477663
Epoch: 199/200
Train loss: 2.2408
0.807152316158733
Epoch: 200/200
Train loss: 2.1428
0.809467203227937
Date :05/20/2023, 20:55:53
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b0
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.3
drop_path_rate: 0.45
76407
Fold: 4
13830
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 42.6320
Epoch: 2/200
Train loss: 5.7946
Epoch: 3/200
Date :05/20/2023, 20:57:46
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b0
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.3
drop_path_rate: 0.45
76407
Fold: 4
13830
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 42.6403
Epoch: 2/200
Train loss: 5.7955
Epoch: 3/200
Train loss: 5.3069
Epoch: 4/200
Train loss: 5.0501
Epoch: 5/200
Train loss: 4.7501
Epoch: 6/200
Train loss: 4.4959
Epoch: 7/200
Train loss: 4.1893
Epoch: 8/200
Train loss: 4.1256
Epoch: 9/200
Train loss: 3.9489
Epoch: 10/200
Train loss: 3.9063
Epoch: 11/200
Train loss: 3.7993
Epoch: 12/200
Train loss: 3.6938
Epoch: 13/200
Train loss: 3.6523
Epoch: 14/200
Train loss: 3.5224
Epoch: 15/200
Train loss: 3.5446
Epoch: 16/200
Train loss: 3.4881
Epoch: 17/200
Train loss: 3.3541
Epoch: 18/200
Train loss: 3.2610
Epoch: 19/200
Train loss: 3.4333
Epoch: 20/200
Train loss: 3.3001
Epoch: 21/200
Train loss: 3.1760
Epoch: 22/200
Train loss: 3.0460
Epoch: 23/200
Train loss: 3.0916
Epoch: 24/200
Train loss: 3.1770
Epoch: 25/200
Train loss: 3.1392
Epoch: 26/200
Train loss: 3.1431
Epoch: 27/200
Train loss: 3.0901
Epoch: 28/200
Train loss: 2.9373
Epoch: 29/200
Train loss: 2.9690
Epoch: 30/200
Train loss: 2.8173
Epoch: 31/200
Train loss: 2.9698
Epoch: 32/200
Train loss: 3.0052
Epoch: 33/200
Train loss: 2.8144
Epoch: 34/200
Train loss: 2.8944
Epoch: 35/200
Train loss: 2.8946
Epoch: 36/200
Train loss: 2.9196
Epoch: 37/200
Train loss: 2.8042
Epoch: 38/200
Train loss: 3.0253
Epoch: 39/200
Train loss: 2.6912
Epoch: 40/200
Train loss: 2.8420
Epoch: 41/200
Train loss: 2.7692
Epoch: 42/200
Train loss: 2.7486
Epoch: 43/200
Train loss: 2.8495
Epoch: 44/200
Train loss: 2.8080
Epoch: 45/200
Train loss: 2.8357
Epoch: 46/200
Train loss: 2.6009
Epoch: 47/200
Train loss: 2.7868
Epoch: 48/200
Train loss: 2.5282
Epoch: 49/200
Train loss: 2.5006
Epoch: 50/200
Train loss: 2.5954
Epoch: 51/200
Train loss: 2.6207
Epoch: 52/200
Train loss: 2.6119
Epoch: 53/200
Train loss: 2.5482
Epoch: 54/200
Train loss: 2.6372
Epoch: 55/200
Train loss: 2.6098
Epoch: 56/200
Train loss: 2.5709
Epoch: 57/200
Train loss: 2.5933
Epoch: 58/200
Train loss: 2.5976
Epoch: 59/200
Train loss: 2.5909
Epoch: 60/200
Train loss: 2.5918
Epoch: 61/200
Train loss: 2.4927
Epoch: 62/200
Train loss: 2.5104
Epoch: 63/200
Train loss: 2.5281
Epoch: 64/200
Train loss: 2.5619
Epoch: 65/200
Train loss: 2.6001
Epoch: 66/200
Train loss: 2.4655
Epoch: 67/200
Train loss: 2.5677
Epoch: 68/200
Train loss: 2.5226
Epoch: 69/200
Train loss: 2.6844
Epoch: 70/200
Train loss: 2.4831
Epoch: 71/200
Train loss: 2.3640
Epoch: 72/200
Train loss: 2.3732
Epoch: 73/200
Train loss: 2.5219
Epoch: 74/200
Train loss: 2.5069
Epoch: 75/200
Train loss: 2.5402
Epoch: 76/200
Train loss: 2.4959
Epoch: 77/200
Train loss: 2.4674
Epoch: 78/200
Train loss: 2.3862
Epoch: 79/200
Train loss: 2.2147
Epoch: 80/200
Train loss: 2.3928
Epoch: 81/200
Train loss: 2.3854
Epoch: 82/200
Train loss: 2.3922
Epoch: 83/200
Train loss: 2.3619
Epoch: 84/200
Train loss: 2.3351
Epoch: 85/200
Train loss: 2.4830
Epoch: 86/200
Train loss: 2.4098
Epoch: 87/200
Train loss: 2.2727
Epoch: 88/200
Train loss: 2.4752
Epoch: 89/200
Train loss: 2.4842
Epoch: 90/200
Train loss: 2.3549
Epoch: 91/200
Train loss: 2.4638
Epoch: 92/200
Train loss: 2.4349
Epoch: 93/200
Train loss: 2.4532
Epoch: 94/200
Train loss: 2.3683
Epoch: 95/200
Train loss: 2.4130
Epoch: 96/200
Train loss: 2.4612
Epoch: 97/200
Train loss: 2.4073
Epoch: 98/200
Train loss: 2.3759
Epoch: 99/200
Train loss: 2.3016
Epoch: 100/200
Train loss: 2.3636
Epoch: 101/200
Train loss: 2.4193
Epoch: 102/200
Train loss: 2.1998
0.8003131310668004
Model improve: 0.000000 -> 0.800313
Epoch: 103/200
Train loss: 2.2744
0.7986861748941859
Epoch: 104/200
Train loss: 2.2608
0.8000202209381123
Epoch: 105/200
Train loss: 2.3071
0.7997351433348643
Epoch: 106/200
Train loss: 2.3362
0.7980466641592624
Epoch: 107/200
Train loss: 2.3787
0.796748823537804
Epoch: 108/200
Train loss: 2.3617
0.7975955258205587
Epoch: 109/200
Train loss: 2.3207
0.7976969539741148
Epoch: 110/200
Train loss: 2.4045
0.79742787434661
Epoch: 111/200
Train loss: 2.1897
0.7999148074323879
Epoch: 112/200
Train loss: 2.2910
0.7980368128552796
Epoch: 113/200
Train loss: 2.2871
0.800450993772745
Model improve: 0.800313 -> 0.800451
Epoch: 114/200
Train loss: 2.3957
0.79794529775821
Epoch: 115/200
Train loss: 2.3047
0.8001051840037432
Epoch: 116/200
Train loss: 2.3317
0.8006753810742802
Model improve: 0.800451 -> 0.800675
Epoch: 117/200
Train loss: 2.2816
0.8001079255875745
Epoch: 118/200
Train loss: 2.2447
0.7992541702335719
Epoch: 119/200
Train loss: 2.3702
0.7992749504580682
Epoch: 120/200
Train loss: 2.2583
0.7997341007468856
Epoch: 121/200
Train loss: 2.2483
0.8010261425698733
Model improve: 0.800675 -> 0.801026
Epoch: 122/200
Train loss: 2.3409
0.801337020318473
Model improve: 0.801026 -> 0.801337
Epoch: 123/200
Train loss: 2.4405
0.7989036913471131
Epoch: 124/200
Train loss: 2.4498
0.8003126263114911
Epoch: 125/200
Train loss: 2.3987
0.8025741644955556
Model improve: 0.801337 -> 0.802574
Epoch: 126/200
Train loss: 2.2289
0.8011860044189009
Epoch: 127/200
Train loss: 2.3097
0.8007662596194701
Epoch: 128/200
Train loss: 2.1834
0.8025979239085487
Model improve: 0.802574 -> 0.802598
Epoch: 129/200
Train loss: 2.2206
0.8017783852338712
Epoch: 130/200
Train loss: 2.1437
0.7992670340448658
Epoch: 131/200
Train loss: 2.2480
0.801316356772988
Epoch: 132/200
Train loss: 2.3875
0.7996208444983791
Epoch: 133/200
Train loss: 2.1745
0.8029507362194295
Model improve: 0.802598 -> 0.802951
Epoch: 134/200
Train loss: 2.3022
0.8011712544940288
Epoch: 135/200
Train loss: 2.1619
0.8030663212349191
Model improve: 0.802951 -> 0.803066
Epoch: 136/200
Train loss: 2.2429
0.8023904652808218
Epoch: 137/200
Train loss: 2.2837
0.8000920299749296
Epoch: 138/200
Train loss: 2.1522
0.803204937789236
Model improve: 0.803066 -> 0.803205
Epoch: 139/200
Train loss: 2.2916
0.8003495264016699
Epoch: 140/200
Train loss: 2.2403
0.8000418982495237
Epoch: 141/200
Train loss: 2.2272
0.8011764370155862
Epoch: 142/200
Train loss: 2.1721
0.8007915839060081
Epoch: 143/200
Train loss: 2.3338
0.8014209572458572
Epoch: 144/200
Train loss: 2.2935
0.8014807638464496
Epoch: 145/200
Train loss: 2.1482
0.8024845693041713
Epoch: 146/200
Train loss: 2.3198
0.8015296434670154
Epoch: 147/200
Train loss: 2.3047
0.8011139244661755
Epoch: 148/200
Train loss: 2.3609
0.8004393291840916
Epoch: 149/200
Train loss: 2.2146
0.800708345883801
Epoch: 150/200
Train loss: 2.0419
0.8009058760269689
Epoch: 151/200
Train loss: 2.2372
0.8010026396300934
Epoch: 152/200
Train loss: 2.1322
0.8013666972478217
Epoch: 153/200
Train loss: 2.1077
0.8014812172489689
Epoch: 154/200
Train loss: 2.2284
0.8017211723521163
Epoch: 155/200
Train loss: 2.3079
0.7992954286709907
Epoch: 156/200
Train loss: 2.2524
0.800958180351078
Epoch: 157/200
Train loss: 2.2294
0.8004131814918175
Epoch: 158/200
Train loss: 2.0547
0.8044746680621858
Model improve: 0.803205 -> 0.804475
Epoch: 159/200
Train loss: 2.2955
0.8012320865182794
Epoch: 160/200
Train loss: 2.1064
0.8033293156251967
Epoch: 161/200
Train loss: 2.3423
0.8025281612474429
Epoch: 162/200
Train loss: 2.2375
0.8003880750417947
Epoch: 163/200
Train loss: 2.1933
0.8015805945013735
Epoch: 164/200
Train loss: 2.3333
0.7996284657553547
Epoch: 165/200
Train loss: 2.3395
0.8002807856590156
Epoch: 166/200
Train loss: 2.2950
0.8023229655683732
Epoch: 167/200
Train loss: 2.3356
0.8011875932060182
Epoch: 168/200
Train loss: 2.2449
0.8023088172122943
Epoch: 169/200
Train loss: 2.3050
0.8016177706124478
Epoch: 170/200
Train loss: 2.1816
0.8012733841783413
Epoch: 171/200
Train loss: 2.3142
0.8012408717579678
Epoch: 172/200
Train loss: 2.1805
0.8018857493228277
Epoch: 173/200
Train loss: 2.3094
0.8013612021020156
Epoch: 174/200
Train loss: 2.1706
0.8010196509423411
Epoch: 175/200
Train loss: 2.2724
0.7979994240462206
Epoch: 176/200
Train loss: 2.0760
0.8050012338597579
Model improve: 0.804475 -> 0.805001
Epoch: 177/200
Train loss: 2.1745
0.8017140283013693
Epoch: 178/200
Train loss: 2.1862
0.8037728138435651
Epoch: 179/200
Train loss: 2.1471
0.8033595737402273
Epoch: 180/200
Train loss: 2.3264
0.7995924821780585
Epoch: 181/200
Train loss: 2.2059
0.8030538825559353
Epoch: 182/200
Train loss: 2.1170
0.8007214842477225
Epoch: 183/200
Train loss: 2.1546
0.8032761994936441
Epoch: 184/200
Train loss: 2.3077
0.8014365754149693
Epoch: 185/200
Train loss: 2.1907
0.803051636419168
Epoch: 186/200
Train loss: 2.3399
0.8012405755935464
Epoch: 187/200
Train loss: 2.1261
0.8022827786214654
Epoch: 188/200
Train loss: 2.3560
0.8008887376272719
Epoch: 189/200
Train loss: 2.2925
0.8004673062058573
Epoch: 190/200
Train loss: 2.1940
0.8041147041338704
Epoch: 191/200
Train loss: 2.3077
0.799962807958698
Epoch: 192/200
Train loss: 2.2451
0.8020610810209658
Epoch: 193/200
Date :05/21/2023, 03:39:51
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_s
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.3
drop_path_rate: 0.45
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 35.5220
Epoch: 2/200
Train loss: 5.7513
Epoch: 3/200
Train loss: 5.3078
Epoch: 4/200
Train loss: 5.0129
Epoch: 5/200
Train loss: 4.6897
Epoch: 6/200
Train loss: 4.4197
Epoch: 7/200
Train loss: 4.1238
Epoch: 8/200
Train loss: 4.0366
Epoch: 9/200
Train loss: 3.8558
Epoch: 10/200
Train loss: 3.7738
Epoch: 11/200
Train loss: 3.6879
Epoch: 12/200
Train loss: 3.5679
Epoch: 13/200
Train loss: 3.5074
Epoch: 14/200
Train loss: 3.3797
Epoch: 15/200
Train loss: 3.3819
Epoch: 16/200
Train loss: 3.3383
Epoch: 17/200
Train loss: 3.1815
Epoch: 18/200
Train loss: 3.0861
Epoch: 19/200
Train loss: 3.2606
Epoch: 20/200
Train loss: 3.1354
Epoch: 21/200
Train loss: 2.9977
Epoch: 22/200
Train loss: 2.8512
Epoch: 23/200
Train loss: 2.8995
Epoch: 24/200
Train loss: 2.9760
Epoch: 25/200
Train loss: 2.9268
Epoch: 26/200
Train loss: 2.9232
Epoch: 27/200
Train loss: 2.8807
Epoch: 28/200
Train loss: 2.7125
Epoch: 29/200
Train loss: 2.7807
Epoch: 30/200
Train loss: 2.6190
Epoch: 31/200
Train loss: 2.7694
Epoch: 32/200
Train loss: 2.8042
Epoch: 33/200
Train loss: 2.5762
Epoch: 34/200
Train loss: 2.6719
Epoch: 35/200
Train loss: 2.6678
Epoch: 36/200
Train loss: 2.7023
Epoch: 37/200
Train loss: 2.5819
Epoch: 38/200
Train loss: 2.8096
Epoch: 39/200
Train loss: 2.4960
Epoch: 40/200
Train loss: 2.6394
Epoch: 41/200
Train loss: 2.5594
Epoch: 42/200
Train loss: 2.5334
Epoch: 43/200
Train loss: 2.6324
Epoch: 44/200
Train loss: 2.5993
Epoch: 45/200
Train loss: 2.6083
Epoch: 46/200
Train loss: 2.3586
Epoch: 47/200
Train loss: 2.5650
Epoch: 48/200
Train loss: 2.3134
Epoch: 49/200
Train loss: 2.3046
Epoch: 50/200
Train loss: 2.3715
Epoch: 51/200
Train loss: 2.4105
Epoch: 52/200
Train loss: 2.3970
Epoch: 53/200
Train loss: 2.3236
Epoch: 54/200
Train loss: 2.4323
Epoch: 55/200
Train loss: 2.4056
Epoch: 56/200
Train loss: 2.3558
Epoch: 57/200
Train loss: 2.3873
Epoch: 58/200
Train loss: 2.3786
Epoch: 59/200
Train loss: 2.3754
Epoch: 60/200
Train loss: 2.3897
Epoch: 61/200
Train loss: 2.2916
Epoch: 62/200
Train loss: 2.3086
Epoch: 63/200
Train loss: 2.3141
Epoch: 64/200
Train loss: 2.3208
Epoch: 65/200
Train loss: 2.3790
Epoch: 66/200
Train loss: 2.2629
Epoch: 67/200
Train loss: 2.3624
Epoch: 68/200
Train loss: 2.2872
Epoch: 69/200
Train loss: 2.4824
Epoch: 70/200
Train loss: 2.2716
Epoch: 71/200
Train loss: 2.1557
Epoch: 72/200
Train loss: 2.1709
Epoch: 73/200
Train loss: 2.3015
Epoch: 74/200
Train loss: 2.2654
Epoch: 75/200
Train loss: 2.3122
Epoch: 76/200
Train loss: 2.2956
Epoch: 77/200
Train loss: 2.2620
Epoch: 78/200
Train loss: 2.1709
Epoch: 79/200
Train loss: 2.0187
Epoch: 80/200
Train loss: 2.1907
Epoch: 81/200
Train loss: 2.1979
Epoch: 82/200
Train loss: 2.1870
Epoch: 83/200
Train loss: 2.1365
Epoch: 84/200
Train loss: 2.1189
Epoch: 85/200
Train loss: 2.2676
Epoch: 86/200
Train loss: 2.1898
Epoch: 87/200
Train loss: 2.0620
Epoch: 88/200
Train loss: 2.2696
Epoch: 89/200
Train loss: 2.2919
Epoch: 90/200
Train loss: 2.1260
Epoch: 91/200
Train loss: 2.2401
Epoch: 92/200
Train loss: 2.2432
Epoch: 93/200
Train loss: 2.2606
Epoch: 94/200
Train loss: 2.1728
Epoch: 95/200
Train loss: 2.2200
Epoch: 96/200
Train loss: 2.2526
Epoch: 97/200
Train loss: 2.1959
Epoch: 98/200
Train loss: 2.1626
Epoch: 99/200
Train loss: 2.1054
Epoch: 100/200
Train loss: 2.1564
Epoch: 101/200
Train loss: 2.2152
Epoch: 102/200
Train loss: 2.0044
0.8092562355130658
Model improve: 0.000000 -> 0.809256
Epoch: 103/200
Train loss: 2.0509
0.8084273551400404
Epoch: 104/200
Train loss: 2.0495
0.8092539362462979
Epoch: 105/200
Train loss: 2.0962
0.8100473015023124
Model improve: 0.809256 -> 0.810047
Epoch: 106/200
Train loss: 2.1208
0.8106325725374128
Model improve: 0.810047 -> 0.810633
Epoch: 107/200
Train loss: 2.1919
0.8082852381618836
Epoch: 108/200
Train loss: 2.1701
0.809192865600948
Epoch: 109/200
Train loss: 2.1355
0.8104842498317519
Epoch: 110/200
Train loss: 2.1743
0.810559210245182
Epoch: 111/200
Train loss: 1.9894
0.8115523175859952
Model improve: 0.810633 -> 0.811552
Epoch: 112/200
Train loss: 2.1051
0.8102011687197544
Epoch: 113/200
Train loss: 2.0956
0.8108206089443432
Epoch: 114/200
Train loss: 2.1946
0.8097617097239843
Epoch: 115/200
Train loss: 2.0651
0.8104261920153452
Epoch: 116/200
Train loss: 2.1171
0.8102075073376992
Epoch: 117/200
Train loss: 2.1018
0.810625709231365
Epoch: 118/200
Train loss: 2.0548
0.8105490408697116
Epoch: 119/200
Train loss: 2.1614
0.8099357592807619
Epoch: 120/200
Train loss: 2.0433
0.8110476682179744
Epoch: 121/200
Train loss: 2.0507
Date :05/21/2023, 07:07:03
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_s
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.25
drop_path_rate: 0.4
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 34.1780
Epoch: 2/200
Train loss: 5.7702
Epoch: 3/200
Train loss: 5.3226
Epoch: 4/200
Train loss: 5.0072
Epoch: 5/200
Train loss: 4.6714
Epoch: 6/200
Train loss: 4.3872
Epoch: 7/200
Train loss: 4.0810
Epoch: 8/200
Train loss: 3.9955
Epoch: 9/200
Train loss: 3.8120
Epoch: 10/200
Train loss: 3.7291
Epoch: 11/200
Train loss: 3.6405
Epoch: 12/200
Train loss: 3.5171
Epoch: 13/200
Train loss: 3.4570
Epoch: 14/200
Train loss: 3.3269
Epoch: 15/200
Train loss: 3.3310
Epoch: 16/200
Train loss: 3.2852
Epoch: 17/200
Train loss: 3.1307
Epoch: 18/200
Train loss: 3.0328
Epoch: 19/200
Train loss: 3.2081
Epoch: 20/200
Train loss: 3.0836
Epoch: 21/200
Train loss: 2.9443
Epoch: 22/200
Train loss: 2.7989
Epoch: 23/200
Train loss: 2.8465
Epoch: 24/200
Train loss: 2.9267
Epoch: 25/200
Train loss: 2.8751
Epoch: 26/200
Train loss: 2.8774
Epoch: 27/200
Train loss: 2.8337
Epoch: 28/200
Train loss: 2.6633
Epoch: 29/200
Train loss: 2.7349
Epoch: 30/200
Train loss: 2.5753
Epoch: 31/200
Train loss: 2.7228
Epoch: 32/200
Train loss: 2.7627
Epoch: 33/200
Train loss: 2.5319
Epoch: 34/200
Train loss: 2.6317
Epoch: 35/200
Train loss: 2.6270
Epoch: 36/200
Train loss: 2.6585
Epoch: 37/200
Train loss: 2.5431
Epoch: 38/200
Train loss: 2.7700
Epoch: 39/200
Train loss: 2.4540
Epoch: 40/200
Train loss: 2.5955
Epoch: 41/200
Train loss: 2.5194
Epoch: 42/200
Train loss: 2.4946
Epoch: 43/200
Train loss: 2.5918
Epoch: 44/200
Train loss: 2.5611
Epoch: 45/200
Train loss: 2.5674
Epoch: 46/200
Train loss: 2.3185
Epoch: 47/200
Train loss: 2.5288
Epoch: 48/200
Train loss: 2.2774
Epoch: 49/200
Train loss: 2.2644
Epoch: 50/200
Train loss: 2.3340
Epoch: 51/200
Train loss: 2.3751
Epoch: 52/200
Train loss: 2.3593
Epoch: 53/200
Train loss: 2.2859
Epoch: 54/200
Train loss: 2.4015
Epoch: 55/200
Train loss: 2.3682
Epoch: 56/200
Train loss: 2.3208
Epoch: 57/200
Train loss: 2.3510
Epoch: 58/200
Train loss: 2.3434
Epoch: 59/200
Train loss: 2.3390
Epoch: 60/200
Train loss: 2.3552
Epoch: 61/200
Train loss: 2.2538
Epoch: 62/200
Train loss: 2.2747
Epoch: 63/200
Train loss: 2.2811
Epoch: 64/200
Train loss: 2.2815
Epoch: 65/200
Train loss: 2.3425
Epoch: 66/200
Train loss: 2.2296
Epoch: 67/200
Train loss: 2.3272
Epoch: 68/200
Train loss: 2.2526
Epoch: 69/200
Train loss: 2.4456
Epoch: 70/200
Train loss: 2.2372
Epoch: 71/200
Train loss: 2.1168
Epoch: 72/200
Train loss: 2.1362
Epoch: 73/200
Train loss: 2.2670
Epoch: 74/200
Train loss: 2.2318
Epoch: 75/200
Train loss: 2.2738
Epoch: 76/200
Train loss: 2.2620
Epoch: 77/200
Train loss: 2.2287
Epoch: 78/200
Train loss: 2.1369
Epoch: 79/200
Train loss: 1.9825
Epoch: 80/200
Train loss: 2.1582
Epoch: 81/200
Train loss: 2.1656
Epoch: 82/200
Train loss: 2.1541
Epoch: 83/200
Train loss: 2.1052
Epoch: 84/200
Train loss: 2.0890
Epoch: 85/200
Train loss: 2.2363
Epoch: 86/200
Train loss: 2.1546
Epoch: 87/200
Train loss: 2.0311
Epoch: 88/200
Train loss: 2.2308
Epoch: 89/200
Train loss: 2.2592
Epoch: 90/200
Train loss: 2.0926
Epoch: 91/200
Train loss: 2.2046
Epoch: 92/200
Train loss: 2.2082
Epoch: 93/200
Train loss: 2.2269
Epoch: 94/200
Train loss: 2.1425
Epoch: 95/200
Train loss: 2.1883
Epoch: 96/200
Train loss: 2.2163
Epoch: 97/200
Train loss: 2.1609
Epoch: 98/200
Train loss: 2.1275
Epoch: 99/200
Train loss: 2.0700
Epoch: 100/200
Train loss: 2.1207
Epoch: 101/200
Train loss: 2.1851
Epoch: 102/200
Train loss: 1.9731
0.8092107935575382
Model improve: 0.000000 -> 0.809211
Epoch: 103/200
Train loss: 2.0221
Date :05/21/2023, 09:16:57
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.5
drop_path_rate: 0.2
76407
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/25
Train loss: 10.8035
Val cmap: 0.778756
Valid loss: 3.429662
Epoch: 2/25
Date :05/21/2023, 09:22:00
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.5
drop_path_rate: 0.2
76407
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/25
Train loss: 7.1229
Val cmap: 0.842205
Valid loss: 2.273779
Epoch: 2/25
Train loss: 3.1636
Val cmap: 0.882205
Valid loss: 1.894931
Epoch: 3/25
Train loss: 2.7921
Val cmap: 0.889505
Valid loss: 1.675425
Epoch: 4/25
Train loss: 2.4616
Val cmap: 0.895320
Valid loss: 1.586685
Epoch: 5/25
Train loss: 2.4079
Val cmap: 0.899490
Valid loss: 1.562965
Epoch: 6/25
Train loss: 2.2731
Val cmap: 0.905474
Valid loss: 1.471526
Epoch: 7/25
Date :05/21/2023, 09:40:01
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.5
drop_path_rate: 0.2
76407
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/25
Date :05/21/2023, 09:40:14
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.5
drop_path_rate: 0.2
76407
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/20
Train loss: 7.1200
Val cmap: 0.841427
Valid loss: 2.286296
Epoch: 2/20
Train loss: 3.1622
Val cmap: 0.881429
Valid loss: 1.909144
Epoch: 3/20
Train loss: 2.7886
Val cmap: 0.890176
Valid loss: 1.665626
Epoch: 4/20
Train loss: 2.4517
Val cmap: 0.898668
Valid loss: 1.546483
Epoch: 5/20
Train loss: 2.3939
Val cmap: 0.902034
Valid loss: 1.530864
Epoch: 6/20
Train loss: 2.2556
Val cmap: 0.906375
Valid loss: 1.477096
Epoch: 7/20
Train loss: 2.0955
Val cmap: 0.907202
Valid loss: 1.452818
Epoch: 8/20
Train loss: 2.0671
Val cmap: 0.909163
Valid loss: 1.461522
Epoch: 9/20
Train loss: 1.9156
Val cmap: 0.911876
Valid loss: 1.395616
Epoch: 10/20
Train loss: 1.9662
Val cmap: 0.914480
Valid loss: 1.350048
Epoch: 11/20
Train loss: 1.8314
Val cmap: 0.913205
Valid loss: 1.367893
Epoch: 12/20
Train loss: 1.7467
Val cmap: 0.913393
Valid loss: 1.323965
0.7920781256995897
Model improve: 0.000000 -> 0.792078
Epoch: 13/20
Train loss: 1.6972
Val cmap: 0.916533
Valid loss: 1.287328
Date :05/21/2023, 10:20:36
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 100
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_s
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.5
drop_path_rate: 0.2
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :05/21/2023, 10:22:19
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 100
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_s
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.5
drop_path_rate: 0.2
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 33.1554
Epoch: 2/100
Train loss: 5.7690
Epoch: 3/100
Train loss: 5.3166
Epoch: 4/100
Train loss: 4.9787
Epoch: 5/100
Train loss: 4.6162
Epoch: 6/100
Train loss: 4.3145
Epoch: 7/100
Train loss: 3.9953
Epoch: 8/100
Train loss: 3.9070
Epoch: 9/100
Train loss: 3.7198
Epoch: 10/100
Train loss: 3.6380
Epoch: 11/100
Train loss: 3.5459
Epoch: 12/100
Train loss: 3.4181
Epoch: 13/100
Train loss: 3.3576
Epoch: 14/100
Train loss: 3.2255
Epoch: 15/100
Train loss: 3.2353
Epoch: 16/100
Train loss: 3.1855
Epoch: 17/100
Train loss: 3.0290
Epoch: 18/100
Train loss: 2.9322
Epoch: 19/100
Train loss: 3.1150
Epoch: 20/100
Train loss: 2.9897
Epoch: 21/100
Train loss: 2.8495
Epoch: 22/100
Train loss: 2.7102
Epoch: 23/100
Train loss: 2.7510
Epoch: 24/100
Train loss: 2.8334
Epoch: 25/100
Train loss: 2.7850
Epoch: 26/100
Train loss: 2.7885
Epoch: 27/100
Train loss: 2.7479
Epoch: 28/100
Train loss: 2.5763
Epoch: 29/100
Train loss: 2.6513
Epoch: 30/100
Train loss: 2.4909
Epoch: 31/100
Train loss: 2.6403
Epoch: 32/100
Train loss: 2.6850
Epoch: 33/100
Train loss: 2.4480
Epoch: 34/100
Train loss: 2.5588
Epoch: 35/100
Train loss: 2.5457
Epoch: 36/100
Train loss: 2.5807
Epoch: 37/100
Train loss: 2.4665
Epoch: 38/100
Train loss: 2.7035
Epoch: 39/100
Train loss: 2.3826
Epoch: 40/100
Train loss: 2.5236
Epoch: 41/100
Train loss: 2.4477
Epoch: 42/100
Train loss: 2.4287
Epoch: 43/100
Train loss: 2.5264
Epoch: 44/100
Train loss: 2.5009
Epoch: 45/100
Train loss: 2.5052
Epoch: 46/100
Train loss: 2.2562
Epoch: 47/100
Train loss: 2.4680
Epoch: 48/100
Train loss: 2.2159
Epoch: 49/100
Train loss: 2.2075
Epoch: 50/100
Train loss: 2.2759
Epoch: 51/100
Train loss: 2.3199
Epoch: 52/100
Train loss: 2.3094
Epoch: 53/100
Train loss: 2.2360
Epoch: 54/100
Train loss: 2.3481
Epoch: 55/100
Train loss: 2.3203
Epoch: 56/100
Train loss: 2.2734
Epoch: 57/100
Train loss: 2.3085
Epoch: 58/100
Train loss: 2.2976
Epoch: 59/100
Train loss: 2.2915
Epoch: 60/100
Train loss: 2.3138
Epoch: 61/100
Train loss: 2.2129
Epoch: 62/100
Train loss: 2.2390
Epoch: 63/100
Train loss: 2.2488
Epoch: 64/100
Train loss: 2.2513
Epoch: 65/100
Train loss: 2.3120
Epoch: 66/100
Train loss: 2.2026
Epoch: 67/100
Train loss: 2.3042
Epoch: 68/100
Train loss: 2.2311
Epoch: 69/100
Train loss: 2.4313
Epoch: 70/100
Train loss: 2.2183
Epoch: 71/100
Train loss: 2.1045
Epoch: 72/100
Train loss: 2.1219
Epoch: 73/100
Train loss: 2.2549
Epoch: 74/100
Train loss: 2.2219
Epoch: 75/100
Train loss: 2.2683
Epoch: 76/100
Train loss: 2.2591
Epoch: 77/100
Train loss: 2.2319
Epoch: 78/100
Train loss: 2.1342
Epoch: 79/100
Train loss: 1.9895
Epoch: 80/100
Train loss: 2.1610
Epoch: 81/100
Train loss: 2.1701
Epoch: 82/100
Train loss: 2.1638
Epoch: 83/100
Train loss: 2.1235
Epoch: 84/100
Train loss: 2.1078
Epoch: 85/100
Train loss: 2.2560
Epoch: 86/100
Train loss: 2.1802
Epoch: 87/100
Train loss: 2.0557
Epoch: 88/100
Train loss: 2.2595
Epoch: 89/100
Train loss: 2.2928
Epoch: 90/100
Train loss: 2.1286
Epoch: 91/100
Train loss: 2.2436
Epoch: 92/100
Train loss: 2.2480
Epoch: 93/100
Train loss: 2.2771
Epoch: 94/100
Train loss: 2.1974
Epoch: 95/100
Train loss: 2.2421
Epoch: 96/100
Train loss: 2.2741
Epoch: 97/100
Train loss: 2.2252
Epoch: 98/100
Train loss: 2.1918
Epoch: 99/100
Train loss: 2.1400
Epoch: 100/100
Train loss: 2.1951
Date :05/21/2023, 12:29:42
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 300
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_s
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.5
drop_path_rate: 0.2
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/300
Train loss: 33.1509
Epoch: 2/300
Train loss: 5.7707
Epoch: 3/300
Train loss: 5.3193
Epoch: 4/300
Train loss: 4.9807
Epoch: 5/300
Train loss: 4.6174
Epoch: 6/300
Train loss: 4.3143
Epoch: 7/300
Train loss: 3.9933
Epoch: 8/300
Train loss: 3.9043
Epoch: 9/300
Train loss: 3.7158
Epoch: 10/300
Train loss: 3.6335
Epoch: 11/300
Train loss: 3.5405
Epoch: 12/300
Train loss: 3.4114
Epoch: 13/300
Train loss: 3.3493
Epoch: 14/300
Train loss: 3.2163
Epoch: 15/300
Train loss: 3.2259
Epoch: 16/300
Train loss: 3.1747
Epoch: 17/300
Train loss: 3.0170
Epoch: 18/300
Train loss: 2.9203
Epoch: 19/300
Train loss: 3.1021
Epoch: 20/300
Train loss: 2.9769
Epoch: 21/300
Train loss: 2.8347
Epoch: 22/300
Train loss: 2.6947
Epoch: 23/300
Train loss: 2.7348
Epoch: 24/300
Train loss: 2.8192
Epoch: 25/300
Train loss: 2.7690
Epoch: 26/300
Train loss: 2.7710
Epoch: 27/300
Train loss: 2.7293
Epoch: 28/300
Train loss: 2.5568
Epoch: 29/300
Train loss: 2.6322
Epoch: 30/300
Train loss: 2.4707
Epoch: 31/300
Train loss: 2.6192
Epoch: 32/300
Train loss: 2.6644
Epoch: 33/300
Train loss: 2.4245
Epoch: 34/300
Train loss: 2.5363
Epoch: 35/300
Train loss: 2.5224
Epoch: 36/300
Train loss: 2.5565
Epoch: 37/300
Train loss: 2.4403
Epoch: 38/300
Train loss: 2.6792
Epoch: 39/300
Train loss: 2.3541
Epoch: 40/300
Train loss: 2.4983
Epoch: 41/300
Train loss: 2.4193
Epoch: 42/300
Train loss: 2.4029
Epoch: 43/300
Train loss: 2.4965
Epoch: 44/300
Train loss: 2.4709
Epoch: 45/300
Train loss: 2.4740
Epoch: 46/300
Train loss: 2.2215
Epoch: 47/300
Train loss: 2.4351
Epoch: 48/300
Train loss: 2.1804
Epoch: 49/300
Train loss: 2.1702
Epoch: 50/300
Train loss: 2.2403
Epoch: 51/300
Train loss: 2.2843
Epoch: 52/300
Train loss: 2.2697
Epoch: 53/300
Train loss: 2.1973
Epoch: 54/300
Train loss: 2.3057
Epoch: 55/300
Train loss: 2.2815
Epoch: 56/300
Train loss: 2.2312
Epoch: 57/300
Train loss: 2.2627
Epoch: 58/300
Train loss: 2.2510
Epoch: 59/300
Train loss: 2.2454
Epoch: 60/300
Train loss: 2.2634
Epoch: 61/300
Train loss: 2.1610
Epoch: 62/300
Train loss: 2.1851
Epoch: 63/300
Train loss: 2.1931
Epoch: 64/300
Train loss: 2.1949
Epoch: 65/300
Train loss: 2.2540
Epoch: 66/300
Train loss: 2.1468
Epoch: 67/300
Train loss: 2.2410
Epoch: 68/300
Train loss: 2.1634
Epoch: 69/300
Train loss: 2.3576
Epoch: 70/300
Train loss: 2.1500
Epoch: 71/300
Train loss: 2.0339
Epoch: 72/300
Train loss: 2.0482
Epoch: 73/300
Train loss: 2.1803
Epoch: 74/300
Train loss: 2.1411
Epoch: 75/300
Train loss: 2.1848
Epoch: 76/300
Train loss: 2.1730
Epoch: 77/300
Train loss: 2.1465
Epoch: 78/300
Train loss: 2.0466
Epoch: 79/300
Train loss: 1.8976
Epoch: 80/300
Train loss: 2.0704
Epoch: 81/300
Train loss: 2.0762
Epoch: 82/300
Train loss: 2.0690
Epoch: 83/300
Train loss: 2.0228
Epoch: 84/300
Train loss: 2.0013
Epoch: 85/300
Train loss: 2.1508
Epoch: 86/300
Train loss: 2.0725
Epoch: 87/300
Train loss: 1.9483
Epoch: 88/300
Train loss: 2.1410
Epoch: 89/300
Train loss: 2.1658
Epoch: 90/300
Train loss: 2.0083
Epoch: 91/300
Train loss: 2.1187
Epoch: 92/300
Train loss: 2.1117
Epoch: 93/300
Train loss: 2.1406
Epoch: 94/300
Train loss: 2.0552
Epoch: 95/300
Train loss: 2.0859
Epoch: 96/300
Train loss: 2.1247
Epoch: 97/300
Train loss: 2.0647
Epoch: 98/300
Train loss: 2.0393
Epoch: 99/300
Train loss: 1.9826
Epoch: 100/300
Train loss: 2.0292
Epoch: 101/300
Train loss: 2.1006
Epoch: 102/300
Train loss: 1.8818
Epoch: 103/300
Train loss: 1.9377
Epoch: 104/300
Train loss: 1.9402
Epoch: 105/300
Train loss: 1.9929
Epoch: 106/300
Train loss: 1.9748
Epoch: 107/300
Train loss: 2.0512
Epoch: 108/300
Train loss: 2.0372
Epoch: 109/300
Train loss: 2.0044
Epoch: 110/300
Train loss: 2.0705
Epoch: 111/300
Train loss: 1.8719
Epoch: 112/300
Train loss: 1.9732
Epoch: 113/300
Train loss: 1.9551
Epoch: 114/300
Train loss: 2.0638
Epoch: 115/300
Train loss: 1.9577
Epoch: 116/300
Train loss: 1.9797
Epoch: 117/300
Train loss: 1.9572
Epoch: 118/300
Train loss: 1.9193
Epoch: 119/300
Train loss: 2.0319
Epoch: 120/300
Train loss: 1.9274
Epoch: 121/300
Train loss: 1.9335
Epoch: 122/300
Train loss: 1.9921
Epoch: 123/300
Train loss: 2.0980
Epoch: 124/300
Train loss: 2.0967
Epoch: 125/300
Train loss: 2.0532
Epoch: 126/300
Train loss: 1.8924
Epoch: 127/300
Train loss: 1.9769
Epoch: 128/300
Train loss: 1.8729
Epoch: 129/300
Train loss: 1.8899
Epoch: 130/300
Train loss: 1.8048
Epoch: 131/300
Train loss: 1.8963
Epoch: 132/300
Train loss: 2.0449
Epoch: 133/300
Train loss: 1.8368
Epoch: 134/300
Train loss: 1.9566
Epoch: 135/300
Train loss: 1.8262
Epoch: 136/300
Train loss: 1.9071
Epoch: 137/300
Train loss: 1.9286
Epoch: 138/300
Train loss: 1.8074
Epoch: 139/300
Train loss: 1.9449
Epoch: 140/300
Train loss: 1.8882
Epoch: 141/300
Train loss: 1.9010
Epoch: 142/300
Train loss: 1.8182
Epoch: 143/300
Train loss: 1.9651
Epoch: 144/300
Train loss: 1.9349
Epoch: 145/300
Train loss: 1.8007
Epoch: 146/300
Train loss: 1.9585
Epoch: 147/300
Train loss: 1.9320
Epoch: 148/300
Train loss: 1.9876
Epoch: 149/300
Train loss: 1.8687
Epoch: 150/300
Train loss: 1.6964
Epoch: 151/300
Train loss: 1.8755
Epoch: 152/300
Train loss: 1.7933
0.8023012486713172
Model improve: 0.000000 -> 0.802301
Epoch: 153/300
Date :05/21/2023, 15:40:20
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 300
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_s
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.4
drop_path_rate: 0.45
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/300
Train loss: 34.2685
Epoch: 2/300
Train loss: 5.8177
Epoch: 3/300
Train loss: 5.3906
Epoch: 4/300
Train loss: 5.0636
Epoch: 5/300
Train loss: 4.7182
Epoch: 6/300
Train loss: 4.4285
Epoch: 7/300
Train loss: 4.1179
Epoch: 8/300
Train loss: 4.0218
Epoch: 9/300
Train loss: 3.8381
Epoch: 10/300
Train loss: 3.7547
Epoch: 11/300
Train loss: 3.6651
Epoch: 12/300
Train loss: 3.5380
Epoch: 13/300
Train loss: 3.4810
Epoch: 14/300
Train loss: 3.3499
Epoch: 15/300
Train loss: 3.3516
Epoch: 16/300
Train loss: 3.3046
Epoch: 17/300
Train loss: 3.1501
Epoch: 18/300
Train loss: 3.0521
Epoch: 19/300
Train loss: 3.2276
Epoch: 20/300
Train loss: 3.1048
Epoch: 21/300
Train loss: 2.9657
Epoch: 22/300
Train loss: 2.8188
Epoch: 23/300
Train loss: 2.8648
Epoch: 24/300
Train loss: 2.9450
Epoch: 25/300
Train loss: 2.8943
Epoch: 26/300
Train loss: 2.8941
Epoch: 27/300
Train loss: 2.8512
Epoch: 28/300
Train loss: 2.6832
Epoch: 29/300
Train loss: 2.7508
Epoch: 30/300
Train loss: 2.5911
Epoch: 31/300
Train loss: 2.7384
Epoch: 32/300
Train loss: 2.7768
Epoch: 33/300
Train loss: 2.5483
Epoch: 34/300
Train loss: 2.6464
Epoch: 35/300
Train loss: 2.6379
Epoch: 36/300
Train loss: 2.6726
Epoch: 37/300
Train loss: 2.5586
Epoch: 38/300
Train loss: 2.7820
Epoch: 39/300
Train loss: 2.4689
Epoch: 40/300
Train loss: 2.6090
Epoch: 41/300
Train loss: 2.5320
Epoch: 42/300
Train loss: 2.5073
Epoch: 43/300
Train loss: 2.6048
Epoch: 44/300
Train loss: 2.5726
Epoch: 45/300
Train loss: 2.5758
Epoch: 46/300
Train loss: 2.3294
Epoch: 47/300
Train loss: 2.5379
Epoch: 48/300
Train loss: 2.2885
Epoch: 49/300
Train loss: 2.2786
Epoch: 50/300
Train loss: 2.3487
Epoch: 51/300
Train loss: 2.3836
Epoch: 52/300
Train loss: 2.3691
Epoch: 53/300
Train loss: 2.2955
Epoch: 54/300
Train loss: 2.4068
Epoch: 55/300
Train loss: 2.3801
Epoch: 56/300
Train loss: 2.3318
Epoch: 57/300
Train loss: 2.3581
Epoch: 58/300
Train loss: 2.3524
Epoch: 59/300
Train loss: 2.3464
Epoch: 60/300
Train loss: 2.3620
Epoch: 61/300
Train loss: 2.2634
Epoch: 62/300
Train loss: 2.2809
Epoch: 63/300
Train loss: 2.2867
Epoch: 64/300
Train loss: 2.2917
Epoch: 65/300
Train loss: 2.3521
Epoch: 66/300
Train loss: 2.2367
Epoch: 67/300
Train loss: 2.3348
Epoch: 68/300
Train loss: 2.2593
Epoch: 69/300
Train loss: 2.4573
Epoch: 70/300
Train loss: 2.2443
Epoch: 71/300
Train loss: 2.1271
Epoch: 72/300
Train loss: 2.1443
Epoch: 73/300
Train loss: 2.2745
Epoch: 74/300
Train loss: 2.2407
Epoch: 75/300
Train loss: 2.2820
Epoch: 76/300
Train loss: 2.2677
Epoch: 77/300
Train loss: 2.2362
Epoch: 78/300
Train loss: 2.1420
Epoch: 79/300
Train loss: 1.9879
Epoch: 80/300
Train loss: 2.1598
Epoch: 81/300
Train loss: 2.1720
Epoch: 82/300
Train loss: 2.1588
Epoch: 83/300
Train loss: 2.1093
Epoch: 84/300
Train loss: 2.0908
Epoch: 85/300
Train loss: 2.2409
Epoch: 86/300
Train loss: 2.1613
Epoch: 87/300
Train loss: 2.0369
Epoch: 88/300
Train loss: 2.2388
Epoch: 89/300
Train loss: 2.2626
Epoch: 90/300
Train loss: 2.0976
Epoch: 91/300
Train loss: 2.2089
Epoch: 92/300
Train loss: 2.2113
Epoch: 93/300
Train loss: 2.2265
Epoch: 94/300
Train loss: 2.1451
Epoch: 95/300
Train loss: 2.1880
Epoch: 96/300
Train loss: 2.2191
Epoch: 97/300
Train loss: 2.1628
Epoch: 98/300
Train loss: 2.1320
Epoch: 99/300
Train loss: 2.0717
Epoch: 100/300
Train loss: 2.1237
Epoch: 101/300
Train loss: 2.1856
Epoch: 102/300
Train loss: 1.9747
Epoch: 103/300
Train loss: 2.0269
Epoch: 104/300
Train loss: 2.0348
Epoch: 105/300
Train loss: 2.0854
Epoch: 106/300
Train loss: 2.0667
Epoch: 107/300
Train loss: 2.1447
Epoch: 108/300
Train loss: 2.1298
Epoch: 109/300
Train loss: 2.0922
Epoch: 110/300
Train loss: 2.1564
Epoch: 111/300
Train loss: 1.9581
Epoch: 112/300
Train loss: 2.0596
Epoch: 113/300
Train loss: 2.0446
Epoch: 114/300
Train loss: 2.1548
Epoch: 115/300
Train loss: 2.0419
Epoch: 116/300
Train loss: 2.0732
Epoch: 117/300
Train loss: 2.0420
Epoch: 118/300
Train loss: 2.0064
Epoch: 119/300
Train loss: 2.1200
Epoch: 120/300
Train loss: 2.0095
Epoch: 121/300
Train loss: 2.0169
Epoch: 122/300
Train loss: 2.0841
Epoch: 123/300
Train loss: 2.1843
Epoch: 124/300
Train loss: 2.1893
Epoch: 125/300
Train loss: 2.1471
Epoch: 126/300
Train loss: 1.9840
Epoch: 127/300
Train loss: 2.0607
Epoch: 128/300
Train loss: 1.9551
Epoch: 129/300
Train loss: 1.9791
Epoch: 130/300
Train loss: 1.8900
Epoch: 131/300
Train loss: 1.9804
Epoch: 132/300
Train loss: 2.1362
Epoch: 133/300
Train loss: 1.9240
Epoch: 134/300
Train loss: 2.0415
Epoch: 135/300
Train loss: 1.9142
Epoch: 136/300
Train loss: 1.9950
Epoch: 137/300
Train loss: 2.0128
Epoch: 138/300
Train loss: 1.8939
Epoch: 139/300
Train loss: 2.0315
Epoch: 140/300
Train loss: 1.9755
Epoch: 141/300
Train loss: 1.9843
Epoch: 142/300
Train loss: 1.9059
Epoch: 143/300
Train loss: 2.0515
Epoch: 144/300
Train loss: 2.0224
Epoch: 145/300
Train loss: 1.8884
Epoch: 146/300
Train loss: 2.0500
Epoch: 147/300
Train loss: 2.0236
Epoch: 148/300
Train loss: 2.0738
Epoch: 149/300
Train loss: 1.9539
Epoch: 150/300
Train loss: 1.7815
0.8127136604553087
Model improve: 0.000000 -> 0.812714
Epoch: 151/300
Train loss: 1.9938
0.8126922528829417
Epoch: 152/300
Train loss: 1.8777
0.8122491722896474
Epoch: 153/300
Train loss: 1.8441
0.8130070583268928
Model improve: 0.812714 -> 0.813007
Epoch: 154/300
Train loss: 1.9571
0.8135808436883321
Model improve: 0.813007 -> 0.813581
Epoch: 155/300
Train loss: 2.0120
0.810802322574707
Epoch: 156/300
Train loss: 1.9818
0.8123517532089567
Epoch: 157/300
Train loss: 1.9466
0.8110734010764814
Epoch: 158/300
Train loss: 1.7988
0.8099740009299882
Epoch: 159/300
Train loss: 2.0062
0.8100161253997945
Epoch: 160/300
Train loss: 1.8271
0.8120107347551402
Epoch: 161/300
Train loss: 2.0708
0.8121520281329486
Epoch: 162/300
Train loss: 1.9226
0.8105041397651678
Epoch: 163/300
Train loss: 1.9103
0.8106725035033282
Epoch: 164/300
Train loss: 2.0358
0.8111097010891515
Epoch: 165/300
Train loss: 2.0384
0.8099894920849287
Epoch: 166/300
Train loss: 2.0147
0.811947143062055
Epoch: 167/300
Train loss: 2.0422
0.8099765679557807
Epoch: 168/300
Train loss: 1.9642
0.8117447714734999
Epoch: 169/300
Train loss: 2.0084
0.8115229991162106
Epoch: 170/300
Train loss: 1.9053
0.8104712925743144
Epoch: 171/300
Train loss: 2.0281
0.8119314481451073
Epoch: 172/300
Train loss: 1.8828
0.8115672879630206
Epoch: 173/300
Train loss: 1.9983
0.8124620828119002
Epoch: 174/300
Train loss: 1.8773
0.8109093289831667
Epoch: 175/300
Train loss: 1.9831
0.8096208881467153
Epoch: 176/300
Train loss: 1.7865
0.8124604245534363
Epoch: 177/300
Train loss: 1.8607
0.8104960912760689
Epoch: 178/300
Train loss: 1.8848
0.8108854959526548
Epoch: 179/300
Train loss: 1.8337
0.8101667798152987
Epoch: 180/300
Train loss: 2.0316
0.8115059028732384
Epoch: 181/300
Train loss: 1.9044
0.8114018575283489
Epoch: 182/300
Train loss: 1.8158
0.811481112479269
Epoch: 183/300
Train loss: 1.8677
0.8110316452827714
Epoch: 184/300
Train loss: 2.0186
0.810817464943352
Epoch: 185/300
Train loss: 1.8801
0.8116639153016932
Epoch: 186/300
Train loss: 2.0270
0.811148159691425
Epoch: 187/300
Train loss: 1.8378
0.8125148393557043
Epoch: 188/300
Train loss: 2.0157
0.8106065146337006
Epoch: 189/300
Train loss: 1.9813
0.8107753525966004
Epoch: 190/300
Train loss: 1.8928
0.8121591847981895
Epoch: 191/300
Train loss: 1.9947
0.8111770622948091
Epoch: 192/300
Train loss: 1.9494
0.8102675336800206
Epoch: 193/300
Train loss: 1.9578
0.8104847068508028
Epoch: 194/300
Train loss: 1.9600
0.8107266353392513
Epoch: 195/300
Train loss: 1.9453
0.8105033934398443
Epoch: 196/300
Train loss: 1.9562
0.8123498643376488
Epoch: 197/300
Train loss: 1.9271
0.8120800550947953
Epoch: 198/300
Train loss: 1.8493
0.8121245470657961
Epoch: 199/300
Train loss: 1.9680
0.8119260651571917
Epoch: 200/300
Train loss: 1.8858
0.8127553555122594
Epoch: 201/300
Train loss: 1.9267
0.8118899707812941
Epoch: 202/300
Train loss: 1.9718
0.8102152301443863
Epoch: 203/300
Train loss: 1.8993
0.8120440277270102
Epoch: 204/300
Train loss: 1.9853
0.8113501687817308
Epoch: 205/300
Train loss: 1.8821
0.8108251825054894
Epoch: 206/300
Train loss: 1.8573
0.8105734298217636
Epoch: 207/300
Train loss: 1.7720
0.8110756927899395
Epoch: 208/300
Train loss: 1.7611
0.8127685789191363
Epoch: 209/300
Train loss: 1.9820
0.8116562741882204
Epoch: 210/300
Train loss: 1.9938
0.8101876294002824
Epoch: 211/300
Train loss: 1.9021
0.8119836301480999
Epoch: 212/300
Train loss: 1.8335
0.8116241455714613
Epoch: 213/300
Train loss: 1.8835
0.8112740877449006
Epoch: 214/300
Train loss: 1.8721
0.8110215408125329
Epoch: 215/300
Train loss: 1.8134
0.8110202472203952
Epoch: 216/300
Train loss: 1.8797
0.8111138217226327
Epoch: 217/300
Train loss: 1.7782
0.8110529387137385
Epoch: 218/300
Train loss: 1.9467
0.8112022852533459
Epoch: 219/300
Train loss: 1.9125
0.8111276879274902
Epoch: 220/300
Train loss: 1.8771
0.8117552431044524
Epoch: 221/300
Train loss: 1.8702
0.8119653998724499
Epoch: 222/300
Train loss: 1.8451
0.8109192831799503
Epoch: 223/300
Train loss: 1.9186
0.8108367316123064
Epoch: 224/300
Train loss: 1.8591
0.811188595454277
Epoch: 225/300
Train loss: 1.9509
0.8114235888529662
Epoch: 226/300
Train loss: 1.9064
0.8108533910444956
Epoch: 227/300
Train loss: 1.8729
0.8111171326316459
Epoch: 228/300
Train loss: 1.9116
0.8116106317240696
Epoch: 229/300
Train loss: 1.7849
0.8113522881547952
Epoch: 230/300
Train loss: 1.8757
0.8099980500228325
Epoch: 231/300
Train loss: 1.8302
0.8115309879463678
Epoch: 232/300
Train loss: 1.8148
0.8113630626589357
Epoch: 233/300
Train loss: 1.7853
0.8120149147288939
Epoch: 234/300
Train loss: 1.7935
0.812422067003935
Epoch: 235/300
Train loss: 1.8093
0.8119920288234156
Epoch: 236/300
Train loss: 1.8871
0.8117719677463029
Epoch: 237/300
Train loss: 1.9373
0.8109546003513425
Epoch: 238/300
Train loss: 1.8426
0.8126175982825157
Epoch: 239/300
Train loss: 1.7539
0.8116760367221584
Epoch: 240/300
Train loss: 1.8019
0.8121492551181162
Epoch: 241/300
Train loss: 1.8192
0.812869577661063
Epoch: 242/300
Train loss: 1.8917
0.8118010520010194
Epoch: 243/300
Train loss: 1.8418
0.8125196020905082
Epoch: 244/300
Train loss: 1.9063
0.8122258998810173
Epoch: 245/300
Train loss: 2.0017
0.8108098731940142
Epoch: 246/300
Date :05/22/2023, 01:27:37
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 0
model_name: tf_efficientnetv2_s
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.3
drop_path_rate: 0.45
76407
Fold: 1
13553
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 35.0366
Epoch: 2/200
Train loss: 5.7915
Epoch: 3/200
Train loss: 5.3339
Epoch: 4/200
Train loss: 4.9958
Epoch: 5/200
Train loss: 4.6650
Epoch: 6/200
Train loss: 4.3930
Epoch: 7/200
Train loss: 4.0970
Epoch: 8/200
Train loss: 3.9953
Epoch: 9/200
Train loss: 3.7960
Epoch: 10/200
Train loss: 3.7263
Epoch: 11/200
Train loss: 3.6224
Epoch: 12/200
Train loss: 3.5204
Epoch: 13/200
Train loss: 3.4428
Epoch: 14/200
Train loss: 3.3426
Epoch: 15/200
Train loss: 3.3191
Epoch: 16/200
Train loss: 3.2894
Epoch: 17/200
Train loss: 3.1352
Epoch: 18/200
Train loss: 3.0921
Epoch: 19/200
Train loss: 3.1981
Epoch: 20/200
Train loss: 3.0809
Epoch: 21/200
Train loss: 3.0386
Epoch: 22/200
Train loss: 2.8829
Epoch: 23/200
Train loss: 2.8676
Epoch: 24/200
Train loss: 2.8634
Epoch: 25/200
Train loss: 3.0231
Epoch: 26/200
Train loss: 2.7700
Epoch: 27/200
Train loss: 2.8616
Epoch: 28/200
Train loss: 2.8094
Epoch: 29/200
Train loss: 2.6982
Epoch: 30/200
Train loss: 2.7248
Epoch: 31/200
Train loss: 2.6029
Epoch: 32/200
Train loss: 2.7635
Epoch: 33/200
Train loss: 2.7322
Epoch: 34/200
Train loss: 2.5142
Epoch: 35/200
Train loss: 2.7012
Epoch: 36/200
Train loss: 2.6078
Epoch: 37/200
Train loss: 2.6613
Epoch: 38/200
Train loss: 2.5539
Epoch: 39/200
Train loss: 2.7512
Epoch: 40/200
Train loss: 2.4769
Epoch: 41/200
Train loss: 2.5829
Epoch: 42/200
Train loss: 2.5805
Epoch: 43/200
Train loss: 2.4632
Epoch: 44/200
Train loss: 2.6224
Epoch: 45/200
Train loss: 2.5650
Epoch: 46/200
Train loss: 2.5740
Epoch: 47/200
Train loss: 2.4458
Epoch: 48/200
Train loss: 2.4972
Epoch: 49/200
Train loss: 2.3920
Epoch: 50/200
Train loss: 2.2384
Epoch: 51/200
Train loss: 2.3934
Epoch: 52/200
Train loss: 2.3315
Epoch: 53/200
Train loss: 2.3648
Epoch: 54/200
Train loss: 2.2708
Epoch: 55/200
Train loss: 2.4681
Epoch: 56/200
Train loss: 2.3633
Epoch: 57/200
Train loss: 2.2182
Epoch: 58/200
Train loss: 2.4528
Epoch: 59/200
Train loss: 2.2785
Epoch: 60/200
Train loss: 2.4484
Epoch: 61/200
Train loss: 2.3188
Epoch: 62/200
Train loss: 2.3402
Epoch: 63/200
Train loss: 2.2893
Epoch: 64/200
Train loss: 2.2191
Epoch: 65/200
Train loss: 2.3369
Epoch: 66/200
Train loss: 2.2636
Epoch: 67/200
Train loss: 2.3767
Epoch: 68/200
Train loss: 2.1771
Epoch: 69/200
Train loss: 2.3476
Epoch: 70/200
Train loss: 2.2900
Epoch: 71/200
Train loss: 2.4123
Epoch: 72/200
Train loss: 2.2742
Epoch: 73/200
Train loss: 2.1144
Epoch: 74/200
Train loss: 2.1528
Epoch: 75/200
Train loss: 2.2421
Epoch: 76/200
Train loss: 2.2795
Epoch: 77/200
Train loss: 2.3568
Epoch: 78/200
Train loss: 2.1549
Epoch: 79/200
Train loss: 2.2483
Epoch: 80/200
Train loss: 2.2129
Epoch: 81/200
Train loss: 2.1149
Epoch: 82/200
Train loss: 2.0518
Epoch: 83/200
Train loss: 2.1961
Epoch: 84/200
Train loss: 2.1451
Epoch: 85/200
Train loss: 2.0919
Epoch: 86/200
Train loss: 2.0840
Epoch: 87/200
Train loss: 2.2148
Epoch: 88/200
Train loss: 2.2602
Epoch: 89/200
Train loss: 1.9903
Epoch: 90/200
Train loss: 2.2030
Epoch: 91/200
Train loss: 2.2081
Epoch: 92/200
Train loss: 2.2716
Epoch: 93/200
Train loss: 2.1517
Epoch: 94/200
Train loss: 2.1484
Epoch: 95/200
Train loss: 2.2710
Epoch: 96/200
Train loss: 2.1647
Epoch: 97/200
Train loss: 2.1117
Epoch: 98/200
Train loss: 2.2092
Epoch: 99/200
Train loss: 2.2781
Epoch: 100/200
Train loss: 2.1405
Epoch: 101/200
Train loss: 2.1342
Epoch: 102/200
Train loss: 2.0715
0.8086911372803334
Model improve: 0.000000 -> 0.808691
Epoch: 103/200
Train loss: 2.1994
0.8082923800591559
Epoch: 104/200
Train loss: 2.1510
0.8071732843897145
Epoch: 105/200
Train loss: 1.9847
0.8097371036209627
Model improve: 0.808691 -> 0.809737
Epoch: 106/200
Train loss: 2.0193
0.809638760244134
Epoch: 107/200
Train loss: 2.0514
0.8116145121967454
Model improve: 0.809737 -> 0.811615
Epoch: 108/200
Train loss: 2.0766
0.8104900157235717
Epoch: 109/200
Train loss: 2.0818
0.8118652824507221
Model improve: 0.811615 -> 0.811865
Epoch: 110/200
Train loss: 2.1392
0.8106440559300454
Epoch: 111/200
Train loss: 2.1250
0.8106388338520893
Epoch: 112/200
Train loss: 2.1353
0.810901978193002
Epoch: 113/200
Train loss: 2.1721
0.8085316345473033
Epoch: 114/200
Train loss: 2.0028
0.8111751777141875
Epoch: 115/200
Train loss: 1.9877
0.8113330434294976
Epoch: 116/200
Train loss: 2.1083
0.8114289041385366
Epoch: 117/200
Train loss: 2.1524
0.809586119954219
Epoch: 118/200
Train loss: 2.0784
0.8103280990196555
Epoch: 119/200
Train loss: 2.1054
0.809200918622646
Epoch: 120/200
Train loss: 2.0958
0.8100272775192744
Epoch: 121/200
Train loss: 2.0027
0.8105793902851394
Epoch: 122/200
Train loss: 2.0762
0.8108134013322699
Epoch: 123/200
Train loss: 2.0695
0.8105673592355406
Epoch: 124/200
Train loss: 2.0210
0.811173780917609
Epoch: 125/200
Train loss: 2.0873
0.8092771450093539
Epoch: 126/200
Train loss: 2.1714
0.8091611513361631
Epoch: 127/200
Train loss: 2.2752
0.8114525787787547
Epoch: 128/200
Train loss: 2.1367
0.8108179710594282
Epoch: 129/200
Train loss: 2.0408
0.8114429689763387
Epoch: 130/200
Train loss: 2.1246
0.8123949558425634
Model improve: 0.811865 -> 0.812395
Epoch: 131/200
Train loss: 2.0040
0.8118009803870444
Epoch: 132/200
Train loss: 2.0253
0.8127627978005384
Model improve: 0.812395 -> 0.812763
Epoch: 133/200
Train loss: 1.8758
0.8132375600032513
Model improve: 0.812763 -> 0.813238
Epoch: 134/200
Train loss: 2.0495
0.8105706090509711
Epoch: 135/200
Train loss: 1.9908
0.8118654525939513
Epoch: 136/200
Train loss: 2.1376
0.8118743803171097
Epoch: 137/200
Train loss: 1.9789
0.8113713650356278
Epoch: 138/200
Train loss: 2.0081
0.8125152393188233
Epoch: 139/200
Train loss: 1.9926
0.8105985562341987
Epoch: 140/200
Train loss: 1.9799
0.811519515310248
Epoch: 141/200
Train loss: 2.0320
0.8116717907317359
Epoch: 142/200
Train loss: 1.8974
0.8115322449804573
Epoch: 143/200
Train loss: 2.1177
0.810912427915946
Epoch: 144/200
Train loss: 1.9962
0.8129875977285401
Epoch: 145/200
Train loss: 2.0109
0.8123455869273079
Epoch: 146/200
Train loss: 1.9476
0.8119103540816851
Epoch: 147/200
Train loss: 2.1144
0.8117687665470091
Epoch: 148/200
Train loss: 2.0924
0.8125165494218821
Epoch: 149/200
Train loss: 1.9288
0.8116245443003907
Epoch: 150/200
Train loss: 2.0567
0.8126758991395338
Epoch: 151/200
Train loss: 2.0718
0.8130554812568621
Epoch: 152/200
Train loss: 2.1450
0.8115328414969177
Epoch: 153/200
Train loss: 1.9649
0.8121690726665752
Epoch: 154/200
Train loss: 1.8326
0.8126601770589004
Epoch: 155/200
Train loss: 2.0274
0.8122572200325658
Epoch: 156/200
Train loss: 1.8832
0.8125776891974742
Epoch: 157/200
Train loss: 1.9092
0.8118799790595488
Epoch: 158/200
Train loss: 2.0683
0.8125346349120212
Epoch: 159/200
Train loss: 1.9440
0.8110374208406662
Epoch: 160/200
Train loss: 2.0324
0.811900689330195
Epoch: 161/200
Train loss: 2.0544
0.8111846041686172
Epoch: 162/200
Train loss: 1.8971
0.8116482428930077
Epoch: 163/200
Train loss: 1.8808
0.8118915694792798
Epoch: 164/200
Train loss: 2.0503
0.8124053943914473
Epoch: 165/200
Train loss: 2.0119
0.8121709163605176
Epoch: 166/200
Train loss: 1.9922
0.8119949402492224
Epoch: 167/200
Train loss: 2.0220
0.8115274053759922
Epoch: 168/200
Train loss: 1.9431
0.8131974867112542
Epoch: 169/200
Train loss: 2.1751
0.8117989492612637
Epoch: 170/200
Train loss: 2.0053
0.8122033922250484
Epoch: 171/200
Train loss: 2.0917
0.8133068630040184
Model improve: 0.813238 -> 0.813307
Epoch: 172/200
Train loss: 2.1316
0.8117008571604747
Epoch: 173/200
Train loss: 1.9938
0.8120403018154322
Epoch: 174/200
Train loss: 2.0301
0.8126571304272957
Epoch: 175/200
Train loss: 1.9775
0.8124308259504591
Epoch: 176/200
Train loss: 2.1029
0.8126581554634933
Epoch: 177/200
Train loss: 1.9500
0.8120418868805382
Epoch: 178/200
Train loss: 2.0390
0.8125535776785793
Epoch: 179/200
Train loss: 1.9337
0.8113024862791214
Epoch: 180/200
Train loss: 2.0593
0.8109457754844005
Epoch: 181/200
Train loss: 1.8606
0.8109473311379883
Epoch: 182/200
Train loss: 1.9281
0.8109464405172612
Epoch: 183/200
Train loss: 1.9853
0.8114392650561555
Epoch: 184/200
Train loss: 1.9235
0.8115528656059949
Epoch: 185/200
Train loss: 2.0508
0.8123232687233916
Epoch: 186/200
Train loss: 2.0214
0.8120996124858847
Epoch: 187/200
Train loss: 1.8462
0.8119328538760963
Epoch: 188/200
Train loss: 1.9513
0.8118978374017651
Epoch: 189/200
Train loss: 2.0143
0.8124255371156945
Epoch: 190/200
Train loss: 2.0827
0.8122964770933133
Epoch: 191/200
Train loss: 2.0612
Date :05/22/2023, 09:42:40
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 0
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.3
drop_path_rate: 0.45
76407
Fold: 1
13553
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Date :05/22/2023, 09:43:01
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 0
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.25
drop_path_rate: 0.4
76407
Fold: 1
13553
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 42.9437
Epoch: 2/200
Train loss: 5.6693
Epoch: 3/200
Train loss: 5.1966
Epoch: 4/200
Train loss: 4.9224
Epoch: 5/200
Train loss: 4.6284
Epoch: 6/200
Train loss: 4.3947
Epoch: 7/200
Train loss: 4.1519
Epoch: 8/200
Train loss: 4.0761
Epoch: 9/200
Train loss: 3.8681
Epoch: 10/200
Train loss: 3.8108
Epoch: 11/200
Train loss: 3.7426
Epoch: 12/200
Train loss: 3.6463
Epoch: 13/200
Train loss: 3.5763
Epoch: 14/200
Train loss: 3.4989
Epoch: 15/200
Train loss: 3.4633
Epoch: 16/200
Train loss: 3.4653
Epoch: 17/200
Train loss: 3.3073
Epoch: 18/200
Train loss: 3.2520
Epoch: 19/200
Train loss: 3.3748
Epoch: 20/200
Train loss: 3.2559
Epoch: 21/200
Train loss: 3.2209
Epoch: 22/200
Train loss: 3.0480
Epoch: 23/200
Train loss: 3.0381
Epoch: 24/200
Train loss: 3.0688
Epoch: 25/200
Train loss: 3.2013
Epoch: 26/200
Train loss: 2.9437
Epoch: 27/200
Train loss: 3.0535
Epoch: 28/200
Train loss: 2.9920
Epoch: 29/200
Train loss: 2.9028
Epoch: 30/200
Train loss: 2.8910
Epoch: 31/200
Train loss: 2.7796
Epoch: 32/200
Train loss: 2.9461
Epoch: 33/200
Train loss: 2.9225
Epoch: 34/200
Train loss: 2.7120
Epoch: 35/200
Train loss: 2.8819
Epoch: 36/200
Train loss: 2.8090
Epoch: 37/200
Train loss: 2.8654
Epoch: 38/200
Train loss: 2.7431
Epoch: 39/200
Train loss: 2.9496
Epoch: 40/200
Train loss: 2.6484
Epoch: 41/200
Train loss: 2.7907
Epoch: 42/200
Train loss: 2.7757
Epoch: 43/200
Train loss: 2.6515
Epoch: 44/200
Train loss: 2.7955
Epoch: 45/200
Train loss: 2.7665
Epoch: 46/200
Train loss: 2.7586
Epoch: 47/200
Train loss: 2.6243
Epoch: 48/200
Train loss: 2.6895
Epoch: 49/200
Train loss: 2.5691
Epoch: 50/200
Train loss: 2.4131
Epoch: 51/200
Train loss: 2.5725
Epoch: 52/200
Train loss: 2.5180
Epoch: 53/200
Train loss: 2.5619
Epoch: 54/200
Train loss: 2.4545
Epoch: 55/200
Train loss: 2.6595
Epoch: 56/200
Train loss: 2.5683
Epoch: 57/200
Train loss: 2.4012
Epoch: 58/200
Train loss: 2.6480
Epoch: 59/200
Train loss: 2.4620
Epoch: 60/200
Train loss: 2.6294
Epoch: 61/200
Train loss: 2.5230
Epoch: 62/200
Train loss: 2.5078
Epoch: 63/200
Train loss: 2.4717
Epoch: 64/200
Train loss: 2.4063
Epoch: 65/200
Train loss: 2.5224
Epoch: 66/200
Train loss: 2.4858
Epoch: 67/200
Train loss: 2.5572
Epoch: 68/200
Train loss: 2.3531
Epoch: 69/200
Train loss: 2.5281
Epoch: 70/200
Train loss: 2.4935
Epoch: 71/200
Train loss: 2.6089
Epoch: 72/200
Train loss: 2.4736
Epoch: 73/200
Train loss: 2.3071
Epoch: 74/200
Train loss: 2.3501
Epoch: 75/200
Train loss: 2.4486
Epoch: 76/200
Train loss: 2.4676
Epoch: 77/200
Train loss: 2.5411
Epoch: 78/200
Train loss: 2.3281
Epoch: 79/200
Train loss: 2.4417
Epoch: 80/200
Train loss: 2.3907
Epoch: 81/200
Train loss: 2.2520
Epoch: 82/200
Train loss: 2.2501
Epoch: 83/200
Train loss: 2.3918
Epoch: 84/200
Train loss: 2.3291
Epoch: 85/200
Train loss: 2.2877
Epoch: 86/200
Train loss: 2.2750
Epoch: 87/200
Train loss: 2.4018
Epoch: 88/200
Train loss: 2.4719
Epoch: 89/200
Train loss: 2.1755
Epoch: 90/200
Train loss: 2.3920
Epoch: 91/200
Train loss: 2.3901
Epoch: 92/200
Train loss: 2.4650
Epoch: 93/200
Train loss: 2.3489
Epoch: 94/200
Train loss: 2.3155
Epoch: 95/200
Train loss: 2.4650
Epoch: 96/200
Train loss: 2.3602
Epoch: 97/200
Train loss: 2.2925
Epoch: 98/200
Train loss: 2.3798
Epoch: 99/200
Train loss: 2.4859
Epoch: 100/200
Train loss: 2.3176
Epoch: 101/200
Train loss: 2.2973
Epoch: 102/200
Train loss: 2.2611
0.8111758843343256
Model improve: 0.000000 -> 0.811176
Epoch: 103/200
Train loss: 2.3892
0.8122326240375821
Model improve: 0.811176 -> 0.812233
Epoch: 104/200
Train loss: 2.3455
0.8108467632465362
Epoch: 105/200
Train loss: 2.1344
0.8125291161278212
Model improve: 0.812233 -> 0.812529
Epoch: 106/200
Train loss: 2.1848
0.812275584475028
Epoch: 107/200
Train loss: 2.2309
0.8118082675658036
Epoch: 108/200
Train loss: 2.2746
0.8129901797837533
Model improve: 0.812529 -> 0.812990
Epoch: 109/200
Train loss: 2.2776
0.8126354790963282
Epoch: 110/200
Train loss: 2.3340
0.811905754045387
Epoch: 111/200
Train loss: 2.3171
0.8122308305001803
Epoch: 112/200
Train loss: 2.3162
0.8124177939591751
Epoch: 113/200
Train loss: 2.3577
0.8107360747885302
Epoch: 114/200
Train loss: 2.1915
0.8132821038378869
Model improve: 0.812990 -> 0.813282
Epoch: 115/200
Train loss: 2.1578
0.8125174951289792
Epoch: 116/200
Train loss: 2.2924
0.813317169551193
Model improve: 0.813282 -> 0.813317
Epoch: 117/200
Train loss: 2.3383
0.8108461908060132
Epoch: 118/200
Train loss: 2.2806
0.8123863380281953
Epoch: 119/200
Train loss: 2.2838
0.8130677298984561
Epoch: 120/200
Train loss: 2.2735
0.8125116709168207
Epoch: 121/200
Train loss: 2.1754
0.8122458683429487
Epoch: 122/200
Train loss: 2.2514
0.8126155691518033
Epoch: 123/200
Train loss: 2.2645
0.8137135877439156
Model improve: 0.813317 -> 0.813714
Epoch: 124/200
Train loss: 2.2028
0.8127059450587726
Epoch: 125/200
Train loss: 2.2756
0.8132823086944813
Epoch: 126/200
Train loss: 2.3592
0.8121260200840721
Epoch: 127/200
Train loss: 2.4720
0.8111473805169764
Epoch: 128/200
Train loss: 2.3146
0.811811096356576
Epoch: 129/200
Train loss: 2.2316
0.813889118574798
Model improve: 0.813714 -> 0.813889
Epoch: 130/200
Train loss: 2.3098
0.8134709763319006
Epoch: 131/200
Train loss: 2.1878
0.8129233888842712
Epoch: 132/200
Train loss: 2.2098
0.8128007984605843
Epoch: 133/200
Train loss: 2.0639
0.8136958993357605
Epoch: 134/200
Train loss: 2.2414
0.8119539695600926
Epoch: 135/200
Train loss: 2.1783
0.8126605579325333
Epoch: 136/200
Train loss: 2.3398
0.8131710794778477
Epoch: 137/200
Train loss: 2.1875
Date :05/22/2023, 13:17:25
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 5
model_name: tf_efficientnetv2_b2
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.3
drop_path_rate: 0.45
76407
Fold: 1
13615
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 42.9934
Epoch: 2/200
Train loss: 5.6993
Epoch: 3/200
Train loss: 5.2011
Epoch: 4/200
Train loss: 4.9431
Epoch: 5/200
Train loss: 4.6436
Epoch: 6/200
Train loss: 4.3995
Epoch: 7/200
Train loss: 4.1257
Epoch: 8/200
Train loss: 4.1197
Epoch: 9/200
Train loss: 3.9042
Epoch: 10/200
Train loss: 3.8343
Epoch: 11/200
Train loss: 3.7723
Epoch: 12/200
Train loss: 3.6493
Epoch: 13/200
Train loss: 3.5934
Epoch: 14/200
Train loss: 3.4708
Epoch: 15/200
Train loss: 3.4595
Epoch: 16/200
Train loss: 3.4296
Epoch: 17/200
Train loss: 3.3154
Epoch: 18/200
Train loss: 3.2193
Epoch: 19/200
Train loss: 3.4084
Epoch: 20/200
Train loss: 3.2171
Epoch: 21/200
Train loss: 3.2468
Epoch: 22/200
Train loss: 2.9982
Epoch: 23/200
Train loss: 3.0034
Epoch: 24/200
Train loss: 3.0952
Epoch: 25/200
Train loss: 3.2441
Epoch: 26/200
Train loss: 2.9307
Epoch: 27/200
Train loss: 3.0564
Epoch: 28/200
Train loss: 2.9362
Epoch: 29/200
Train loss: 2.8608
Epoch: 30/200
Train loss: 2.8719
Epoch: 31/200
Train loss: 2.7534
Epoch: 32/200
Train loss: 2.9737
Epoch: 33/200
Train loss: 2.8425
Epoch: 34/200
Train loss: 2.7392
Epoch: 35/200
Train loss: 2.8720
Epoch: 36/200
Train loss: 2.8697
Epoch: 37/200
Train loss: 2.7581
Epoch: 38/200
Train loss: 2.7390
Epoch: 39/200
Train loss: 2.9758
Epoch: 40/200
Train loss: 2.6214
Epoch: 41/200
Train loss: 2.7660
Epoch: 42/200
Train loss: 2.6974
Epoch: 43/200
Train loss: 2.6813
Epoch: 44/200
Train loss: 2.8017
Epoch: 45/200
Train loss: 2.7275
Epoch: 46/200
Train loss: 2.7552
Epoch: 47/200
Train loss: 2.5483
Epoch: 48/200
Train loss: 2.6219
Epoch: 49/200
Train loss: 2.5089
Epoch: 50/200
Train loss: 2.4453
Epoch: 51/200
Train loss: 2.5363
Epoch: 52/200
Train loss: 2.5670
Epoch: 53/200
Train loss: 2.5398
Epoch: 54/200
Train loss: 2.4744
Epoch: 55/200
Train loss: 2.5664
Epoch: 56/200
Train loss: 2.5691
Epoch: 57/200
Train loss: 2.4670
Epoch: 58/200
Train loss: 2.5928
Epoch: 59/200
Train loss: 2.4854
Epoch: 60/200
Train loss: 2.5219
Epoch: 61/200
Train loss: 2.5292
Epoch: 62/200
Train loss: 2.4944
Epoch: 63/200
Train loss: 2.3921
Epoch: 64/200
Train loss: 2.4911
Epoch: 65/200
Train loss: 2.5145
Epoch: 66/200
Train loss: 2.4814
Epoch: 67/200
Train loss: 2.4780
Epoch: 68/200
Train loss: 2.4289
Epoch: 69/200
Train loss: 2.4756
Epoch: 70/200
Train loss: 2.5722
Epoch: 71/200
Train loss: 2.4786
Epoch: 72/200
Train loss: 2.3170
Epoch: 73/200
Train loss: 2.3751
Epoch: 74/200
Train loss: 2.3575
Epoch: 75/200
Train loss: 2.4745
Epoch: 76/200
Train loss: 2.4968
Epoch: 77/200
Train loss: 2.3529
Epoch: 78/200
Train loss: 2.4035
Epoch: 79/200
Train loss: 2.4120
Epoch: 80/200
Train loss: 2.3078
Epoch: 81/200
Train loss: 2.1987
Epoch: 82/200
Train loss: 2.3561
Epoch: 83/200
Train loss: 2.3505
Epoch: 84/200
Train loss: 2.2498
Epoch: 85/200
Train loss: 2.2199
Epoch: 86/200
Train loss: 2.3722
Epoch: 87/200
Train loss: 2.4140
Epoch: 88/200
Train loss: 2.2879
Epoch: 89/200
Train loss: 2.2502
Epoch: 90/200
Train loss: 2.4229
Epoch: 91/200
Train loss: 2.4508
Epoch: 92/200
Train loss: 2.3568
Epoch: 93/200
Train loss: 2.3112
Epoch: 94/200
Train loss: 2.4339
Epoch: 95/200
Train loss: 2.3337
Epoch: 96/200
Train loss: 2.2908
Epoch: 97/200
Train loss: 2.3672
Epoch: 98/200
Train loss: 2.4330
Epoch: 99/200
Train loss: 2.3127
Epoch: 100/200
Train loss: 2.3271
Epoch: 101/200
Train loss: 2.2053
Epoch: 102/200
Train loss: 2.3906
0.8133956151258791
Model improve: 0.000000 -> 0.813396
Epoch: 103/200
Train loss: 2.3181
0.8139452316313649
Model improve: 0.813396 -> 0.813945
Epoch: 104/200
Train loss: 2.1511
0.8134345587151366
Epoch: 105/200
Train loss: 2.1820
0.8133314997099356
Epoch: 106/200
Train loss: 2.2055
0.8152337856320983
Model improve: 0.813945 -> 0.815234
Epoch: 107/200
Train loss: 2.2488
0.8151059793589381
Epoch: 108/200
Train loss: 2.2587
0.8140849118704003
Epoch: 109/200
Train loss: 2.3308
0.8135913681596681
Epoch: 110/200
Train loss: 2.2942
0.8126082648437714
Epoch: 111/200
Train loss: 2.2782
0.8153865240819559
Model improve: 0.815234 -> 0.815387
Epoch: 112/200
Train loss: 2.3291
0.813167465661489
Epoch: 113/200
Train loss: 2.1787
0.8137470841975878
Epoch: 114/200
Train loss: 2.1656
0.813634291000814
Epoch: 115/200
Train loss: 2.2574
0.8142498883036913
Epoch: 116/200
Train loss: 2.3183
0.8129620175695872
Epoch: 117/200
Train loss: 2.2498
0.8140123671197977
Epoch: 118/200
Train loss: 2.2705
0.8139272137673604
Epoch: 119/200
Train loss: 2.2237
0.814909406667772
Epoch: 120/200
Train loss: 2.1579
0.8150056699356425
Epoch: 121/200
Train loss: 2.2593
0.8147622434648295
Epoch: 122/200
Train loss: 2.2447
0.8152440118728761
Epoch: 123/200
Train loss: 2.2177
0.8153293359038978
Epoch: 124/200
Train loss: 2.2729
0.8149893612043305
Epoch: 125/200
Train loss: 2.3260
0.8158910194024193
Model improve: 0.815387 -> 0.815891
Epoch: 126/200
Train loss: 2.4770
0.81462961789767
Epoch: 127/200
Train loss: 2.3329
0.8150987542596916
Epoch: 128/200
Train loss: 2.1703
0.8160989478438356
Model improve: 0.815891 -> 0.816099
Epoch: 129/200
Train loss: 2.2645
0.815979905468302
Epoch: 130/200
Train loss: 2.1083
0.8169338620975538
Model improve: 0.816099 -> 0.816934
Epoch: 131/200
Train loss: 2.2251
0.8158089272190934
Epoch: 132/200
Train loss: 2.0293
0.8165214544051643
Epoch: 133/200
Train loss: 2.2143
0.8181558132748206
Model improve: 0.816934 -> 0.818156
Epoch: 134/200
Train loss: 2.2976
0.8144155779278206
Epoch: 135/200
Train loss: 2.1781
0.8169081443073097
Epoch: 136/200
Train loss: 2.1562
0.8179169366686141
Epoch: 137/200
Train loss: 2.2099
0.8171490293916728
Epoch: 138/200
Train loss: 2.0688
0.8172588284111121
Epoch: 139/200
Train loss: 2.1864
0.8178319699042049
Epoch: 140/200
Train loss: 2.2011
0.8178657549426415
Epoch: 141/200
Train loss: 2.0901
0.8162370922838704
Epoch: 142/200
Train loss: 2.2165
0.8171381055405968
Epoch: 143/200
Train loss: 2.1875
0.816446976814771
Epoch: 144/200
Train loss: 2.1260
0.817890874966055
Epoch: 145/200
Train loss: 2.1862
0.8152452461255083
Epoch: 146/200
Train loss: 2.2224
0.8159423883606501
Epoch: 147/200
Train loss: 2.2064
0.8174060079075108
Epoch: 148/200
Train loss: 2.1092
0.8162407959652461
Epoch: 149/200
Train loss: 2.2302
0.8166073783975621
Epoch: 150/200
Train loss: 2.2875
0.816487194495289
Epoch: 151/200
Train loss: 2.2990
0.8167074936448031
Epoch: 152/200
Train loss: 2.0821
0.8179547170645799
Epoch: 153/200
Train loss: 2.0662
0.8171252393492973
Epoch: 154/200
Train loss: 2.1176
0.8182448301873686
Model improve: 0.818156 -> 0.818245
Epoch: 155/200
Train loss: 2.0901
0.818405892693424
Model improve: 0.818245 -> 0.818406
Epoch: 156/200
Train loss: 2.1117
0.8158391661271017
Epoch: 157/200
Train loss: 2.1485
0.8173448010125476
Epoch: 158/200
Train loss: 2.2546
0.8160590287322171
Epoch: 159/200
Train loss: 2.1780
0.8166708834852777
Epoch: 160/200
Train loss: 2.1507
0.8167946775282119
Epoch: 161/200
Train loss: 2.0136
0.8174520740936657
Epoch: 162/200
Train loss: 2.2187
0.8164723145431215
Epoch: 163/200
Train loss: 2.0417
0.8182094157899377
Epoch: 164/200
Train loss: 2.2929
0.8170516420694891
Epoch: 165/200
Train loss: 2.1078
0.8167963773791364
Epoch: 166/200
Train loss: 2.1508
0.815784226066129
Epoch: 167/200
Train loss: 2.2373
0.8160899974045946
Epoch: 168/200
Train loss: 2.2514
0.8169128218345053
Epoch: 169/200
Train loss: 2.3044
0.8174402337733675
Epoch: 170/200
Train loss: 2.2567
0.8153742394318223
Epoch: 171/200
Train loss: 2.2228
0.8171809638357261
Epoch: 172/200
Train loss: 2.2047
0.817142753682566
Epoch: 173/200
Train loss: 2.1407
0.8167757472871991
Epoch: 174/200
Train loss: 2.2798
0.8156600385795193
Epoch: 175/200
Train loss: 2.1074
0.8171272511391485
Epoch: 176/200
Train loss: 2.2282
0.8164647580205048
Epoch: 177/200
Train loss: 2.0922
0.8171756437126116
Epoch: 178/200
Train loss: 2.1261
0.8173745803120611
Epoch: 179/200
Train loss: 2.2500
0.8171092727808185
Epoch: 180/200
Train loss: 1.9759
0.8174067594826827
Epoch: 181/200
Train loss: 2.1186
0.817025281830269
Epoch: 182/200
Train loss: 2.0437
0.8189095609407347
Model improve: 0.818406 -> 0.818910
Epoch: 183/200
Train loss: 2.2115
0.8163699993944666
Epoch: 184/200
Train loss: 2.1805
0.8177337847844576
Epoch: 185/200
Train loss: 2.1357
0.8175562627689562
Epoch: 186/200
Train loss: 2.0624
0.816711803341369
Epoch: 187/200
Train loss: 2.1531
0.8171939600537013
Epoch: 188/200
Train loss: 2.2081
0.817787931102259
Epoch: 189/200
Train loss: 2.2396
0.8169871765406901
Epoch: 190/200
Train loss: 2.1831
0.8177301724157726
Epoch: 191/200
Train loss: 2.1516
0.8167920360055718
Epoch: 192/200
Train loss: 2.1852
0.8163031303657595
Epoch: 193/200
Train loss: 2.2925
0.817351737991931
Epoch: 194/200
Train loss: 2.1465
0.8176643717947252
Epoch: 195/200
Train loss: 2.2227
0.8178960015898471
Epoch: 196/200
Train loss: 2.2489
0.8165946485634955
Epoch: 197/200
Train loss: 2.2368
0.8156559234120856
Epoch: 198/200
Train loss: 2.1726
0.8176932704095022
Epoch: 199/200
Train loss: 2.2885
0.816637704561624
Epoch: 200/200
Train loss: 2.1207
0.8163202262346525
Date :05/22/2023, 20:38:57
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 5
model_name: tf_efficientnetv2_b2
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.3
drop_path_rate: 0.45
76407
Fold: 4
13611
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 43.1710
Epoch: 2/200
Train loss: 5.7055
Epoch: 3/200
Train loss: 5.2023
Epoch: 4/200
Train loss: 4.9467
Epoch: 5/200
Train loss: 4.6448
Epoch: 6/200
Train loss: 4.4038
Epoch: 7/200
Train loss: 4.1323
Epoch: 8/200
Train loss: 4.1072
Epoch: 9/200
Train loss: 3.8620
Epoch: 10/200
Train loss: 3.8309
Epoch: 11/200
Train loss: 3.7795
Epoch: 12/200
Train loss: 3.6536
Epoch: 13/200
Train loss: 3.5701
Epoch: 14/200
Train loss: 3.4542
Epoch: 15/200
Train loss: 3.4626
Epoch: 16/200
Train loss: 3.4265
Epoch: 17/200
Train loss: 3.3070
Epoch: 18/200
Train loss: 3.2085
Epoch: 19/200
Train loss: 3.3773
Epoch: 20/200
Train loss: 3.2081
Epoch: 21/200
Train loss: 3.2270
Epoch: 22/200
Train loss: 2.9683
Epoch: 23/200
Train loss: 2.9951
Epoch: 24/200
Train loss: 3.0905
Epoch: 25/200
Train loss: 3.2353
Epoch: 26/200
Train loss: 2.9061
Epoch: 27/200
Train loss: 3.0418
Epoch: 28/200
Train loss: 2.9327
Epoch: 29/200
Train loss: 2.8657
Epoch: 30/200
Train loss: 2.8707
Epoch: 31/200
Train loss: 2.7899
Epoch: 32/200
Train loss: 2.9622
Epoch: 33/200
Train loss: 2.8139
Epoch: 34/200
Train loss: 2.7243
Epoch: 35/200
Train loss: 2.8706
Epoch: 36/200
Train loss: 2.8703
Epoch: 37/200
Train loss: 2.7662
Epoch: 38/200
Train loss: 2.7409
Epoch: 39/200
Train loss: 2.9612
Epoch: 40/200
Train loss: 2.6180
Epoch: 41/200
Train loss: 2.7747
Epoch: 42/200
Train loss: 2.6776
Epoch: 43/200
Train loss: 2.6603
Epoch: 44/200
Train loss: 2.8090
Epoch: 45/200
Train loss: 2.7413
Epoch: 46/200
Train loss: 2.7477
Epoch: 47/200
Train loss: 2.5192
Epoch: 48/200
Train loss: 2.6097
Epoch: 49/200
Train loss: 2.5091
Epoch: 50/200
Train loss: 2.4212
Epoch: 51/200
Train loss: 2.5550
Epoch: 52/200
Train loss: 2.5566
Epoch: 53/200
Train loss: 2.5299
Epoch: 54/200
Train loss: 2.4717
Epoch: 55/200
Train loss: 2.5491
Epoch: 56/200
Train loss: 2.5623
Epoch: 57/200
Train loss: 2.4764
Epoch: 58/200
Train loss: 2.5823
Epoch: 59/200
Train loss: 2.4863
Epoch: 60/200
Train loss: 2.5157
Epoch: 61/200
Train loss: 2.5151
Epoch: 62/200
Train loss: 2.4903
Epoch: 63/200
Train loss: 2.3819
Epoch: 64/200
Train loss: 2.4911
Epoch: 65/200
Train loss: 2.4969
Epoch: 66/200
Train loss: 2.4917
Epoch: 67/200
Train loss: 2.4709
Epoch: 68/200
Train loss: 2.4121
Epoch: 69/200
Train loss: 2.4698
Epoch: 70/200
Train loss: 2.5443
Epoch: 71/200
Train loss: 2.4860
Epoch: 72/200
Train loss: 2.3121
Epoch: 73/200
Train loss: 2.3756
Epoch: 74/200
Train loss: 2.3459
Epoch: 75/200
Train loss: 2.4730
Epoch: 76/200
Train loss: 2.4858
Epoch: 77/200
Train loss: 2.3403
Epoch: 78/200
Train loss: 2.4033
Epoch: 79/200
Train loss: 2.4104
Epoch: 80/200
Train loss: 2.2902
Epoch: 81/200
Train loss: 2.1771
Epoch: 82/200
Train loss: 2.3512
Epoch: 83/200
Train loss: 2.3476
Epoch: 84/200
Train loss: 2.2422
Epoch: 85/200
Train loss: 2.2296
Epoch: 86/200
Train loss: 2.3922
Epoch: 87/200
Train loss: 2.3979
Epoch: 88/200
Train loss: 2.2812
Epoch: 89/200
Train loss: 2.2282
Epoch: 90/200
Train loss: 2.4151
Epoch: 91/200
Train loss: 2.4412
Epoch: 92/200
Train loss: 2.3582
Epoch: 93/200
Train loss: 2.3199
Epoch: 94/200
Train loss: 2.4335
Epoch: 95/200
Train loss: 2.3165
Epoch: 96/200
Train loss: 2.2983
Epoch: 97/200
Train loss: 2.3700
Epoch: 98/200
Train loss: 2.4396
Epoch: 99/200
Train loss: 2.3086
Epoch: 100/200
Train loss: 2.3211
Epoch: 101/200
Train loss: 2.1733
Epoch: 102/200
Train loss: 2.3768
0.8045821304754253
Model improve: 0.000000 -> 0.804582
Epoch: 103/200
Train loss: 2.3059
0.8049232865243782
Model improve: 0.804582 -> 0.804923
Epoch: 104/200
Train loss: 2.1334
0.8041500162193886
Epoch: 105/200
Train loss: 2.1767
0.804420965852057
Epoch: 106/200
Train loss: 2.2063
0.8045185954442676
Epoch: 107/200
Train loss: 2.2413
0.8053412459055738
Model improve: 0.804923 -> 0.805341
Epoch: 108/200
Train loss: 2.2598
0.8047577386728906
Epoch: 109/200
Train loss: 2.3155
0.8031731746759815
Epoch: 110/200
Train loss: 2.2866
0.8038899762293483
Epoch: 111/200
Train loss: 2.2698
0.8023274405927697
Epoch: 112/200
Train loss: 2.3194
0.8033761637383143
Epoch: 113/200
Train loss: 2.1786
0.803554196411998
Epoch: 114/200
Train loss: 2.1429
0.8038947301835662
Epoch: 115/200
Train loss: 2.2658
0.805161460682367
Epoch: 116/200
Train loss: 2.3267
0.8058393596190412
Model improve: 0.805341 -> 0.805839
Epoch: 117/200
Train loss: 2.2440
0.8054873379612351
Epoch: 118/200
Train loss: 2.2473
0.8060506214219204
Model improve: 0.805839 -> 0.806051
Epoch: 119/200
Train loss: 2.2209
0.8047426052573254
Epoch: 120/200
Train loss: 2.1355
0.8066233297860805
Model improve: 0.806051 -> 0.806623
Epoch: 121/200
Train loss: 2.2556
0.805288700435497
Epoch: 122/200
Date :05/22/2023, 23:22:10
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 15
model_name: tf_efficientnetv2_b2
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.3
drop_path_rate: 0.45
76407
Fold: 4
14189
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 41.7168
Epoch: 2/200
Train loss: 5.7900
Epoch: 3/200
Train loss: 5.3260
Epoch: 4/200
Train loss: 5.0460
Epoch: 5/200
Train loss: 4.7555
Epoch: 6/200
Train loss: 4.4803
Epoch: 7/200
Train loss: 4.2525
Epoch: 8/200
Train loss: 4.1184
Epoch: 9/200
Train loss: 3.9876
Epoch: 10/200
Train loss: 3.8516
Epoch: 11/200
Train loss: 3.7644
Epoch: 12/200
Train loss: 3.7203
Epoch: 13/200
Train loss: 3.6160
Epoch: 14/200
Train loss: 3.4986
Epoch: 15/200
Train loss: 3.5091
Epoch: 16/200
Train loss: 3.3645
Epoch: 17/200
Train loss: 3.2672
Epoch: 18/200
Train loss: 3.3616
Epoch: 19/200
Train loss: 3.2952
Epoch: 20/200
Train loss: 3.2342
Epoch: 21/200
Train loss: 3.0911
Epoch: 22/200
Train loss: 3.0217
Epoch: 23/200
Train loss: 3.0921
Epoch: 24/200
Train loss: 3.2227
Epoch: 25/200
Train loss: 2.9310
Epoch: 26/200
Train loss: 3.0568
Epoch: 27/200
Train loss: 2.9658
Epoch: 28/200
Train loss: 2.8800
Epoch: 29/200
Train loss: 2.8525
Epoch: 30/200
Train loss: 2.8030
Epoch: 31/200
Train loss: 2.9729
Epoch: 32/200
Train loss: 2.7926
Epoch: 33/200
Train loss: 2.7668
Epoch: 34/200
Train loss: 2.8415
Epoch: 35/200
Train loss: 2.8965
Epoch: 36/200
Train loss: 2.7113
Epoch: 37/200
Train loss: 2.9033
Epoch: 38/200
Train loss: 2.7904
Epoch: 39/200
Train loss: 2.7112
Epoch: 40/200
Train loss: 2.7590
Epoch: 41/200
Train loss: 2.6576
Epoch: 42/200
Train loss: 2.7865
Epoch: 43/200
Train loss: 2.7365
Epoch: 44/200
Train loss: 2.7814
Epoch: 45/200
Train loss: 2.5717
Epoch: 46/200
Train loss: 2.7050
Epoch: 47/200
Train loss: 2.4715
Epoch: 48/200
Train loss: 2.4447
Epoch: 49/200
Train loss: 2.5483
Epoch: 50/200
Train loss: 2.5513
Epoch: 51/200
Train loss: 2.5631
Epoch: 52/200
Train loss: 2.4608
Epoch: 53/200
Train loss: 2.5607
Epoch: 54/200
Train loss: 2.5709
Epoch: 55/200
Train loss: 2.5269
Epoch: 56/200
Train loss: 2.5235
Epoch: 57/200
Train loss: 2.5268
Epoch: 58/200
Train loss: 2.5759
Epoch: 59/200
Train loss: 2.4842
Epoch: 60/200
Train loss: 2.4613
Epoch: 61/200
Train loss: 2.4477
Epoch: 62/200
Train loss: 2.4749
Epoch: 63/200
Train loss: 2.4756
Epoch: 64/200
Train loss: 2.5546
Epoch: 65/200
Train loss: 2.3697
Epoch: 66/200
Train loss: 2.5389
Epoch: 67/200
Train loss: 2.5030
Epoch: 68/200
Train loss: 2.5294
Epoch: 69/200
Train loss: 2.3955
Epoch: 70/200
Train loss: 2.3729
Epoch: 71/200
Train loss: 2.2568
Epoch: 72/200
Train loss: 2.5229
Epoch: 73/200
Train loss: 2.5002
Epoch: 74/200
Train loss: 2.3303
Epoch: 75/200
Train loss: 2.4212
Epoch: 76/200
Train loss: 2.4069
Epoch: 77/200
Train loss: 2.2565
Epoch: 78/200
Train loss: 2.2577
Epoch: 79/200
Train loss: 2.3528
Epoch: 80/200
Train loss: 2.3593
Epoch: 81/200
Train loss: 2.2609
Epoch: 82/200
Train loss: 2.2312
Epoch: 83/200
Train loss: 2.3826
Epoch: 84/200
Train loss: 2.4602
Epoch: 85/200
Train loss: 2.1895
Epoch: 86/200
Train loss: 2.3577
Epoch: 87/200
Train loss: 2.4206
Epoch: 88/200
Train loss: 2.4031
Epoch: 89/200
Train loss: 2.3409
Epoch: 90/200
Train loss: 2.3012
Epoch: 91/200
Train loss: 2.4368
Epoch: 92/200
Train loss: 2.2593
Epoch: 93/200
Train loss: 2.4653
Epoch: 94/200
Train loss: 2.3176
Epoch: 95/200
Train loss: 2.3958
Epoch: 96/200
Train loss: 2.3597
Epoch: 97/200
Train loss: 2.1843
Epoch: 98/200
Train loss: 2.3636
Epoch: 99/200
Train loss: 2.3057
Epoch: 100/200
Train loss: 2.1786
Epoch: 101/200
Train loss: 2.1485
Epoch: 102/200
Train loss: 2.2505
0.8077661175800785
Model improve: 0.000000 -> 0.807766
Epoch: 103/200
Train loss: 2.2544
0.807432994453499
Epoch: 104/200
Train loss: 2.2458
0.8085480200241134
Model improve: 0.807766 -> 0.808548
Epoch: 105/200
Train loss: 2.2925
0.8080983097265706
Epoch: 106/200
Train loss: 2.2919
0.8078185520274765
Epoch: 107/200
Train loss: 2.2619
0.8068398121140549
Epoch: 108/200
Train loss: 2.3365
0.807978843278171
Epoch: 109/200
Train loss: 2.1419
0.8080770037043985
Epoch: 110/200
Train loss: 2.2263
0.8069738255163769
Epoch: 111/200
Train loss: 2.2425
0.8074583263471903
Epoch: 112/200
Train loss: 2.3362
0.8061054453529161
Epoch: 113/200
Train loss: 2.2446
0.8079330803068019
Epoch: 114/200
Train loss: 2.2311
0.8097204854368226
Model improve: 0.808548 -> 0.809720
Epoch: 115/200
Train loss: 2.2493
0.8083444191741213
Epoch: 116/200
Train loss: 2.1954
0.8079978132326234
Epoch: 117/200
Train loss: 2.2640
0.8088045000767442
Epoch: 118/200
Train loss: 2.1815
0.8097725714289081
Model improve: 0.809720 -> 0.809773
Epoch: 119/200
Train loss: 2.2300
0.8073277213734532
Epoch: 120/200
Train loss: 2.2824
0.8087301308532648
Epoch: 121/200
Train loss: 2.4350
0.8088093373715124
Epoch: 122/200
Train loss: 2.3278
0.80765059602389
Epoch: 123/200
Train loss: 2.3048
0.8087897602202815
Epoch: 124/200
Train loss: 2.2622
0.8069776155023872
Epoch: 125/200
Train loss: 2.1535
0.8092883964168965
Epoch: 126/200
Train loss: 2.1863
0.8080383657276712
Epoch: 127/200
Train loss: 2.0524
0.8092020284278831
Epoch: 128/200
Train loss: 2.1936
0.8105396695964873
Model improve: 0.809773 -> 0.810540
Epoch: 129/200
Train loss: 2.2152
0.8073868999152765
Epoch: 130/200
Train loss: 2.2590
0.8082012087724796
Epoch: 131/200
Train loss: 2.1526
0.8105087326394805
Epoch: 132/200
Train loss: 2.2079
0.8099499233586903
Epoch: 133/200
Train loss: 2.0857
0.8108908201778453
Model improve: 0.810540 -> 0.810891
Epoch: 134/200
Train loss: 2.1957
0.810467963695837
Epoch: 135/200
Train loss: 2.1892
0.8109422198360285
Model improve: 0.810891 -> 0.810942
Epoch: 136/200
Train loss: 2.0903
0.8116439772491079
Model improve: 0.810942 -> 0.811644
Epoch: 137/200
Train loss: 2.2250
0.8111241305987026
Epoch: 138/200
Train loss: 2.2218
0.8115334308623148
Epoch: 139/200
Train loss: 2.1142
0.8104590938661212
Epoch: 140/200
Train loss: 2.2460
0.8100937509889784
Epoch: 141/200
Train loss: 2.2681
0.8090416934766227
Epoch: 142/200
Train loss: 2.0771
0.8124969826377068
Model improve: 0.811644 -> 0.812497
Epoch: 143/200
Train loss: 2.2612
0.8105728282039366
Epoch: 144/200
Train loss: 2.2240
0.8098937563463441
Epoch: 145/200
Train loss: 2.2900
0.8108243945833701
Epoch: 146/200
Train loss: 2.1916
0.8111122301285725
Epoch: 147/200
Train loss: 1.9793
0.8121113588299481
Epoch: 148/200
Train loss: 2.2013
0.8105220309580077
Epoch: 149/200
Train loss: 2.0331
0.8109604183782259
Epoch: 150/200
Train loss: 2.0908
0.8106742954379123
Epoch: 151/200
Train loss: 2.1766
0.8105246665968734
Epoch: 152/200
Train loss: 2.1394
0.8109486451157248
Epoch: 153/200
Train loss: 2.2041
0.8108772672302401
Epoch: 154/200
Train loss: 2.1764
0.8112652465696928
Epoch: 155/200
Train loss: 2.0847
0.810360642015254
Epoch: 156/200
Train loss: 2.1640
0.8102967063514854
Epoch: 157/200
Train loss: 2.0598
0.8121537826156966
Epoch: 158/200
Train loss: 2.2800
0.8109248652038067
Epoch: 159/200
Train loss: 2.0990
0.8106078798317589
Epoch: 160/200
Train loss: 2.1466
0.8093091381598347
Epoch: 161/200
Train loss: 2.2309
0.8100957136431497
Epoch: 162/200
Train loss: 2.2667
0.8089442495552537
Epoch: 163/200
Train loss: 2.2401
0.8109158378057276
Epoch: 164/200
Train loss: 2.2819
0.8109100740887107
Epoch: 165/200
Train loss: 2.1934
0.8107172528085181
Epoch: 166/200
Train loss: 2.2136
0.8109312026750383
Epoch: 167/200
Train loss: 2.1427
0.8089652854020613
Epoch: 168/200
Train loss: 2.2426
0.8098278432250069
Epoch: 169/200
Train loss: 2.1402
0.8103972986249327
Epoch: 170/200
Train loss: 2.2191
0.809613456023259
Epoch: 171/200
Train loss: 2.0400
0.8116266091763793
Epoch: 172/200
Train loss: 2.2062
0.8104180084394423
Epoch: 173/200
Train loss: 2.0032
0.8119416642914995
Epoch: 174/200
Train loss: 2.1456
0.811464178682867
Epoch: 175/200
Train loss: 2.0913
0.8114171369186156
Epoch: 176/200
Train loss: 2.0715
0.8104975748376696
Epoch: 177/200
Train loss: 2.2532
0.8106233760597361
Epoch: 178/200
Train loss: 2.1151
0.8103625921148454
Epoch: 179/200
Train loss: 2.0534
0.8109108748496915
Epoch: 180/200
Train loss: 2.1554
0.810671935894204
Epoch: 181/200
Train loss: 2.2062
0.8113915324414276
Epoch: 182/200
Train loss: 2.2242
0.8096783026048027
Epoch: 183/200
Train loss: 2.1835
0.8092174714367008
Epoch: 184/200
Train loss: 2.1173
0.8101021949731977
Epoch: 185/200
Train loss: 2.1898
0.8097812006628584
Epoch: 186/200
Train loss: 2.2597
0.8120159671545784
Epoch: 187/200
Train loss: 2.1715
0.8116235275899792
Epoch: 188/200
Train loss: 2.2226
0.8112451926617785
Epoch: 189/200
Train loss: 2.2204
0.8104769398871813
Epoch: 190/200
Train loss: 2.2248
0.8096635254296918
Epoch: 191/200
Train loss: 2.1335
0.8109235395076879
Epoch: 192/200
Train loss: 2.2635
0.8098265811935218
Epoch: 193/200
Train loss: 2.1497
0.80982661673755
Epoch: 194/200
Train loss: 2.0727
0.8105606250706819
Epoch: 195/200
Train loss: 2.1442
0.8117173383473908
Epoch: 196/200
Train loss: 2.3051
0.811493684577317
Epoch: 197/200
Train loss: 2.1668
0.8100726970759458
Epoch: 198/200
Train loss: 2.1870
0.8094561376628994
Epoch: 199/200
Train loss: 2.1248
0.811057498064939
Epoch: 200/200
Train loss: 2.3159
0.8100995328887475
Date :05/23/2023, 06:56:28
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 5
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.25
drop_path_rate: 0.4
76407
Fold: 1
13615
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Date :05/23/2023, 06:56:55
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 5
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.8
drop_path_rate: 0.4
76407
Fold: 1
13615
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 42.6039
Epoch: 2/200
Train loss: 5.6847
Epoch: 3/200
Train loss: 5.2109
Epoch: 4/200
Train loss: 4.9284
Epoch: 5/200
Train loss: 4.6438
Epoch: 6/200
Train loss: 4.3900
Epoch: 7/200
Train loss: 4.1405
Epoch: 8/200
Train loss: 4.1119
Epoch: 9/200
Train loss: 3.8639
Epoch: 10/200
Train loss: 3.8361
Epoch: 11/200
Train loss: 3.7749
Epoch: 12/200
Train loss: 3.6321
Epoch: 13/200
Train loss: 3.5873
Epoch: 14/200
Train loss: 3.4784
Epoch: 15/200
Train loss: 3.4594
Epoch: 16/200
Train loss: 3.4274
Epoch: 17/200
Train loss: 3.3102
Epoch: 18/200
Train loss: 3.2075
Epoch: 19/200
Train loss: 3.3910
Epoch: 20/200
Train loss: 3.2252
Epoch: 21/200
Train loss: 3.2378
Epoch: 22/200
Train loss: 3.0113
Epoch: 23/200
Train loss: 3.0221
Epoch: 24/200
Train loss: 3.1116
Epoch: 25/200
Train loss: 3.2575
Epoch: 26/200
Train loss: 2.9253
Epoch: 27/200
Train loss: 3.0609
Epoch: 28/200
Train loss: 2.9278
Epoch: 29/200
Train loss: 2.8880
Epoch: 30/200
Train loss: 2.8625
Epoch: 31/200
Train loss: 2.7954
Epoch: 32/200
Train loss: 2.9839
Epoch: 33/200
Train loss: 2.8459
Epoch: 34/200
Train loss: 2.7371
Epoch: 35/200
Train loss: 2.8976
Epoch: 36/200
Train loss: 2.8877
Epoch: 37/200
Train loss: 2.7906
Epoch: 38/200
Train loss: 2.7617
Epoch: 39/200
Train loss: 2.9765
Epoch: 40/200
Train loss: 2.6518
Epoch: 41/200
Train loss: 2.7840
Epoch: 42/200
Train loss: 2.7112
Epoch: 43/200
Train loss: 2.7056
Epoch: 44/200
Train loss: 2.8154
Epoch: 45/200
Train loss: 2.7601
Epoch: 46/200
Train loss: 2.7686
Epoch: 47/200
Train loss: 2.5545
Epoch: 48/200
Train loss: 2.6423
Epoch: 49/200
Train loss: 2.5100
Epoch: 50/200
Train loss: 2.4481
Epoch: 51/200
Train loss: 2.5589
Epoch: 52/200
Train loss: 2.5857
Epoch: 53/200
Train loss: 2.5583
Epoch: 54/200
Train loss: 2.4800
Epoch: 55/200
Train loss: 2.5777
Epoch: 56/200
Train loss: 2.5735
Epoch: 57/200
Train loss: 2.5025
Epoch: 58/200
Train loss: 2.6125
Epoch: 59/200
Train loss: 2.5005
Epoch: 60/200
Train loss: 2.5435
Epoch: 61/200
Train loss: 2.5393
Epoch: 62/200
Train loss: 2.5215
Epoch: 63/200
Train loss: 2.4044
Epoch: 64/200
Train loss: 2.5130
Epoch: 65/200
Train loss: 2.5364
Epoch: 66/200
Train loss: 2.4992
Epoch: 67/200
Train loss: 2.4967
Epoch: 68/200
Train loss: 2.4416
Epoch: 69/200
Train loss: 2.4805
Epoch: 70/200
Train loss: 2.5686
Epoch: 71/200
Train loss: 2.4720
Epoch: 72/200
Train loss: 2.3411
Epoch: 73/200
Train loss: 2.3948
Epoch: 74/200
Train loss: 2.3606
Epoch: 75/200
Train loss: 2.5159
Epoch: 76/200
Train loss: 2.5197
Epoch: 77/200
Train loss: 2.3410
Epoch: 78/200
Train loss: 2.4202
Epoch: 79/200
Train loss: 2.4279
Epoch: 80/200
Train loss: 2.3288
Epoch: 81/200
Train loss: 2.2190
Epoch: 82/200
Train loss: 2.3794
Epoch: 83/200
Train loss: 2.3814
Epoch: 84/200
Train loss: 2.2665
Epoch: 85/200
Train loss: 2.2354
Epoch: 86/200
Train loss: 2.3941
Epoch: 87/200
Train loss: 2.4185
Epoch: 88/200
Train loss: 2.3024
Epoch: 89/200
Train loss: 2.2753
Epoch: 90/200
Train loss: 2.4402
Epoch: 91/200
Train loss: 2.4539
Epoch: 92/200
Train loss: 2.3867
Epoch: 93/200
Train loss: 2.3124
Epoch: 94/200
Train loss: 2.4475
Epoch: 95/200
Train loss: 2.3376
Epoch: 96/200
Train loss: 2.3098
Epoch: 97/200
Train loss: 2.3825
Epoch: 98/200
Train loss: 2.4733
Epoch: 99/200
Train loss: 2.3434
Epoch: 100/200
Train loss: 2.3231
Epoch: 101/200
Train loss: 2.1943
Epoch: 102/200
Train loss: 2.4234
Epoch: 103/200
Train loss: 2.3156
Epoch: 104/200
Train loss: 2.1479
Epoch: 105/200
Train loss: 2.1920
Epoch: 106/200
Train loss: 2.2201
Epoch: 107/200
Train loss: 2.2643
Epoch: 108/200
Train loss: 2.2910
Epoch: 109/200
Train loss: 2.3421
Epoch: 110/200
Train loss: 2.3078
Epoch: 111/200
Train loss: 2.2803
Epoch: 112/200
Train loss: 2.3482
Epoch: 113/200
Train loss: 2.1949
Epoch: 114/200
Train loss: 2.1735
Epoch: 115/200
Train loss: 2.2943
Epoch: 116/200
Train loss: 2.3561
Epoch: 117/200
Train loss: 2.2655
Epoch: 118/200
Train loss: 2.2842
Epoch: 119/200
Train loss: 2.2502
Epoch: 120/200
Train loss: 2.1693
Epoch: 121/200
Train loss: 2.2775
Epoch: 122/200
Train loss: 2.2760
Epoch: 123/200
Train loss: 2.2236
Epoch: 124/200
Train loss: 2.2747
Epoch: 125/200
Train loss: 2.3420
Epoch: 126/200
Train loss: 2.4783
Epoch: 127/200
Train loss: 2.3601
Epoch: 128/200
Train loss: 2.1825
Epoch: 129/200
Train loss: 2.2885
Epoch: 130/200
Train loss: 2.1325
Epoch: 131/200
Train loss: 2.2392
Epoch: 132/200
Train loss: 2.0495
0.8168135885923055
Model improve: 0.000000 -> 0.816814
Epoch: 133/200
Train loss: 2.2219
0.8168505740855592
Model improve: 0.816814 -> 0.816851
Epoch: 134/200
Train loss: 2.3456
0.8126607703497086
Epoch: 135/200
Train loss: 2.1830
0.8150189139993227
Epoch: 136/200
Train loss: 2.1895
0.8147220588530173
Epoch: 137/200
Train loss: 2.2360
0.8146113818209111
Epoch: 138/200
Train loss: 2.0862
0.8151759136186525
Epoch: 139/200
Train loss: 2.2129
0.8154719296818398
Epoch: 140/200
Train loss: 2.2381
0.8147960919241592
Epoch: 141/200
Train loss: 2.1199
0.814501615069326
Epoch: 142/200
Train loss: 2.2508
0.8160166583763306
Epoch: 143/200
Train loss: 2.2259
0.8162103963840753
Epoch: 144/200
Train loss: 2.1472
0.8165935015523296
Epoch: 145/200
Train loss: 2.2260
0.8141182417688306
Epoch: 146/200
Train loss: 2.2631
0.8146424229506561
Epoch: 147/200
Train loss: 2.2380
0.815618725907343
Epoch: 148/200
Train loss: 2.1509
0.815578777846913
Epoch: 149/200
Train loss: 2.2455
0.8162693130516425
Epoch: 150/200
Train loss: 2.3149
0.8157942008317616
Epoch: 151/200
Train loss: 2.3255
0.8157723249864328
Epoch: 152/200
Train loss: 2.0818
0.8173052560861402
Model improve: 0.816851 -> 0.817305
Epoch: 153/200
Train loss: 2.0809
0.8148947176313741
Epoch: 154/200
Train loss: 2.1209
0.8163235157863882
Epoch: 155/200
Train loss: 2.1276
0.8167554894784725
Epoch: 156/200
Train loss: 2.1230
0.8137441382215158
Epoch: 157/200
Train loss: 2.1540
0.8153799992995855
Epoch: 158/200
Train loss: 2.2662
0.8136176701385459
Epoch: 159/200
Train loss: 2.2116
0.8152858475190567
Epoch: 160/200
Train loss: 2.1839
0.8160715217898481
Epoch: 161/200
Train loss: 2.0389
0.816856896119783
Epoch: 162/200
Train loss: 2.2357
0.815500174815996
Epoch: 163/200
Train loss: 2.0725
0.8166593219785575
Epoch: 164/200
Train loss: 2.3292
0.815603540922752
Epoch: 165/200
Train loss: 2.1394
0.8149977875370185
Epoch: 166/200
Train loss: 2.1715
0.8137950879921091
Epoch: 167/200
Train loss: 2.2623
0.8144345911830487
Epoch: 168/200
Train loss: 2.2747
0.8154019964281911
Epoch: 169/200
Train loss: 2.3144
0.8153587975985217
Epoch: 170/200
Train loss: 2.2602
0.813445142990228
Epoch: 171/200
Train loss: 2.2480
0.8162012596146088
Epoch: 172/200
Train loss: 2.2270
0.8154480553223108
Epoch: 173/200
Train loss: 2.1379
0.8148366443576006
Epoch: 174/200
Train loss: 2.2968
0.8138140215506061
Epoch: 175/200
Train loss: 2.1335
0.8159015312115533
Epoch: 176/200
Train loss: 2.2389
0.8145166529210268
Epoch: 177/200
Train loss: 2.1053
0.8153492242772228
Epoch: 178/200
Train loss: 2.1662
0.8162068584803165
Epoch: 179/200
Train loss: 2.2519
0.8154550723868018
Epoch: 180/200
Train loss: 1.9924
0.8162124927236538
Epoch: 181/200
Train loss: 2.1588
0.815325996906453
Epoch: 182/200
Train loss: 2.0500
0.8170629392755876
Epoch: 183/200
Train loss: 2.2286
0.8139214012232483
Epoch: 184/200
Train loss: 2.1928
0.8152390961499266
Epoch: 185/200
Train loss: 2.1399
0.8156164220807425
Epoch: 186/200
Train loss: 2.0756
0.8154276314320921
Epoch: 187/200
Train loss: 2.1891
0.8148061380435329
Epoch: 188/200
Train loss: 2.2198
0.8159928173055706
Epoch: 189/200
Train loss: 2.2633
0.814978008190528
Epoch: 190/200
Train loss: 2.2022
0.8160303764860152
Epoch: 191/200
Train loss: 2.1736
0.8146470366521887
Epoch: 192/200
Date :05/23/2023, 12:28:01
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 15
model_name: tf_efficientnetv2_s
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.3
drop_path_rate: 0.45
76407
Fold: 1
14198
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Date :05/23/2023, 12:28:19
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 15
model_name: tf_efficientnetv2_s
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.3
drop_path_rate: 0.45
76407
Fold: 1
14198
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 33.0111
Epoch: 2/200
Train loss: 5.7617
Epoch: 3/200
Train loss: 5.3185
Epoch: 4/200
Train loss: 5.0021
Epoch: 5/200
Train loss: 4.6936
Epoch: 6/200
Train loss: 4.4011
Epoch: 7/200
Train loss: 4.1628
Epoch: 8/200
Train loss: 4.0141
Epoch: 9/200
Train loss: 3.8733
Epoch: 10/200
Train loss: 3.7336
Epoch: 11/200
Train loss: 3.6490
Epoch: 12/200
Train loss: 3.5887
Epoch: 13/200
Train loss: 3.4870
Epoch: 14/200
Train loss: 3.3562
Epoch: 15/200
Train loss: 3.3565
Epoch: 16/200
Train loss: 3.2222
Epoch: 17/200
Train loss: 3.1164
Epoch: 18/200
Train loss: 3.2030
Epoch: 19/200
Train loss: 3.1434
Epoch: 20/200
Train loss: 3.0988
Epoch: 21/200
Train loss: 2.9248
Epoch: 22/200
Train loss: 2.8622
Epoch: 23/200
Train loss: 2.9182
Epoch: 24/200
Train loss: 3.0617
Epoch: 25/200
Train loss: 2.7657
Epoch: 26/200
Train loss: 2.8790
Epoch: 27/200
Train loss: 2.7820
Epoch: 28/200
Train loss: 2.6968
Epoch: 29/200
Train loss: 2.6952
Epoch: 30/200
Train loss: 2.6362
Epoch: 31/200
Train loss: 2.8007
Epoch: 32/200
Train loss: 2.6392
Epoch: 33/200
Train loss: 2.5941
Epoch: 34/200
Train loss: 2.6899
Epoch: 35/200
Train loss: 2.7165
Epoch: 36/200
Train loss: 2.5260
Epoch: 37/200
Train loss: 2.7355
Epoch: 38/200
Train loss: 2.6071
Epoch: 39/200
Train loss: 2.5424
Epoch: 40/200
Train loss: 2.5932
Epoch: 41/200
Train loss: 2.4978
Epoch: 42/200
Train loss: 2.5993
Epoch: 43/200
Train loss: 2.5663
Epoch: 44/200
Train loss: 2.6073
Epoch: 45/200
Train loss: 2.3942
Epoch: 46/200
Train loss: 2.5386
Epoch: 47/200
Train loss: 2.2994
Epoch: 48/200
Train loss: 2.2526
Epoch: 49/200
Train loss: 2.3852
Epoch: 50/200
Train loss: 2.3891
Epoch: 51/200
Train loss: 2.4187
Epoch: 52/200
Train loss: 2.2866
Epoch: 53/200
Train loss: 2.3803
Epoch: 54/200
Train loss: 2.4118
Epoch: 55/200
Train loss: 2.3733
Epoch: 56/200
Train loss: 2.3619
Epoch: 57/200
Train loss: 2.3476
Epoch: 58/200
Train loss: 2.3957
Epoch: 59/200
Train loss: 2.3228
Epoch: 60/200
Train loss: 2.3098
Epoch: 61/200
Train loss: 2.2751
Epoch: 62/200
Train loss: 2.3049
Epoch: 63/200
Train loss: 2.3140
Epoch: 64/200
Train loss: 2.3925
Epoch: 65/200
Train loss: 2.2242
Epoch: 66/200
Train loss: 2.4070
Epoch: 67/200
Train loss: 2.3137
Epoch: 68/200
Train loss: 2.3581
Epoch: 69/200
Train loss: 2.1987
Epoch: 70/200
Train loss: 2.2408
Epoch: 71/200
Train loss: 2.0907
Epoch: 72/200
Train loss: 2.3528
Epoch: 73/200
Train loss: 2.3373
Epoch: 74/200
Train loss: 2.1563
Epoch: 75/200
Train loss: 2.2755
Epoch: 76/200
Train loss: 2.2554
Epoch: 77/200
Train loss: 2.1164
Epoch: 78/200
Train loss: 2.1109
Epoch: 79/200
Train loss: 2.1714
Epoch: 80/200
Train loss: 2.1741
Epoch: 81/200
Train loss: 2.0981
Epoch: 82/200
Train loss: 2.0761
Epoch: 83/200
Train loss: 2.2183
Epoch: 84/200
Train loss: 2.2826
Epoch: 85/200
Train loss: 2.0331
Epoch: 86/200
Train loss: 2.1843
Epoch: 87/200
Train loss: 2.2451
Epoch: 88/200
Train loss: 2.2573
Epoch: 89/200
Train loss: 2.1886
Epoch: 90/200
Train loss: 2.1518
Epoch: 91/200
Train loss: 2.2648
Epoch: 92/200
Train loss: 2.0949
Epoch: 93/200
Train loss: 2.2921
Epoch: 94/200
Train loss: 2.1623
Epoch: 95/200
Train loss: 2.2491
Epoch: 96/200
Train loss: 2.1969
Epoch: 97/200
Train loss: 2.0253
Epoch: 98/200
Train loss: 2.2195
Epoch: 99/200
Train loss: 2.1501
Epoch: 100/200
Train loss: 2.0223
Epoch: 101/200
Train loss: 1.9882
Epoch: 102/200
Train loss: 2.1029
Epoch: 103/200
Train loss: 2.0859
Epoch: 104/200
Train loss: 2.0694
Epoch: 105/200
Train loss: 2.1488
Epoch: 106/200
Train loss: 2.1516
Epoch: 107/200
Train loss: 2.1048
Epoch: 108/200
Train loss: 2.1975
Epoch: 109/200
Train loss: 1.9769
Epoch: 110/200
Train loss: 2.0786
Epoch: 111/200
Train loss: 2.0825
Epoch: 112/200
Train loss: 2.1862
Epoch: 113/200
Train loss: 2.0851
Epoch: 114/200
Train loss: 2.0678
Epoch: 115/200
Train loss: 2.0902
Epoch: 116/200
Train loss: 2.0121
Epoch: 117/200
Train loss: 2.0995
Epoch: 118/200
Train loss: 2.0176
Epoch: 119/200
Train loss: 2.1006
Epoch: 120/200
Train loss: 2.1180
Epoch: 121/200
Train loss: 2.2580
Epoch: 122/200
Train loss: 2.1545
Epoch: 123/200
Train loss: 2.1593
Epoch: 124/200
Train loss: 2.1107
Epoch: 125/200
Train loss: 1.9994
Epoch: 126/200
Train loss: 2.0211
Epoch: 127/200
Train loss: 1.8994
Epoch: 128/200
Train loss: 2.0361
Epoch: 129/200
Train loss: 2.0618
Epoch: 130/200
Train loss: 2.0789
Epoch: 131/200
Train loss: 1.9665
Epoch: 132/200
Train loss: 2.0519
0.8112245200622101
Model improve: 0.000000 -> 0.811225
Epoch: 133/200
Train loss: 1.9257
0.8115798246553644
Model improve: 0.811225 -> 0.811580
Epoch: 134/200
Train loss: 2.0339
0.8109549611587664
Epoch: 135/200
Train loss: 2.0276
0.8115803240231438
Model improve: 0.811580 -> 0.811580
Epoch: 136/200
Train loss: 1.9212
0.8119698625442072
Model improve: 0.811580 -> 0.811970
Epoch: 137/200
Train loss: 2.0689
0.8110604710880609
Epoch: 138/200
Train loss: 2.0585
0.811368353824179
Epoch: 139/200
Train loss: 1.9657
0.811200141987699
Epoch: 140/200
Train loss: 2.0797
0.8112556826856175
Epoch: 141/200
Train loss: 2.0907
0.8113633027217895
Epoch: 142/200
Train loss: 1.9044
0.8128169034831838
Model improve: 0.811970 -> 0.812817
Epoch: 143/200
Train loss: 2.0960
0.8122343781145213
Epoch: 144/200
Train loss: 2.0504
0.8109165903115109
Epoch: 145/200
Train loss: 2.1452
0.8110445879427268
Epoch: 146/200
Train loss: 2.0343
0.8117255580799133
Epoch: 147/200
Train loss: 1.8206
0.8122940562604135
Epoch: 148/200
Train loss: 2.0420
0.8120468367214545
Epoch: 149/200
Train loss: 1.8700
0.8117862995387108
Epoch: 150/200
Train loss: 1.9240
0.8119436314452158
Epoch: 151/200
Train loss: 2.0167
0.8122880956225451
Epoch: 152/200
Train loss: 1.9856
0.8123176236962298
Epoch: 153/200
Train loss: 2.0446
0.812287436988402
Epoch: 154/200
Train loss: 2.0066
0.8116755193524038
Epoch: 155/200
Train loss: 1.9293
0.8127139596210282
Epoch: 156/200
Train loss: 2.0103
0.812499118097794
Epoch: 157/200
Train loss: 1.8947
0.8120932524826967
Epoch: 158/200
Train loss: 2.1075
0.8121543403781071
Epoch: 159/200
Train loss: 1.9329
0.8116960019083985
Epoch: 160/200
Train loss: 1.9806
0.8119166235739054
Epoch: 161/200
Train loss: 2.0818
0.8115568050426436
Epoch: 162/200
Train loss: 2.0987
0.8121369795863592
Epoch: 163/200
Train loss: 2.0724
0.81253947927274
Epoch: 164/200
Train loss: 2.0877
0.812605691746754
Epoch: 165/200
Train loss: 2.0200
0.81291279931083
Model improve: 0.812817 -> 0.812913
Epoch: 166/200
Train loss: 2.0400
0.8133298777170589
Model improve: 0.812913 -> 0.813330
Epoch: 167/200
Train loss: 1.9900
0.8126798235957434
Epoch: 168/200
Train loss: 2.0820
0.8124920115826177
Epoch: 169/200
Train loss: 1.9592
0.8122911384060374
Epoch: 170/200
Train loss: 2.0621
0.8120535962271513
Epoch: 171/200
Train loss: 1.8793
0.812167478059837
Epoch: 172/200
Train loss: 2.0388
0.8123807231973128
Epoch: 173/200
Train loss: 1.8419
0.8128819332673441
Epoch: 174/200
Train loss: 1.9983
0.8126009215223662
Epoch: 175/200
Train loss: 1.9413
0.8129915835902729
Epoch: 176/200
Train loss: 1.9127
0.8122285242668172
Epoch: 177/200
Train loss: 2.1047
0.8131400379152346
Epoch: 178/200
Train loss: 1.9491
0.813195355986154
Epoch: 179/200
Train loss: 1.9015
0.8130882932885981
Epoch: 180/200
Train loss: 1.9699
0.812882633750759
Epoch: 181/200
Train loss: 2.0325
0.8128051574394506
Epoch: 182/200
Train loss: 2.0712
0.812568175832359
Epoch: 183/200
Train loss: 2.0171
0.812968722211982
Epoch: 184/200
Train loss: 1.9652
0.8134353091256017
Model improve: 0.813330 -> 0.813435
Epoch: 185/200
Train loss: 2.0305
0.8135045390409013
Model improve: 0.813435 -> 0.813505
Epoch: 186/200
Train loss: 2.0860
0.813329690262584
Epoch: 187/200
Train loss: 2.0067
0.8126315983574434
Epoch: 188/200
Train loss: 2.0618
0.8124979198155456
Epoch: 189/200
Train loss: 2.0697
0.8129801195263029
Epoch: 190/200
Train loss: 2.0670
0.8124095210685919
Epoch: 191/200
Train loss: 1.9732
0.812316900328803
Epoch: 192/200
Train loss: 2.0913
0.8123421053521945
Epoch: 193/200
Train loss: 1.9835
0.8122318571513142
Epoch: 194/200
Train loss: 1.9142
0.8128208392490508
Epoch: 195/200
Train loss: 1.9775
Date :05/23/2023, 19:44:39
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 15
model_name: tf_efficientnetv2_b2
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.3
drop_path_rate: 0.45
76407
Fold: 1
13975
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 42.1978
Epoch: 2/200
Date :05/23/2023, 19:46:08
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 15
model_name: tf_efficientnetv2_b2
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.3
drop_path_rate: 0.45
76407
Fold: 1
13975
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 42.2053
Epoch: 2/200
Train loss: 5.7428
Epoch: 3/200
Train loss: 5.2658
Epoch: 4/200
Train loss: 4.9784
Epoch: 5/200
Train loss: 4.7237
Epoch: 6/200
Train loss: 4.4234
Epoch: 7/200
Train loss: 4.1785
Epoch: 8/200
Train loss: 4.0769
Epoch: 9/200
Train loss: 3.9394
Epoch: 10/200
Train loss: 3.8054
Epoch: 11/200
Train loss: 3.7431
Epoch: 12/200
Train loss: 3.6536
Epoch: 13/200
Train loss: 3.5969
Epoch: 14/200
Train loss: 3.4653
Epoch: 15/200
Train loss: 3.5153
Epoch: 16/200
Train loss: 3.3744
Epoch: 17/200
Train loss: 3.2382
Epoch: 18/200
Train loss: 3.2822
Epoch: 19/200
Train loss: 3.3038
Epoch: 20/200
Train loss: 3.2797
Epoch: 21/200
Train loss: 3.1058
Epoch: 22/200
Train loss: 2.9472
Epoch: 23/200
Train loss: 3.1281
Epoch: 24/200
Train loss: 3.1753
Epoch: 25/200
Train loss: 2.9832
Epoch: 26/200
Train loss: 3.0487
Epoch: 27/200
Train loss: 2.9823
Epoch: 28/200
Train loss: 2.8342
Epoch: 29/200
Train loss: 2.9468
Epoch: 30/200
Train loss: 2.7533
Epoch: 31/200
Train loss: 2.9851
Epoch: 32/200
Train loss: 2.8031
Epoch: 33/200
Train loss: 2.7832
Epoch: 34/200
Train loss: 2.8474
Epoch: 35/200
Train loss: 2.9030
Epoch: 36/200
Train loss: 2.7355
Epoch: 37/200
Train loss: 2.7975
Epoch: 38/200
Train loss: 2.9265
Epoch: 39/200
Train loss: 2.6156
Epoch: 40/200
Train loss: 2.7908
Epoch: 41/200
Train loss: 2.6831
Epoch: 42/200
Train loss: 2.7459
Epoch: 43/200
Train loss: 2.7274
Epoch: 44/200
Train loss: 2.7733
Epoch: 45/200
Train loss: 2.7106
Epoch: 46/200
Train loss: 2.6148
Epoch: 47/200
Train loss: 2.5713
Epoch: 48/200
Train loss: 2.4664
Epoch: 49/200
Train loss: 2.5099
Epoch: 50/200
Train loss: 2.4725
Epoch: 51/200
Train loss: 2.5943
Epoch: 52/200
Train loss: 2.4385
Epoch: 53/200
Train loss: 2.6430
Epoch: 54/200
Train loss: 2.5482
Epoch: 55/200
Train loss: 2.4073
Epoch: 56/200
Train loss: 2.5815
Epoch: 57/200
Train loss: 2.4727
Epoch: 58/200
Train loss: 2.5426
Epoch: 59/200
Train loss: 2.5289
Epoch: 60/200
Train loss: 2.5023
Epoch: 61/200
Train loss: 2.3802
Epoch: 62/200
Train loss: 2.5053
Epoch: 63/200
Train loss: 2.4818
Epoch: 64/200
Train loss: 2.4732
Epoch: 65/200
Train loss: 2.4931
Epoch: 66/200
Train loss: 2.3749
Epoch: 67/200
Train loss: 2.5351
Epoch: 68/200
Train loss: 2.5052
Epoch: 69/200
Train loss: 2.4731
Epoch: 70/200
Train loss: 2.3645
Epoch: 71/200
Train loss: 2.3456
Epoch: 72/200
Train loss: 2.3633
Epoch: 73/200
Train loss: 2.4994
Epoch: 74/200
Train loss: 2.5221
Epoch: 75/200
Train loss: 2.3008
Epoch: 76/200
Train loss: 2.4433
Epoch: 77/200
Train loss: 2.3917
Epoch: 78/200
Train loss: 2.2485
Epoch: 79/200
Train loss: 2.2081
Epoch: 80/200
Train loss: 2.3864
Epoch: 81/200
Train loss: 2.3094
Epoch: 82/200
Train loss: 2.3096
Epoch: 83/200
Train loss: 2.2453
Epoch: 84/200
Train loss: 2.4034
Epoch: 85/200
Train loss: 2.3819
Epoch: 86/200
Train loss: 2.1539
Epoch: 87/200
Train loss: 2.4088
Epoch: 88/200
Train loss: 2.3790
Epoch: 89/200
Train loss: 2.3188
Epoch: 90/200
Train loss: 2.3303
Epoch: 91/200
Train loss: 2.3839
Epoch: 92/200
Train loss: 2.4295
Epoch: 93/200
Train loss: 2.2456
Epoch: 94/200
Train loss: 2.3843
Epoch: 95/200
Train loss: 2.3815
Epoch: 96/200
Train loss: 2.3778
Epoch: 97/200
Train loss: 2.3469
Epoch: 98/200
Train loss: 2.1882
Epoch: 99/200
Train loss: 2.3507
Epoch: 100/200
Train loss: 2.3284
Epoch: 101/200
Train loss: 2.1794
Epoch: 102/200
Train loss: 2.1381
Epoch: 103/200
Train loss: 2.2036
Epoch: 104/200
Train loss: 2.2621
Epoch: 105/200
Train loss: 2.2513
Epoch: 106/200
Train loss: 2.3141
Epoch: 107/200
Train loss: 2.3172
Epoch: 108/200
Train loss: 2.2701
Epoch: 109/200
Train loss: 2.3155
Epoch: 110/200
Train loss: 2.1324
Epoch: 111/200
Train loss: 2.2201
Epoch: 112/200
Train loss: 2.2502
Epoch: 113/200
Train loss: 2.2985
Epoch: 114/200
Train loss: 2.2310
Epoch: 115/200
Train loss: 2.2729
Epoch: 116/200
Train loss: 2.2107
Epoch: 117/200
Train loss: 2.1915
Epoch: 118/200
Train loss: 2.2595
Epoch: 119/200
Train loss: 2.1891
Epoch: 120/200
Train loss: 2.2019
Epoch: 121/200
Train loss: 2.2674
Epoch: 122/200
Train loss: 2.3962
Epoch: 123/200
Train loss: 2.3382
Epoch: 124/200
Train loss: 2.3141
Epoch: 125/200
Train loss: 2.2266
Epoch: 126/200
Train loss: 2.1943
Epoch: 127/200
Train loss: 2.1979
Epoch: 128/200
Train loss: 2.0507
Epoch: 129/200
Train loss: 2.1673
Epoch: 130/200
Train loss: 2.1894
Epoch: 131/200
Train loss: 2.3034
Epoch: 132/200
Train loss: 2.1306
0.8154994280372313
Model improve: 0.000000 -> 0.815499
Epoch: 133/200
Train loss: 2.1895
0.8158548173886171
Model improve: 0.815499 -> 0.815855
Epoch: 134/200
Train loss: 2.1336
0.8162173848159229
Model improve: 0.815855 -> 0.816217
Epoch: 135/200
Train loss: 2.1906
0.8155198122141503
Epoch: 136/200
Train loss: 2.2013
0.8153881413280945
Epoch: 137/200
Train loss: 2.0329
0.8168023415177728
Model improve: 0.816217 -> 0.816802
Epoch: 138/200
Train loss: 2.2605
0.8163137049148206
Epoch: 139/200
Train loss: 2.1886
0.8155528563348279
Epoch: 140/200
Train loss: 2.1088
0.8164139145298247
Epoch: 141/200
Train loss: 2.1729
0.8149289014022836
Epoch: 142/200
Train loss: 2.2408
0.8147835672154605
Epoch: 143/200
Train loss: 2.1971
0.8161542138445212
Epoch: 144/200
Train loss: 2.1769
0.8132976914915512
Epoch: 145/200
Train loss: 2.1610
0.8172010077614986
Model improve: 0.816802 -> 0.817201
Epoch: 146/200
Train loss: 2.2794
0.8163684668936818
Epoch: 147/200
Train loss: 2.2812
0.8163818259225631
Epoch: 148/200
Train loss: 1.9813
0.8180895494109298
Model improve: 0.817201 -> 0.818090
Epoch: 149/200
Train loss: 2.1052
0.8166486449965072
Epoch: 150/200
Train loss: 2.1055
0.8157921508957826
Epoch: 151/200
Train loss: 2.0752
0.8177903421376939
Epoch: 152/200
Train loss: 2.1628
0.8160980350874678
Epoch: 153/200
Train loss: 2.1455
0.8157896354919494
Epoch: 154/200
Train loss: 2.1988
0.8164076455293867
Epoch: 155/200
Train loss: 2.2416
0.8168074153083371
Epoch: 156/200
Train loss: 2.0237
0.8162922100573778
Epoch: 157/200
Train loss: 2.0503
0.8183117139319516
Model improve: 0.818090 -> 0.818312
Epoch: 158/200
Train loss: 2.2214
0.8169721733503109
Epoch: 159/200
Train loss: 2.1884
0.8148004430256837
Epoch: 160/200
Train loss: 2.1353
0.8165610280703134
Epoch: 161/200
Train loss: 2.1443
0.8172255327094383
Epoch: 162/200
Train loss: 2.1411
0.816451986975175
Epoch: 163/200
Train loss: 2.3457
0.8154852571655903
Epoch: 164/200
Train loss: 2.2960
0.8144981865163876
Epoch: 165/200
Train loss: 2.1192
0.8173245415644078
Epoch: 166/200
Train loss: 2.3089
0.81591877134702
Epoch: 167/200
Train loss: 2.1872
0.8157189193927663
Epoch: 168/200
Train loss: 2.1532
0.8164397949809143
Epoch: 169/200
Train loss: 2.1983
0.815194538336068
Epoch: 170/200
Train loss: 2.1370
0.8158290582033993
Epoch: 171/200
Train loss: 2.1817
0.8159885699312988
Epoch: 172/200
Train loss: 2.1229
0.8170945867197613
Epoch: 173/200
Train loss: 2.1334
0.8157960974033714
Epoch: 174/200
Train loss: 2.2177
0.8171578535633106
Epoch: 175/200
Train loss: 1.9879
0.8162210433167745
Epoch: 176/200
Train loss: 2.1413
0.8146309419111806
Epoch: 177/200
Train loss: 2.0049
0.8180673849856722
Epoch: 178/200
Train loss: 2.2128
0.8158532548085589
Epoch: 179/200
Train loss: 2.1755
0.8164145457654113
Epoch: 180/200
Train loss: 2.0808
0.8174930986856181
Epoch: 181/200
Train loss: 2.1011
0.8158023631906142
Epoch: 182/200
Train loss: 2.1463
0.8163384425051998
Epoch: 183/200
Train loss: 2.2448
0.8159536381806259
Epoch: 184/200
Train loss: 2.2153
0.8160946865017741
Epoch: 185/200
Train loss: 2.1408
Date :05/24/2023, 00:57:36
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 30
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.25
drop_path_rate: 0.4
76407
Fold: 1
15267
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 38.9429
Epoch: 2/200
Train loss: 6.0700
Epoch: 3/200
Train loss: 5.6765
Epoch: 4/200
Train loss: 5.3340
Epoch: 5/200
Train loss: 4.9894
Epoch: 6/200
Train loss: 4.6863
Epoch: 7/200
Train loss: 4.4456
Epoch: 8/200
Train loss: 4.1828
Epoch: 9/200
Train loss: 4.0766
Epoch: 10/200
Train loss: 3.9488
Epoch: 11/200
Train loss: 3.8238
Epoch: 12/200
Train loss: 3.7671
Epoch: 13/200
Train loss: 3.6354
Epoch: 14/200
Train loss: 3.6102
Epoch: 15/200
Train loss: 3.4008
Epoch: 16/200
Train loss: 3.3565
Epoch: 17/200
Train loss: 3.4981
Epoch: 18/200
Train loss: 3.4097
Epoch: 19/200
Train loss: 3.2669
Epoch: 20/200
Train loss: 3.1004
Epoch: 21/200
Train loss: 3.1627
Epoch: 22/200
Train loss: 3.2740
Epoch: 23/200
Train loss: 3.0989
Epoch: 24/200
Train loss: 3.1379
Epoch: 25/200
Train loss: 3.0455
Epoch: 26/200
Train loss: 2.9872
Epoch: 27/200
Train loss: 2.8728
Epoch: 28/200
Train loss: 2.9601
Epoch: 29/200
Train loss: 3.0369
Epoch: 30/200
Train loss: 2.7871
Epoch: 31/200
Train loss: 2.9563
Epoch: 32/200
Train loss: 2.9225
Epoch: 33/200
Train loss: 2.8427
Epoch: 34/200
Train loss: 2.9310
Epoch: 35/200
Train loss: 2.9307
Epoch: 36/200
Train loss: 2.7595
Epoch: 37/200
Train loss: 2.8495
Epoch: 38/200
Train loss: 2.6957
Epoch: 39/200
Train loss: 2.8901
Epoch: 40/200
Train loss: 2.8605
Epoch: 41/200
Train loss: 2.8057
Epoch: 42/200
Train loss: 2.6618
Epoch: 43/200
Train loss: 2.6687
Epoch: 44/200
Train loss: 2.5638
Epoch: 45/200
Train loss: 2.5839
Epoch: 46/200
Train loss: 2.5419
Epoch: 47/200
Train loss: 2.6488
Epoch: 48/200
Train loss: 2.5106
Epoch: 49/200
Train loss: 2.6384
Epoch: 50/200
Train loss: 2.5825
Epoch: 51/200
Train loss: 2.6228
Epoch: 52/200
Train loss: 2.5479
Epoch: 53/200
Train loss: 2.6684
Epoch: 54/200
Train loss: 2.5601
Epoch: 55/200
Train loss: 2.5724
Epoch: 56/200
Train loss: 2.4541
Epoch: 57/200
Train loss: 2.5619
Epoch: 58/200
Train loss: 2.5468
Epoch: 59/200
Train loss: 2.6058
Epoch: 60/200
Train loss: 2.4434
Epoch: 61/200
Train loss: 2.6409
Epoch: 62/200
Train loss: 2.5369
Epoch: 63/200
Train loss: 2.5879
Epoch: 64/200
Train loss: 2.4370
Epoch: 65/200
Train loss: 2.3824
Epoch: 66/200
Train loss: 2.4389
Epoch: 67/200
Train loss: 2.5538
Epoch: 68/200
Train loss: 2.5559
Epoch: 69/200
Train loss: 2.4610
Epoch: 70/200
Train loss: 2.4437
Epoch: 71/200
Train loss: 2.3546
Epoch: 72/200
Train loss: 2.3372
Epoch: 73/200
Train loss: 2.3864
Epoch: 74/200
Train loss: 2.4048
Epoch: 75/200
Train loss: 2.3144
Epoch: 76/200
Train loss: 2.3591
Epoch: 77/200
Train loss: 2.4749
Epoch: 78/200
Train loss: 2.4014
Epoch: 79/200
Train loss: 2.2935
Epoch: 80/200
Train loss: 2.4603
Epoch: 81/200
Train loss: 2.4496
Epoch: 82/200
Train loss: 2.4197
Epoch: 83/200
Train loss: 2.3686
Epoch: 84/200
Train loss: 2.5032
Epoch: 85/200
Train loss: 2.3379
Epoch: 86/200
Train loss: 2.4976
Epoch: 87/200
Train loss: 2.4110
Epoch: 88/200
Train loss: 2.4130
Epoch: 89/200
Train loss: 2.3560
Epoch: 90/200
Train loss: 2.2789
Epoch: 91/200
Train loss: 2.3741
Epoch: 92/200
Train loss: 2.4084
Epoch: 93/200
Train loss: 2.1513
Epoch: 94/200
Train loss: 2.2690
Epoch: 95/200
Train loss: 2.3034
Epoch: 96/200
Train loss: 2.3036
Epoch: 97/200
Train loss: 2.3159
Epoch: 98/200
Train loss: 2.3958
Epoch: 99/200
Train loss: 2.3293
Epoch: 100/200
Train loss: 2.3588
Epoch: 101/200
Train loss: 2.2235
Epoch: 102/200
Train loss: 2.2431
Epoch: 103/200
Train loss: 2.3804
Epoch: 104/200
Train loss: 2.3598
Epoch: 105/200
Train loss: 2.2969
Epoch: 106/200
Train loss: 2.3094
Epoch: 107/200
Train loss: 2.2047
Epoch: 108/200
Train loss: 2.3968
Epoch: 109/200
Train loss: 2.2467
Epoch: 110/200
Train loss: 2.2583
Epoch: 111/200
Train loss: 2.3059
Epoch: 112/200
Train loss: 2.5545
Epoch: 113/200
Train loss: 2.3774
Epoch: 114/200
Train loss: 2.2632
Epoch: 115/200
Train loss: 2.2647
Epoch: 116/200
Train loss: 2.2670
Epoch: 117/200
Train loss: 2.2032
Epoch: 118/200
Train loss: 2.0899
Epoch: 119/200
Train loss: 2.2660
Epoch: 120/200
Train loss: 2.3406
Epoch: 121/200
Train loss: 2.2125
Epoch: 122/200
Train loss: 2.2612
Epoch: 123/200
Train loss: 2.1550
Epoch: 124/200
Train loss: 2.2080
Epoch: 125/200
Train loss: 2.2407
Epoch: 126/200
Train loss: 2.2024
Epoch: 127/200
Train loss: 2.1995
Epoch: 128/200
Train loss: 2.2392
Epoch: 129/200
Train loss: 2.1774
Epoch: 130/200
Train loss: 2.2958
Epoch: 131/200
Train loss: 2.2940
Epoch: 132/200
Train loss: 2.2255
0.8103586062341422
Model improve: 0.000000 -> 0.810359
Epoch: 133/200
Train loss: 2.2137
0.811578031167354
Model improve: 0.810359 -> 0.811578
Epoch: 134/200
Train loss: 2.3762
0.8096113307932076
Epoch: 135/200
Train loss: 2.2236
0.8131512017868534
Model improve: 0.811578 -> 0.813151
Epoch: 136/200
Train loss: 2.0835
0.810701831817531
Epoch: 137/200
Train loss: 2.1924
0.8130243515224527
Epoch: 138/200
Train loss: 2.1405
0.8120101115878416
Epoch: 139/200
Train loss: 2.1468
0.8104120931380125
Epoch: 140/200
Train loss: 2.1943
Date :05/24/2023, 03:40:37
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 20
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.25
drop_path_rate: 0.5
76407
Fold: 1
14704
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 41.7854
Epoch: 2/200
Train loss: 5.8834
Epoch: 3/200
Train loss: 5.4591
Epoch: 4/200
Train loss: 5.1553
Epoch: 5/200
Train loss: 4.8630
Epoch: 6/200
Train loss: 4.6100
Epoch: 7/200
Train loss: 4.3797
Epoch: 8/200
Train loss: 4.1994
Epoch: 9/200
Train loss: 4.0509
Epoch: 10/200
Train loss: 3.9989
Epoch: 11/200
Train loss: 3.8310
Epoch: 12/200
Train loss: 3.7595
Epoch: 13/200
Train loss: 3.6438
Epoch: 14/200
Train loss: 3.5956
Epoch: 15/200
Train loss: 3.5792
Epoch: 16/200
Train loss: 3.4224
Epoch: 17/200
Train loss: 3.3148
Epoch: 18/200
Train loss: 3.5127
Epoch: 19/200
Train loss: 3.3688
Epoch: 20/200
Train loss: 3.1958
Epoch: 21/200
Train loss: 3.0866
Epoch: 22/200
Train loss: 3.2273
Epoch: 23/200
Train loss: 3.2843
Epoch: 24/200
Train loss: 3.0874
Epoch: 25/200
Train loss: 3.1638
Epoch: 26/200
Train loss: 3.0281
Epoch: 27/200
Train loss: 3.0043
Epoch: 28/200
Train loss: 2.9772
Epoch: 29/200
Train loss: 2.9448
Epoch: 30/200
Train loss: 3.0463
Epoch: 31/200
Train loss: 2.8492
Epoch: 32/200
Train loss: 2.8701
Epoch: 33/200
Train loss: 2.9642
Epoch: 34/200
Train loss: 2.9725
Epoch: 35/200
Train loss: 2.8109
Epoch: 36/200
Train loss: 3.0666
Epoch: 37/200
Train loss: 2.7208
Epoch: 38/200
Train loss: 2.8986
Epoch: 39/200
Train loss: 2.7497
Epoch: 40/200
Train loss: 2.8309
Epoch: 41/200
Train loss: 2.8508
Epoch: 42/200
Train loss: 2.8707
Epoch: 43/200
Train loss: 2.7914
Epoch: 44/200
Train loss: 2.7192
Epoch: 45/200
Train loss: 2.6440
Epoch: 46/200
Train loss: 2.5583
Epoch: 47/200
Train loss: 2.6044
Epoch: 48/200
Train loss: 2.5939
Epoch: 49/200
Train loss: 2.6809
Epoch: 50/200
Train loss: 2.5320
Epoch: 51/200
Train loss: 2.6556
Epoch: 52/200
Train loss: 2.7012
Epoch: 53/200
Train loss: 2.5332
Epoch: 54/200
Train loss: 2.6612
Epoch: 55/200
Train loss: 2.6167
Epoch: 56/200
Train loss: 2.6820
Epoch: 57/200
Train loss: 2.5396
Epoch: 58/200
Train loss: 2.5645
Epoch: 59/200
Train loss: 2.4964
Epoch: 60/200
Train loss: 2.6466
Epoch: 61/200
Train loss: 2.5038
Epoch: 62/200
Train loss: 2.6742
Epoch: 63/200
Train loss: 2.3934
Epoch: 64/200
Train loss: 2.6775
Epoch: 65/200
Train loss: 2.5407
Epoch: 66/200
Train loss: 2.5649
Epoch: 67/200
Train loss: 2.4364
Epoch: 68/200
Train loss: 2.4100
Epoch: 69/200
Train loss: 2.5047
Epoch: 70/200
Train loss: 2.5316
Epoch: 71/200
Train loss: 2.5690
Epoch: 72/200
Train loss: 2.5130
Epoch: 73/200
Train loss: 2.4618
Epoch: 74/200
Train loss: 2.3993
Epoch: 75/200
Train loss: 2.2413
Epoch: 76/200
Train loss: 2.4538
Epoch: 77/200
Train loss: 2.4531
Epoch: 78/200
Train loss: 2.3705
Epoch: 79/200
Train loss: 2.2985
Epoch: 80/200
Train loss: 2.4760
Epoch: 81/200
Train loss: 2.4758
Epoch: 82/200
Train loss: 2.2930
Epoch: 83/200
Train loss: 2.4348
Epoch: 84/200
Train loss: 2.4718
Epoch: 85/200
Train loss: 2.4656
Epoch: 86/200
Train loss: 2.4275
Epoch: 87/200
Train loss: 2.4455
Epoch: 88/200
Train loss: 2.4938
Epoch: 89/200
Train loss: 2.3580
Epoch: 90/200
Train loss: 2.4566
Epoch: 91/200
Train loss: 2.4655
Epoch: 92/200
Train loss: 2.4250
Epoch: 93/200
Train loss: 2.4148
Epoch: 94/200
Train loss: 2.3342
Epoch: 95/200
Train loss: 2.3926
Epoch: 96/200
Train loss: 2.4459
Epoch: 97/200
Train loss: 2.1742
Epoch: 98/200
Train loss: 2.2717
Epoch: 99/200
Train loss: 2.3158
Epoch: 100/200
Train loss: 2.2918
Epoch: 101/200
Train loss: 2.3614
Epoch: 102/200
Train loss: 2.3805
Epoch: 103/200
Train loss: 2.3913
Epoch: 104/200
Train loss: 2.3620
Epoch: 105/200
Train loss: 2.2884
Epoch: 106/200
Train loss: 2.2283
Epoch: 107/200
Train loss: 2.3293
Epoch: 108/200
Train loss: 2.4109
Epoch: 109/200
Train loss: 2.3149
Epoch: 110/200
Train loss: 2.3197
Epoch: 111/200
Train loss: 2.3124
Epoch: 112/200
Train loss: 2.2537
Epoch: 113/200
Train loss: 2.3680
Epoch: 114/200
Train loss: 2.2744
Epoch: 115/200
Train loss: 2.2757
Epoch: 116/200
Train loss: 2.3774
Epoch: 117/200
Train loss: 2.5431
Epoch: 118/200
Train loss: 2.4294
Epoch: 119/200
Train loss: 2.2775
Epoch: 120/200
Train loss: 2.3333
Epoch: 121/200
Train loss: 2.2315
Epoch: 122/200
Train loss: 2.2182
Epoch: 123/200
Train loss: 2.1348
Epoch: 124/200
Train loss: 2.2723
Epoch: 125/200
Train loss: 2.3532
Epoch: 126/200
Train loss: 2.2015
Epoch: 127/200
Train loss: 2.3065
Epoch: 128/200
Train loss: 2.1963
Epoch: 129/200
Train loss: 2.2497
Epoch: 130/200
Train loss: 2.2718
Epoch: 131/200
Train loss: 2.1221
Epoch: 132/200
Train loss: 2.3399
0.8150492310240957
Model improve: 0.000000 -> 0.815049
Epoch: 133/200
Train loss: 2.2451
0.8161263164606157
Model improve: 0.815049 -> 0.816126
Epoch: 134/200
Train loss: 2.1925
0.8154381054058353
Epoch: 135/200
Train loss: 2.3158
0.8135162706586547
Epoch: 136/200
Train loss: 2.3195
0.8123046924848851
Epoch: 137/200
Train loss: 2.1575
0.816354967375021
Model improve: 0.816126 -> 0.816355
Epoch: 138/200
Train loss: 2.3342
0.8136017608858067
Epoch: 139/200
Train loss: 2.3006
0.811329081071617
Epoch: 140/200
Train loss: 2.4026
0.8137096482451739
Epoch: 141/200
Train loss: 2.1835
0.8158772845634326
Epoch: 142/200
Train loss: 2.1307
0.8137540302747484
Epoch: 143/200
Train loss: 2.2117
0.8159359015827339
Epoch: 144/200
Train loss: 2.1777
0.8150942243625346
Epoch: 145/200
Train loss: 2.1317
0.81504526722542
Epoch: 146/200
Train loss: 2.2323
0.8156621532572389
Epoch: 147/200
Train loss: 2.2899
0.8163438457752091
Epoch: 148/200
Train loss: 2.2966
0.8136000183961106
Epoch: 149/200
Train loss: 2.1627
0.8156028827282268
Epoch: 150/200
Train loss: 2.1427
0.8162040317091258
Epoch: 151/200
Train loss: 2.2527
0.8152738446927814
Epoch: 152/200
Train loss: 2.2442
0.8120233844996297
Epoch: 153/200
Train loss: 2.2522
0.8144882287437248
Epoch: 154/200
Train loss: 2.1776
0.8157702156770436
Epoch: 155/200
Train loss: 2.2433
0.8149979643638752
Epoch: 156/200
Train loss: 2.3946
0.8142932912180458
Epoch: 157/200
Train loss: 2.3324
0.8140346449439896
Epoch: 158/200
Train loss: 2.2693
0.8139525805821193
Epoch: 159/200
Train loss: 2.3171
0.8157223873721793
Epoch: 160/200
Train loss: 2.2966
0.8127759774922223
Epoch: 161/200
Train loss: 2.1693
0.8141296660057439
Epoch: 162/200
Train loss: 2.3206
0.8141402736394046
Epoch: 163/200
Train loss: 2.2030
0.8134896855817039
Epoch: 164/200
Train loss: 2.2779
0.8146700883300929
Epoch: 165/200
Train loss: 2.1387
0.8164037462312095
Model improve: 0.816355 -> 0.816404
Epoch: 166/200
Train loss: 2.2809
0.8138421172622435
Epoch: 167/200
Train loss: 2.0438
0.8150164837287878
Epoch: 168/200
Train loss: 2.2483
0.8147747393164021
Epoch: 169/200
Train loss: 2.1588
0.8147842937479176
Epoch: 170/200
Train loss: 2.1645
0.8141022357356897
Epoch: 171/200
Train loss: 2.3023
0.8147567469242778
Epoch: 172/200
Train loss: 2.1834
0.8150002468372421
Epoch: 173/200
Train loss: 2.1543
0.8150852983774832
Epoch: 174/200
Train loss: 2.1862
0.8152965403036339
Epoch: 175/200
Train loss: 2.3094
0.8154293334251351
Epoch: 176/200
Train loss: 2.3041
0.8150320970873497
Epoch: 177/200
Train loss: 2.1588
0.8159320382650058
Epoch: 178/200
Train loss: 2.3251
0.8142965423575037
Epoch: 179/200
