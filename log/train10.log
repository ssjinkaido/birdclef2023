Date :04/05/2023, 07:36:39
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 0
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s_in21ft1k
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
Fold: 0
19577
Date :04/05/2023, 07:43:54
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 0
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
Fold: 0
19577
Date :04/05/2023, 07:52:54
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 0
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 10.4284, val loss: 7.2872
Model improve: 0.0000 -> 0.5588
Epoch: 2/100
Train loss: 8.9680, val loss: 5.4160
Model improve: 0.5588 -> 0.6508
Epoch: 3/100
Train loss: 8.1004, val loss: 4.6071
Model improve: 0.6508 -> 0.7113
Epoch: 4/100
Train loss: 7.6306, val loss: 4.0157
Model improve: 0.7113 -> 0.7458
Epoch: 5/100
Train loss: 7.3850, val loss: 3.8701
Model improve: 0.7458 -> 0.7671
Epoch: 6/100
Train loss: 7.1127, val loss: 3.5701
Model improve: 0.7671 -> 0.7816
Epoch: 7/100
Train loss: 6.7863, val loss: 3.4643
Model improve: 0.7816 -> 0.7920
Epoch: 8/100
Train loss: 6.6863, val loss: 3.1211
Model improve: 0.7920 -> 0.8045
Epoch: 9/100
Train loss: 6.6964, val loss: 3.2300
Model improve: 0.8045 -> 0.8109
Epoch: 10/100
Train loss: 6.5047, val loss: 2.9695
Model improve: 0.8109 -> 0.8175
Epoch: 11/100
Train loss: 6.3281, val loss: 2.9344
Model improve: 0.8175 -> 0.8266
Epoch: 12/100
Train loss: 6.3115, val loss: 2.9389
Epoch: 13/100
Train loss: 6.1530, val loss: 2.6833
Model improve: 0.8266 -> 0.8391
Epoch: 14/100
Train loss: 6.0189, val loss: 2.7539
Epoch: 15/100
Train loss: 6.0798, val loss: 2.6341
Model improve: 0.8391 -> 0.8413
Epoch: 16/100
Train loss: 5.9623, val loss: 2.6660
Model improve: 0.8413 -> 0.8456
Epoch: 17/100
Train loss: 5.8416, val loss: 2.6027
Epoch: 18/100
Train loss: 5.8909, val loss: 2.5046
Model improve: 0.8456 -> 0.8501
Epoch: 19/100
Train loss: 5.8624, val loss: 2.7010
Model improve: 0.8501 -> 0.8503
Epoch: 20/100
Train loss: 5.8597, val loss: 2.5992
Epoch: 21/100
Train loss: 5.8043, val loss: 2.4419
Model improve: 0.8503 -> 0.8579
Epoch: 22/100
Train loss: 5.6674, val loss: 2.5518
Epoch: 23/100
Train loss: 5.5624, val loss: 2.5573
Model improve: 0.8579 -> 0.8581
Epoch: 24/100
Train loss: 5.6431, val loss: 2.4302
Model improve: 0.8581 -> 0.8600
Epoch: 25/100
Train loss: 5.4919, val loss: 2.2943
Model improve: 0.8600 -> 0.8637
Epoch: 26/100
Train loss: 5.4893, val loss: 2.3257
Model improve: 0.8637 -> 0.8639
Epoch: 27/100
Train loss: 5.5481, val loss: 2.4165
Model improve: 0.8639 -> 0.8678
Epoch: 28/100
Train loss: 5.4783, val loss: 2.2943
Model improve: 0.8678 -> 0.8690
Epoch: 29/100
Train loss: 5.4238, val loss: 2.3143
Model improve: 0.8690 -> 0.8729
Epoch: 30/100
Train loss: 5.3572, val loss: 2.2702
Epoch: 31/100
Train loss: 5.4200, val loss: 2.2596
Epoch: 32/100
Train loss: 5.3504, val loss: 2.2189
Epoch: 33/100
Train loss: 5.1800, val loss: 2.2636
Epoch: 34/100
Train loss: 5.1875, val loss: 2.2567
Model improve: 0.8729 -> 0.8732
Epoch: 35/100
Train loss: 5.3324, val loss: 2.3572
Epoch: 36/100
Train loss: 5.1851, val loss: 2.1966
Epoch: 37/100
Train loss: 5.3431, val loss: 2.1783
Model improve: 0.8732 -> 0.8742
Epoch: 38/100
Train loss: 5.2024, val loss: 2.1131
Epoch: 39/100
Train loss: 5.1897, val loss: 2.1415
Model improve: 0.8742 -> 0.8754
Epoch: 40/100
Train loss: 5.1394, val loss: 2.2141
Model improve: 0.8754 -> 0.8791
Epoch: 41/100
Train loss: 5.0090, val loss: 2.1347
Epoch: 42/100
Train loss: 5.2206, val loss: 2.2057
Epoch: 43/100
Train loss: 5.1059, val loss: 2.1419
Model improve: 0.8791 -> 0.8819
Epoch: 44/100
Train loss: 5.1072, val loss: 2.1003
Epoch: 45/100
Train loss: 5.1106, val loss: 2.1168
Epoch: 46/100
Train loss: 4.9417, val loss: 2.1653
Epoch: 47/100
Train loss: 4.9652, val loss: 2.1614
Epoch: 48/100
Train loss: 5.1334, val loss: 2.1088
Epoch: 49/100
Train loss: 4.9386, val loss: 2.1101
Epoch: 50/100
Train loss: 4.9296, val loss: 2.0663
Model improve: 0.8819 -> 0.8828
Epoch: 51/100
Train loss: 4.9171, val loss: 2.1337
Epoch: 52/100
Train loss: 5.0002, val loss: 2.1146
Epoch: 53/100
Train loss: 4.8573, val loss: 2.0630
Epoch: 54/100
Train loss: 4.7334, val loss: 1.9860
Model improve: 0.8828 -> 0.8834
Epoch: 55/100
Train loss: 4.8068, val loss: 2.0314
Model improve: 0.8834 -> 0.8840
Epoch: 56/100
Train loss: 4.8280, val loss: 2.0735
Epoch: 57/100
Train loss: 4.8709, val loss: 2.0391
Model improve: 0.8840 -> 0.8866
Epoch: 58/100
Train loss: 4.8937, val loss: 1.9892
Epoch: 59/100
Train loss: 4.8206, val loss: 2.0051
Epoch: 60/100
Train loss: 4.8928, val loss: 2.0007
Model improve: 0.8866 -> 0.8891
Epoch: 61/100
Train loss: 4.6759, val loss: 1.9936
Epoch: 62/100
Train loss: 4.8177, val loss: 1.9562
Model improve: 0.8891 -> 0.8900
Epoch: 63/100
Train loss: 4.6958, val loss: 1.9980
Epoch: 64/100
Train loss: 4.7546, val loss: 1.9939
Epoch: 65/100
Train loss: 4.8079, val loss: 1.9891
Epoch: 66/100
Train loss: 4.7385, val loss: 2.0057
Epoch: 67/100
Train loss: 4.7475, val loss: 1.9612
Epoch: 68/100
Train loss: 4.5529, val loss: 2.0088
Epoch: 69/100
Train loss: 4.8523, val loss: 1.9707
Epoch: 70/100
Train loss: 4.6217, val loss: 1.9563
Epoch: 71/100
Train loss: 4.6045, val loss: 2.0470
Epoch: 72/100
Train loss: 4.6368, val loss: 1.9619
Epoch: 73/100
Train loss: 4.4988, val loss: 1.9434
Epoch: 74/100
Train loss: 4.5676, val loss: 1.9319
Epoch: 75/100
Train loss: 4.6256, val loss: 1.9594
Epoch: 76/100
Train loss: 4.5022, val loss: 1.9139
Epoch: 77/100
Train loss: 4.5705, val loss: 1.9283
Model improve: 0.8900 -> 0.8904
Epoch: 78/100
Train loss: 4.5028, val loss: 1.9689
Model improve: 0.8904 -> 0.8911
Epoch: 79/100
Train loss: 4.6395, val loss: 1.8980
Model improve: 0.8911 -> 0.8924
Epoch: 80/100
Train loss: 4.4494, val loss: 1.8988
Epoch: 81/100
Train loss: 4.6200, val loss: 1.9396
Epoch: 82/100
Train loss: 4.6788, val loss: 1.9006
Epoch: 83/100
Train loss: 4.5383, val loss: 1.9387
Epoch: 84/100
Train loss: 4.5559, val loss: 1.8988
Model improve: 0.8924 -> 0.8924
Epoch: 85/100
Train loss: 4.5112, val loss: 1.9093
Epoch: 86/100
Train loss: 4.4672, val loss: 1.9232
Epoch: 87/100
Train loss: 4.4397, val loss: 1.8860
Epoch: 88/100
Train loss: 4.5774, val loss: 1.8870
Model improve: 0.8924 -> 0.8932
Epoch: 89/100
Train loss: 4.5247, val loss: 1.8902
Epoch: 90/100
Train loss: 4.6037, val loss: 1.9235
Epoch: 91/100
Train loss: 4.4302, val loss: 1.8669
Model improve: 0.8932 -> 0.8935
Epoch: 92/100
Train loss: 4.4284, val loss: 1.9047
Epoch: 93/100
Train loss: 4.5711, val loss: 1.8914
Epoch: 94/100
Train loss: 4.4388, val loss: 1.8804
Epoch: 95/100
Train loss: 4.6262, val loss: 1.8875
Epoch: 96/100
Train loss: 4.5706, val loss: 1.8827
Model improve: 0.8935 -> 0.8938
Epoch: 97/100
Train loss: 4.4452, val loss: 1.9095
Epoch: 98/100
Train loss: 4.4961, val loss: 1.8796
Epoch: 99/100
Train loss: 4.7311, val loss: 1.8802
Epoch: 100/100
Train loss: 4.5249, val loss: 1.8820
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 10.4590, val loss: 7.7227
Model improve: 0.0000 -> 0.5422
Epoch: 2/100
Date :04/05/2023, 14:24:12
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
No noise injections
Date :04/05/2023, 14:24:39
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 200
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 9.8884, val loss: 6.5733
Model improve: 0.0000 -> 0.5951
Epoch: 2/200
Train loss: 7.5052, val loss: 4.9373
Model improve: 0.5951 -> 0.6779
Epoch: 3/200
Train loss: 6.2628, val loss: 4.3941
Model improve: 0.6779 -> 0.7299
Epoch: 4/200
Train loss: 5.7127, val loss: 3.8552
Model improve: 0.7299 -> 0.7585
Epoch: 5/200
Train loss: 5.3678, val loss: 3.6471
Model improve: 0.7585 -> 0.7772
Epoch: 6/200
Train loss: 5.0093, val loss: 3.5629
Model improve: 0.7772 -> 0.7858
Epoch: 7/200
Train loss: 4.6179, val loss: 3.4101
Model improve: 0.7858 -> 0.7991
Epoch: 8/200
Train loss: 4.4724, val loss: 3.1758
Model improve: 0.7991 -> 0.8137
Epoch: 9/200
Train loss: 4.5469, val loss: 3.2956
Epoch: 10/200
Train loss: 4.2521, val loss: 3.0791
Model improve: 0.8137 -> 0.8161
Epoch: 11/200
Train loss: 4.1379, val loss: 3.2777
Model improve: 0.8161 -> 0.8164
Epoch: 12/200
Train loss: 4.0372, val loss: 3.0113
Model improve: 0.8164 -> 0.8233
Epoch: 13/200
Train loss: 3.8924, val loss: 3.0701
Model improve: 0.8233 -> 0.8238
Epoch: 14/200
Train loss: 3.6877, val loss: 3.0381
Model improve: 0.8238 -> 0.8267
Epoch: 15/200
Train loss: 3.7910, val loss: 2.9245
Model improve: 0.8267 -> 0.8293
Epoch: 16/200
Train loss: 3.6529, val loss: 2.9736
Model improve: 0.8293 -> 0.8325
Epoch: 17/200
Train loss: 3.4922, val loss: 2.8758
Model improve: 0.8325 -> 0.8351
Epoch: 18/200
Train loss: 3.5187, val loss: 2.8696
Model improve: 0.8351 -> 0.8395
Epoch: 19/200
Train loss: 3.5385, val loss: 3.0662
Epoch: 20/200
Train loss: 3.5849, val loss: 2.8505
Epoch: 21/200
Train loss: 3.5052, val loss: 3.0032
Epoch: 22/200
Train loss: 3.2890, val loss: 2.8379
Model improve: 0.8395 -> 0.8410
Epoch: 23/200
Train loss: 3.1742, val loss: 2.9594
Epoch: 24/200
Train loss: 3.3452, val loss: 2.8749
Model improve: 0.8410 -> 0.8415
Epoch: 25/200
Train loss: 3.2204, val loss: 2.8540
Epoch: 26/200
Train loss: 3.1578, val loss: 2.8083
Model improve: 0.8415 -> 0.8423
Epoch: 27/200
Train loss: 3.1693, val loss: 2.7389
Model improve: 0.8423 -> 0.8513
Epoch: 28/200
Train loss: 3.1589, val loss: 2.8224
Epoch: 29/200
Train loss: 3.0650, val loss: 2.7562
Model improve: 0.8513 -> 0.8516
Epoch: 30/200
Train loss: 2.9569, val loss: 2.8504
Epoch: 31/200
Train loss: 3.0269, val loss: 2.8371
Epoch: 32/200
Train loss: 3.0035, val loss: 2.7734
Epoch: 33/200
Train loss: 2.8036, val loss: 2.7036
Epoch: 34/200
Train loss: 2.7959, val loss: 2.8078
Epoch: 35/200
Train loss: 3.0027, val loss: 2.8384
Epoch: 36/200
Train loss: 2.7866, val loss: 2.6886
Epoch: 37/200
Train loss: 2.9841, val loss: 2.7714
Epoch: 38/200
Train loss: 2.8558, val loss: 2.7499
Epoch: 39/200
Train loss: 2.8344, val loss: 2.7440
Epoch: 40/200
Train loss: 2.8789, val loss: 2.6322
Model improve: 0.8516 -> 0.8536
Epoch: 41/200
Train loss: 2.6256, val loss: 2.7130
Epoch: 42/200
Train loss: 2.9100, val loss: 2.6525
Epoch: 43/200
Train loss: 2.7855, val loss: 2.8000
Epoch: 44/200
Train loss: 2.8387, val loss: 2.7965
Epoch: 45/200
Train loss: 2.8446, val loss: 2.7488
Epoch: 46/200
Train loss: 2.5517, val loss: 2.6241
Epoch: 47/200
Train loss: 2.6898, val loss: 2.5406
Model improve: 0.8536 -> 0.8565
Epoch: 48/200
Train loss: 2.8752, val loss: 2.6839
Epoch: 49/200
Train loss: 2.6800, val loss: 2.6655
Epoch: 50/200
Train loss: 2.6413, val loss: 2.6788
Epoch: 51/200
Train loss: 2.6295, val loss: 2.7420
Epoch: 52/200
Train loss: 2.7445, val loss: 2.6877
Epoch: 53/200
Train loss: 2.5641, val loss: 2.6289
Model improve: 0.8565 -> 0.8591
Epoch: 54/200
Train loss: 2.4421, val loss: 2.7460
Epoch: 55/200
Train loss: 2.5520, val loss: 2.6832
Epoch: 56/200
Train loss: 2.5408, val loss: 2.7073
Epoch: 57/200
Train loss: 2.6612, val loss: 2.7393
Epoch: 58/200
Train loss: 2.6378, val loss: 2.7819
Epoch: 59/200
Train loss: 2.5073, val loss: 2.7543
Epoch: 60/200
Train loss: 2.6852, val loss: 2.6622
Epoch: 61/200
Train loss: 2.4543, val loss: 2.6622
Epoch: 62/200
Train loss: 2.5598, val loss: 2.6020
Epoch: 63/200
Train loss: 2.4692, val loss: 2.6958
Epoch: 64/200
Train loss: 2.5324, val loss: 2.6049
Epoch: 65/200
Train loss: 2.5565, val loss: 2.6199
Epoch: 66/200
Train loss: 2.5377, val loss: 2.5839
Epoch: 67/200
Train loss: 2.5172, val loss: 2.6203
Epoch: 68/200
Train loss: 2.3096, val loss: 2.5830
Epoch: 69/200
Train loss: 2.5892, val loss: 2.6482
Epoch: 70/200
Train loss: 2.3622, val loss: 2.6938
Epoch: 71/200
Train loss: 2.3530, val loss: 2.6274
Epoch: 72/200
Train loss: 2.4442, val loss: 2.6449
Epoch: 73/200
Train loss: 2.1986, val loss: 2.5589
Epoch: 74/200
Train loss: 2.3538, val loss: 2.6370
Epoch: 75/200
Train loss: 2.4558, val loss: 2.6275
Epoch: 76/200
Train loss: 2.3007, val loss: 2.6075
Epoch: 77/200
Train loss: 2.3514, val loss: 2.6943
Epoch: 78/200
Train loss: 2.2812, val loss: 2.5836
Model improve: 0.8591 -> 0.8593
Epoch: 79/200
Train loss: 2.4096, val loss: 2.5105
Model improve: 0.8593 -> 0.8617
Epoch: 80/200
Train loss: 2.1615, val loss: 2.8390
Epoch: 81/200
Train loss: 2.3322, val loss: 2.6360
Epoch: 82/200
Train loss: 2.4831, val loss: 2.5649
Epoch: 83/200
Train loss: 2.2842, val loss: 2.8599
Epoch: 84/200
Train loss: 2.3176, val loss: 2.5953
Epoch: 85/200
Train loss: 2.2251, val loss: 2.5850
Epoch: 86/200
Train loss: 2.2139, val loss: 2.5453
Model improve: 0.8617 -> 0.8625
Epoch: 87/200
Train loss: 2.1386, val loss: 2.6059
Epoch: 88/200
Train loss: 2.3356, val loss: 2.5396
Epoch: 89/200
Train loss: 2.2418, val loss: 2.5973
Epoch: 90/200
Train loss: 2.3441, val loss: 2.5718
Model improve: 0.8625 -> 0.8638
Epoch: 91/200
Train loss: 2.1139, val loss: 2.6432
Epoch: 92/200
Train loss: 2.0993, val loss: 2.6462
Epoch: 93/200
Train loss: 2.2740, val loss: 2.6055
Epoch: 94/200
Train loss: 2.1627, val loss: 2.5940
Epoch: 95/200
Train loss: 2.3009, val loss: 2.5171
Epoch: 96/200
Train loss: 2.2329, val loss: 2.5387
Epoch: 97/200
Train loss: 2.1236, val loss: 2.6048
Epoch: 98/200
Train loss: 2.1674, val loss: 2.5509
Epoch: 99/200
Train loss: 2.3850, val loss: 2.5996
Epoch: 100/200
Train loss: 2.1526, val loss: 2.6074
Epoch: 101/200
Train loss: 2.0298, val loss: 2.7490
Epoch: 102/200
Train loss: 2.0416, val loss: 2.6320
Epoch: 103/200
Train loss: 2.3993, val loss: 2.6596
Epoch: 104/200
Train loss: 2.2135, val loss: 2.5965
Epoch: 105/200
Train loss: 1.9368, val loss: 2.5705
Epoch: 106/200
Train loss: 2.2547, val loss: 2.6294
Epoch: 107/200
Train loss: 2.1624, val loss: 2.5876
Epoch: 108/200
Train loss: 2.0432, val loss: 2.5150
Epoch: 109/200
Train loss: 2.0685, val loss: 2.5583
Epoch: 110/200
Train loss: 2.0167, val loss: 2.5296
Epoch: 111/200
Train loss: 2.2179, val loss: 2.5412
Epoch: 112/200
Train loss: 2.0695, val loss: 2.5555
Epoch: 113/200
Train loss: 2.0751, val loss: 2.6652
Epoch: 114/200
Train loss: 2.1016, val loss: 2.5186
Epoch: 115/200
Train loss: 1.9223, val loss: 2.5304
Epoch: 116/200
Train loss: 2.1901, val loss: 2.5478
Epoch: 117/200
Train loss: 2.1007, val loss: 2.4060
Model improve: 0.8638 -> 0.8667
Epoch: 118/200
Train loss: 1.9721, val loss: 2.5175
Epoch: 119/200
Train loss: 2.0410, val loss: 2.6063
Epoch: 120/200
Train loss: 1.9571, val loss: 2.4313
Model improve: 0.8667 -> 0.8678
Epoch: 121/200
Train loss: 2.1181, val loss: 2.5444
Epoch: 122/200
Train loss: 2.0212, val 123/200
Train loss: 2.1671, val loss: 2.5790
Epoch: 124/200
Train loss: 2.0472, val loss: 2.5793
Epoch: 125/200
Train loss: 1.9906, val loss: 2.5647
Epoch: 126/200
Train loss: 2.0331, val loss: 2.5751
Epoch: 127/200
Train loss: 1.9048, val loss: 2.6768
Epoch: 128/200
Train loss: 2.1103, val loss: 2.5008
Epoch: 129/200
Train loss: 1.8378, val loss: 2.5876
Epoch: 130/200
Train loss: 1.8867, val loss: 2.5101
Epoch: 131/200
Train loss: 1.9579, val loss: 2.4794
Epoch: 132/200
Train loss: 1.9809, val loss: 2.5324
Epoch: 133/200
Date :04/06/2023, 00:52:49
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 50
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 3
use_spec_augmenter: False
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/50
Train loss: 9.8058, val loss: 6.5331
Model improve: 0.0000 -> 0.6000
Epoch: 2/50
Train loss: 7.3948, val loss: 4.9518
Model improve: 0.6000 -> 0.6886
Epoch: 3/50
Train loss: 6.1908, val loss: 4.2247
Model improve: 0.6886 -> 0.7334
Epoch: 4/50
Train loss: 5.6454, val loss: 3.9572
Model improve: 0.7334 -> 0.7598
Epoch: 5/50
Train loss: 5.3188, val loss: 3.6685
Model improve: 0.7598 -> 0.7823
Epoch: 6/50
Train loss: 4.9743, val loss: 3.5458
Model improve: 0.7823 -> 0.7920
Epoch: 7/50
Train loss: 4.5465, val loss: 3.3408
Model improve: 0.7920 -> 0.8066
Epoch: 8/50
Train loss: 4.4320, val loss: 3.0766
Model improve: 0.8066 -> 0.8112
Epoch: 9/50
Train loss: 4.4597, val loss: 3.1507
Model improve: 0.8112 -> 0.8191
Epoch: 10/50
Train loss: 4.1644, val loss: 3.0491
Model improve: 0.8191 -> 0.8202
Epoch: 11/50
Train loss: 4.0184, val loss: 3.0788
Model improve: 0.8202 -> 0.8247
Epoch: 12/50
Train loss: 3.9469, val loss: 2.9362
Model improve: 0.8247 -> 0.8299
Epoch: 13/50
Train loss: 3.7980, val loss: 3.0048
Epoch: 14/50
Train loss: 3.6176, val loss: 2.8872
Model improve: 0.8299 -> 0.8321
Epoch: 15/50
Train loss: 3.7015, val loss: 2.7960
Model improve: 0.8321 -> 0.8372
Epoch: 16/50
Train loss: 3.5467, val loss: 2.8651
Model improve: 0.8372 -> 0.8381
Epoch: 17/50
Train loss: 3.3664, val loss: 2.8829
Epoch: 18/50
Train loss: 3.4248, val loss: 2.8159
Model improve: 0.8381 -> 0.8394
Epoch: 19/50
Train loss: 3.4089, val loss: 2.8370
Model improve: 0.8394 -> 0.8461
Epoch: 20/50
Train loss: 3.4431, val loss: 2.7862
Epoch: 21/50
Train loss: 3.3279, val loss: 2.7694
Model improve: 0.8461 -> 0.8470
Epoch: 22/50
Train loss: 3.1113, val loss: 2.7566
Epoch: 23/50
Train loss: 2.9687, val loss: 2.6463
Model improve: 0.8470 -> 0.8495
Epoch: 24/50
Train loss: 3.1589, val loss: 2.6512
Epoch: 25/50
                    Train loss: 3.0164, val loss: 2.6620
Epoch: 26/50
Train loss: 2.9136, val loss: 2.5491
Model improve: 0.8495 -> Train loss: 2.9453, val loss: 2.6628
Epoch: 28/50
Train loss: 2.9217, val loss: 2.5206
Model improve: 0.8528 -> 0.8575
Epoch: 29/50
Train loss: 2.7859, val loss: 2.4625
Model improve: 0.8575 -> 0.8593
Epoch: 30/50
Train loss: 2.6888, val loss: 2.5148
Epoch: 31/50
Train loss: 2.7369, val loss: 2.5409
Epoch: 32/50
Train loss: 2.7386, val loss: 2.5454
Epoch: 33/50
Train loss: 2.4962, val loss: Train loss: 2.5161, val loss: 2.5026
Epoch: 35/50
Train loss: 2.7061, val loss: 2.5705
Epoch: 36/50
Train loss: 2.4794, val loss: 2.4274
Model improve: 0.8593 -> 0.8617
Epoch: 37/50
Train loss: 2.6719, val loss: 2.4955
Epoch: 38/50
Train loss: 2.5668, val loss: 2.4543
Epoch: 39/50
Train loss: 2.5115, val loss: 2.4501
Epoch: 40/50
Train loss: 2.5392, val loss: 2.4113
Epoch: 41/50
Train loss: 2.3012, val loss: 2.4843
Epoch: 42/50
Train loss: 2.5809, val loss: 2.3653
Model improve: 0.8617 -> 0.8619
Epoch: 43/50
Train loss: 2.5060, val loss: 2.4515
Epoch: 44/50
Train loss: 2.5395, val loss: 2.4394
Epoch: 45/50
Train loss: 2.5628, val loss: 2.4215
Epoch: 46/50
Train loss: 2.2931, val loss: 2.4942
Epoch: 47/50
Date :04/11/2023, 03:06:41
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 50
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 3
use_spec_augmenter: False
Date :04/11/2023, 04:04:22
Date :04/11/2023, 04:04:22
Duration: 5
Duration: 5
Sample rate: 32000
Sample rate: 32000
nfft: 2048
nfft: 2048
fmin: 50
fmin: 50
nmels: 128
nmels: 128
fmax: 16000
fmax: 16000
trainbs: 64
trainbs: 64
validbs: 256
validbs: 256
epochwarmup: 0
epochwarmup: 0
totalepoch: 50
totalepoch: 50
learningrate: 0.0004
learningrate: 0.0004
weightdecay: 0.01
weightdecay: 0.01
thrupsample: 50
thrupsample: 50
model_name: tf_efficientnetv2_s
model_name: tf_efficientnetv2_s
mix_up: 0.2
mix_up: 0.2
hop_length: 256
hop_length: 256
train_with_mixup: True
train_with_mixup: True
num_channels: 1
num_channels: 1
use_spec_augmenter: False
use_spec_augmenter: False
Fold: 0
Fold: 0
19577
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/50
Epoch: 1/50
Fold: 0
Fold: 0
19577
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/50
Epoch: 1/50
Date :04/11/2023, 04:28:36
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 50
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
Date :04/12/2023, 22:26:12
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 9.6226, val loss: 5.8646
0.6225078453435574
Model improve: 0.0000 -> 0.6225
Epoch: 2/100
Date :04/13/2023, 02:34:05
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 9.8505, val loss: 6.1600
0.6052253235667585
Model improve: 0.0000 -> 0.6052
Epoch: 2/100
alse
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 7.3811, val loss: 4.3015
0.7080418637869482
Model improve: 0.6052 -> 0.7080
Epoch: 3/100
Date :04/13/2023, 02:40:55
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 9.6598, val loss: 6.0180
0.6370715698789069
Model improve: 0.0000 -> 0.6371
Epoch: 2/100
Date :04/13/2023, 02:47:53
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 9.5629, val loss: 5.7350
0.6476635105143511
Model improve: 0.0000 -> 0.6477
Epoch: 2/100
Train loss: 7.3754, val loss: 4.4227
0.7399123460313594
Model improve: 0.6477 -> 0.7399
Epoch: 3/100
Train loss: 6.4821, val loss: 3.8035
0.7713371057700626
Model improve: 0.7399 -> 0.7713
Epoch: 4/100
Date :04/13/2023, 03:02:28
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 14000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 9.5152, val loss: 5.6609
0.6524322872149226
Model improve: 0.0000 -> 0.6524
Epoch: 2/100
Train loss: 7.0404, val loss: 4.2313
0.7433061530133344
Model improve: 0.6524 -> 0.7433
Epoch: 3/100
Train loss: 6.0295, val loss: 3.5867
0.7858577455635821
Model improve: 0.7433 -> 0.7859
Epoch: 4/100
Train loss: 5.3055, val loss: 3.1812
0.8085883917533481
Model improve: 0.7859 -> 0.8086
Epoch: 5/100
Train loss: 5.0870, val loss: 2.9625
0.8282264209663471
Model improve: 0.8086 -> 0.8282
Epoch: 6/100
Train loss: 4.7322, val loss: 2.8464
0.8336323771304361
Model improve: 0.8282 -> 0.8336
Epoch: 7/100
Train loss: 4.4587, val loss: 2.8395
0.8410510053847094
Model improve: 0.8336 -> 0.8411
Epoch: 8/100
Train loss: 4.3637, val loss: 2.5474
0.84997702488677
Model improve: 0.8411 -> 0.8500
Epoch: 9/100
Train loss: 4.1392, val loss: 2.6152
0.8555501169717721
Model improve: 0.8500 -> 0.8556
Epoch: 10/100
Train loss: 4.1692, val loss: 2.6093
0.8636482603049541
Model improve: 0.8556 -> 0.8636
Epoch: 11/100
Train loss: 3.9656, val loss: 2.4361
0.8665776004567944
Model improve: 0.8636 -> 0.8666
Epoch: 12/100
Train loss: 3.8523, val loss: 2.3909
0.8687790798095744
Model improve: 0.8666 -> 0.8688
Epoch: 13/100
Train loss: 3.7340, val loss: 2.3052
0.8706134271706465
Model improve: 0.8688 -> 0.8706
Epoch: 14/100
Train loss: 3.6775, val loss: 2.3123
0.8671177658061319
Epoch: 15/100
Train loss: 3.5580, val loss: 2.4867
0.8625394967205869
Epoch: 16/100
Train loss: 3.5283, val loss: 2.3013
0.8732021681636247
Model improve: 0.8706 -> 0.8732
Epoch: 17/100
Train loss: 3.3298, val loss: 2.2968
0.8759285969116877
Model improve: 0.8732 -> 0.8759
Epoch: 18/100
Train loss: 3.4361, val loss: 2.2879
0.8757403242585857
Epoch: 19/100
Train loss: 3.3957, val loss: 2.2319
0.874056601714156
Epoch: 20/100
Train loss: 3.3389, val loss: 2.2319
0.8734897851445691
Epoch: 21/100
Train loss: 3.2517, val loss: 2.3114
0.8711755648046717
Epoch: 22/100
Train loss: 3.2849, val loss: 2.1690
0.877272766045107
Model improve: 0.8759 -> 0.8773
Epoch: 23/100
Train loss: 3.1912, val loss: 2.1729
0.8776105373541214
Model improve: 0.8773 -> 0.8776
Epoch: 24/100
Train loss: 3.2629, val loss: 2.1319
0.88075756854293
Model improve: 0.8776 -> 0.8808
Epoch: 25/100
Train loss: 3.1307, val loss: 2.2664
0.8769175710597275
Epoch: 26/100
Train loss: 3.0850, val loss: 2.1287
0.8786522962124379
Epoch: 27/100
Train loss: 2.9959, val loss: 2.1845
0.8759958917554154
Epoch: 28/100
Train loss: 2.9928, val loss: 2.1706
0.8790661624443934
Epoch: 29/100
Train loss: 3.0674, val loss: 2.2328
0.8737745569282201
Epoch: 30/100
Train loss: 3.0142, val loss: 2.0706
0.8856976654314758
Model improve: 0.8808 -> 0.8857
Epoch: 31/100
Train loss: 2.9282, val loss: 2.1305
0.8829174517961275
Epoch: 32/100
Train loss: 2.9373, val loss: 2.0392
0.8845601529448357
Epoch: 33/100
Train loss: 3.0027, val loss: 2.1423
0.882466293539329
Epoch: 34/100
Train loss: 2.8538, val loss: 2.1034
0.8821358909778959
Epoch: 35/100
Train loss: 2.8451, val loss: 2.0542
0.8834614850096032
Epoch: 36/100
Train loss: 2.8430, val loss: 2.1083
0.8817162250232234
Epoch: 37/100
Train loss: 2.7016, val loss: 2.0172
0.885624880911909
Epoch: 38/100
Train loss: 2.7429, val loss: 1.9783
0.8869471434175972
Model improve: 0.8857 -> 0.8869
Epoch: 39/100
Train loss: 2.6687, val loss: 1.9804
0.8883708857757353
Model improve: 0.8869 -> 0.8884
Epoch: 40/100
Train loss: 2.7175, val loss: 2.0266
0.8836661286701131
Epoch: 41/100
Train loss: 2.8070, val loss: 2.0054
0.8850064324732717
Epoch: 42/100
Train loss: 2.6863, val loss: 2.0703
0.8826734570582099
Epoch: 43/100
Train loss: 2.6141, val loss: 2.0113
0.8876038810742486
Epoch: 44/100
Train loss: 2.7137, val loss: 1.9316
0.8909073296704839
Model improve: 0.8884 -> 0.8909
Epoch: 45/100
Train loss: 2.6219, val loss: 1.9665
0.8882295285626766
Epoch: 46/100
Train loss: 2.4888, val loss: 1.9219
0.8903852231479936
Epoch: 47/100
Train loss: 2.6097, val loss: 1.9697
0.8863093633876238
Epoch: 48/100
Train loss: 2.6322, val loss: 1.9513
0.8884565239998959
Epoch: 49/100
Train loss: 2.5367, val loss: 2.0244
0.8875866263479695
Epoch: 50/100
Train loss: 2.6369, val loss: 1.9764
0.8870392770998307
Epoch: 51/100
Train loss: 2.3991, val loss: 2.0480
0.8869787330889162
Epoch: 52/100
Train loss: 2.7161, val loss: 1.8903
0.8914483284940818
Model improve: 0.8909 -> 0.8914
Epoch: 53/100
Train loss: 2.4140, val loss: 1.8919
0.8913469847167814
Epoch: 54/100
Train loss: 2.4734, val loss: 1.8989
0.8900063759112253
Epoch: 55/100
Train loss: 2.4064, val loss: 1.9221
0.8911088125842408
Epoch: 56/100
Date :04/13/2023, 07:06:51
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 14000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :04/13/2023, 07:07:52
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 14000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
Date :04/13/2023, 07:08:05
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 14000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 9.2699, val loss: 5.5534
0.6598419114164347
Model improve: 0.0000 -> 0.6598
Epoch: 2/100
Train loss: 6.7147, val loss: 4.1126
0.7477467550596991
Model improve: 0.6598 -> 0.7477
Epoch: 3/100
Train loss: 5.8590, val loss: 3.4719
0.7918311811069085
Model improve: 0.7477 -> 0.7918
Epoch: 4/100
Train loss: 5.0971, val loss: 3.2519
0.8052703719594796
Model improve: 0.7918 -> 0.8053
Epoch: 5/100
Train loss: 4.8816, val loss: 3.0371
0.8235971572086289
Model improve: 0.8053 -> 0.8236
Epoch: 6/100
Train loss: 4.5194, val loss: 2.9653
0.830616915802992
Model improve: 0.8236 -> 0.8306
Epoch: 7/100
Train loss: 4.2103, val loss: 2.7258
0.8453750188792986
Model improve: 0.8306 -> 0.8454
Epoch: 8/100
Train loss: 4.1049, val loss: 2.6568
0.8493534922342798
Model improve: 0.8454 -> 0.8494
Epoch: 9/100
Train loss: 3.9074, val loss: 2.5858
0.8550146579660591
Model improve: 0.8494 -> 0.8550
Epoch: 10/100
Train loss: 3.9504, val loss: 2.6831
0.8612964647808752
Model improve: 0.8550 -> 0.8613
Epoch: 11/100
Train loss: 3.7443, val loss: 2.4950
0.8604433323742096
Epoch: 12/100
Train loss: 3.6079, val loss: 2.3934
0.8663383103351409
Model improve: 0.8613 -> 0.8663
Epoch: 13/100
Train loss: 3.5108, val loss: 2.4809
0.8605325560730062
Epoch: 14/100
Train loss: 3.4655, val loss: 2.3707
0.866031817834571
Epoch: 15/100
Train loss: 3.3421, val loss: 2.3327
0.869147183266645
Model improve: 0.8663 -> 0.8691
Epoch: 16/100
Train loss: 3.3348, val loss: 2.3840
0.864015524288499
Epoch: 17/100
Train loss: 3.1206, val loss: 2.3253
0.8673761269136565
Epoch: 18/100
Train loss: 3.2062, val loss: 2.3252
0.8724773959270138
Model improve: 0.8691 -> 0.8725
Epoch: 19/100
Train loss: 3.1623, val loss: 2.3103
0.869323185238046
Epoch: 20/100
Train loss: 3.1433, val loss: 2.2284
0.8733644796262469
Model improve: 0.8725 -> 0.8734
Epoch: 21/100
Date :04/13/2023, 08:27:43
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 14000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 9.7467, val loss: 6.2735
0.6234686383013124
Model improve: 0.0000 -> 0.6235
Epoch: 2/100
Train loss: 7.4447, val loss: 4.5042
0.7277009774843881
Model improve: 0.6235 -> 0.7277
Epoch: 3/100
Date :04/13/2023, 08:38:31
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 14000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 9.3519, val loss: 5.4487
0.6680674221391723
Model improve: 0.0000 -> 0.6681
Epoch: 2/100
Train loss: 6.9146, val loss: 4.0278
0.7520490691506934
Model improve: 0.6681 -> 0.7520
Epoch: 3/100
Train loss: 5.9895, val loss: 3.6317
0.7875402943052667
Model improve: 0.7520 -> 0.7875
Epoch: 4/100
Train loss: 5.3004, val loss: 3.1688
0.815813085436923
Model improve: 0.7875 -> 0.8158
Epoch: 5/100
Train loss: 5.0823, val loss: 2.9557
0.8259364723619675
Model improve: 0.8158 -> 0.8259
Epoch: 6/100
Train loss: 4.7128, val loss: 2.8198
0.8349899836218757
Model improve: 0.8259 -> 0.8350
Epoch: 7/100
Train loss: 4.4095, val loss: 2.7851
0.8454452372930192
Model improve: 0.8350 -> 0.8454
Epoch: 8/100
Train loss: 4.2923, val loss: 2.6814
0.8481139935620033
Model improve: 0.8454 -> 0.8481
Epoch: 9/100
Date :04/13/2023, 09:12:44
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 14000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: eca_nfnet_l0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 10.6762, val loss: 9.7079
0.48217037997666223
Model improve: 0.0000 -> 0.4822
Epoch: 2/100
Train loss: 10.1800, val loss: 8.7294
0.5011555409735933
Model improve: 0.4822 -> 0.5012
Epoch: 3/100
Train loss: 9.4797, val loss: 7.4809
0.5507934044957916
Model improve: 0.5012 -> 0.5508
Epoch: 4/100
Train loss: 8.3971, val loss: 5.9988
0.6123871193712698
Model improve: 0.5508 -> 0.6124
Epoch: 5/100
Train loss: 7.6080, val loss: 5.2300
0.6633051395361049
Model improve: 0.6124 -> 0.6633
Epoch: 6/100
Train loss: 6.8899, val loss: 4.6883
0.7022066437379915
Model improve: 0.6633 -> 0.7022
Epoch: 7/100
Train loss: 6.2915, val loss: 4.0132
0.7461268600727352
Model improve: 0.7022 -> 0.7461
Epoch: 8/100
Train loss: 5.9228, val loss: 3.8689
0.766927155981328
Model improve: 0.7461 -> 0.7669
Epoch: 9/100
Train loss: 5.5100, val loss: 3.4943
0.7855957331684563
Model improve: 0.7669 -> 0.7856
Epoch: 10/100
Train loss: 5.3902, val loss: 3.4122
0.8007187525883437
Model improve: 0.7856 -> 0.8007
Epoch: 11/100
Train loss: 5.0354, val loss: 3.0459
0.8146378455038044
Model improve: 0.8007 -> 0.8146
Epoch: 12/100
Train loss: 4.8847, val loss: 3.0341
0.823300980427288
Model improve: 0.8146 -> 0.8233
Epoch: 13/100
Train loss: 4.7051, val loss: 2.9045
0.8313278279497732
Model improve: 0.8233 -> 0.8313
Epoch: 14/100
Train loss: 4.5699, val loss: 2.7336
0.8382999937835514
Model improve: 0.8313 -> 0.8383
Epoch: 15/100
Train loss: 4.3691, val loss: 2.7077
0.8457026401714928
Model improve: 0.8383 -> 0.8457
Epoch: 16/100
Train loss: 4.4350, val loss: 2.6336
0.8429706399820954
Epoch: 17/100
Train loss: 4.0695, val loss: 2.5411
0.8541875371381729
Model improve: 0.8457 -> 0.8542
Epoch: 18/100
Train loss: 4.1167, val loss: 2.5011
0.8536071261741487
Epoch: 19/100
Train loss: 4.0699, val loss: 2.5143
0.8531508210066668
Epoch: 20/100
Train loss: 3.9678, val loss: 2.4949
0.8577995191642069
Model improve: 0.8542 -> 0.8578
Epoch: 21/100
Train loss: 3.8694, val loss: 2.4596
0.8606459599356354
Model improve: 0.8578 -> 0.8606
Epoch: 22/100
Train loss: 3.8646, val loss: 2.4234
0.8602927578751808
Epoch: 23/100
Train loss: 3.7976, val loss: 2.3171
0.8699368995205076
Model improve: 0.8606 -> 0.8699
Epoch: 24/100
Train loss: 3.8254, val loss: 2.4287
0.8643231905866402
Epoch: 25/100
Train loss: 3.6766, val loss: 2.3127
0.870906792719401
Model improve: 0.8699 -> 0.8709
Epoch: 26/100
Train loss: 3.6039, val loss: 2.2712
0.8661899245154969
Epoch: 27/100
Train loss: 3.4876, val loss: 2.3414
0.8678362456207911
Epoch: 28/100
Train loss: 3.4798, val loss: 2.2659
0.8714892515293111
Model improve: 0.8709 -> 0.8715
Epoch: 29/100
Train loss: 3.4974, val loss: 2.2784
0.8745799587281445
Model improve: 0.8715 -> 0.8746
Epoch: 30/100
Train loss: 3.5120, val loss: 2.1981
0.8732366066203988
Epoch: 31/100
Train loss: 3.3534, val loss: 2.2251
0.8723590922551706
Epoch: 32/100
Train loss: 3.3454, val loss: 2.2322
0.874461459939719
Epoch: 33/100
Train loss: 3.4277, val loss: 2.2248
0.8735601982385262
Epoch: 34/100
Date :04/13/2023, 11:07:41
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 14000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.3
hop_length: 320
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 9.6263, val loss: 5.9496
0.6415449972911837
Model improve: 0.0000 -> 0.6415
Epoch: 2/100
Train loss: 7.3456, val loss: 4.2227
0.7427415987236148
Model improve: 0.6415 -> 0.7427
Epoch: 3/100
Date :04/13/2023, 11:15:35
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 64
fmax: 14000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.3
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 9.8593, val loss: 6.2681
0.6216250853653009
Model improve: 0.0000 -> 0.6216
Epoch: 2/100
Train loss: 7.6328, val loss: 4.6576
0.7170664154178751
Model improve: 0.6216 -> 0.7171
Epoch: 3/100
Date :04/13/2023, 11:22:58
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 224
fmax: 14000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.3
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 9.3421, val loss: 5.6099
0.659937677857849
Model improve: 0.0000 -> 0.6599
Epoch: 2/100
Train loss: 7.0343, val loss: 4.0474
0.7523827221795746
Model improve: 0.6599 -> 0.7524
Epoch: 3/100
Train loss: 6.1087, val loss: 3.5375
0.7967768561640357
Model improve: 0.7524 -> 0.7968
Epoch: 4/100
Train loss: 5.4023, val loss: 3.0947
0.8139172653995262
Model improve: 0.7968 -> 0.8139
Epoch: 5/100
Train loss: 5.1581, val loss: 3.0634
0.8286011173745125
Model improve: 0.8139 -> 0.8286
Epoch: 6/100
Train loss: 4.9603, val loss: 2.9485
0.8402663035018462
Model improve: 0.8286 -> 0.8403
Epoch: 7/100
Train loss: 4.6864, val loss: 2.8536
0.8452155411949309
Model improve: 0.8403 -> 0.8452
Epoch: 8/100
Train loss: 4.4466, val loss: 2.6638
0.8481817104412152
Model improve: 0.8452 -> 0.8482
Epoch: 9/100
Train loss: 4.3488, val loss: 2.5213
0.8540755649083995
Model improve: 0.8482 -> 0.8541
Epoch: 10/100
Train loss: 4.3551, val loss: 2.7693
0.8573120338953555
Model improve: 0.8541 -> 0.8573
Epoch: 11/100
Train loss: 4.0664, val loss: 2.5618
0.8582464687900866
Model improve: 0.8573 -> 0.8582
Epoch: 12/100
Train loss: 4.2249, val loss: 2.4841
0.8676402657665043
Model improve: 0.8582 -> 0.8676
Epoch: 13/100
Train loss: 3.9671, val loss: 2.3618
0.8667976409572681
Epoch: 14/100
Train loss: 3.8661, val loss: 2.2850
0.8686843007662699
Model improve: 0.8676 -> 0.8687
Epoch: 15/100
Train loss: 3.8581, val loss: 2.4937
0.8694379737265704
Model improve: 0.8687 -> 0.8694
Epoch: 16/100
Date :04/13/2023, 12:34:10
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 14000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :04/13/2023, 12:34:33
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 14000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
0.9018508765670533
Epoch: 2/100
Date :04/13/2023, 12:37:36
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 14000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
0.8708360361308859
Epoch: 2/100
Date :04/13/2023, 12:41:20
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 14000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
0.8708360361308859
Epoch: 2/100
Date :04/13/2023, 12:44:36
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 14000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: True
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 9.6075, val loss: 6.2017
0.6272095369427265
Model improve: 0.0000 -> 0.6272
Epoch: 2/100
Train loss: 7.2665, val loss: 4.7206
0.7134480209218003
Model improve: 0.6272 -> 0.7134
Epoch: 3/100
Date :04/13/2023, 12:56:27
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 14000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: False
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 8.6553, val loss: 5.5226
0.6571587391982286
Model improve: 0.0000 -> 0.6572
Epoch: 2/100
Date :04/13/2023, 13:01:32
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 14000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: False
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :04/13/2023, 13:02:54
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 14000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: False
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
15599
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 9.5794, val loss: 6.7006
0.6377826179204373
Model improve: 0.0000 -> 0.6378
Epoch: 2/100
Train loss: 6.2452, val loss: 5.2013
0.7051609787486405
Model improve: 0.6378 -> 0.7052
Epoch: 3/100
Date :04/13/2023, 13:10:08
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 14000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: False
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 8.5714, val loss: 5.4481
0.6685736380908107
Model improve: 0.0000 -> 0.6686
Epoch: 2/100
Train loss: 5.3312, val loss: 4.4071
0.7336982667511799
Model improve: 0.6686 -> 0.7337
Epoch: 3/100
Date :04/13/2023, 13:17:47
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 14000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 9.3741, val loss: 5.8208
0.6493618755667917
Model improve: 0.0000 -> 0.6494
Epoch: 2/100
Train loss: 6.9401, val loss: 4.4319
0.7348537214259918
Model improve: 0.6494 -> 0.7349
Epoch: 3/100
Train loss: 6.0133, val loss: 4.0367
0.7691688445820564
Model improve: 0.7349 -> 0.7692
Epoch: 4/100
Train loss: 5.2753, val loss: 3.6342
0.789079997077397
Model improve: 0.7692 -> 0.7891
Epoch: 5/100
Train loss: 5.0631, val loss: 3.3115
0.8072429840810379
Model improve: 0.7891 -> 0.8072
Epoch: 6/100
Train loss: 4.7337, val loss: 3.3820
0.8099171590012972
Model improve: 0.8072 -> 0.8099
Epoch: 7/100
Train loss: 4.3969, val loss: 3.2798
0.819008935914425
Model improve: 0.8099 -> 0.8190
Epoch: 8/100
Train loss: 4.3360, val loss: 3.1528
0.8239247893069416
Model improve: 0.8190 -> 0.8239
Epoch: 9/100
Train loss: 4.0859, val loss: 3.0773
0.8286932080400612
Model improve: 0.8239 -> 0.8287
Epoch: 10/100
Train loss: 4.1567, val loss: 3.1855
0.8295794710313501
Model improve: 0.8287 -> 0.8296
Epoch: 11/100
Train loss: 3.9545, val loss: 3.0323
0.8300554545428225
Model improve: 0.8296 -> 0.8301
Epoch: 12/100
Train loss: 3.8515, val loss: 2.9053
0.8414551922474236
Model improve: 0.8301 -> 0.8415
Epoch: 13/100
Train loss: 3.6936, val loss: 2.9325
0.8323836797503271
Epoch: 14/100
Train loss: 3.6577, val loss: 2.7484
0.8446529387995211
Model improve: 0.8415 -> 0.8447
Epoch: 15/100
Train loss: 3.5520, val loss: 2.8606
0.8388798072747811
Epoch: 16/100
Train loss: 3.5283, val loss: 2.7490
0.8450777995043134
Model improve: 0.8447 -> 0.8451
Epoch: 17/100
Train loss: 3.2895, val loss: 2.8504
0.8393641739564672
Epoch: 18/100
Train loss: 3.3989, val loss: 2.8679
0.8416991960140197
Epoch: 19/100
Train loss: 3.3760, val loss: 2.8068
0.843751860422212
Epoch: 20/100
Train loss: 3.3357, val loss: 2.7643
0.8480786134096636
Model improve: 0.8451 -> 0.8481
Epoch: 21/100
Train loss: 3.2498, val loss: 2.7317
0.8449033924585416
Epoch: 22/100
Train loss: 3.2477, val loss: 2.7294
0.846178926566401
Epoch: 23/100
Train loss: 3.1741, val loss: 2.7016
0.849446638952653
Model improve: 0.8481 -> 0.8494
Epoch: 24/100
Train loss: 3.2428, val loss: 2.6278
0.8516165646124706
Model improve: 0.8494 -> 0.8516
Epoch: 25/100
Train loss: 3.1375, val loss: 2.6730
0.8562082851222911
Model improve: 0.8516 -> 0.8562
Epoch: 26/100
Train loss: 3.1133, val loss: 2.6840
0.8486848120852929
Epoch: 27/100
Train loss: 3.0124, val loss: 2.7354
0.8483736720807031
Epoch: 28/100
Train loss: 2.9544, val loss: 2.6601
0.853736679701474
Epoch: 29/100
Train loss: 3.1002, val loss: 2.6494
0.8508075667141494
Epoch: 30/100
Train loss: 3.0150, val loss: 2.5066
0.8601237179036879
Model improve: 0.8562 -> 0.8601
Epoch: 31/100
Train loss: 2.9031, val loss: 2.5986
0.8537205778309153
Epoch: 32/100
Train loss: 2.8979, val loss: 2.5533
0.8568363436130307
Epoch: 33/100
Train loss: 3.0025, val loss: 2.7239
0.8494412555261324
Epoch: 34/100
Train loss: 2.8346, val loss: 2.5669
0.8581753578508541
Epoch: 35/100
Train loss: 2.8523, val loss: 2.6644
0.8530958126750934
Epoch: 36/100
Train loss: 2.8382, val loss: 2.5852
0.8569272877727344
Epoch: 37/100
Train loss: 2.6982, val loss: 2.5510
0.8558328131101016
Epoch: 38/100
Train loss: 2.7528, val loss: 2.5655
0.8573319028741855
Epoch: 39/100
Train loss: 2.6972, val loss: 2.5792
0.8547143822622936
Epoch: 40/100
Train loss: 2.6842, val loss: 2.4491
0.8598853751675536
Epoch: 41/100
Train loss: 2.8028, val loss: 2.5688
0.8520157266639558
Epoch: 42/100
Train loss: 2.6968, val loss: 2.5789
0.8562677971635458
Epoch: 43/100
Date :04/13/2023, 15:59:19
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 14000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13552
Date :04/13/2023, 16:00:02
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 14000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13552
Date :04/13/2023, 16:00:26
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 14000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13552
Date :04/13/2023, 16:05:13
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 14000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13552
Date :04/13/2023, 16:05:20
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 14000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13552
Date :04/13/2023, 16:05:24
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 14000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13552
Date :04/13/2023, 16:05:27
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 14000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13552
Date :04/13/2023, 16:05:33
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 14000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13552
Date :04/13/2023, 16:05:37
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 14000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13552
Date :04/13/2023, 16:08:40
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 14000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13552
Date :04/13/2023, 16:09:23
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 14000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13552
Date :04/13/2023, 16:09:44
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 14000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13552
Date :04/13/2023, 16:10:09
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 14000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13552
Date :04/13/2023, 16:10:53
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 14000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13552
Date :04/13/2023, 16:11:42
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 14000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13552
Date :04/13/2023, 16:12:15
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 14000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13552
Date :04/13/2023, 16:14:23
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 14000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13552
Date :04/13/2023, 16:15:27
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 14000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13552
Date :04/13/2023, 16:15:59
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 14000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13552
Date :04/13/2023, 16:16:57
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 14000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13552
Date :04/13/2023, 16:18:02
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 14000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13552
Date :04/13/2023, 16:18:34
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 14000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13552
Date :04/13/2023, 16:19:03
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 14000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13552
Date :04/13/2023, 16:22:01
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 14000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13552
Date :04/13/2023, 16:22:49
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 14000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13552
Date :04/13/2023, 16:24:16
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 14000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13552
0.7019608938822928
Date :04/13/2023, 16:42:51
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 14000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13552
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :04/13/2023, 16:43:01
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 14000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13552
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :04/13/2023, 16:46:37
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 14000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13552
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
0.8770009640773117
Model improve: 0.0000 -> 0.8770
Epoch: 2/100
Date :04/13/2023, 16:47:44
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 14000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13552
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :04/13/2023, 16:48:33
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 14000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
13552
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
0.8770009640773117
Model improve: 0.0000 -> 0.8770
Epoch: 2/100
Date :04/13/2023, 16:52:10
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 14000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 9.3962, val loss: 5.8424
0.640534114237079
Model improve: 0.0000 -> 0.6405
Epoch: 2/100
Train loss: 7.0064, val loss: 4.4740
0.7318283765188376
Model improve: 0.6405 -> 0.7318
Epoch: 3/100
Date :04/13/2023, 17:01:29
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 14000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
0.7019608938822928
Date :04/13/2023, 17:12:29
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 14000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 9.3824, val loss: 5.8991
0.641460382086903
Model improve: 0.0000 -> 0.6415
Epoch: 2/100
Train loss: 6.9801, val loss: 4.4489
0.7342012201302299
Model improve: 0.6415 -> 0.7342
Epoch: 3/100
Train loss: 6.0634, val loss: 4.1275
0.7617720265006925
Model improve: 0.7342 -> 0.7618
Epoch: 4/100
Train loss: 5.3415, val loss: 3.6783
0.7809835175980174
Model improve: 0.7618 -> 0.7810
Epoch: 5/100
Train loss: 5.1064, val loss: 3.4361
0.7995080245710019
Model improve: 0.7810 -> 0.7995
Epoch: 6/100
Train loss: 4.7752, val loss: 3.3919
0.807642705495752
Model improve: 0.7995 -> 0.8076
Epoch: 7/100
Train loss: 4.4548, val loss: 3.2486
0.8198649364314721
Model improve: 0.8076 -> 0.8199
Epoch: 8/100
Train loss: 4.3660, val loss: 3.1671
0.8239440441518124
Model improve: 0.8199 -> 0.8239
Epoch: 9/100
Train loss: 4.1343, val loss: 3.0537
0.8290752858732892
Model improve: 0.8239 -> 0.8291
Epoch: 10/100
Train loss: 4.1946, val loss: 3.0955
0.8308384013841305
Model improve: 0.8291 -> 0.8308
Epoch: 11/100
Train loss: 3.9740, val loss: 3.0664
0.8281567972085323
Epoch: 12/100
Train loss: 3.8521, val loss: 2.9626
0.8399262689624282
Model improve: 0.8308 -> 0.8399
Epoch: 13/100
Train loss: 3.7200, val loss: 3.0161
0.8319393305058327
Epoch: 14/100
Train loss: 3.7025, val loss: 2.7151
0.8446979405793518
Model improve: 0.8399 -> 0.8447
Epoch: 15/100
Train loss: 3.5780, val loss: 2.8959
0.8412596753117012
Epoch: 16/100
Train loss: 3.5506, val loss: 2.8662
0.8394970607570679
Epoch: 17/100
Train loss: 3.3316, val loss: 2.9229
0.8366054501532721
Epoch: 18/100
Train loss: 3.4113, val loss: 2.8563
0.8395591065767763
Epoch: 19/100
Train loss: 3.4070, val loss: 2.8991
0.8415632595465621
Epoch: 20/100
Train loss: 3.3456, val loss: 2.7748
0.8454828656100783
Model improve: 0.8447 -> 0.8455
Epoch: 21/100
Train loss: 3.2630, val loss: 2.7196
0.8481984293127472
Model improve: 0.8455 -> 0.8482
Epoch: 22/100
Train loss: 3.2537, val loss: 2.8023
0.843328713800255
Epoch: 23/100
Train loss: 3.2074, val loss: 2.7220
0.8468031938775298
Epoch: 24/100
Train loss: 3.2633, val loss: 2.7134
0.8467721513729101
Epoch: 25/100
Train loss: 3.1387, val loss: 2.8152
0.8451137574498014
Epoch: 26/100
Train loss: 3.1403, val loss: 2.8131
0.8400611689225951
Epoch: 27/100
Train loss: 3.0278, val loss: 2.7260
0.8478972114882627
Epoch: 28/100
Train loss: 2.9696, val loss: 2.6783
0.8467032662578015
Epoch: 29/100
Train loss: 3.1200, val loss: 2.6352
0.8524112853378885
Model improve: 0.8482 -> 0.8524
Epoch: 30/100
Train loss: 3.0219, val loss: 2.5066
0.8578816086098953
Model improve: 0.8524 -> 0.8579
Epoch: 31/100
Train loss: 2.9209, val loss: 2.5809
0.8542051609554091
Epoch: 32/100
Train loss: 2.9141, val loss: 2.5633
0.8584237876735406
Model improve: 0.8579 -> 0.8584
Epoch: 33/100
Train loss: 3.0303, val loss: 2.6989
0.8518849600187949
Epoch: 34/100
Train loss: 2.8495, val loss: 2.5872
0.8579720457323516
Epoch: 35/100
Train loss: 2.8791, val loss: 2.6165
0.8553782364050935
Epoch: 36/100
Train loss: 2.8607, val loss: 2.5761
0.85839200535347
Epoch: 37/100
Train loss: 2.7262, val loss: 2.5432
0.8582253261091842
Epoch: 38/100
Train loss: 2.7711, val loss: 2.5738
0.8554525978959714
Epoch: 39/100
Train loss: 2.7202, val loss: 2.5061
0.8564567109876644
Epoch: 40/100
Train loss: 2.6954, val loss: 2.4283
0.8618098266534444
Model improve: 0.8584 -> 0.8618
Epoch: 41/100
Train loss: 2.8275, val loss: 2.5311
0.8559439838641412
Epoch: 42/100
Train loss: 2.7106, val loss: 2.6709
0.8521629307737082
Epoch: 43/100
Train loss: 2.6172, val loss: 2.5112
0.8602105334403147
Epoch: 44/100
Train loss: 2.7006, val loss: 2.5420
0.8535198224590516
Epoch: 45/100
Train loss: 2.6457, val loss: 2.5891
0.8537490315658011
Epoch: 46/100
Train loss: 2.4794, val loss: 2.5921
0.8509015822764553
Epoch: 47/100
Train loss: 2.6020, val loss: 2.5017
0.8605070773279927
Epoch: 48/100
Train loss: 2.6679, val loss: 2.5468
0.8578857543587739
Epoch: 49/100
Train loss: 2.5358, val loss: 2.4577
0.8596986520049746
Epoch: 50/100
Train loss: 2.6397, val loss: 2.4535
0.8599058062452527
Epoch: 51/100
Train loss: 2.4099, val loss: 2.4995
0.8582415571140792
Epoch: 52/100
Train loss: 2.6991, val loss: 2.5532
0.857803810140195
Epoch: 53/100
Train loss: 2.4383, val loss: 2.4617
0.8615696674668705
Epoch: 54/100
Train loss: 2.4504, val loss: 2.6257
0.8565496837237615
Epoch: 55/100
Train loss: 2.4365, val loss: 2.5034
0.8619990357901727
Model improve: 0.8618 -> 0.8620
Epoch: 56/100
Train loss: 2.4886, val loss: 2.4248
0.8623442912449013
Model improve: 0.8620 -> 0.8623
Epoch: 57/100
Train loss: 2.4477, val loss: 2.4554
0.8651461734389555
Model improve: 0.8623 -> 0.8651
Epoch: 58/100
Train loss: 2.4532, val loss: 2.4493
0.8657779794912306
Model improve: 0.8651 -> 0.8658
Epoch: 59/100
Train loss: 2.4226, val loss: 2.4053
0.8649918107449212
Epoch: 60/100
Train loss: 2.3672, val loss: 2.4239
0.8683004635150161
Model improve: 0.8658 -> 0.8683
Epoch: 61/100
Train loss: 2.4288, val loss: 2.4772
0.8583620845674066
Epoch: 62/100
Train loss: 2.4684, val loss: 2.3868
0.8600653997477966
Epoch: 63/100
Train loss: 2.3685, val loss: 2.4370
0.8623165895431232
Epoch: 64/100
Train loss: 2.3562, val loss: 2.4369
0.861983608824958
Epoch: 65/100
Train loss: 2.1909, val loss: 2.4675
0.8603530285511164
Epoch: 66/100
Train loss: 2.3284, val loss: 2.5274
0.8626802744377553
Epoch: 67/100
Train loss: 2.2816, val loss: 2.4845
0.8599152644494469
Epoch: 68/100
Train loss: 2.3026, val loss: 2.4084
0.8654444644705895
Epoch: 69/100
Train loss: 2.2103, val loss: 2.3193
0.8712365253398804
Model improve: 0.8683 -> 0.8712
Epoch: 70/100
Train loss: 2.2273, val loss: 2.3967
0.8664105223260068
Epoch: 71/100
Train loss: 2.2687, val loss: 2.3654
0.8672835757265664
Epoch: 72/100
Train loss: 2.1279, val loss: 2.4511
0.8627310622498858
Epoch: 73/100
Train loss: 2.1275, val loss: 2.4278
0.8668425442186514
Epoch: 74/100
Train loss: 2.1605, val loss: 2.4496
0.8636341749251346
Epoch: 75/100
Train loss: 2.1935, val loss: 2.3863
0.8664295050188381
Epoch: 76/100
Train loss: 2.2890, val loss: 2.3783
0.8642455096527709
Epoch: 77/100
Train loss: 2.0048, val loss: 2.4935
0.8679304083804533
Epoch: 78/100
Train loss: 2.1320, val loss: 2.3364
0.8677558753688508
Epoch: 79/100
Train loss: 2.1599, val loss: 2.3207
0.866026682533498
Epoch: 80/100
Train loss: 2.0958, val loss: 2.3857
0.8673510404130745
Epoch: 81/100
Train loss: 2.2882, val loss: 2.3983
0.8659672860926702
Epoch: 82/100
Train loss: 2.1351, val loss: 2.3580
0.865729849593965
Epoch: 83/100
Train loss: 2.0306, val loss: 2.3475
0.867133343598887
Epoch: 84/100
Train loss: 2.1964, val loss: 2.3765
0.8663327967840265
Epoch: 85/100
Train loss: 2.0737, val loss: 2.4648
0.8677979823423435
Epoch: 86/100
Train loss: 2.0946, val loss: 2.3603
0.866909326425221
Epoch: 87/100
Train loss: 2.1274, val loss: 2.4022
0.8655635525678647
Epoch: 88/100
Train loss: 2.1201, val loss: 2.3162
0.8717322605786927
Model improve: 0.8712 -> 0.8717
Epoch: 89/100
Train loss: 2.2401, val loss: 2.3705
0.8697920934672065
Epoch: 90/100
Train loss: 2.0565, val loss: 2.3244
0.8707297489014181
Epoch: 91/100
Train loss: 2.1645, val loss: 2.3394
0.867397471734394
Epoch: 92/100
Train loss: 2.1107, val loss: 2.4637
0.8681689370276753
Epoch: 93/100
Train loss: 2.0822, val loss: 2.3151
0.8703859584447936
Epoch: 94/100
Train loss: 2.0917, val loss: 2.3922
0.8711630693752329
Epoch: 95/100
Train loss: 1.9969, val loss: 2.4528
0.8676326611809762
Epoch: 96/100
Train loss: 2.0982, val loss: 2.2506
0.8718218469845536
Model improve: 0.8717 -> 0.8718
Epoch: 97/100
Train loss: 2.1308, val loss: 2.2790
0.8719442945286098
Model improve: 0.8718 -> 0.8719
Epoch: 98/100
Train loss: 2.0323, val loss: 2.3248
0.8695294662393811
Epoch: 99/100
Train loss: 2.0868, val loss: 2.4019
0.8682869830549657
Epoch: 100/100
Train loss: 2.1001, val loss: 2.3500
0.8686409946799633
Date :04/13/2023, 23:57:30
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
19629
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 9.3547, val loss: 5.8169
0.6488112938193266
Model improve: 0.0000 -> 0.6488
Epoch: 2/100
Train loss: 6.8829, val loss: 4.5760
0.724405212217088
Model improve: 0.0000 -> 0.7244
Epoch: 3/100
Train loss: 6.0147, val loss: 3.9367
0.7661589128432684
Model improve: 0.0000 -> 0.7662
Epoch: 4/100
Train loss: 5.2655, val loss: 3.6196
0.784333204451299
Model improve: 0.0000 -> 0.7843
Epoch: 5/100
Train loss: 5.0823, val loss: 3.4990
0.8039612814993572
Model improve: 0.0000 -> 0.8040
Epoch: 6/100
Train loss: 4.7416, val loss: 3.3768
0.8072843257262355
Model improve: 0.0000 -> 0.8073
Epoch: 7/100
Train loss: 4.4006, val loss: 3.3597
0.8128614425317429
Model improve: 0.0000 -> 0.8129
Epoch: 8/100
Train loss: 4.3315, val loss: 3.3002
0.8198075118370046
Model improve: 0.0000 -> 0.8198
Epoch: 9/100
Train loss: 4.1119, val loss: 3.1573
0.8225392952730098
Model improve: 0.0000 -> 0.8225
Epoch: 10/100
Train loss: 4.1411, val loss: 2.9031
0.8338873140920819
Model improve: 0.0000 -> 0.8339
Epoch: 11/100
Date :04/14/2023, 08:43:33
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
Date :04/14/2023, 08:45:30
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :04/14/2023, 08:46:31
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 7.7672, val loss: 3.1179
0.7705113888209291
Model improve: 0.0000 -> 0.7705
Epoch: 2/100
Train loss: 3.8201, val loss: 2.2284
0.853277105785922
Model improve: 0.7705 -> 0.8533
Epoch: 3/100
Train loss: 3.1249, val loss: 1.9678
0.8738958980220655
Model improve: 0.8533 -> 0.8739
Epoch: 4/100
Train loss: 2.8681, val loss: 1.8324
0.8848675209731649
Model improve: 0.8739 -> 0.8849
Epoch: 5/100
Train loss: 2.7518, val loss: 1.7577
0.8904529342861245
Model improve: 0.8849 -> 0.8905
Epoch: 6/100
Train loss: 2.5633, val loss: 1.6793
0.8953116554265198
Model improve: 0.8905 -> 0.8953
Epoch: 7/100
Train loss: 2.3392, val loss: 1.5377
0.9035409971182377
Model improve: 0.8953 -> 0.9035
Epoch: 8/100
Train loss: 2.2891, val loss: 1.4564
0.9055027622066515
Model improve: 0.9035 -> 0.9055
Epoch: 9/100
Train loss: 2.3696, val loss: 1.4953
0.9093308771292323
Model improve: 0.9055 -> 0.9093
Epoch: 10/100
Train loss: 2.1461, val loss: 1.3360
0.9123130872513032
Model improve: 0.9093 -> 0.9123
Epoch: 11/100
Train loss: 2.0708, val loss: 1.3813
0.9133885153003735
Model improve: 0.9123 -> 0.9134
Epoch: 12/100
Train loss: 2.0075, val loss: 1.2821
0.9164171181806063
Model improve: 0.9134 -> 0.9164
Epoch: 13/100
Train loss: 1.9252, val loss: 1.2862
0.9168302099244646
Model improve: 0.9164 -> 0.9168
Epoch: 14/100
Train loss: 1.8149, val loss: 1.2885
0.916580080829149
Epoch: 15/100
Train loss: 1.8846, val loss: 1.2372
0.9187812542196411
Model improve: 0.9168 -> 0.9188
Epoch: 16/100
Train loss: 1.8121, val loss: 1.2056
0.9205735729361687
Model improve: 0.9188 -> 0.9206
Epoch: 17/100
Train loss: 1.6850, val loss: 1.1845
0.9211586823318019
Model improve: 0.9206 -> 0.9212
Epoch: 18/100
Train loss: 1.7565, val loss: 1.2116
0.9215976986172678
Model improve: 0.9212 -> 0.9216
Epoch: 19/100
Train loss: 1.7683, val loss: 1.2561
0.9223070461221513
Model improve: 0.9216 -> 0.9223
Epoch: 20/100
Train loss: 1.8301, val loss: 1.2127
0.922420515647161
Model improve: 0.9223 -> 0.9224
Epoch: 21/100
Train loss: 2.3796, val loss: 1.7618
0.8896445342212189
Epoch: 22/100
Train loss: 2.2495, val loss: 1.5844
0.8969449588677824
Epoch: 23/100
Train loss: 2.1175, val loss: 1.6065
0.8965863521463325
Epoch: 24/100
Train loss: 2.1713, val loss: 1.5275
0.9037867142276987
Epoch: 25/100
Train loss: 2.0642, val loss: 1.4584
0.9039176558368225
Epoch: 26/100
Train loss: 2.0093, val loss: 1.4480
0.9034109992308887
Epoch: 27/100
Train loss: 1.9883, val loss: 1.4419
0.9111590504066011
Epoch: 28/100
Train loss: 1.9026, val loss: 1.4110
0.9094451008380804
Epoch: 29/100
Train loss: 1.8298, val loss: 1.3920
0.9097574773323227
Epoch: 30/100
Train loss: 1.7414, val loss: 1.3349
0.9112191051065093
Epoch: 31/100
Train loss: 1.7391, val loss: 1.3155
0.9123788058441278
Epoch: 32/100
Train loss: 1.7386, val loss: 1.2664
0.9168345300378423
Epoch: 33/100
Train loss: 1.5772, val loss: 1.2827
0.9170344633865305
Epoch: 34/100
Train loss: 1.5437, val loss: 1.2684
0.9187484760541996
Epoch: 35/100
Train loss: 1.6507, val loss: 1.3130
0.9167978744192914
Epoch: 36/100
Train loss: 1.5045, val loss: 1.2495
0.9178457817618417
Epoch: 37/100
Train loss: 1.6148, val loss: 1.2130
0.9190733664054577
Epoch: 38/100
Train loss: 1.5493, val loss: 1.2112
0.9194365181676789
Epoch: 39/100
Train loss: 1.5473, val loss: 1.2144
0.9199778983542434
Epoch: 40/100
Date :04/14/2023, 10:53:55
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 30
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/30
Date :04/14/2023, 10:55:11
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 30
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
19577
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/30
Train loss: 10.0295, val loss: 3.7825
0.7189526129420609
Model improve: 0.0000 -> 0.7190
Epoch: 2/30
Train loss: 4.2184, val loss: 2.3397
0.8425017020406564
Model improve: 0.7190 -> 0.8425
Epoch: 3/30
Train loss: 3.3115, val loss: 1.9923
0.8750450414711558
Model improve: 0.8425 -> 0.8750
Epoch: 4/30
Train loss: 2.9533, val loss: 1.7893
0.8892568237656846
Model improve: 0.8750 -> 0.8893
Epoch: 5/30
Train loss: 2.7968, val loss: 1.6845
0.8965947349415252
Model improve: 0.8893 -> 0.8966
Epoch: 6/30
Train loss: 2.6059, val loss: 1.6138
0.8982512196296542
Model improve: 0.8966 -> 0.8983
Epoch: 7/30
Train loss: 2.3790, val loss: 1.5141
0.9065710408064217
Model improve: 0.8983 -> 0.9066
Epoch: 8/30
Train loss: 2.3331, val loss: 1.4227
0.9110210193696402
Model improve: 0.9066 -> 0.9110
Epoch: 9/30
Train loss: 2.4228, val loss: 1.5012
0.9123382326088247
Model improve: 0.9110 -> 0.9123
Epoch: 10/30
Train loss: 2.2133, val loss: 1.3761
0.9106672436286389
Epoch: 11/30
Train loss: 2.1661, val loss: 1.4051
0.9140920739819469
Model improve: 0.9123 -> 0.9141
Epoch: 12/30
Train loss: 2.1136, val loss: 1.3347
0.9157940020590133
Model improve: 0.9141 -> 0.9158
Epoch: 13/30
Train loss: 2.0397, val loss: 1.3404
0.9169803384051795
Model improve: 0.9158 -> 0.9170
Epoch: 14/30
Train loss: 1.9376, val loss: 1.3351
0.9167855427580814
Epoch: 15/30
Train loss: 2.0144, val loss: 1.2991
0.9160107204654571
Epoch: 16/30
Train loss: 1.9390, val loss: 1.2930
0.917672248927723
Model improve: 0.9170 -> 0.9177
Epoch: 17/30
Train loss: 1.8098, val loss: 1.2476
0.9168583705085798
Epoch: 18/30
Train loss: 1.8751, val loss: 1.2606
0.9184372383791141
Model improve: 0.9177 -> 0.9184
Epoch: 19/30
Train loss: 1.8741, val loss: 1.3206
0.9191693864563408
Model improve: 0.9184 -> 0.9192
Epoch: 20/30
Train loss: 1.9119, val loss: 1.2713
0.9207164846835058
Model improve: 0.9192 -> 0.9207
Epoch: 21/30
Train loss: 1.8700, val loss: 1.3137
0.9192934717849821
Epoch: 22/30
Train loss: 1.7283, val loss: 1.2128
0.920532969887331
Epoch: 23/30
Train loss: 1.6734, val loss: 1.2405
0.9220228750354714
Model improve: 0.9207 -> 0.9220
Epoch: 24/30
Train loss: 1.7793, val loss: 1.2493
0.9212214028073569
Epoch: 25/30
Train loss: 1.7347, val loss: 1.1841
0.9218862908776078
Epoch: 26/30
Train loss: 1.6919, val loss: 1.1790
0.9214582317419272
Epoch: 27/30
Train loss: 1.7389, val loss: 1.2614
0.9223728621772024
Model improve: 0.9220 -> 0.9224
Epoch: 28/30
Train loss: 1.7016, val loss: 1.2106
0.9223540658608839
Epoch: 29/30
Train loss: 1.6761, val loss: 1.2028
0.9222627908387413
Epoch: 30/30
Train loss: 1.6276, val loss: 1.1917
0.9223634328397069
Date :04/26/2023, 17:47:29
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 300
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/300
Train loss: 14.5507
Epoch: 2/300
Train loss: 4.5116
Epoch: 3/300
Train loss: 4.0771
Epoch: 4/300
Train loss: 3.8195
Epoch: 5/300
Date :04/26/2023, 17:57:14
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 300
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/300
Train loss: 14.5516
Epoch: 2/300
Date :04/26/2023, 18:00:03
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 300
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/300
Train loss: 14.5524
Epoch: 2/300
Train loss: 4.5118
Epoch: 3/300
Train loss: 4.0764
Epoch: 4/300
Train loss: 3.8194
Epoch: 5/300
Train loss: 3.6419
Epoch: 6/300
Train loss: 3.4163
Epoch: 7/300
Train loss: 3.3936
Epoch: 8/300
Train loss: 3.2796
Epoch: 9/300
Train loss: 3.2084
Epoch: 10/300
Train loss: 3.1760
Epoch: 11/300
Train loss: 3.1298
Epoch: 12/300
Train loss: 3.0277
Epoch: 13/300
Train loss: 2.9366
Epoch: 14/300
Train loss: 2.9583
Epoch: 15/300
Train loss: 2.9629
Epoch: 16/300
Train loss: 2.8764
Epoch: 17/300
Train loss: 2.8867
Epoch: 18/300
Train loss: 2.8249
Epoch: 19/300
Train loss: 2.8594
Epoch: 20/300
Train loss: 2.7162
Epoch: 21/300
Train loss: 2.7238
Epoch: 22/300
Train loss: 2.7579
Epoch: 23/300
Train loss: 2.7877
Epoch: 24/300
Train loss: 2.7544
Epoch: 25/300
Train loss: 2.7012
Epoch: 26/300
Train loss: 2.6317
Epoch: 27/300
Train loss: 2.6976
Epoch: 28/300
Train loss: 2.6378
Epoch: 29/300
Train loss: 2.6602
Epoch: 30/300
Train loss: 2.6209
Epoch: 31/300
Train loss: 2.7003
Epoch: 32/300
Train loss: 2.6035
Epoch: 33/300
Train loss: 2.5714
Epoch: 34/300
Train loss: 2.5339
Epoch: 35/300
Train loss: 2.5465
Epoch: 36/300
Train loss: 2.5587
Epoch: 37/300
Train loss: 2.5883
Epoch: 38/300
Train loss: 2.4612
Epoch: 39/300
Train loss: 2.5017
Epoch: 40/300
Train loss: 2.4558
Epoch: 41/300
Train loss: 2.5421
Epoch: 42/300
Train loss: 2.5859
Epoch: 43/300
Train loss: 2.5147
Epoch: 44/300
Train loss: 2.4727
Epoch: 45/300
Train loss: 2.4752
Epoch: 46/300
Train loss: 2.4552
Epoch: 47/300
Train loss: 2.4956
Epoch: 48/300
Train loss: 2.5119
Epoch: 49/300
Train loss: 2.5241
Epoch: 50/300
Train loss: 2.4742
Epoch: 51/300
Train loss: 2.4954
Epoch: 52/300
Train loss: 2.3569
Epoch: 53/300
Train loss: 2.4829
Epoch: 54/300
Train loss: 2.4028
Epoch: 55/300
Train loss: 2.4194
Epoch: 56/300
Train loss: 2.4061
Epoch: 57/300
Train loss: 2.4538
Epoch: 58/300
Train loss: 2.3307
Epoch: 59/300
Train loss: 2.3353
Epoch: 60/300
Train loss: 2.3642
Epoch: 61/300
Train loss: 2.3630
Epoch: 62/300
Train loss: 2.4073
Epoch: 63/300
Train loss: 2.3744
Epoch: 64/300
Train loss: 2.3497
Epoch: 65/300
Train loss: 2.3755
Epoch: 66/300
Train loss: 2.3716
Epoch: 67/300
Train loss: 2.2906
Epoch: 68/300
Date :05/15/2023, 23:53:40
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.3
drop_path_rate: 0.4
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 42.1205
Epoch: 2/200
Train loss: 5.7189
Epoch: 3/200
Train loss: 5.2756
Epoch: 4/200
Train loss: 5.0099
Epoch: 5/200
Train loss: 4.6982
Epoch: 6/200
Train loss: 4.4572
Epoch: 7/200
Train loss: 4.1691
Epoch: 8/200
Train loss: 4.1038
Epoch: 9/200
Train loss: 3.9424
Epoch: 10/200
Train loss: 3.8586
Epoch: 11/200
Train loss: 3.7760
Epoch: 12/200
Train loss: 3.6581
Epoch: 13/200
Train loss: 3.5997
Epoch: 14/200
Train loss: 3.4898
Epoch: 15/200
Train loss: 3.5010
Epoch: 16/200
Train loss: 3.4556
Epoch: 17/200
Train loss: 3.3107
Epoch: 18/200
Train loss: 3.2172
Epoch: 19/200
Train loss: 3.4092
Epoch: 20/200
Train loss: 3.2799
Epoch: 21/200
Train loss: 3.1335
Epoch: 22/200
Train loss: 3.0328
Epoch: 23/200
Train loss: 3.0587
Epoch: 24/200
Train loss: 3.1400
Epoch: 25/200
Train loss: 3.0929
Epoch: 26/200
Train loss: 3.0825
Epoch: 27/200
Train loss: 3.0329
Epoch: 28/200
Train loss: 2.9110
Epoch: 29/200
Train loss: 2.9349
Epoch: 30/200
Train loss: 2.7883
Epoch: 31/200
Train loss: 2.9045
Epoch: 32/200
Train loss: 2.9567
Epoch: 33/200
Train loss: 2.7539
Epoch: 34/200
Train loss: 2.8623
Epoch: 35/200
Train loss: 2.8472
Epoch: 36/200
Train loss: 2.8616
Epoch: 37/200
Train loss: 2.7537
Epoch: 38/200
Train loss: 2.9742
Epoch: 39/200
Train loss: 2.6805
Epoch: 40/200
Train loss: 2.7838
Epoch: 41/200
Train loss: 2.7204
Epoch: 42/200
Train loss: 2.7109
Epoch: 43/200
Train loss: 2.7865
Epoch: 44/200
Train loss: 2.7819
Epoch: 45/200
Train loss: 2.7933
Epoch: 46/200
Train loss: 2.5533
Epoch: 47/200
Train loss: 2.7373
Epoch: 48/200
Train loss: 2.4695
Epoch: 49/200
Train loss: 2.4859
Epoch: 50/200
Train loss: 2.5350
Epoch: 51/200
Train loss: 2.5825
Epoch: 52/200
Train loss: 2.5616
Epoch: 53/200
Train loss: 2.5006
Epoch: 54/200
Train loss: 2.5984
Epoch: 55/200
Train loss: 2.5457
Epoch: 56/200
Train loss: 2.5289
Epoch: 57/200
Train loss: 2.5571
Epoch: 58/200
Train loss: 2.5333
Epoch: 59/200
Train loss: 2.5594
Epoch: 60/200
Train loss: 2.5578
Epoch: 61/200
Train loss: 2.4433
Epoch: 62/200
Train loss: 2.4729
Epoch: 63/200
Train loss: 2.4855
Epoch: 64/200
Train loss: 2.5182
Epoch: 65/200
Train loss: 2.5562
Epoch: 66/200
Train loss: 2.4318
Epoch: 67/200
Train loss: 2.5234
Epoch: 68/200
Train loss: 2.4854
Epoch: 69/200
Train loss: 2.6367
Epoch: 70/200
Train loss: 2.4621
Epoch: 71/200
Train loss: 2.2992
Epoch: 72/200
Train loss: 2.3361
Epoch: 73/200
Train loss: 2.4661
Epoch: 74/200
Train loss: 2.4408
Epoch: 75/200
Train loss: 2.4959
Epoch: 76/200
Train loss: 2.4627
Epoch: 77/200
Train loss: 2.4408
Epoch: 78/200
Train loss: 2.3327
Epoch: 79/200
Train loss: 2.1997
Epoch: 80/200
Train loss: 2.3649
Epoch: 81/200
Train loss: 2.3664
Epoch: 82/200
Train loss: 2.3524
Epoch: 83/200
Train loss: 2.3249
Epoch: 84/200
Train loss: 2.3007
Epoch: 85/200
Train loss: 2.4219
Epoch: 86/200
Train loss: 2.3705
Epoch: 87/200
Train loss: 2.2344
Epoch: 88/200
Train loss: 2.4372
Epoch: 89/200
Train loss: 2.4579
Epoch: 90/200
Train loss: 2.2991
Epoch: 91/200
Train loss: 2.4147
Epoch: 92/200
Train loss: 2.4009
Epoch: 93/200
Train loss: 2.4169
Epoch: 94/200
Train loss: 2.3256
Epoch: 95/200
Train loss: 2.3862
Epoch: 96/200
Train loss: 2.4099
Epoch: 97/200
Train loss: 2.3641
Epoch: 98/200
Train loss: 2.3395
Epoch: 99/200
Train loss: 2.2753
Epoch: 100/200
Train loss: 2.3280
Epoch: 101/200
Train loss: 2.3776
Epoch: 102/200
Train loss: 2.1666
0.8123105047638106
Model improve: 0.000000 -> 0.812311
Epoch: 103/200
Train loss: 2.2193
0.8122369770984872
Epoch: 104/200
Train loss: 2.2215
0.813193336719592
Model improve: 0.812311 -> 0.813193
Epoch: 105/200
Train loss: 2.2701
0.8144131626636915
Model improve: 0.813193 -> 0.814413
Epoch: 106/200
Date :05/16/2023, 03:35:48
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.2
drop_path_rate: 0.35
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 42.1979
Epoch: 2/200
Train loss: 5.7267
Epoch: 3/200
Train loss: 5.2794
Epoch: 4/200
Train loss: 5.0026
Epoch: 5/200
Train loss: 4.7111
Epoch: 6/200
Train loss: 4.4692
Epoch: 7/200
Train loss: 4.1710
Epoch: 8/200
Train loss: 4.1051
Epoch: 9/200
Train loss: 3.9488
Epoch: 10/200
Train loss: 3.8592
Epoch: 11/200
Train loss: 3.7700
Epoch: 12/200
Train loss: 3.6552
Epoch: 13/200
Train loss: 3.6016
Epoch: 14/200
Train loss: 3.4887
Epoch: 15/200
Train loss: 3.4955
Epoch: 16/200
Train loss: 3.4629
Epoch: 17/200
Train loss: 3.3117
Epoch: 18/200
Train loss: 3.2218
Epoch: 19/200
Train loss: 3.4081
Epoch: 20/200
Train loss: 3.2940
Epoch: 21/200
Train loss: 3.1367
Epoch: 22/200
Train loss: 3.0252
Epoch: 23/200
Train loss: 3.0356
Epoch: 24/200
Train loss: 3.1168
Epoch: 25/200
Train loss: 3.0799
Epoch: 26/200
Train loss: 3.0735
Epoch: 27/200
Train loss: 3.0480
Epoch: 28/200
Train loss: 2.9077
Epoch: 29/200
Train loss: 2.9376
Epoch: 30/200
Train loss: 2.7732
Epoch: 31/200
Train loss: 2.9297
Epoch: 32/200
Train loss: 2.9614
Epoch: 33/200
Train loss: 2.7532
Epoch: 34/200
Train loss: 2.8337
Epoch: 35/200
Train loss: 2.8467
Epoch: 36/200
Train loss: 2.8620
Epoch: 37/200
Train loss: 2.7609
Epoch: 38/200
Train loss: 2.9820
Epoch: 39/200
Train loss: 2.6605
Epoch: 40/200
Train loss: 2.7858
Epoch: 41/200
Train loss: 2.7206
Epoch: 42/200
Train loss: 2.7111
Epoch: 43/200
Train loss: 2.7890
Epoch: 44/200
Train loss: 2.7722
Epoch: 45/200
Train loss: 2.7806
Epoch: 46/200
Train loss: 2.5441
Epoch: 47/200
Train loss: 2.7286
Epoch: 48/200
Train loss: 2.4953
Epoch: 49/200
Train loss: 2.4759
Epoch: 50/200
Train loss: 2.5338
Epoch: 51/200
Train loss: 2.5874
Epoch: 52/200
Train loss: 2.5945
Epoch: 53/200
Train loss: 2.4885
Epoch: 54/200
Train loss: 2.5841
Epoch: 55/200
Train loss: 2.5613
Epoch: 56/200
Train loss: 2.5248
Epoch: 57/200
Train loss: 2.5766
Epoch: 58/200
Train loss: 2.5334
Epoch: 59/200
Train loss: 2.5512
Epoch: 60/200
Train loss: 2.5525
Epoch: 61/200
Train loss: 2.4545
Epoch: 62/200
Train loss: 2.4743
Epoch: 63/200
Train loss: 2.4972
Epoch: 64/200
Train loss: 2.5247
Epoch: 65/200
Train loss: 2.5569
Epoch: 66/200
Train loss: 2.4280
Epoch: 67/200
Train loss: 2.5190
Epoch: 68/200
Train loss: 2.4882
Epoch: 69/200
Train loss: 2.6497
Epoch: 70/200
Train loss: 2.4622
Epoch: 71/200
Train loss: 2.3112
Epoch: 72/200
Train loss: 2.3349
Epoch: 73/200
Train loss: 2.4649
Epoch: 74/200
Train loss: 2.4388
Epoch: 75/200
Train loss: 2.5136
Epoch: 76/200
Train loss: 2.4597
Epoch: 77/200
Train loss: 2.4326
Epoch: 78/200
Train loss: 2.3417
Epoch: 79/200
Train loss: 2.1994
Epoch: 80/200
Train loss: 2.3583
Epoch: 81/200
Train loss: 2.3697
Epoch: 82/200
Train loss: 2.3326
Epoch: 83/200
Train loss: 2.3194
Epoch: 84/200
Train loss: 2.3064
Epoch: 85/200
Train loss: 2.4271
Epoch: 86/200
Train loss: 2.3788
Epoch: 87/200
Train loss: 2.2378
Epoch: 88/200
Train loss: 2.4366
Epoch: 89/200
Train loss: 2.4686
Epoch: 90/200
Train loss: 2.2941
Epoch: 91/200
Train loss: 2.4161
Epoch: 92/200
Train loss: 2.3922
Epoch: 93/200
Train loss: 2.4229
Epoch: 94/200
Train loss: 2.3357
Epoch: 95/200
Train loss: 2.3798
Epoch: 96/200
Train loss: 2.4235
Epoch: 97/200
Train loss: 2.3639
Epoch: 98/200
Train loss: 2.3395
Epoch: 99/200
Train loss: 2.2670
Epoch: 100/200
Train loss: 2.3275
Epoch: 101/200
Train loss: 2.3777
Epoch: 102/200
Train loss: 2.1676
0.8126407159314982
Model improve: 0.000000 -> 0.812641
Epoch: 103/200
Train loss: 2.2228
0.812390061022016
Epoch: 104/200
Train loss: 2.2254
0.8125957121035383
Epoch: 105/200
Train loss: 2.2735
0.8135203757085843
Model improve: 0.812641 -> 0.813520
Epoch: 106/200
Date :05/16/2023, 07:35:05
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.15
drop_path_rate: 0.4
norandomvolume
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 42.0807
Epoch: 2/200
Train loss: 5.7103
Epoch: 3/200
Train loss: 5.2649
Epoch: 4/200
Train loss: 4.9996
Epoch: 5/200
Train loss: 4.6881
Epoch: 6/200
Train loss: 4.4430
Epoch: 7/200
Train loss: 4.1670
Epoch: 8/200
Train loss: 4.0952
Epoch: 9/200
Train loss: 3.9431
Epoch: 10/200
Train loss: 3.8594
Epoch: 11/200
Train loss: 3.7788
Epoch: 12/200
Train loss: 3.6664
Epoch: 13/200
Train loss: 3.5992
Epoch: 14/200
Train loss: 3.4870
Epoch: 15/200
Train loss: 3.4925
Epoch: 16/200
Train loss: 3.4318
Epoch: 17/200
Train loss: 3.3222
Epoch: 18/200
Train loss: 3.2061
Epoch: 19/200
Train loss: 3.4115
Epoch: 20/200
Train loss: 3.2746
Epoch: 21/200
Train loss: 3.1611
Epoch: 22/200
Train loss: 3.0194
Epoch: 23/200
Train loss: 3.0322
Epoch: 24/200
Train loss: 3.1207
Epoch: 25/200
Train loss: 3.0832
Epoch: 26/200
Train loss: 3.0813
Epoch: 27/200
Train loss: 3.0498
Epoch: 28/200
Train loss: 2.9171
Epoch: 29/200
Train loss: 2.9347
Epoch: 30/200
Train loss: 2.7810
Epoch: 31/200
Train loss: 2.9105
Epoch: 32/200
Train loss: 2.9773
Epoch: 33/200
Train loss: 2.7324
Epoch: 34/200
Train loss: 2.8341
Epoch: 35/200
Train loss: 2.8342
Epoch: 36/200
Train loss: 2.8622
Epoch: 37/200
Train loss: 2.7381
Epoch: 38/200
Train loss: 2.9714
Epoch: 39/200
Train loss: 2.6603
Epoch: 40/200
Train loss: 2.7843
Epoch: 41/200
Train loss: 2.7347
Epoch: 42/200
Train loss: 2.7088
Epoch: 43/200
Train loss: 2.7983
Epoch: 44/200
Train loss: 2.7828
Epoch: 45/200
Train loss: 2.7727
Epoch: 46/200
Train loss: 2.5233
Epoch: 47/200
Train loss: 2.7309
Epoch: 48/200
Train loss: 2.4724
Epoch: 49/200
Train loss: 2.4739
Epoch: 50/200
Train loss: 2.5445
Epoch: 51/200
Train loss: 2.5863
Epoch: 52/200
Train loss: 2.5656
Epoch: 53/200
Train loss: 2.4887
Epoch: 54/200
Train loss: 2.5973
Epoch: 55/200
Train loss: 2.5602
Epoch: 56/200
Train loss: 2.5276
Epoch: 57/200
Train loss: 2.5567
Epoch: 58/200
Train loss: 2.5442
Epoch: 59/200
Train loss: 2.5405
Epoch: 60/200
Train loss: 2.5510
Epoch: 61/200
Train loss: 2.4690
Epoch: 62/200
Train loss: 2.4850
Epoch: 63/200
Train loss: 2.4774
Epoch: 64/200
Train loss: 2.5196
Epoch: 65/200
Train loss: 2.5502
Epoch: 66/200
Train loss: 2.4242
Epoch: 67/200
Train loss: 2.5124
Epoch: 68/200
Train loss: 2.4876
Epoch: 69/200
Train loss: 2.6303
Epoch: 70/200
Train loss: 2.4618
Epoch: 71/200
Train loss: 2.3176
Epoch: 72/200
Train loss: 2.3215
Epoch: 73/200
Train loss: 2.4695
Epoch: 74/200
Train loss: 2.4385
Epoch: 75/200
Train loss: 2.4929
Epoch: 76/200
Train loss: 2.4679
Epoch: 77/200
Train loss: 2.4405
Epoch: 78/200
Train loss: 2.3450
Epoch: 79/200
Train loss: 2.1836
Epoch: 80/200
Train loss: 2.3638
Epoch: 81/200
Train loss: 2.3635
Epoch: 82/200
Train loss: 2.3549
Epoch: 83/200
Train loss: 2.3134
Epoch: 84/200
Train loss: 2.2926
Epoch: 85/200
Train loss: 2.4233
Epoch: 86/200
Train loss: 2.3416
Epoch: 87/200
Train loss: 2.2340
Epoch: 88/200
Train loss: 2.4346
Epoch: 89/200
Train loss: 2.4417
Epoch: 90/200
Train loss: 2.2949
Epoch: 91/200
Train loss: 2.4082
Epoch: 92/200
Train loss: 2.3978
Epoch: 93/200
Train loss: 2.4166
Epoch: 94/200
Train loss: 2.3273
Epoch: 95/200
Train loss: 2.3822
Epoch: 96/200
Train loss: 2.4252
Epoch: 97/200
Train loss: 2.3700
Epoch: 98/200
Train loss: 2.3434
Epoch: 99/200
Train loss: 2.2623
Epoch: 100/200
Train loss: 2.3302
Epoch: 101/200
Train loss: 2.3765
Epoch: 102/200
Train loss: 2.1580
0.8118372035757953
Model improve: 0.000000 -> 0.811837
Epoch: 103/200
Train loss: 2.2138
0.8105622174844496
Epoch: 104/200
Train loss: 2.2317
Date :05/16/2023, 11:02:49
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.15
drop_path_rate: 0.4
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 42.3783
Epoch: 2/200
Train loss: 5.7854
Epoch: 3/200
Train loss: 5.3761
Epoch: 4/200
Train loss: 5.1201
Epoch: 5/200
Train loss: 4.8420
Epoch: 6/200
Train loss: 4.5925
Epoch: 7/200
Train loss: 4.3198
Epoch: 8/200
Train loss: 4.2405
Epoch: 9/200
Train loss: 4.0732
Epoch: 10/200
Train loss: 3.9951
Epoch: 11/200
Train loss: 3.8876
Epoch: 12/200
Train loss: 3.7903
Epoch: 13/200
Train loss: 3.7285
Epoch: 14/200
Train loss: 3.6144
Epoch: 15/200
Train loss: 3.6252
Epoch: 16/200
Train loss: 3.5691
Epoch: 17/200
Train loss: 3.4380
Epoch: 18/200
Train loss: 3.3468
Epoch: 19/200
Train loss: 3.5108
Epoch: 20/200
Train loss: 3.3907
Epoch: 21/200
Train loss: 3.2564
Epoch: 22/200
Train loss: 3.1421
Epoch: 23/200
Train loss: 3.1867
Epoch: 24/200
Train loss: 3.2340
Epoch: 25/200
Train loss: 3.2164
Epoch: 26/200
Train loss: 3.1986
Epoch: 27/200
Train loss: 3.1530
Epoch: 28/200
Train loss: 3.0395
Epoch: 29/200
Train loss: 3.0655
Epoch: 30/200
Train loss: 2.8944
Epoch: 31/200
Train loss: 3.0133
Epoch: 32/200
Train loss: 3.0663
Epoch: 33/200
Train loss: 2.8620
Epoch: 34/200
Train loss: 2.9634
Epoch: 35/200
Train loss: 2.9451
Epoch: 36/200
Train loss: 2.9709
Epoch: 37/200
Train loss: 2.8793
Epoch: 38/200
Train loss: 3.0950
Epoch: 39/200
Train loss: 2.7837
Epoch: 40/200
Train loss: 2.8788
Epoch: 41/200
Train loss: 2.8408
Epoch: 42/200
Train loss: 2.8295
Epoch: 43/200
Train loss: 2.9089
Epoch: 44/200
Train loss: 2.8855
Epoch: 45/200
Train loss: 2.8809
Epoch: 46/200
Train loss: 2.6328
Epoch: 47/200
Train loss: 2.8439
Epoch: 48/200
Train loss: 2.5889
Epoch: 49/200
Train loss: 2.5829
Epoch: 50/200
Train loss: 2.6293
Epoch: 51/200
Train loss: 2.6882
Epoch: 52/200
Train loss: 2.6724
Epoch: 53/200
Train loss: 2.6066
Epoch: 54/200
Train loss: 2.6964
Epoch: 55/200
Train loss: 2.6758
Epoch: 56/200
Train loss: 2.6202
Epoch: 57/200
Train loss: 2.6691
Epoch: 58/200
Train loss: 2.6272
Epoch: 59/200
Train loss: 2.6389
Epoch: 60/200
Train loss: 2.6561
Epoch: 61/200
Train loss: 2.5667
Epoch: 62/200
Train loss: 2.5814
Epoch: 63/200
Train loss: 2.5789
Epoch: 64/200
Train loss: 2.6161
Epoch: 65/200
Train loss: 2.6526
Epoch: 66/200
Train loss: 2.5212
Epoch: 67/200
Train loss: 2.5938
Epoch: 68/200
Train loss: 2.5651
Epoch: 69/200
Train loss: 2.7411
Epoch: 70/200
Train loss: 2.5655
Epoch: 71/200
Train loss: 2.4101
Epoch: 72/200
Train loss: 2.4163
Epoch: 73/200
Train loss: 2.5636
Epoch: 74/200
Train loss: 2.5312
Epoch: 75/200
Train loss: 2.5947
Epoch: 76/200
Train loss: 2.5517
Epoch: 77/200
Train loss: 2.5271
Epoch: 78/200
Train loss: 2.4124
Epoch: 79/200
Train loss: 2.2758
Epoch: 80/200
Train loss: 2.4613
Epoch: 81/200
Train loss: 2.4698
Epoch: 82/200
Train loss: 2.4393
Epoch: 83/200
Train loss: 2.4143
Epoch: 84/200
Train loss: 2.3922
Epoch: 85/200
Train loss: 2.5263
Epoch: 86/200
Train loss: 2.4537
Epoch: 87/200
Train loss: 2.3306
Epoch: 88/200
Train loss: 2.5182
Epoch: 89/200
Train loss: 2.5383
Epoch: 90/200
Train loss: 2.3870
Epoch: 91/200
Train loss: 2.5060
Epoch: 92/200
Train loss: 2.4892
Epoch: 93/200
Train loss: 2.5122
Epoch: 94/200
Train loss: 2.4311
Epoch: 95/200
Train loss: 2.4794
Epoch: 96/200
Train loss: 2.5169
Epoch: 97/200
Train loss: 2.4631
Epoch: 98/200
Train loss: 2.4178
Epoch: 99/200
Train loss: 2.3492
Epoch: 100/200
Train loss: 2.4281
Epoch: 101/200
Train loss: 2.4822
Epoch: 102/200
Train loss: 2.2305
0.8077542077442097
Model improve: 0.000000 -> 0.807754
Epoch: 103/200
Date :05/16/2023, 14:23:38
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.15
drop_path_rate: 0.4
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Date :05/16/2023, 14:25:13
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.0
drop_path_rate: 0.4
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 42.2036
Epoch: 2/200
Train loss: 5.7272
Epoch: 3/200
Train loss: 5.2801
Epoch: 4/200
Train loss: 5.0034
Epoch: 5/200
Train loss: 4.7115
Epoch: 6/200
Train loss: 4.4692
Epoch: 7/200
Train loss: 4.1710
Epoch: 8/200
Train loss: 4.1053
Epoch: 9/200
Train loss: 3.9485
Epoch: 10/200
Train loss: 3.8588
Epoch: 11/200
Train loss: 3.7698
Epoch: 12/200
Train loss: 3.6549
Epoch: 13/200
Train loss: 3.6015
Epoch: 14/200
Train loss: 3.4882
Epoch: 15/200
Train loss: 3.4947
Epoch: 16/200
Train loss: 3.4622
Epoch: 17/200
Train loss: 3.3114
Epoch: 18/200
Train loss: 3.2218
Epoch: 19/200
Train loss: 3.4081
Epoch: 20/200
Train loss: 3.2936
Epoch: 21/200
Train loss: 3.1364
Epoch: 22/200
Train loss: 3.0250
Epoch: 23/200
Train loss: 3.0354
Epoch: 24/200
Train loss: 3.1167
Epoch: 25/200
Train loss: 3.0794
Epoch: 26/200
Train loss: 3.0733
Epoch: 27/200
Train loss: 3.0481
Epoch: 28/200
Train loss: 2.9073
Epoch: 29/200
Train loss: 2.9377
Epoch: 30/200
Train loss: 2.7739
Epoch: 31/200
Train loss: 2.9296
Epoch: 32/200
Train loss: 2.9614
Epoch: 33/200
Train loss: 2.7532
Epoch: 34/200
Train loss: 2.8334
Epoch: 35/200
Train loss: 2.8461
Epoch: 36/200
Train loss: 2.8619
Epoch: 37/200
Train loss: 2.7614
Epoch: 38/200
Train loss: 2.9809
Epoch: 39/200
Train loss: 2.6600
Epoch: 40/200
Train loss: 2.7860
Epoch: 41/200
Train loss: 2.7206
Epoch: 42/200
Train loss: 2.7111
Epoch: 43/200
Train loss: 2.7883
Epoch: 44/200
Train loss: 2.7719
Epoch: 45/200
Train loss: 2.7802
Epoch: 46/200
Train loss: 2.5443
Epoch: 47/200
Train loss: 2.7279
Epoch: 48/200
Train loss: 2.4963
Epoch: 49/200
Train loss: 2.4757
Epoch: 50/200
Train loss: 2.5337
Epoch: 51/200
Train loss: 2.5877
Epoch: 52/200
Train loss: 2.5939
Epoch: 53/200
Train loss: 2.4881
Epoch: 54/200
Train loss: 2.5838
Epoch: 55/200
Train loss: 2.5614
Epoch: 56/200
Train loss: 2.5250
Epoch: 57/200
Train loss: 2.5772
Epoch: 58/200
Train loss: 2.5329
Epoch: 59/200
Train loss: 2.5507
Epoch: 60/200
Train loss: 2.5519
Epoch: 61/200
Train loss: 2.4545
Epoch: 62/200
Train loss: 2.4749
Epoch: 63/200
Train loss: 2.4968
Epoch: 64/200
Train loss: 2.5248
Epoch: 65/200
Train loss: 2.5571
Epoch: 66/200
Train loss: 2.4278
Epoch: 67/200
Train loss: 2.5185
Epoch: 68/200
Train loss: 2.4876
Epoch: 69/200
Train loss: 2.6493
Epoch: 70/200
Train loss: 2.4619
Epoch: 71/200
Train loss: 2.3115
Epoch: 72/200
Train loss: 2.3345
Epoch: 73/200
Train loss: 2.4644
Epoch: 74/200
Train loss: 2.4389
Epoch: 75/200
Train loss: 2.5141
Epoch: 76/200
Train loss: 2.4597
Epoch: 77/200
Train loss: 2.4330
Epoch: 78/200
Train loss: 2.3421
Epoch: 79/200
Train loss: 2.1994
Epoch: 80/200
Train loss: 2.3590
Epoch: 81/200
Train loss: 2.3704
Epoch: 82/200
Train loss: 2.3323
Epoch: 83/200
Train loss: 2.3199
Epoch: 84/200
Train loss: 2.3070
Epoch: 85/200
Train loss: 2.4277
Epoch: 86/200
Train loss: 2.3783
Epoch: 87/200
Train loss: 2.2382
Epoch: 88/200
Train loss: 2.4370
Epoch: 89/200
Train loss: 2.4680
Epoch: 90/200
Train loss: 2.2943
Epoch: 91/200
Train loss: 2.4161
Epoch: 92/200
Train loss: 2.3923
Epoch: 93/200
Train loss: 2.4233
Epoch: 94/200
Train loss: 2.3359
Epoch: 95/200
Train loss: 2.3792
Epoch: 96/200
Train loss: 2.4238
Epoch: 97/200
Train loss: 2.3644
Epoch: 98/200
Train loss: 2.3398
Epoch: 99/200
Train loss: 2.2673
Epoch: 100/200
Train loss: 2.3281
Epoch: 101/200
Train loss: 2.3773
Epoch: 102/200
Train loss: 2.1680
0.8126374229757214
Model improve: 0.000000 -> 0.812637
Epoch: 103/200
Train loss: 2.2228
0.812421585574417
Epoch: 104/200
Train loss: 2.2257
0.8124715690961981
Epoch: 105/200
Train loss: 2.2731
0.8136145105990401
Model improve: 0.812637 -> 0.813615
Epoch: 106/200
Train loss: 2.2771
0.8114906746862088
Epoch: 107/200
Train loss: 2.3375
0.8112955549296845
Epoch: 108/200
Train loss: 2.3338
0.811389634619831
Epoch: 109/200
Train loss: 2.3150
0.8112503194709232
Epoch: 110/200
Train loss: 2.3464
0.8133200378141039
Epoch: 111/200
Train loss: 2.1330
0.8152986654920078
Model improve: 0.813615 -> 0.815299
Epoch: 112/200
Train loss: 2.2520
0.8118773913265197
Epoch: 113/200
Train loss: 2.2618
0.813538842322027
Epoch: 114/200
Train loss: 2.3749
0.8123368085808789
Epoch: 115/200
Train loss: 2.2438
Date :05/16/2023, 19:14:23
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.0
drop_path_rate: 0.4
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 42.1975
Epoch: 2/200
Train loss: 5.7271
Epoch: 3/200
Train loss: 5.2795
Epoch: 4/200
Train loss: 5.0023
Epoch: 5/200
Train loss: 4.7098
Epoch: 6/200
Train loss: 4.4681
Epoch: 7/200
Train loss: 4.1704
Epoch: 8/200
Train loss: 4.1049
Epoch: 9/200
Train loss: 3.9480
Epoch: 10/200
Train loss: 3.8580
Epoch: 11/200
Train loss: 3.7691
Epoch: 12/200
Train loss: 3.6556
Epoch: 13/200
Train loss: 3.6015
Epoch: 14/200
Train loss: 3.4886
Epoch: 15/200
Train loss: 3.4953
Epoch: 16/200
Train loss: 3.4623
Epoch: 17/200
Train loss: 3.3114
Epoch: 18/200
Train loss: 3.2221
Epoch: 19/200
Train loss: 3.4083
Epoch: 20/200
Train loss: 3.2934
Epoch: 21/200
Train loss: 3.1366
Epoch: 22/200
Train loss: 3.0250
Epoch: 23/200
Train loss: 3.0358
Epoch: 24/200
Train loss: 3.1168
Epoch: 25/200
Train loss: 3.0797
Epoch: 26/200
Train loss: 3.0734
Epoch: 27/200
Train loss: 3.0478
Epoch: 28/200
Train loss: 2.9068
Epoch: 29/200
Train loss: 2.9381
Epoch: 30/200
Train loss: 2.7739
Epoch: 31/200
Train loss: 2.9297
Epoch: 32/200
Train loss: 2.9618
Epoch: 33/200
Train loss: 2.7539
Epoch: 34/200
Train loss: 2.8337
Epoch: 35/200
Train loss: 2.8465
Epoch: 36/200
Train loss: 2.8614
Epoch: 37/200
Train loss: 2.7614
Epoch: 38/200
Train loss: 2.9822
Epoch: 39/200
Train loss: 2.6596
Epoch: 40/200
Train loss: 2.7859
Epoch: 41/200
Train loss: 2.7213
Epoch: 42/200
Train loss: 2.7119
Epoch: 43/200
Train loss: 2.7890
Epoch: 44/200
Train loss: 2.7719
Epoch: 45/200
Train loss: 2.7801
Epoch: 46/200
Train loss: 2.5443
Epoch: 47/200
Train loss: 2.7286
Epoch: 48/200
Train loss: 2.4960
Epoch: 49/200
Train loss: 2.4753
Epoch: 50/200
Train loss: 2.5349
Epoch: 51/200
Train loss: 2.5878
Epoch: 52/200
Train loss: 2.5939
Epoch: 53/200
Train loss: 2.4877
Epoch: 54/200
Train loss: 2.5837
Epoch: 55/200
Train loss: 2.5614
Epoch: 56/200
Train loss: 2.5248
Epoch: 57/200
Train loss: 2.5769
Epoch: 58/200
Train loss: 2.5331
Epoch: 59/200
Train loss: 2.5508
Epoch: 60/200
Train loss: 2.5519
Epoch: 61/200
Train loss: 2.4551
Epoch: 62/200
Train loss: 2.4747
Epoch: 63/200
Train loss: 2.4964
Epoch: 64/200
Train loss: 2.5249
Epoch: 65/200
Train loss: 2.5568
Epoch: 66/200
Train loss: 2.4282
Epoch: 67/200
Train loss: 2.5195
Epoch: 68/200
Train loss: 2.4882
Epoch: 69/200
Train loss: 2.6494
Epoch: 70/200
Train loss: 2.4624
Epoch: 71/200
Train loss: 2.3124
Epoch: 72/200
Train loss: 2.3349
Epoch: 73/200
Train loss: 2.4644
Epoch: 74/200
Train loss: 2.4384
Epoch: 75/200
Train loss: 2.5141
Epoch: 76/200
Train loss: 2.4588
Epoch: 77/200
Train loss: 2.4325
Epoch: 78/200
Train loss: 2.3416
Epoch: 79/200
Train loss: 2.1989
Epoch: 80/200
Train loss: 2.3594
Epoch: 81/200
Train loss: 2.3700
Epoch: 82/200
Train loss: 2.3321
Epoch: 83/200
Train loss: 2.3188
Epoch: 84/200
Train loss: 2.3065
Epoch: 85/200
Train loss: 2.4280
Epoch: 86/200
Train loss: 2.3789
Epoch: 87/200
Train loss: 2.2386
Epoch: 88/200
Train loss: 2.4377
Epoch: 89/200
Train loss: 2.4680
Epoch: 90/200
Train loss: 2.2940
Epoch: 91/200
Train loss: 2.4168
Epoch: 92/200
Train loss: 2.3920
Epoch: 93/200
Train loss: 2.4229
Epoch: 94/200
Train loss: 2.3357
Epoch: 95/200
Train loss: 2.3798
Epoch: 96/200
Train loss: 2.4239
Epoch: 97/200
Train loss: 2.3645
Epoch: 98/200
Train loss: 2.3403
Epoch: 99/200
Train loss: 2.2668
Epoch: 100/200
Train loss: 2.3277
Epoch: 101/200
Train loss: 2.3779
Epoch: 102/200
Train loss: 2.1676
0.8126510946343773
Model improve: 0.000000 -> 0.812651
Epoch: 103/200
Train loss: 2.2227
0.8123904901522446
Epoch: 104/200
Train loss: 2.2255
0.8125401622951732
Epoch: 105/200
Train loss: 2.2739
0.813714248535301
Model improve: 0.812651 -> 0.813714
Epoch: 106/200
Train loss: 2.2776
0.8115912132749437
Epoch: 107/200
Train loss: 2.3375
Date :05/16/2023, 23:05:28
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.3
drop_path_rate: 0.4
76407
Fold: 1
13839
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0003
    lr: 0.0003
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/200
Train loss: 42.2072
Epoch: 2/200
Train loss: 5.7270
Epoch: 3/200
Train loss: 5.2792
Epoch: 4/200
Train loss: 5.0021
Epoch: 5/200
Train loss: 4.7100
Epoch: 6/200
Train loss: 4.4686
Epoch: 7/200
Train loss: 4.1701
Epoch: 8/200
Train loss: 4.1042
Epoch: 9/200
Train loss: 3.9485
Epoch: 10/200
Train loss: 3.8582
Epoch: 11/200
Train loss: 3.7697
Epoch: 12/200
Train loss: 3.6548
Epoch: 13/200
Train loss: 3.6011
Epoch: 14/200
Train loss: 3.4888
Epoch: 15/200
Train loss: 3.4951
Epoch: 16/200
Train loss: 3.4620
Epoch: 17/200
Train loss: 3.3110
Epoch: 18/200
Train loss: 3.2219
Epoch: 19/200
Train loss: 3.4078
Epoch: 20/200
Train loss: 3.2935
Epoch: 21/200
Train loss: 3.1367
Epoch: 22/200
Train loss: 3.0249
Epoch: 23/200
Train loss: 3.0358
Epoch: 24/200
Train loss: 3.1164
Epoch: 25/200
Train loss: 3.0800
Epoch: 26/200
Train loss: 3.0730
Epoch: 27/200
Train loss: 3.0479
Epoch: 28/200
Train loss: 2.9068
Epoch: 29/200
Train loss: 2.9380
Epoch: 30/200
Train loss: 2.7737
Epoch: 31/200
Train loss: 2.9296
Epoch: 32/200
Train loss: 2.9614
Epoch: 33/200
Train loss: 2.7535
Epoch: 34/200
Train loss: 2.8336
Epoch: 35/200
Train loss: 2.8462
Epoch: 36/200
Train loss: 2.8611
Epoch: 37/200
Train loss: 2.7611
Epoch: 38/200
Train loss: 2.9817
Epoch: 39/200
Train loss: 2.6606
Epoch: 40/200
Train loss: 2.7860
Epoch: 41/200
Train loss: 2.7211
Epoch: 42/200
Train loss: 2.7113
Epoch: 43/200
Train loss: 2.7890
Epoch: 44/200
Train loss: 2.7720
Epoch: 45/200
Train loss: 2.7805
Epoch: 46/200
Train loss: 2.5440
Epoch: 47/200
Train loss: 2.7289
Epoch: 48/200
Train loss: 2.4955
Epoch: 49/200
Train loss: 2.4757
Epoch: 50/200
Train loss: 2.5344
Epoch: 51/200
Train loss: 2.5875
Epoch: 52/200
Train loss: 2.5952
Epoch: 53/200
Train loss: 2.4877
Epoch: 54/200
Train loss: 2.5835
Epoch: 55/200
Train loss: 2.5610
Epoch: 56/200
Train loss: 2.5250
Epoch: 57/200
Train loss: 2.5775
Epoch: 58/200
Train loss: 2.5330
Epoch: 59/200
Train loss: 2.5507
Epoch: 60/200
Train loss: 2.5516
Epoch: 61/200
Train loss: 2.4553
Epoch: 62/200
Train loss: 2.4746
Epoch: 63/200
Train loss: 2.4970
Epoch: 64/200
Train loss: 2.5250
Epoch: 65/200
Train loss: 2.5571
Epoch: 66/200
Train loss: 2.4289
Epoch: 67/200
Train loss: 2.5187
Epoch: 68/200
Train loss: 2.4880
Epoch: 69/200
Train loss: 2.6487
Epoch: 70/200
Train loss: 2.4623
Epoch: 71/200
Train loss: 2.3116
Epoch: 72/200
Train loss: 2.3348
Epoch: 73/200
Train loss: 2.4647
Epoch: 74/200
Train loss: 2.4379
Epoch: 75/200
Train loss: 2.5138
Epoch: 76/200
Train loss: 2.4590
Epoch: 77/200
Train loss: 2.4325
Epoch: 78/200
Train loss: 2.3429
Epoch: 79/200
Train loss: 2.1989
Epoch: 80/200
Train loss: 2.3590
Epoch: 81/200
Train loss: 2.3700
Epoch: 82/200
Train loss: 2.3328
Epoch: 83/200
Train loss: 2.3200
Epoch: 84/200
Train loss: 2.3064
Epoch: 85/200
Train loss: 2.4272
Epoch: 86/200
Train loss: 2.3784
Epoch: 87/200
Train loss: 2.2381
Epoch: 88/200
Train loss: 2.4371
Epoch: 89/200
Train loss: 2.4681
Epoch: 90/200
Train loss: 2.2944
Epoch: 91/200
Train loss: 2.4160
Epoch: 92/200
Train loss: 2.3920
Epoch: 93/200
Train loss: 2.4233
Epoch: 94/200
Train loss: 2.3357
Epoch: 95/200
Train loss: 2.3793
Epoch: 96/200
Train loss: 2.4236
Epoch: 97/200
Train loss: 2.3645
Epoch: 98/200
Train loss: 2.3400
Epoch: 99/200
Train loss: 2.2673
Epoch: 100/200
Train loss: 2.3277
Epoch: 101/200
Train loss: 2.3774
Epoch: 102/200
Train loss: 2.1670
0.8126069807501699
Model improve: 0.000000 -> 0.812607
Epoch: 103/200
Train loss: 2.2228
0.8124260470026243
Epoch: 104/200
Train loss: 2.2261
0.8124068191837663
Epoch: 105/200
Train loss: 2.2736
Date :05/17/2023, 03:11:50
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 512
epochwarmup: 0
totalepoch: 200
learningrate: 0.0003
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnetv2_b0
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
drop_rate: 0.25
drop_path_rate: 0.4
76407
Fold: 1
13839
