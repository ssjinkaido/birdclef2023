Date :04/06/2023, 04:33:59
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 50
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 3
use_spec_augmenter: False
Fold: 0
65299
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/50
Date :04/06/2023, 05:19:35
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 10
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
Fold: 0
65299
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Fold: 0
65299
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Fold: 0
65299
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 13.0549, val loss: 12.3438
Model improve: 0.0000 -> 0.2604
Epoch: 2/100
Fold: 0
65299
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :04/06/2023, 09:05:37
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 10
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
Fold: 0
65299
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :04/06/2023, 11:04:38
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 10
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
Fold: 0
65299
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :04/06/2023, 11:57:38
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 28
validbs: 112
epochwarmup: 10
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
Fold: 0
65299
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :04/06/2023, 08:21:23
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 28
validbs: 112
epochwarmup: 10
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
Fold: 0
65299
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :04/06/2023, 08:22:33
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 10
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
Fold: 0
65299
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 13.1488, val loss: 12.3270
Model improve: 0.0000 -> 0.2602
Epoch: 2/100
Date :04/06/2023, 09:54:08
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 10
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
Fold: 0
50592
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Fold: 0
50592
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Fold: 0
50592
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 13.0125, val loss: 12.0018
Model improve: 0.0000 -> 0.3333
Epoch: 2/100
Train loss: 12.0522, val loss: 11.4432
Model improve: 0.3333 -> 0.3402
Epoch: 3/100
Date :04/06/2023, 11:40:21
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
Fold: 0
50592
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :04/06/2023, 21:09:24
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
Fold: 0
10133
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :04/06/2023, 21:10:58
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
Fold: 0
10133
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 9.3945, val loss: 7.3325
Model improve: 0.0000 -> 0.6227
Epoch: 2/100
Train loss: 7.3971, val loss: 5.0158
Model improve: 0.6227 -> 0.7036
Epoch: 3/100
Fold: 0
10133
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 9.4701, val loss: 7.5522
Model improve: 0.0000 -> 0.6114
Epoch: 2/100
Train loss: 7.7599, val loss: 5.4022
Model improve: 0.6114 -> 0.6972
Epoch: 3/100
Train loss: 6.5638, val loss: 4.3282
Model improve: 0.6972 -> 0.7495
Epoch: 4/100
Train loss: 5.6205, val loss: 3.7532
Model improve: 0.7495 -> 0.7777
Epoch: 5/100
Train loss: 4.9441, val loss: 3.3507
Model improve: 0.7777 -> 0.8019
Epoch: 6/100
Train loss: 4.7713, val loss: 3.0148
Model improve: 0.8019 -> 0.8152
Epoch: 7/100
Train loss: 4.4504, val loss: 2.8924
Model improve: 0.8152 -> 0.8317
Epoch: 8/100
Train loss: 4.2316, val loss: 2.6446
Model improve: 0.8317 -> 0.8402
Epoch: 9/100
Train loss: 4.1344, val loss: 2.6119
Model improve: 0.8402 -> 0.8534
Epoch: 10/100
Train loss: 3.9658, val loss: 2.5512
Model improve: 0.8534 -> 0.8544
Epoch: 11/100
Train loss: 3.9552, val loss: 2.4728
Model improve: 0.8544 -> 0.8636
Epoch: 12/100
Train loss: 3.8353, val loss: 2.4437
Model improve: 0.8636 -> 0.8694
Epoch: 13/100
Train loss: 3.3202, val loss: 2.2679
Model improve: 0.8694 -> 0.8697
Epoch: 14/100
Train loss: 3.6391, val loss: 2.3999
Model improve: 0.8697 -> 0.8795
Epoch: 15/100
Train loss: 3.2663, val loss: 2.3090
Epoch: 16/100
Train loss: 3.5388, val loss: 2.3325
Epoch: 17/100
Train loss: 3.4387, val loss: 2.2801
Model improve: 0.8795 -> 0.8848
Epoch: 18/100
Train loss: 3.2368, val loss: 2.3339
Epoch: 19/100
Train loss: 3.3127, val loss: 2.2035
Model improve: 0.8848 -> 0.8881
Epoch: 20/100
Train loss: 3.1588, val loss: 2.1578
Epoch: 21/100
Train loss: 3.1308, val loss: 2.1207
Model improve: 0.8881 -> 0.8884
Epoch: 22/100
Train loss: 2.9874, val loss: 2.1215
Model improve: 0.8884 -> 0.8897
Epoch: 23/100
Train loss: 3.2044, val loss: 2.1483
Model improve: 0.8897 -> 0.8906
Epoch: 24/100
Train loss: 2.8441, val loss: 1.9979
Model improve: 0.8906 -> 0.8916
Epoch: 25/100
Train loss: 3.0183, val loss: 2.0065
Model improve: 0.8916 -> 0.8930
Epoch: 26/100
Train loss: 2.7150, val loss: 2.0921
Epoch: 27/100
Train loss: 2.8926, val loss: 2.0201
Model improve: 0.8930 -> 0.9005
Epoch: 28/100
Train loss: 3.0334, val loss: 2.1206
Epoch: 29/100
Train loss: 2.8043, val loss: 2.0959
Epoch: 30/100
Train loss: 2.8742, val loss: 2.0135
Epoch: 31/100
Train loss: 2.6714, val loss: 2.0629
Epoch: 32/100
Train loss: 2.6167, val loss: 2.0538
Epoch: 33/100
Train loss: 2.4941, val loss: 1.9215
Epoch: 34/100
Train loss: 2.8572, val loss: 1.9139
Model improve: 0.9005 -> 0.9018
Epoch: 35/100
Train loss: 2.5898, val loss: 1.9228
Epoch: 36/100
Train loss: 2.7599, val loss: 1.9356
Model improve: 0.9018 -> 0.9029
Epoch: 37/100
Train loss: 2.7715, val loss: 1.9934
Model improve: 0.9029 -> 0.9031
Epoch: 38/100
Train loss: 2.8245, val loss: 1.9550
Epoch: 39/100
Train loss: 2.6473, val loss: 2.0675
Model improve: 0.9031 -> 0.9046
Epoch: 40/100
Train loss: 2.5895, val loss: 1.8849
Model improve: 0.9046 -> 0.9049
Epoch: 41/100
Train loss: 2.8948, val loss: 1.9266
Epoch: 42/100
Train loss: 2.3257, val loss: 1.8926
Epoch: 43/100
Train loss: 2.3855, val loss: 1.9678
Model improve: 0.9049 -> 0.9075
Epoch: 44/100
Train loss: 2.3025, val loss: 2.0028
Epoch: 45/100
Train loss: 2.4466, val loss: 1.9332
Epoch: 46/100
Train loss: 2.4224, val loss: 2.0393
Epoch: 47/100
Train loss: 2.5747, val loss: 1.9347
Epoch: 48/100
Train loss: 2.3756, val loss: 1.8821
Epoch: 49/100
Train loss: 2.3760, val loss: 1.9472
Epoch: 50/100
Train loss: 2.3318, val loss: 1.8805
Epoch: 51/100
Train loss: 2.3999, val loss: 2.0028
Model improve: 0.9075 -> 0.9099
Epoch: 52/100
Train loss: 2.3232, val loss: 1.8763
Epoch: 53/100
Train loss: 2.3252, val loss: 2.0007
Epoch: 54/100
Train loss: 2.3782, val loss: 1.9668
Epoch: 55/100
Train loss: 2.1881, val loss: 1.8542
Epoch: 56/100
Train loss: 2.3296, val loss: 1.8120
Epoch: 57/100
Train loss: 2.2348, val loss: 1.9185
Epoch: 58/100
Train loss: 2.1621, val loss: 1.7923
Epoch: 59/100
Train loss: 2.4383, val loss: 1.7957
Epoch: 60/100
Train loss: 2.0671, val loss: 1.8239
Epoch: 61/100
Train loss: 2.2304, val loss: 1.7843
Epoch: 62/100
Train loss: 2.2619, val loss: 1.8343
Epoch: 63/100
Train loss: 1.9669, val loss: 1.8924
Epoch: 64/100
Train loss: 2.0598, val loss: 1.8211
Epoch: 65/100
Train loss: 2.1228, val loss: 1.8543
Epoch: 66/100
Train loss: 2.0744, val loss: 1.8245
Model improve: 0.9099 -> 0.9109
Epoch: 67/100
Train loss: 2.2064, val loss: 1.8337
Epoch: 68/100
Date :04/08/2023, 02:50:14
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.0004
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
Fold: 0
10133
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Fold: 0
10133
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Fold: 0
10133
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Fold: 0
10133
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0004
    lr: 0.0004
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 8.9968, val loss: 6.7195
Model improve: 0.0000 -> 0.6554
Epoch: 2/100
Date :04/14/2023, 01:53:23
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
56754
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/20
Date :04/14/2023, 01:55:00
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
56754
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/20
Date :04/14/2023, 01:55:21
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
56754
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/20
Train loss: 10.1668, val loss: 6.7136
Model improve: 0.0000 -> 0.4654
Epoch: 2/20
Train loss: 8.1360, val loss: 5.2042
Model improve: 0.4654 -> 0.5851
Epoch: 3/20
Train loss: 7.3334, val loss: 4.3216
Model improve: 0.5851 -> 0.6591
Epoch: 4/20
Train loss: 6.8761, val loss: 3.7459
Model improve: 0.6591 -> 0.7003
Epoch: 5/20
Train loss: 6.4976, val loss: 3.6992
Model improve: 0.7003 -> 0.7236
Epoch: 6/20
Train loss: 6.2022, val loss: 3.4087
Model improve: 0.7236 -> 0.7406
Epoch: 7/20
Date :04/14/2023, 03:31:57
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
56754
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/20
Date :04/14/2023, 03:32:08
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
56754
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/20
Date :04/14/2023, 03:32:52
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
56754
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/20
Date :04/14/2023, 03:33:31
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
56754
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/20
Date :04/14/2023, 03:45:24
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
56754
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/20
Date :04/14/2023, 03:46:00
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
56754
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/20
Train loss: 8.2933, val loss: 4.1878
Model improve: 0.0000 -> 0.5423
Epoch: 2/20
Train loss: 4.9820, val loss: 3.3301
Model improve: 0.5423 -> 0.6709
Epoch: 3/20
Train loss: 4.4401, val loss: 2.7490
Model improve: 0.6709 -> 0.7460
Epoch: 4/20
Train loss: 4.1741, val loss: 2.5154
Model improve: 0.7460 -> 0.7774
Epoch: 5/20
Train loss: 3.9238, val loss: 2.3848
Model improve: 0.7774 -> 0.8005
Epoch: 6/20
Train loss: 3.7046, val loss: 2.1743
Model improve: 0.8005 -> 0.8227
Epoch: 7/20
Train loss: 3.6365, val loss: 2.0324
Model improve: 0.8227 -> 0.8356
Epoch: 8/20
Train loss: 3.4501, val loss: 1.8346
Model improve: 0.8356 -> 0.8493
Epoch: 9/20
Train loss: 3.3263, val loss: 1.7819
Model improve: 0.8493 -> 0.8552
Epoch: 10/20
Train loss: 3.2202, val loss: 1.6395
Model improve: 0.8552 -> 0.8680
Epoch: 11/20
Train loss: 3.0804, val loss: 1.6285
Model improve: 0.8680 -> 0.8744
Epoch: 12/20
Train loss: 2.9353, val loss: 1.5778
Model improve: 0.8744 -> 0.8817
Epoch: 13/20
Train loss: 2.9198, val loss: 1.4899
Model improve: 0.8817 -> 0.8862
Epoch: 14/20
Train loss: 2.7639, val loss: 1.5435
Model improve: 0.8862 -> 0.8892
Epoch: 15/20
Train loss: 2.7747, val loss: 1.3792
Model improve: 0.8892 -> 0.8937
Epoch: 16/20
Train loss: 2.6856, val loss: 1.3883
Model improve: 0.8937 -> 0.8959
Epoch: 17/20
Train loss: 2.6606, val loss: 1.3890
Model improve: 0.8959 -> 0.8974
Epoch: 18/20
Train loss: 2.6436, val loss: 1.3862
Model improve: 0.8974 -> 0.8978
Epoch: 19/20
Train loss: 2.5188, val loss: 1.4093
Model improve: 0.8978 -> 0.8987
Epoch: 20/20
Train loss: 2.6000, val loss: 1.3195
Model improve: 0.8987 -> 0.8989
Date :04/14/2023, 18:05:28
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
56754
Date :04/14/2023, 12:33:48
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
61126
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/20
Date :04/14/2023, 12:36:33
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Date :04/14/2023, 12:37:31
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Date :04/14/2023, 12:38:56
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Date :04/14/2023, 12:40:46
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Date :04/14/2023, 12:42:17
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Date :04/14/2023, 12:43:51
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
61126
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :04/14/2023, 12:44:41
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
61126
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :04/14/2023, 12:45:29
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
61126
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :04/14/2023, 12:58:56
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
61125
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :04/14/2023, 13:00:25
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
61125
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :04/14/2023, 13:09:04
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
61125
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :04/14/2023, 13:10:17
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
61125
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 7.3534, val loss: 4.2805
Model improve: 0.0000 -> 0.5571
Epoch: 2/60
Date :04/14/2023, 13:26:47
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
61125
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 7.6623, val loss: 4.5836
Model improve: 0.0000 -> 0.5268
Epoch: 2/60
Date :04/14/2023, 13:50:53
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
61125
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 7.3534, val loss: 4.2805
Model improve: 0.0000 -> 0.5571
Epoch: 2/60
Train loss: 5.0069, val loss: 3.2531
Model improve: 0.5571 -> 0.6872
Epoch: 3/60
Train loss: 4.4887, val loss: 2.8726
Model improve: 0.6872 -> 0.7377
Epoch: 4/60
Train loss: 4.2071, val loss: 2.5539
Model improve: 0.7377 -> 0.7812
Epoch: 5/60
Train loss: 3.9574, val loss: 2.3337
Model improve: 0.7812 -> 0.8044
Epoch: 6/60
Train loss: 3.7914, val loss: 2.2420
Model improve: 0.8044 -> 0.8216
Epoch: 7/60
Train loss: 3.6426, val loss: 2.2943
Model improve: 0.8216 -> 0.8296
Epoch: 8/60
Train loss: 3.5703, val loss: 2.0785
Model improve: 0.8296 -> 0.8396
Epoch: 9/60
Train loss: 3.4328, val loss: 1.9975
Model improve: 0.8396 -> 0.8469
Epoch: 10/60
Train loss: 3.3822, val loss: 1.9190
Model improve: 0.8469 -> 0.8545
Epoch: 11/60
Train loss: 3.3395, val loss: 1.8554
Model improve: 0.8545 -> 0.8603
Epoch: 12/60
Train loss: 3.2149, val loss: 1.7696
Model improve: 0.8603 -> 0.8653
Epoch: 13/60
Train loss: 3.1625, val loss: 1.8155
Model improve: 0.8653 -> 0.8657
Epoch: 14/60
Train loss: 3.1224, val loss: 1.7521
Model improve: 0.8657 -> 0.8718
Epoch: 15/60
Train loss: 3.0853, val loss: 1.7452
Model improve: 0.8718 -> 0.8735
Epoch: 16/60
Train loss: 3.0668, val loss: 1.7176
Model improve: 0.8735 -> 0.8741
Epoch: 17/60
Train loss: 3.0045, val loss: 1.6985
Model improve: 0.8741 -> 0.8799
Epoch: 18/60
Train loss: 2.9624, val loss: 1.5981
Model improve: 0.8799 -> 0.8822
Epoch: 19/60
Train loss: 2.9334, val loss: 1.6459
Model improve: 0.8822 -> 0.8839
Epoch: 20/60
Train loss: 2.9116, val loss: 1.5850
Model improve: 0.8839 -> 0.8853
Epoch: 21/60
Train loss: 2.8612, val loss: 1.6097
Model improve: 0.8853 -> 0.8868
Epoch: 22/60
Train loss: 2.8355, val loss: 1.6416
Model improve: 0.8868 -> 0.8884
Epoch: 23/60
Train loss: 2.7678, val loss: 1.5235
Epoch: 24/60
Train loss: 2.7301, val loss: 1.5325
Model improve: 0.8884 -> 0.8895
Epoch: 25/60
Train loss: 2.7056, val loss: 1.5480
Model improve: 0.8895 -> 0.8931
Epoch: 26/60
Train loss: 2.7013, val loss: 1.5336
Epoch: 27/60
Train loss: 2.6660, val loss: 1.4835
Model improve: 0.8931 -> 0.8946
Epoch: 28/60
Train loss: 2.6011, val loss: 1.5106
Model improve: 0.8946 -> 0.8965
Epoch: 29/60
Train loss: 2.6352, val loss: 1.5866
Epoch: 30/60
Train loss: 2.5860, val loss: 1.4594
Model improve: 0.8965 -> 0.8969
Epoch: 31/60
Train loss: 2.5205, val loss: 1.5364
Model improve: 0.8969 -> 0.8987
Epoch: 32/60
Train loss: 2.5062, val loss: 1.4120
Model improve: 0.8987 -> 0.9002
Epoch: 33/60
Train loss: 2.4814, val loss: 1.5455
Model improve: 0.9002 -> 0.9013
Epoch: 34/60
Train loss: 2.4601, val loss: 1.4580
Epoch: 35/60
Train loss: 2.4363, val loss: 1.4490
Model improve: 0.9013 -> 0.9026
Epoch: 36/60
Train loss: 2.3975, val loss: 1.3868
Model improve: 0.9026 -> 0.9050
Epoch: 37/60
Train loss: 2.4217, val loss: 1.4663
Epoch: 38/60
Train loss: 2.3903, val loss: 1.4101
Epoch: 39/60
Train loss: 2.3248, val loss: 1.3774
Model improve: 0.9050 -> 0.9053
Epoch: 40/60
Train loss: 2.2562, val loss: 1.3813
Model improve: 0.9053 -> 0.9063
Epoch: 41/60
Train loss: 2.2895, val loss: 1.3372
Model improve: 0.9063 -> 0.9083
Epoch: 42/60
Train loss: 2.2660, val loss: 1.3443
Epoch: 43/60
Train loss: 2.2495, val loss: 1.3557
Epoch: 44/60
Train loss: 2.2272, val loss: 1.3262
Model improve: 0.9083 -> 0.9090
Epoch: 45/60
Train loss: 2.2323, val loss: 1.3185
Epoch: 46/60
Train loss: 2.1951, val loss: 1.3113
Model improve: 0.9090 -> 0.9104
Epoch: 47/60
Train loss: 2.2142, val loss: 1.3272
Epoch: 48/60
Train loss: 2.2093, val loss: 1.3044
Model improve: 0.9104 -> 0.9106
Epoch: 49/60
Train loss: 2.1050, val loss: 1.3029
Model improve: 0.9106 -> 0.9110
Epoch: 50/60
Train loss: 2.1595, val loss: 1.2930
Epoch: 51/60
Train loss: 2.1268, val loss: 1.2834
Epoch: 52/60
Train loss: 2.1551, val loss: 1.3157
Model improve: 0.9110 -> 0.9113
Epoch: 53/60
Train loss: 2.1510, val loss: 1.3183
Model improve: 0.9113 -> 0.9118
Epoch: 54/60
Train loss: 2.0764, val loss: 1.2748
Epoch: 55/60
Train loss: 2.1203, val loss: 1.2691
Epoch: 56/60
Train loss: 2.0848, val loss: 1.2828
Epoch: 57/60
Date :04/15/2023, 03:35:56
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/20
Date :04/15/2023, 03:37:18
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/20
Train loss: 7.5087, val loss: 4.2434
Val cmap: 0.5739024452197213
Model improve: 0.0000 -> 0.5739
Epoch: 2/20
Train loss: 5.0782, val loss: 3.2777
Val cmap: 0.6940529615649186
Model improve: 0.5739 -> 0.6941
Epoch: 3/20
Train loss: 4.5494, val loss: 2.7395
Val cmap: 0.7678023189804866
Model improve: 0.6941 -> 0.7678
Epoch: 4/20
Train loss: 4.1963, val loss: 2.4670
Val cmap: 0.7966026643077108
Model improve: 0.7678 -> 0.7966
Epoch: 5/20
Train loss: 3.9216, val loss: 2.2840
Val cmap: 0.8141824562779284
Model improve: 0.7966 -> 0.8142
Epoch: 6/20
Date :04/15/2023, 04:56:46
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.005
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
61125
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.005
    lr: 0.005
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 7.7162, val loss: 6.0162
Val cmap: 0.3328251737205878
Model improve: 0.0000 -> 0.3328
Epoch: 2/60
Date :04/15/2023, 05:19:20
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 14000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
61125
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 7.2091, val loss: 4.2409
Val cmap: 0.5655011990597029
Model improve: 0.0000 -> 0.5655
Epoch: 2/60
Train loss: 5.0151, val loss: 3.3602
Val cmap: 0.6775810945239362
Model improve: 0.5655 -> 0.6776
Epoch: 3/60
Train loss: 4.5269, val loss: 2.8638
Val cmap: 0.7425355381219121
Model improve: 0.6776 -> 0.7425
Epoch: 4/60
Train loss: 4.2372, val loss: 2.6059
Val cmap: 0.7756608295651634
Model improve: 0.7425 -> 0.7757
Epoch: 5/60
Train loss: 3.9869, val loss: 2.3326
Val cmap: 0.8023306739910359
Model improve: 0.7757 -> 0.8023
Epoch: 6/60
Train loss: 3.8106, val loss: 2.3230
Val cmap: 0.8158590768161635
Model improve: 0.8023 -> 0.8159
Epoch: 7/60
Train loss: 3.6687, val loss: 2.2961
Val cmap: 0.8238467482671255
Model improve: 0.8159 -> 0.8238
Epoch: 8/60
Train loss: 3.5943, val loss: 2.0899
Val cmap: 0.8369504884913734
Model improve: 0.8238 -> 0.8370
Epoch: 9/60
Date :04/15/2023, 07:15:02
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 50
nmels: 128
fmax: 14000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
61125
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 7.6378, val loss: 4.4718
Val cmap: 0.5253258481029788
Model improve: 0.0000 -> 0.5253
Epoch: 2/60
Date :04/15/2023, 07:30:06
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
61125
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 7.5285, val loss: 4.5460
Val cmap: 0.5275614252251506
Model improve: 0.0000 -> 0.5276
Epoch: 2/60
Date :04/15/2023, 07:47:08
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 7.3371, val loss: 4.2853
Val cmap: 0.5719765346815018
Model improve: 0.0000 -> 0.5720
Epoch: 2/60
Train loss: 4.9862, val loss: 3.1964
Val cmap: 0.7023262337082612
Model improve: 0.5720 -> 0.7023
Epoch: 3/60
Train loss: 4.4724, val loss: 2.7612
Val cmap: 0.7661189412765902
Model improve: 0.7023 -> 0.7661
Epoch: 4/60
Train loss: 4.1339, val loss: 2.4889
Val cmap: 0.800680399466851
Model improve: 0.7661 -> 0.8007
Epoch: 5/60
Train loss: 3.8843, val loss: 2.2767
Val cmap: 0.8174613824191926
Model improve: 0.8007 -> 0.8175
Epoch: 6/60
Train loss: 3.7188, val loss: 2.1163
Val cmap: 0.8335205202882776
Model improve: 0.8175 -> 0.8335
Epoch: 7/60
Train loss: 3.5916, val loss: 2.1250
Val cmap: 0.8384074252810061
Model improve: 0.8335 -> 0.8384
Epoch: 8/60
Train loss: 3.4829, val loss: 1.9990
Val cmap: 0.8521579992519387
Model improve: 0.8384 -> 0.8522
Epoch: 9/60
Train loss: 3.3730, val loss: 1.9647
Val cmap: 0.8545366034251302
Model improve: 0.8522 -> 0.8545
Epoch: 10/60
Train loss: 3.3120, val loss: 1.8872
Val cmap: 0.8619017369366908
Model improve: 0.8545 -> 0.8619
Epoch: 11/60
Train loss: 3.2119, val loss: 1.8359
Val cmap: 0.8666893642957509
Model improve: 0.8619 -> 0.8667
Epoch: 12/60
Train loss: 3.1417, val loss: 1.8371
Val cmap: 0.8705944199076181
Model improve: 0.8667 -> 0.8706
Epoch: 13/60
Train loss: 3.1105, val loss: 1.7612
Val cmap: 0.8765051022518145
Model improve: 0.8706 -> 0.8765
Epoch: 14/60
Train loss: 3.0579, val loss: 1.6667
Val cmap: 0.8791711138909163
Model improve: 0.8765 -> 0.8792
Epoch: 15/60
Train loss: 3.0201, val loss: 1.7469
Val cmap: 0.8789183285406812
Epoch: 16/60
Train loss: 2.9763, val loss: 1.7924
Val cmap: 0.8784394050009084
Epoch: 17/60
Train loss: 2.9288, val loss: 1.7117
Val cmap: 0.8830612988063756
Model improve: 0.8792 -> 0.8831
Epoch: 18/60
Train loss: 2.8999, val loss: 1.7951
Val cmap: 0.8836905331511954
Model improve: 0.8831 -> 0.8837
Epoch: 19/60
Train loss: 2.8884, val loss: 1.6333
Val cmap: 0.8890801071635379
Model improve: 0.8837 -> 0.8891
Epoch: 20/60
Train loss: 2.8166, val loss: 1.6541
Val cmap: 0.8897063990660793
Model improve: 0.8891 -> 0.8897
Epoch: 21/60
Train loss: 2.7693, val loss: 1.6330
Val cmap: 0.8919599301338158
Model improve: 0.8897 -> 0.8920
Epoch: 22/60
Train loss: 2.7107, val loss: 1.5658
Val cmap: 0.8932038385552024
Model improve: 0.8920 -> 0.8932
Epoch: 23/60
Train loss: 2.7185, val loss: 1.6003
Val cmap: 0.8931650775158455
Epoch: 24/60
Train loss: 2.6611, val loss: 1.5415
Val cmap: 0.8934773890895
Model improve: 0.8932 -> 0.8935
Epoch: 25/60
Train loss: 2.6651, val loss: 1.4984
Val cmap: 0.8970492151818088
Model improve: 0.8935 -> 0.8970
Epoch: 26/60
Train loss: 2.5951, val loss: 1.5571
Val cmap: 0.898003773538247
Model improve: 0.8970 -> 0.8980
Epoch: 27/60
Train loss: 2.6077, val loss: 1.5110
Val cmap: 0.8986333643224154
Model improve: 0.8980 -> 0.8986
Epoch: 28/60
Train loss: 2.5752, val loss: 1.5501
Val cmap: 0.8985233629773276
Epoch: 29/60
Train loss: 2.5054, val loss: 1.5609
Val cmap: 0.8996638346428538
Model improve: 0.8986 -> 0.8997
Epoch: 30/60
Train loss: 2.5013, val loss: 1.4825
Val cmap: 0.9020326678293537
Model improve: 0.8997 -> 0.9020
Epoch: 31/60
Train loss: 2.4788, val loss: 1.5051
Val cmap: 0.9029746771463409
Model improve: 0.9020 -> 0.9030
Epoch: 32/60
Train loss: 2.4458, val loss: 1.5239
Val cmap: 0.9026938485538338
Epoch: 33/60
Train loss: 2.3977, val loss: 1.4015
Val cmap: 0.9040895264676923
Model improve: 0.9030 -> 0.9041
Epoch: 34/60
Train loss: 2.4018, val loss: 1.3895
Val cmap: 0.9059580398211418
Model improve: 0.9041 -> 0.9060
Epoch: 35/60
Train loss: 2.3908, val loss: 1.4440
Val cmap: 0.9062614592489858
Model improve: 0.9060 -> 0.9063
Epoch: 36/60
Train loss: 2.3747, val loss: 1.3555
Val cmap: 0.9072319354619133
Model improve: 0.9063 -> 0.9072
Epoch: 37/60
Train loss: 2.2760, val loss: 1.4298
Val cmap: 0.9074766569816084
Model improve: 0.9072 -> 0.9075
Epoch: 38/60
Train loss: 2.2604, val loss: 1.3558
Val cmap: 0.9059411554008894
Epoch: 39/60
Train loss: 2.2936, val loss: 1.3756
Val cmap: 0.907324640217552
Epoch: 40/60
Train loss: 2.2578, val loss: 1.3948
Val cmap: 0.9091711676669647
Model improve: 0.9075 -> 0.9092
Epoch: 41/60
Train loss: 2.2305, val loss: 1.3264
Val cmap: 0.908887204254144
Epoch: 42/60
Train loss: 2.2319, val loss: 1.3366
Val cmap: 0.9090808004752954
Epoch: 43/60
Train loss: 2.1805, val loss: 1.3264
Val cmap: 0.909869075908185
Model improve: 0.9092 -> 0.9099
Epoch: 44/60
Train loss: 2.1869, val loss: 1.3599
Val cmap: 0.9108897214853465
Model improve: 0.9099 -> 0.9109
Epoch: 45/60
Train loss: 2.1938, val loss: 1.3194
Val cmap: 0.9110870781460553
Model improve: 0.9109 -> 0.9111
Epoch: 46/60
Train loss: 2.0928, val loss: 1.3302
Val cmap: 0.9119667324181094
Model improve: 0.9111 -> 0.9120
Epoch: 47/60
Train loss: 2.1123, val loss: 1.3206
Val cmap: 0.9116307858537915
Epoch: 48/60
Train loss: 2.1159, val loss: 1.3124
Val cmap: 0.9118838915612666
Epoch: 49/60
Train loss: 2.1101, val loss: 1.3105
Val cmap: 0.9118036365213257
Epoch: 50/60
Train loss: 2.1158, val loss: 1.3354
Val cmap: 0.9121943059872198
Model improve: 0.9120 -> 0.9122
Epoch: 51/60
Train loss: 2.0592, val loss: 1.3230
Val cmap: 0.9131904140239537
Model improve: 0.9122 -> 0.9132
Epoch: 52/60
Train loss: 2.0842, val loss: 1.2805
Val cmap: 0.9138890874287176
Model improve: 0.9132 -> 0.9139
Epoch: 53/60
Train loss: 2.0514, val loss: 1.2933
Val cmap: 0.9135247173976098
Epoch: 54/60
Train loss: 1.9939, val loss: 1.3400
Val cmap: 0.9132656280364647
Epoch: 55/60
Train loss: 2.0441, val loss: 1.3429
Val cmap: 0.9136092860868414
Epoch: 56/60
Train loss: 2.0299, val loss: 1.3044
Val cmap: 0.9139878683968342
Model improve: 0.9139 -> 0.9140
Epoch: 57/60
Train loss: 2.0271, val loss: 1.3191
Val cmap: 0.9140674077992431
Model improve: 0.9140 -> 0.9141
Epoch: 58/60
Train loss: 2.0639, val loss: 1.3003
Val cmap: 0.9138435493443604
Epoch: 59/60
Train loss: 2.0251, val loss: 1.2822
Val cmap: 0.9138000432461768
Epoch: 60/60
Train loss: 2.0384, val loss: 1.2962
Val cmap: 0.9144199963478172
Model improve: 0.9141 -> 0.9144
Date :04/16/2023, 00:00:32
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
79784
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Model improve: 999.0000 -> 7.0615
Epoch: 2/60
Model improve: 7.0615 -> 4.8011
Epoch: 3/60
Model improve: 4.8011 -> 4.3084
Epoch: 4/60
Model improve: 4.3084 -> 3.9852
Epoch: 5/60
Model improve: 3.9852 -> 3.7584
Epoch: 6/60
Model improve: 3.7584 -> 3.6463
Epoch: 7/60
Model improve: 3.6463 -> 3.4951
Epoch: 8/60
Model improve: 3.4951 -> 3.4093
Epoch: 9/60
Model improve: 3.4093 -> 3.3111
Epoch: 10/60
Model improve: 3.3111 -> 3.2353
Epoch: 11/60
Model improve: 3.2353 -> 3.1644
Epoch: 12/60
Model improve: 3.1644 -> 3.1158
Epoch: 13/60
Model improve: 3.1158 -> 3.0991
Epoch: 14/60
Model improve: 3.0991 -> 3.0314
Epoch: 15/60
Model improve: 3.0314 -> 3.0247
Epoch: 16/60
Model improve: 3.0247 -> 2.9529
Epoch: 17/60
Model improve: 2.9529 -> 2.9262
Epoch: 18/60
Model improve: 2.9262 -> 2.8291
Epoch: 19/60
Epoch: 20/60
Model improve: 2.8291 -> 2.8028
Epoch: 21/60
Model improve: 2.8028 -> 2.7647
Epoch: 22/60
Epoch: 23/60
Model improve: 2.7647 -> 2.7347
Epoch: 24/60
Model improve: 2.7347 -> 2.6731
Epoch: 25/60
Model improve: 2.6731 -> 2.6526
Epoch: 26/60
Model improve: 2.6526 -> 2.6405
Epoch: 27/60
Model improve: 2.6405 -> 2.6032
Epoch: 28/60
Model improve: 2.6032 -> 2.5763
Epoch: 29/60
Epoch: 30/60
Model improve: 2.5763 -> 2.4802
Epoch: 31/60
Model improve: 2.4802 -> 2.4651
Epoch: 32/60
Epoch: 33/60
Model improve: 2.4651 -> 2.4633
Epoch: 34/60
Model improve: 2.4633 -> 2.4224
Epoch: 35/60
Model improve: 2.4224 -> 2.3989
Epoch: 36/60
Model improve: 2.3989 -> 2.3772
Epoch: 37/60
Epoch: 38/60
Model improve: 2.3772 -> 2.2858
Epoch: 39/60
Epoch: 40/60
Epoch: 41/60
Model improve: 2.2858 -> 2.2805
Epoch: 42/60
Model improve: 2.2805 -> 2.2369
Epoch: 43/60
Model improve: 2.2369 -> 2.1981
Epoch: 44/60
Model improve: 2.1981 -> 2.1968
Epoch: 45/60
Model improve: 2.1968 -> 2.1737
Epoch: 46/60
Epoch: 47/60
Model improve: 2.1737 -> 2.1523
Epoch: 48/60
Model improve: 2.1523 -> 2.1517
Epoch: 49/60
Model improve: 2.1517 -> 2.1306
Epoch: 50/60
Model improve: 2.1306 -> 2.1256
Epoch: 51/60
Epoch: 52/60
Model improve: 2.1256 -> 2.1166
Epoch: 53/60
Model improve: 2.1166 -> 2.0963
Epoch: 54/60
Model improve: 2.0963 -> 2.0695
Epoch: 55/60
Epoch: 56/60
Model improve: 2.0695 -> 2.0507
Epoch: 57/60
Model improve: 2.0507 -> 2.0491
Epoch: 58/60
Model improve: 2.0491 -> 2.0452
Epoch: 59/60
Epoch: 60/60
Date :04/16/2023, 16:30:06
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
79784
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 7.0615, val loss: 3.7811
Val cmap: 0.6467040193028535
Model improve: 0.0000 -> 0.6467
Epoch: 2/60
Train loss: 4.8121, val loss: 2.8679
Val cmap: 0.7560348167501948
Model improve: 0.6467 -> 0.7560
Epoch: 3/60
Train loss: 4.3080, val loss: 2.3606
Val cmap: 0.8134844163540257
Model improve: 0.7560 -> 0.8135
Epoch: 4/60
Train loss: 3.9697, val loss: 2.0317
Val cmap: 0.8553141974689629
Model improve: 0.8135 -> 0.8553
Epoch: 5/60
Train loss: 3.7305, val loss: 1.8787
Val cmap: 0.8750235498741146
Model improve: 0.8553 -> 0.8750
Epoch: 6/60
Train loss: 3.6435, val loss: 1.7470
Val cmap: 0.8876862775702891
Model improve: 0.8750 -> 0.8877
Epoch: 7/60
Train loss: 3.4868, val loss: 1.7206
Val cmap: 0.9013745917377229
Model improve: 0.8877 -> 0.9014
Epoch: 8/60
Train loss: 3.3957, val loss: 1.5287
Val cmap: 0.9082636136728113
Model improve: 0.9014 -> 0.9083
Epoch: 9/60
Train loss: 3.3190, val loss: 1.4779
Val cmap: 0.9199215651085361
Model improve: 0.9083 -> 0.9199
Epoch: 10/60
Train loss: 3.2402, val loss: 1.3646
Val cmap: 0.9252394703440949
Model improve: 0.9199 -> 0.9252
Epoch: 11/60
Train loss: 3.1691, val loss: 1.3910
Val cmap: 0.9301750804278204
Model improve: 0.9252 -> 0.9302
Epoch: 12/60
Train loss: 3.1143, val loss: 1.3090
Val cmap: 0.9345789273400166
Model improve: 0.9302 -> 0.9346
Epoch: 13/60
Train loss: 3.0877, val loss: 1.2194
Val cmap: 0.938027800564871
Model improve: 0.9346 -> 0.9380
Epoch: 14/60
Train loss: 3.0305, val loss: 1.2361
Val cmap: 0.9400111997988655
Model improve: 0.9380 -> 0.9400
Epoch: 15/60
Train loss: 3.0142, val loss: 1.2767
Val cmap: 0.9432032980464017
Model improve: 0.9400 -> 0.9432
Epoch: 16/60
Train loss: 2.9566, val loss: 1.1456
Val cmap: 0.9473856004567961
Model improve: 0.9432 -> 0.9474
Epoch: 17/60
Train loss: 2.9191, val loss: 1.1851
Val cmap: 0.9501086652032155
Model improve: 0.9474 -> 0.9501
Epoch: 18/60
Train loss: 2.8150, val loss: 1.0539
Val cmap: 0.9523088524516805
Model improve: 0.9501 -> 0.9523
Epoch: 19/60
Train loss: 2.8492, val loss: 1.0701
Val cmap: 0.9541654456097023
Model improve: 0.9523 -> 0.9542
Epoch: 20/60
Train loss: 2.8062, val loss: 1.0773
Val cmap: 0.9559392827720204
Model improve: 0.9542 -> 0.9559
Epoch: 21/60
Train loss: 2.7551, val loss: 0.9885
Val cmap: 0.9579578534750731
Model improve: 0.9559 -> 0.9580
Epoch: 22/60
Train loss: 2.7734, val loss: 1.0501
Val cmap: 0.9613138847311975
Model improve: 0.9580 -> 0.9613
Epoch: 23/60
Train loss: 2.7379, val loss: 1.0258
Val cmap: 0.9620800815890351
Model improve: 0.9613 -> 0.9621
Epoch: 24/60
Train loss: 2.6623, val loss: 0.9304
Val cmap: 0.9650353749124174
Model improve: 0.9621 -> 0.9650
Epoch: 25/60
Train loss: 2.6400, val loss: 0.8653
Val cmap: 0.9661350652268624
Model improve: 0.9650 -> 0.9661
Epoch: 26/60
Train loss: 2.6304, val loss: 0.9034
Val cmap: 0.967663331352765
Model improve: 0.9661 -> 0.9677
Epoch: 27/60
Train loss: 2.6050, val loss: 0.9123
Val cmap: 0.9680101888921767
Model improve: 0.9677 -> 0.9680
Epoch: 28/60
Train loss: 2.5680, val loss: 0.8928
Val cmap: 0.9703713140896182
Model improve: 0.9680 -> 0.9704
Epoch: 29/60
Train loss: 2.5789, val loss: 0.7762
Val cmap: 0.971191579388965
Model improve: 0.9704 -> 0.9712
Epoch: 30/60
Train loss: 2.4788, val loss: 0.7201
Val cmap: 0.9732842840214296
Model improve: 0.9712 -> 0.9733
Epoch: 31/60
Train loss: 2.4594, val loss: 0.7957
Val cmap: 0.9737668737440089
Model improve: 0.9733 -> 0.9738
Epoch: 32/60
Train loss: 2.4856, val loss: 0.6891
Val cmap: 0.9756594425874187
Model improve: 0.9738 -> 0.9757
Epoch: 33/60
Train loss: 2.4563, val loss: 0.7206
Val cmap: 0.9762330465080726
Model improve: 0.9757 -> 0.9762
Epoch: 34/60
Train loss: 2.4140, val loss: 0.7619
Val cmap: 0.9762798299209428
Model improve: 0.9762 -> 0.9763
Epoch: 35/60
Train loss: 2.3994, val loss: 0.6880
Val cmap: 0.9783117924127448
Model improve: 0.9763 -> 0.9783
Epoch: 36/60
Train loss: 2.3751, val loss: 0.6313
Val cmap: 0.9788406811552585
Model improve: 0.9783 -> 0.9788
Epoch: 37/60
Train loss: 2.3873, val loss: 0.6026
Val cmap: 0.9798234757414259
Model improve: 0.9788 -> 0.9798
Epoch: 38/60
Train loss: 2.2949, val loss: 0.6587
Val cmap: 0.9807149976703768
Model improve: 0.9798 -> 0.9807
Epoch: 39/60
Train loss: 2.3179, val loss: 0.5862
Val cmap: 0.9818211319979476
Model improve: 0.9807 -> 0.9818
Epoch: 40/60
Train loss: 2.2966, val loss: 0.5711
Val cmap: 0.9826953087548883
Model improve: 0.9818 -> 0.9827
Epoch: 41/60
Train loss: 2.2847, val loss: 0.5955
Val cmap: 0.9829620271154785
Model improve: 0.9827 -> 0.9830
Epoch: 42/60
Train loss: 2.2404, val loss: 0.5801
Val cmap: 0.983952146375429
Model improve: 0.9830 -> 0.9840
Epoch: 43/60
Train loss: 2.2099, val loss: 0.5382
Val cmap: 0.9843683881973592
Model improve: 0.9840 -> 0.9844
Epoch: 44/60
Train loss: 2.1974, val loss: 0.5109
Val cmap: 0.985161506623653
Model improve: 0.9844 -> 0.9852
Epoch: 45/60
Train loss: 2.1613, val loss: 0.5141
Val cmap: 0.985200127213334
Model improve: 0.9852 -> 0.9852
Epoch: 46/60
Train loss: 2.1772, val loss: 0.4717
Val cmap: 0.9858562929045426
Model improve: 0.9852 -> 0.9859
Epoch: 47/60
Train loss: 2.1474, val loss: 0.4507
Val cmap: 0.9861656456896325
Model improve: 0.9859 -> 0.9862
Epoch: 48/60
Train loss: 2.1491, val loss: 0.4722
Val cmap: 0.9868446235836675
Model improve: 0.9862 -> 0.9868
Epoch: 49/60
Train loss: 2.1320, val loss: 0.4567
Val cmap: 0.9867294605977652
Epoch: 50/60
Train loss: 2.1182, val loss: 0.4608
Val cmap: 0.9874262296805144
Model improve: 0.9868 -> 0.9874
Epoch: 51/60
Train loss: 2.1456, val loss: 0.4486
Val cmap: 0.9876800567802675
Model improve: 0.9874 -> 0.9877
Epoch: 52/60
Train loss: 2.1214, val loss: 0.4908
Val cmap: 0.9876738102808422
Epoch: 53/60
Train loss: 2.0948, val loss: 0.4442
Val cmap: 0.9880112408867505
Model improve: 0.9877 -> 0.9880
Epoch: 54/60
Train loss: 2.0739, val loss: 0.4295
Val cmap: 0.9882517919111593
Model improve: 0.9880 -> 0.9883
Epoch: 55/60
Train loss: 2.0729, val loss: 0.4477
Val cmap: 0.9884222486850658
Model improve: 0.9883 -> 0.9884
Epoch: 56/60
Train loss: 2.0488, val loss: 0.4136
Val cmap: 0.9880522238235497
Epoch: 57/60
Train loss: 2.0465, val loss: 0.4281
Val cmap: 0.9880438336095452
Epoch: 58/60
Train loss: 2.0373, val loss: 0.4519
Val cmap: 0.9886717412415477
Model improve: 0.9884 -> 0.9887
Epoch: 59/60
Train loss: 2.0782, val loss: 0.4517
Val cmap: 0.9882349697341121
Epoch: 60/60
Train loss: 2.0858, val loss: 0.4521
Val cmap: 0.9883272656454712
Date :04/17/2023, 12:32:30
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Date :04/17/2023, 12:33:02
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Date :04/17/2023, 12:33:28
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Date :04/17/2023, 12:33:43
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
78014
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :04/17/2023, 12:34:48
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Date :04/17/2023, 12:35:13
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Date :04/17/2023, 12:35:29
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
78014
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :04/17/2023, 12:37:17
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Date :04/17/2023, 12:37:34
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
78014
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :04/17/2023, 12:40:23
Duration: 15
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
78014
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :04/17/2023, 12:41:32
Duration: 15
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
60154
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :04/17/2023, 12:41:53
Duration: 15
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
60154
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :04/17/2023, 12:43:55
Duration: 15
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
60154
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 6.9264, val loss: 3.6119
Val cmap: 0.6376127126431803
Model improve: 0.0000 -> 0.6376
Epoch: 2/60
Train loss: 4.2538, val loss: 2.8323
Val cmap: 0.734643892309197
Model improve: 0.6376 -> 0.7346
Epoch: 3/60
Train loss: 3.6809, val loss: 2.3590
Val cmap: 0.7932737614222567
Model improve: 0.7346 -> 0.7933
Epoch: 4/60
Train loss: 3.3529, val loss: 2.2326
Val cmap: 0.8173724946066332
Model improve: 0.7933 -> 0.8174
Epoch: 5/60
Date :04/17/2023, 14:23:54
Duration: 15
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
60154
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 6.9264, val loss: 3.6119
Val cmap: 0.6376127126431803
Model improve: 0.0000 -> 0.6376
Epoch: 2/60
Train loss: 4.2538, val loss: 2.8323
Val cmap: 0.734643892309197
Model improve: 0.6376 -> 0.7346
Epoch: 3/60
Train loss: 3.6809, val loss: 2.3590
Val cmap: 0.7932737614222567
Model improve: 0.7346 -> 0.7933
Epoch: 4/60
Train loss: 3.3529, val loss: 2.2326
Val cmap: 0.8173724946066332
Model improve: 0.7933 -> 0.8174
Epoch: 5/60
Train loss: 3.1035, val loss: 2.0118
Val cmap: 0.8286676111034846
Model improve: 0.8174 -> 0.8287
Epoch: 6/60
Train loss: 2.9043, val loss: 1.8832
Val cmap: 0.8463047895061279
Model improve: 0.8287 -> 0.8463
Epoch: 7/60
Train loss: 2.8091, val loss: 1.8316
Val cmap: 0.8620394518739557
Model improve: 0.8463 -> 0.8620
Epoch: 8/60
Train loss: 2.7073, val loss: 1.7183
Val cmap: 0.8677309634131629
Model improve: 0.8620 -> 0.8677
Epoch: 9/60
Train loss: 2.5723, val loss: 1.7733
Val cmap: 0.8647846759650498
Epoch: 10/60
Train loss: 2.4981, val loss: 1.6880
Val cmap: 0.872947985384639
Model improve: 0.8677 -> 0.8729
Epoch: 11/60
Train loss: 2.4400, val loss: 1.6284
Val cmap: 0.8776013005751374
Model improve: 0.8729 -> 0.8776
Epoch: 12/60
Train loss: 2.3498, val loss: 1.6395
Val cmap: 0.8730993083129026
Epoch: 13/60
Train loss: 2.3368, val loss: 1.5503
Val cmap: 0.8802313422318873
Model improve: 0.8776 -> 0.8802
Epoch: 14/60
Train loss: 2.2353, val loss: 1.5755
Val cmap: 0.8817189348750385
Model improve: 0.8802 -> 0.8817
Epoch: 15/60
Train loss: 2.1786, val loss: 1.5606
Val cmap: 0.8880133549896407
Model improve: 0.8817 -> 0.8880
Epoch: 16/60
Train loss: 2.1667, val loss: 1.5399
Val cmap: 0.8842093125332788
Epoch: 17/60
Train loss: 2.1276, val loss: 1.4542
Val cmap: 0.8950754429270436
Model improve: 0.8880 -> 0.8951
Epoch: 18/60
Train loss: 2.0657, val loss: 1.5205
Val cmap: 0.8901538512558995
Epoch: 19/60
Train loss: 2.0833, val loss: 1.4640
Val cmap: 0.893470954063013
Epoch: 20/60
Train loss: 1.9962, val loss: 1.5317
Val cmap: 0.8928548153683261
Epoch: 21/60
Train loss: 2.0170, val loss: 1.4474
Val cmap: 0.8966421954822947
Model improve: 0.8951 -> 0.8966
Epoch: 22/60
Train loss: 1.9503, val loss: 1.5182
Val cmap: 0.8936872242908815
Epoch: 23/60
Train loss: 1.9370, val loss: 1.4692
Val cmap: 0.8977975985373703
Model improve: 0.8966 -> 0.8978
Epoch: 24/60
Train loss: 1.8792, val loss: 1.4624
Val cmap: 0.8943444870970255
Epoch: 25/60
Train loss: 1.8724, val loss: 1.4382
Val cmap: 0.8990453564132403
Model improve: 0.8978 -> 0.8990
Epoch: 26/60
Train loss: 1.8486, val loss: 1.4636
Val cmap: 0.8989972069971074
Epoch: 27/60
Train loss: 1.8129, val loss: 1.4346
Val cmap: 0.8997776212716665
Model improve: 0.8990 -> 0.8998
Epoch: 28/60
Train loss: 1.7636, val loss: 1.4206
Val cmap: 0.8986160361328147
Epoch: 29/60
Train loss: 1.7687, val loss: 1.4162
Val cmap: 0.9004092520325242
Model improve: 0.8998 -> 0.9004
Epoch: 30/60
Train loss: 1.7789, val loss: 1.3996
Val cmap: 0.9028770048581559
Model improve: 0.9004 -> 0.9029
Epoch: 31/60
Train loss: 1.6839, val loss: 1.4001
Val cmap: 0.9016401606843608
Epoch: 32/60
Train loss: 1.6599, val loss: 1.3698
Val cmap: 0.9039060186662204
Model improve: 0.9029 -> 0.9039
Epoch: 33/60
Train loss: 1.6595, val loss: 1.3368
Val cmap: 0.901415209548351
Epoch: 34/60
Train loss: 1.6579, val loss: 1.3943
Val cmap: 0.9023152664591699
Epoch: 35/60
Train loss: 1.6497, val loss: 1.4013
Val cmap: 0.9034937969443932
Epoch: 36/60
Train loss: 1.6824, val loss: 1.3565
Val cmap: 0.9037787401129309
Epoch: 37/60
Train loss: 1.5070, val loss: 1.3495
Val cmap: 0.9024445500782169
Epoch: 38/60
Train loss: 1.5901, val loss: 1.3258
Val cmap: 0.9044568522194254
Model improve: 0.9039 -> 0.9045
Epoch: 39/60
Train loss: 1.5807, val loss: 1.3063
Val cmap: 0.907526855958584
Model improve: 0.9045 -> 0.9075
Epoch: 40/60
Train loss: 1.5712, val loss: 1.2970
Val cmap: 0.9076638237391912
Model improve: 0.9075 -> 0.9077
Epoch: 41/60
Train loss: 1.5172, val loss: 1.3103
Val cmap: 0.9068241330758238
Epoch: 42/60
Train loss: 1.4805, val loss: 1.2975
Val cmap: 0.9091702285817388
Model improve: 0.9077 -> 0.9092
Epoch: 43/60
Train loss: 1.4895, val loss: 1.3022
Val cmap: 0.9092022362007494
Model improve: 0.9092 -> 0.9092
Epoch: 44/60
Train loss: 1.4597, val loss: 1.2646
Val cmap: 0.9079415853694023
Epoch: 45/60
Train loss: 1.4663, val loss: 1.2927
Val cmap: 0.9085545628647048
Epoch: 46/60
Train loss: 1.4499, val loss: 1.3007
Val cmap: 0.9075803355291795
Epoch: 47/60
Train loss: 1.4752, val loss: 1.2509
Val cmap: 0.9121205251208241
Model improve: 0.9092 -> 0.9121
Epoch: 48/60
Train loss: 1.4117, val loss: 1.2826
Val cmap: 0.9091871769520106
Epoch: 49/60
Train loss: 1.4218, val loss: 1.2823
Val cmap: 0.9089629036084926
Epoch: 50/60
Train loss: 1.4219, val loss: 1.2635
Val cmap: 0.9079669924352382
Epoch: 51/60
Train loss: 1.3809, val loss: 1.2636
Val cmap: 0.9101597408819639
Epoch: 52/60
Train loss: 1.4218, val loss: 1.2280
Val cmap: 0.9114677010456655
Epoch: 53/60
Train loss: 1.4184, val loss: 1.2661
Val cmap: 0.9083273366794102
Epoch: 54/60
Train loss: 1.3797, val loss: 1.2621
Val cmap: 0.909733014829836
Epoch: 55/60
Train loss: 1.3865, val loss: 1.2777
Val cmap: 0.9087334311330569
Epoch: 56/60
Train loss: 1.4396, val loss: 1.2693
Val cmap: 0.9101818271445872
Epoch: 57/60
Date :04/18/2023, 12:18:30
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 16
validbs: 64
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
60154
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 6.7916, val loss: 4.5952
Val cmap: 0.5137446511921107
Model improve: 0.0000 -> 0.5137
Epoch: 2/60
Date :04/18/2023, 12:37:29
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
60154
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :04/18/2023, 12:38:12
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
60154
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 9.2649, val loss: 4.5665
Val cmap: 0.508622026972172
Model improve: 0.0000 -> 0.5086
Epoch: 2/60
Date :04/18/2023, 12:58:13
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnet_b1_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
60154
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 7.7664, val loss: 4.3788
Val cmap: 0.5328385312995046
Model improve: 0.0000 -> 0.5328
Epoch: 2/60
Date :04/18/2023, 13:20:25
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnet_b1_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
78014
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :04/18/2023, 13:21:07
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnet_b1_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
Date :04/18/2023, 13:21:25
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnet_b1_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
78014
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :04/18/2023, 13:21:50
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnet_b1_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
78014
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :04/18/2023, 13:23:01
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnet_b1_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
78014
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :04/18/2023, 13:27:11
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnet_b1_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
78014
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/60
Train loss: 7.2496, val loss: 4.0000
Val cmap: 0.6011031962314854
Model improve: 0.0000 -> 0.6011
Epoch: 2/60
Train loss: 4.7664, val loss: 3.2256
Val cmap: 0.7107043423842135
Model improve: 0.6011 -> 0.7107
Epoch: 3/60
Train loss: 4.2690, val loss: 2.8187
Val cmap: 0.7553547043190754
Model improve: 0.7107 -> 0.7554
Epoch: 4/60
Train loss: 3.9801, val loss: 2.5268
Val cmap: 0.7877396300314884
Model improve: 0.7554 -> 0.7877
Epoch: 5/60
Train loss: 3.7582, val loss: 2.3800
Val cmap: 0.8089028710853863
Model improve: 0.7877 -> 0.8089
Epoch: 6/60
Train loss: 3.6484, val loss: 2.2521
Val cmap: 0.8239555262795049
Model improve: 0.8089 -> 0.8240
Epoch: 7/60
Train loss: 3.5030, val loss: 2.0581
Val cmap: 0.840718696234567
Model improve: 0.8240 -> 0.8407
Epoch: 8/60
Train loss: 3.4638, val loss: 1.9988
Val cmap: 0.8479769038738174
Model improve: 0.8407 -> 0.8480
Epoch: 9/60
Train loss: 3.3712, val loss: 1.9827
Val cmap: 0.8545122274934271
Model improve: 0.8480 -> 0.8545
Epoch: 10/60
Train loss: 3.2657, val loss: 1.9171
Val cmap: 0.8599031187771699
Model improve: 0.8545 -> 0.8599
Epoch: 11/60
Train loss: 3.2409, val loss: 1.8682
Val cmap: 0.863388574107244
Model improve: 0.8599 -> 0.8634
Epoch: 12/60
Train loss: 3.2002, val loss: 1.7792
Val cmap: 0.872640460435464
Model improve: 0.8634 -> 0.8726
Epoch: 13/60
Train loss: 3.1685, val loss: 1.7517
Val cmap: 0.8762150594272967
Model improve: 0.8726 -> 0.8762
Epoch: 14/60
Train loss: 3.0930, val loss: 1.6437
Val cmap: 0.8825816868998212
Model improve: 0.8762 -> 0.8826
Epoch: 15/60
Train loss: 3.0766, val loss: 1.6872
Val cmap: 0.8837075707334798
Model improve: 0.8826 -> 0.8837
Epoch: 16/60
Train loss: 3.0703, val loss: 1.6428
Val cmap: 0.89005835483724
Model improve: 0.8837 -> 0.8901
Epoch: 17/60
Train loss: 2.9895, val loss: 1.5485
Val cmap: 0.8956029190294253
Model improve: 0.8901 -> 0.8956
Epoch: 18/60
Train loss: 2.9402, val loss: 1.5038
Val cmap: 0.8998450398012241
Model improve: 0.8956 -> 0.8998
Epoch: 19/60
Train loss: 2.9161, val loss: 1.5204
Val cmap: 0.9039576647616236
Model improve: 0.8998 -> 0.9040
Epoch: 20/60
Train loss: 2.8799, val loss: 1.4822
Val cmap: 0.9042479702913829
Model improve: 0.9040 -> 0.9042
Epoch: 21/60
Train loss: 2.8740, val loss: 1.4528
Val cmap: 0.9066908872563503
Model improve: 0.9042 -> 0.9067
Epoch: 22/60
Train loss: 2.8227, val loss: 1.5263
Val cmap: 0.9083483146152735
Model improve: 0.9067 -> 0.9083
Epoch: 23/60
Train loss: 2.8673, val loss: 1.3259
Val cmap: 0.9139088643383764
Model improve: 0.9083 -> 0.9139
Epoch: 24/60
Train loss: 2.7614, val loss: 1.3577
Val cmap: 0.9146833671980281
Model improve: 0.9139 -> 0.9147
Epoch: 25/60
Train loss: 2.7643, val loss: 1.3807
Val cmap: 0.9142563740230714
Epoch: 26/60
Train loss: 2.7370, val loss: 1.2783
Val cmap: 0.9201026693725421
Model improve: 0.9147 -> 0.9201
Epoch: 27/60
Train loss: 2.7127, val loss: 1.2763
Val cmap: 0.9221740705483746
Model improve: 0.9201 -> 0.9222
Epoch: 28/60
Train loss: 2.6954, val loss: 1.2100
Val cmap: 0.9247694613047871
Model improve: 0.9222 -> 0.9248
Epoch: 29/60
Train loss: 2.7022, val loss: 1.2933
Val cmap: 0.924316561661176
Epoch: 30/60
Train loss: 2.6724, val loss: 1.1779
Val cmap: 0.9282263052589108
Model improve: 0.9248 -> 0.9282
Epoch: 31/60
Train loss: 2.5830, val loss: 1.1777
Val cmap: 0.9279640988355478
Epoch: 32/60
Train loss: 2.5882, val loss: 1.1786
Val cmap: 0.9281622104267903
Epoch: 33/60
Train loss: 2.5681, val loss: 1.1569
Val cmap: 0.9302339217539632
Model improve: 0.9282 -> 0.9302
Epoch: 34/60
Train loss: 2.5896, val loss: 1.1691
Val cmap: 0.9322853461964188
Model improve: 0.9302 -> 0.9323
Epoch: 35/60
Train loss: 2.5747, val loss: 1.1436
Val cmap: 0.9358821816258235
Model improve: 0.9323 -> 0.9359
Epoch: 36/60
Train loss: 2.5235, val loss: 1.1881
Val cmap: 0.9352714629026218
Epoch: 37/60
Train loss: 2.5461, val loss: 1.0872
Val cmap: 0.937987731659398
Model improve: 0.9359 -> 0.9380
Epoch: 38/60
Train loss: 2.5251, val loss: 1.0878
Val cmap: 0.9422638211240374
Model improve: 0.9380 -> 0.9423
Epoch: 39/60
Train loss: 2.4624, val loss: 1.1441
Val cmap: 0.940353529226968
Epoch: 40/60
Train loss: 2.5007, val loss: 1.0815
Val cmap: 0.9395027458443322
Epoch: 41/60
Train loss: 2.4741, val loss: 1.0258
Val cmap: 0.9434363359005709
Model improve: 0.9423 -> 0.9434
Epoch: 42/60
Train loss: 2.4739, val loss: 1.0064
Val cmap: 0.9456385437110423
Model improve: 0.9434 -> 0.9456
Epoch: 43/60
Train loss: 2.4419, val loss: 0.9774
Val cmap: 0.9448715821710942
Epoch: 44/60
Train loss: 2.3946, val loss: 0.9988
Val cmap: 0.9432112559307445
Epoch: 45/60
Train loss: 2.4177, val loss: 0.9846
Val cmap: 0.9451551136698808
Epoch: 46/60
Train loss: 2.3978, val loss: 0.9437
Val cmap: 0.948391285474929
Model improve: 0.9456 -> 0.9484
Epoch: 47/60
Train loss: 2.4140, val loss: 0.8920
Val cmap: 0.9503222220478923
Model improve: 0.9484 -> 0.9503
Epoch: 48/60
Train loss: 2.3791, val loss: 0.9588
Val cmap: 0.949046439278414
Epoch: 49/60
Train loss: 2.3953, val loss: 0.9048
Val cmap: 0.949133458413161
Epoch: 50/60
Train loss: 2.3675, val loss: 0.9233
Val cmap: 0.9499647601450455
Epoch: 51/60
Train loss: 2.3651, val loss: 0.9338
Val cmap: 0.9484458831421457
Epoch: 52/60
Train loss: 2.4061, val loss: 0.9119
Val cmap: 0.9502788059383743
Epoch: 53/60
Train loss: 2.3966, val loss: 0.9200
Val cmap: 0.9505134826059859
Model improve: 0.9503 -> 0.9505
Epoch: 54/60
Train loss: 2.3648, val loss: 0.9192
Val cmap: 0.9510427214265166
Model improve: 0.9505 -> 0.9510
Epoch: 55/60
Train loss: 2.3352, val loss: 0.9216
Val cmap: 0.9513610234459785
Model improve: 0.9510 -> 0.9514
Epoch: 56/60
Train loss: 2.3646, val loss: 0.9652
Val cmap: 0.9510118200556793
Epoch: 57/60
Train loss: 2.3564, val loss: 0.9334
Val cmap: 0.9516105049552099
Model improve: 0.9514 -> 0.9516
Epoch: 58/60
Train loss: 2.3117, val loss: 1.0093
Val cmap: 0.9498429149542236
Epoch: 59/60
Train loss: 2.3446, val loss: 0.9255
Val cmap: 0.9532897081546693
Model improve: 0.9516 -> 0.9533
Epoch: 60/60
Train loss: 2.3172, val loss: 0.8931
Val cmap: 0.9525106359024029
Date :04/19/2023, 06:35:23
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnet_b1_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
78014
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/60
Date :04/19/2023, 06:36:10
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnet_b1_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
78014
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/60
Train loss: 382.9805, val loss: 382.6175
Val cmap: 0.24194607800122597
Model improve: 0.0000 -> 0.2419
Epoch: 2/60
Date :04/19/2023, 06:54:18
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: eca_nfnet_l0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
78014
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/60
Train loss: 7.6429, val loss: 4.0190
Val cmap: 0.6226383111331832
Model improve: 0.0000 -> 0.6226
Epoch: 2/60
Train loss: 4.6896, val loss: 3.2415
Val cmap: 0.7244594370543261
Model improve: 0.6226 -> 0.7245
Epoch: 3/60
Train loss: 4.1818, val loss: 2.8301
Val cmap: 0.7672853724023962
Model improve: 0.7245 -> 0.7673
Epoch: 4/60
Train loss: 3.8599, val loss: 2.4555
Val cmap: 0.8043566430744674
Model improve: 0.7673 -> 0.8044
Epoch: 5/60
Train loss: 3.6396, val loss: 2.2150
Val cmap: 0.830037964542214
Model improve: 0.8044 -> 0.8300
Epoch: 6/60
Train loss: 3.5034, val loss: 2.1408
Val cmap: 0.8492653740875832
Model improve: 0.8300 -> 0.8493
Epoch: 7/60
Train loss: 3.3463, val loss: 1.9158
Val cmap: 0.859818115985406
Model improve: 0.8493 -> 0.8598
Epoch: 8/60
Train loss: 3.2918, val loss: 1.8421
Val cmap: 0.8711337927110563
Model improve: 0.8598 -> 0.8711
Epoch: 9/60
Train loss: 3.2094, val loss: 1.7409
Val cmap: 0.8781062391286992
Model improve: 0.8711 -> 0.8781
Epoch: 10/60
Train loss: 3.0834, val loss: 1.6600
Val cmap: 0.8861481116308229
Model improve: 0.8781 -> 0.8861
Epoch: 11/60
Train loss: 3.0586, val loss: 1.5918
Val cmap: 0.8925371174956416
Model improve: 0.8861 -> 0.8925
Epoch: 12/60
Train loss: 2.9934, val loss: 1.5301
Val cmap: 0.8989254908230402
Model improve: 0.8925 -> 0.8989
Epoch: 13/60
Train loss: 2.9617, val loss: 1.5097
Val cmap: 0.8997117104444158
Model improve: 0.8989 -> 0.8997
Epoch: 14/60
Train loss: 2.8996, val loss: 1.4421
Val cmap: 0.9068598931557768
Model improve: 0.8997 -> 0.9069
Epoch: 15/60
Train loss: 2.8590, val loss: 1.4205
Val cmap: 0.905776102911358
Epoch: 16/60
Train loss: 2.8626, val loss: 1.3967
Val cmap: 0.9098930512011995
Model improve: 0.9069 -> 0.9099
Epoch: 17/60
Train loss: 2.7760, val loss: 1.3064
Val cmap: 0.9178001222417556
Model improve: 0.9099 -> 0.9178
Epoch: 18/60
Train loss: 2.7257, val loss: 1.2605
Val cmap: 0.9230373185487242
Model improve: 0.9178 -> 0.9230
Epoch: 19/60
Train loss: 2.7056, val loss: 1.2488
Val cmap: 0.9232803080379514
Model improve: 0.9230 -> 0.9233
Epoch: 20/60
Train loss: 2.6600, val loss: 1.2131
Val cmap: 0.9257984535700069
Model improve: 0.9233 -> 0.9258
Epoch: 21/60
Train loss: 2.6496, val loss: 1.1795
Val cmap: 0.9293658182634529
Model improve: 0.9258 -> 0.9294
Epoch: 22/60
Train loss: 2.5981, val loss: 1.1749
Val cmap: 0.9321346684151609
Model improve: 0.9294 -> 0.9321
Epoch: 23/60
Train loss: 2.6377, val loss: 1.1011
Val cmap: 0.9345188840614049
Model improve: 0.9321 -> 0.9345
Epoch: 24/60
Train loss: 2.5426, val loss: 1.1085
Val cmap: 0.9335629890923174
Epoch: 25/60
Train loss: 2.5308, val loss: 1.0594
Val cmap: 0.937043451068797
Model improve: 0.9345 -> 0.9370
Epoch: 26/60
Train loss: 2.5180, val loss: 1.0308
Val cmap: 0.9405706600923945
Model improve: 0.9370 -> 0.9406
Epoch: 27/60
Train loss: 2.4822, val loss: 1.0028
Val cmap: 0.9427337080118208
Model improve: 0.9406 -> 0.9427
Epoch: 28/60
Train loss: 2.4477, val loss: 0.9893
Val cmap: 0.9445208507953927
Model improve: 0.9427 -> 0.9445
Epoch: 29/60
Train loss: 2.4665, val loss: 0.9554
Val cmap: 0.9481105305583954
Model improve: 0.9445 -> 0.9481
Epoch: 30/60
Train loss: 2.4531, val loss: 0.9792
Val cmap: 0.9458849757779206
Epoch: 31/60
Train loss: 2.3525, val loss: 0.9678
Val cmap: 0.9479588550285718
Epoch: 32/60
Train loss: 2.3672, val loss: 0.9152
Val cmap: 0.9518556964013797
Model improve: 0.9481 -> 0.9519
Epoch: 33/60
Train loss: 2.3440, val loss: 0.8895
Val cmap: 0.9519540821110639
Model improve: 0.9519 -> 0.9520
Epoch: 34/60
Train loss: 2.3571, val loss: 0.8616
Val cmap: 0.9548245591848685
Model improve: 0.9520 -> 0.9548
Epoch: 35/60
Train loss: 2.3492, val loss: 0.8548
Val cmap: 0.9559617291319913
Model improve: 0.9548 -> 0.9560
Epoch: 36/60
Train loss: 2.2931, val loss: 0.8702
Val cmap: 0.9546777673815932
Epoch: 37/60
Train loss: 2.3148, val loss: 0.8030
Val cmap: 0.9584121908245118
Model improve: 0.9560 -> 0.9584
Epoch: 38/60
Train loss: 2.2778, val loss: 0.8148
Val cmap: 0.9572083310334387
Epoch: 39/60
Train loss: 2.2233, val loss: 0.8149
Val cmap: 0.960212911885764
Model improve: 0.9584 -> 0.9602
Epoch: 40/60
Train loss: 2.2524, val loss: 0.8032
Val cmap: 0.9592552197846909
Epoch: 41/60
Train loss: 2.2375, val loss: 0.7553
Val cmap: 0.9628673630150872
Model improve: 0.9602 -> 0.9629
Epoch: 42/60
Train loss: 2.2286, val loss: 0.7654
Val cmap: 0.9626871900200754
Epoch: 43/60
Train loss: 2.2091, val loss: 0.7528
Val cmap: 0.9638831058072314
Model improve: 0.9629 -> 0.9639
Epoch: 44/60
Train loss: 2.1494, val loss: 0.7448
Val cmap: 0.9638513759287803
Epoch: 45/60
Train loss: 2.1693, val loss: 0.6986
Val cmap: 0.9666737434406444
Model improve: 0.9639 -> 0.9667
Epoch: 46/60
Train loss: 2.1405, val loss: 0.6829
Val cmap: 0.9676880185849016
Model improve: 0.9667 -> 0.9677
Epoch: 47/60
Train loss: 2.1590, val loss: 0.6989
Val cmap: 0.9659933018527148
Epoch: 48/60
Train loss: 2.1449, val loss: 0.6940
Val cmap: 0.9657952445196707
Epoch: 49/60
Train loss: 2.1418, val loss: 0.6745
Val cmap: 0.9689549413862526
Model improve: 0.9677 -> 0.9690
Epoch: 50/60
Train loss: 2.1154, val loss: 0.6666
Val cmap: 0.968259418180388
Epoch: 51/60
Train loss: 2.1164, val loss: 0.6657
Val cmap: 0.9676112999861869
Epoch: 52/60
Train loss: 2.1511, val loss: 0.6605
Val cmap: 0.9690889097157388
Model improve: 0.9690 -> 0.9691
Epoch: 53/60
Train loss: 2.1390, val loss: 0.6610
Val cmap: 0.968518668937644
Epoch: 54/60
Train loss: 2.1067, val loss: 0.6491
Val cmap: 0.9695192055927709
Model improve: 0.9691 -> 0.9695
Epoch: 55/60
Train loss: 2.0819, val loss: 0.6523
Val cmap: 0.9694154823684026
Epoch: 56/60
Train loss: 2.1149, val loss: 0.6498
Val cmap: 0.96891170863917
Epoch: 57/60
Train loss: 2.1045, val loss: 0.6508
Val cmap: 0.9702440916705826
Model improve: 0.9695 -> 0.9702
Epoch: 58/60
Train loss: 2.0516, val loss: 0.6426
Val cmap: 0.9700960906198522
Epoch: 59/60
Train loss: 2.0921, val loss: 0.6499
Val cmap: 0.9698197270983093
Epoch: 60/60
Train loss: 2.0645, val loss: 0.6452
Val cmap: 0.9706144915733054
Model improve: 0.9702 -> 0.9706
Date :04/20/2023, 06:54:15
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 256
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: eca_nfnet_l0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
78014
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/60
Train loss: 7.7993, val loss: 4.3180
Val cmap: 0.6010476610322356
Model improve: 0.0000 -> 0.6010
Epoch: 2/60
Date :04/20/2023, 07:32:43
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 224
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: eca_nfnet_l0
mix_up: 0.2
hop_length: 320
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
78014
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/60
Train loss: 8.0848, val loss: 4.3764
Val cmap: 0.5708797773502262
Model improve: 0.0000 -> 0.5709
Epoch: 2/60
Date :04/20/2023, 07:58:11
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: eca_nfnet_l0
mix_up: 0.2
hop_length: 128
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
78014
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/60
Date :04/20/2023, 03:23:25
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnet_b1_ns
mix_up: 0.2
hop_length: 128
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
78014
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Train loss: 10.9272, val loss: 6.9236
Val cmap: 0.4405586999982737
Model improve: 0.0000 -> 0.4406
Epoch: 2/25
Train loss: 7.3153, val loss: 5.6933
Val cmap: 0.56781097570851
Model improve: 0.4406 -> 0.5678
Epoch: 3/25
Date :04/20/2023, 04:11:23
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnet_b1_ns
mix_up: 0.2
hop_length: 128
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
60154
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Train loss: 9.4064, val loss: 4.8880
Val cmap: 0.47327569888900134
Model improve: 0.0000 -> 0.4733
Epoch: 2/25
Train loss: 5.0633, val loss: 3.7145
Val cmap: 0.6175676921659115
Model improve: 0.4733 -> 0.6176
Epoch: 3/25
Date :04/20/2023, 04:50:15
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnet_b1_ns
mix_up: 0.2
hop_length: 512
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
60154
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Date :04/20/2023, 04:54:24
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnet_b0_ns
mix_up: 0.2
hop_length: 512
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
60154
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Train loss: 9.4397, val loss: 4.7564
Val cmap: 0.469168911549604
Model improve: 0.0000 -> 0.4692
Epoch: 2/25
Date :04/20/2023, 05:07:29
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnet_b0_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
63369
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Train loss: 9.1079, val loss: 4.7506
Val cmap: 0.48143681414073447
Model improve: 0.0000 -> 0.4814
Epoch: 2/25
Train loss: 5.1968, val loss: 3.7770
Val cmap: 0.6167357336348261
Model improve: 0.4814 -> 0.6167
Epoch: 3/25
Train loss: 4.5931, val loss: 3.3962
Val cmap: 0.6738686652418129
Model improve: 0.6167 -> 0.6739
Epoch: 4/25
Train loss: 4.2768, val loss: 3.1193
Val cmap: 0.7047683341366882
Model improve: 0.6739 -> 0.7048
Epoch: 5/25
Train loss: 4.0287, val loss: 2.9096
Val cmap: 0.7316871538299242
Model improve: 0.7048 -> 0.7317
Epoch: 6/25
Train loss: 3.9082, val loss: 2.7668
Val cmap: 0.74356010211183
Model improve: 0.7317 -> 0.7436
Epoch: 7/25
Train loss: 3.7074, val loss: 2.6644
Val cmap: 0.7569334787409481
Model improve: 0.7436 -> 0.7569
Epoch: 8/25
Date :04/20/2023, 06:59:45
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnet_b0_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
63369
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Train loss: 7.8617, val loss: 4.5010
Val cmap: 0.5287784725502451
Model improve: 0.0000 -> 0.5288
Epoch: 2/25
Date :04/20/2023, 07:15:13
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.01
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnet_b0_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
63369
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.01
    lr: 0.01
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Date :04/20/2023, 07:23:10
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.01
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnet_b0_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
60154
63369
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.01
    lr: 0.01
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Date :04/20/2023, 07:25:01
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.01
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnet_b0_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
Date :04/20/2023, 07:25:11
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.01
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnet_b0_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
Date :04/20/2023, 07:25:39
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.01
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnet_b0_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
60167
63682
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.01
    lr: 0.01
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Date :04/20/2023, 07:29:01
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.01
weightdecay: 0.0
thrupsample: 100
model_name: tf_efficientnet_b0_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
60167
75026
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.01
    lr: 0.01
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Date :04/20/2023, 07:30:06
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.01
weightdecay: 0.0
thrupsample: 100
model_name: tf_efficientnet_b0_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Date :04/20/2023, 07:30:50
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.01
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnet_b0_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
61125
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.01
    lr: 0.01
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Date :04/20/2023, 07:31:19
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.01
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnet_b0_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
64694
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.01
    lr: 0.01
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Date :04/20/2023, 07:33:13
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 25
learningrate: 0.01
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnet_b0_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
64694
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.01
    lr: 0.01
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/25
Train loss: 8.5280, val loss: 6.8622
Val cmap: 0.2622083033828723
Model improve: 0.0000 -> 0.2622
Epoch: 2/25
Date :04/20/2023, 07:48:42
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 3
totalepoch: 30
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnet_b0_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
64694
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.0
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/30
Train loss: 26.7973, val loss: 6.9401
Val cmap: 0.26917794914862436
Model improve: 0.0000 -> 0.2692
Epoch: 2/30
Train loss: 6.4359, val loss: 4.8870
Val cmap: 0.4850283444707206
Model improve: 0.2692 -> 0.4850
Epoch: 3/30
Train loss: 5.4046, val loss: 4.0817
Val cmap: 0.591829512055652
Model improve: 0.4850 -> 0.5918
Epoch: 4/30
Train loss: 4.8493, val loss: 3.5196
Val cmap: 0.6610417578434086
Model improve: 0.5918 -> 0.6610
Epoch: 5/30
Train loss: 4.4435, val loss: 3.2256
Val cmap: 0.6935362608124118
Model improve: 0.6610 -> 0.6935
Epoch: 6/30
Train loss: 4.2012, val loss: 3.0871
Val cmap: 0.7149580124015182
Model improve: 0.6935 -> 0.7150
Epoch: 7/30
Date :04/20/2023, 09:25:23
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 64
validbs: 256
epochwarmup: 0
totalepoch: 30
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnet_b0_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
64694
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/30
Train loss: 9.0041, val loss: 4.7759
Val cmap: 0.478398686432218
Model improve: 0.0000 -> 0.4784
Epoch: 2/30
Train loss: 5.1981, val loss: 3.7523
Val cmap: 0.6206678611196972
Model improve: 0.4784 -> 0.6207
Epoch: 3/30
Train loss: 4.6267, val loss: 3.4024
Val cmap: 0.6718858093224807
Model improve: 0.6207 -> 0.6719
Epoch: 4/30
Train loss: 4.2889, val loss: 3.0763
Val cmap: 0.7115531718485406
Model improve: 0.6719 -> 0.7116
Epoch: 5/30
Date :04/20/2023, 10:26:26
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnet_b0_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
61125
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 7.8694, val loss: 4.5330
Val cmap: 0.5339384326583355
Model improve: 0.0000 -> 0.5339
Epoch: 2/60
Date :04/20/2023, 10:44:48
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 224
fmax: 16000
trainbs: 24
validbs: 144
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnet_b0_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
61125
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 7.6444, val loss: 4.4977
Val cmap: 0.5402632631194374
Model improve: 0.0000 -> 0.5403
Epoch: 2/60
Train loss: 5.0864, val loss: 3.6326
Val cmap: 0.6504203609625593
Model improve: 0.5403 -> 0.6504
Epoch: 3/60
Train loss: 4.5440, val loss: 3.3269
Val cmap: 0.688816277575173
Model improve: 0.6504 -> 0.6888
Epoch: 4/60
Date :04/20/2023, 11:45:38
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 24
validbs: 144
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnet_b0_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
61125
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 7.3847, val loss: 4.4067
Val cmap: 0.5550937047338131
Model improve: 0.0000 -> 0.5551
Epoch: 2/60
Train loss: 5.0624, val loss: 3.6735
Val cmap: 0.6492193560224139
Model improve: 0.5551 -> 0.6492
Epoch: 3/60
Train loss: 4.5638, val loss: 3.2905
Val cmap: 0.6882217115957766
Model improve: 0.6492 -> 0.6882
Epoch: 4/60
Train loss: 4.2323, val loss: 3.0579
Val cmap: 0.7163303708706109
Model improve: 0.6882 -> 0.7163
Epoch: 5/60
Date :04/20/2023, 12:57:03
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnet_b0_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
61125
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 7.7541, val loss: 4.4611
Val cmap: 0.5478766957191729
Model improve: 0.0000 -> 0.5479
Epoch: 2/60
Train loss: 4.9918, val loss: 3.7248
Val cmap: 0.6436940311851556
Model improve: 0.5479 -> 0.6437
Epoch: 3/60
Train loss: 4.4759, val loss: 3.2824
Val cmap: 0.6947663223698113
Model improve: 0.6437 -> 0.6948
Epoch: 4/60
Train loss: 4.1690, val loss: 3.0473
Val cmap: 0.7171477197558264
Model improve: 0.6948 -> 0.7171
Epoch: 5/60
Train loss: 3.9224, val loss: 2.8618
Val cmap: 0.7411640340930292
Model improve: 0.7171 -> 0.7412
Epoch: 6/60
Train loss: 3.7816, val loss: 2.7776
Val cmap: 0.747670307999254
Model improve: 0.7412 -> 0.7477
Epoch: 7/60
Train loss: 3.6645, val loss: 2.7152
Val cmap: 0.758352929417401
Model improve: 0.7477 -> 0.7584
Epoch: 8/60
Train loss: 3.5684, val loss: 2.6256
Val cmap: 0.767824324782561
Model improve: 0.7584 -> 0.7678
Epoch: 9/60
Train loss: 3.4966, val loss: 2.5493
Val cmap: 0.7764690336477373
Model improve: 0.7678 -> 0.7765
Epoch: 10/60
Train loss: 3.4246, val loss: 2.5172
Val cmap: 0.7802371115063779
Model improve: 0.7765 -> 0.7802
Epoch: 11/60
Train loss: 3.3263, val loss: 2.4643
Val cmap: 0.7805465847058987
Model improve: 0.7802 -> 0.7805
Epoch: 12/60
Train loss: 3.2810, val loss: 2.4850
Val cmap: 0.7841934102422871
Model improve: 0.7805 -> 0.7842
Epoch: 13/60
Train loss: 3.2511, val loss: 2.4321
Val cmap: 0.7914548685040613
Model improve: 0.7842 -> 0.7915
Epoch: 14/60
Train loss: 3.1888, val loss: 2.3647
Val cmap: 0.7931497815610908
Model improve: 0.7915 -> 0.7931
Epoch: 15/60
Train loss: 3.1906, val loss: 2.3832
Val cmap: 0.7947172904710252
Model improve: 0.7931 -> 0.7947
Epoch: 16/60
Train loss: 3.1361, val loss: 2.3671
Val cmap: 0.7952864733177525
Model improve: 0.7947 -> 0.7953
Epoch: 17/60
Train loss: 3.1055, val loss: 2.3206
Val cmap: 0.7976676311643877
Model improve: 0.7953 -> 0.7977
Epoch: 18/60
Train loss: 3.0744, val loss: 2.3700
Val cmap: 0.7993815693278509
Model improve: 0.7977 -> 0.7994
Epoch: 19/60
Train loss: 3.0711, val loss: 2.2297
Val cmap: 0.8075188529606216
Model improve: 0.7994 -> 0.8075
Epoch: 20/60
Train loss: 2.9937, val loss: 2.2438
Val cmap: 0.8054416011886926
Epoch: 21/60
Train loss: 2.9664, val loss: 2.2552
Val cmap: 0.8079386402418469
Model improve: 0.8075 -> 0.8079
Epoch: 22/60
Train loss: 2.8945, val loss: 2.2296
Val cmap: 0.8078480174366737
Epoch: 23/60
Train loss: 2.9109, val loss: 2.2294
Val cmap: 0.8054295749248226
Epoch: 24/60
Train loss: 2.8611, val loss: 2.2178
Val cmap: 0.8090681936227598
Model improve: 0.8079 -> 0.8091
Epoch: 25/60
Train loss: 2.8795, val loss: 2.1557
Val cmap: 0.8126541669566701
Model improve: 0.8091 -> 0.8127
Epoch: 26/60
Train loss: 2.8143, val loss: 2.1842
Val cmap: 0.8113858960239668
Epoch: 27/60
Train loss: 2.8105, val loss: 2.1654
Val cmap: 0.8136784941634229
Model improve: 0.8127 -> 0.8137
Epoch: 28/60
Train loss: 2.7878, val loss: 2.1574
Val cmap: 0.8184389600131979
Model improve: 0.8137 -> 0.8184
Epoch: 29/60
Train loss: 2.7256, val loss: 2.1484
Val cmap: 0.8162591829794781
Epoch: 30/60
Train loss: 2.7132, val loss: 2.1123
Val cmap: 0.8213821130019584
Model improve: 0.8184 -> 0.8214
Epoch: 31/60
Train loss: 2.6964, val loss: 2.0991
Val cmap: 0.8205823626639589
Epoch: 32/60
Train loss: 2.6695, val loss: 2.1025
Val cmap: 0.8231644671043967
Model improve: 0.8214 -> 0.8232
Epoch: 33/60
Train loss: 2.6323, val loss: 2.0869
Val cmap: 0.8212170634349677
Epoch: 34/60
Train loss: 2.6428, val loss: 2.0386
Val cmap: 0.826539621955124
Model improve: 0.8232 -> 0.8265
Epoch: 35/60
Train loss: 2.6371, val loss: 2.0619
Val cmap: 0.8245450010223854
Epoch: 36/60
Train loss: 2.6220, val loss: 2.0339
Val cmap: 0.8250163585211071
Epoch: 37/60
Train loss: 2.5243, val loss: 2.0431
Val cmap: 0.8269699320503229
Model improve: 0.8265 -> 0.8270
Epoch: 38/60
Train loss: 2.5069, val loss: 2.0301
Val cmap: 0.8264991871659416
Epoch: 39/60
Train loss: 2.5429, val loss: 2.0180
Val cmap: 0.8295801023896504
Model improve: 0.8270 -> 0.8296
Epoch: 40/60
Train loss: 2.5167, val loss: 1.9759
Val cmap: 0.8334342788992312
Model improve: 0.8296 -> 0.8334
Epoch: 41/60
Train loss: 2.5007, val loss: 1.9950
Val cmap: 0.8292410168431793
Epoch: 42/60
Train loss: 2.4971, val loss: 1.9768
Val cmap: 0.8301852715939573
Epoch: 43/60
Train loss: 2.4472, val loss: 1.9936
Val cmap: 0.8329325042033223
Epoch: 44/60
Train loss: 2.4632, val loss: 2.0179
Val cmap: 0.8310157449387064
Epoch: 45/60
Train loss: 2.4805, val loss: 1.9778
Val cmap: 0.8319428852770877
Epoch: 46/60
Train loss: 2.3773, val loss: 1.9696
Val cmap: 0.8322079895980732
Epoch: 47/60
Train loss: 2.4026, val loss: 1.9725
Val cmap: 0.8335229284608325
Model improve: 0.8334 -> 0.8335
Epoch: 48/60
Train loss: 2.3977, val loss: 1.9371
Val cmap: 0.8365509756221722
Model improve: 0.8335 -> 0.8366
Epoch: 49/60
Train loss: 2.3997, val loss: 1.9261
Val cmap: 0.837522794418193
Model improve: 0.8366 -> 0.8375
Epoch: 50/60
Train loss: 2.3985, val loss: 1.9450
Val cmap: 0.8349730517186301
Epoch: 51/60
Train loss: 2.3555, val loss: 1.9654
Val cmap: 0.8345353784710091
Epoch: 52/60
Train loss: 2.3771, val loss: 1.9071
Val cmap: 0.8385194189521666
Model improve: 0.8375 -> 0.8385
Epoch: 53/60
Train loss: 2.3513, val loss: 1.9302
Val cmap: 0.8359498578106074
Epoch: 54/60
Train loss: 2.3006, val loss: 1.9681
Val cmap: 0.8317592495960063
Epoch: 55/60
Train loss: 2.3443, val loss: 1.9696
Val cmap: 0.8354335238737342
Epoch: 56/60
Train loss: 2.3223, val loss: 1.8951
Val cmap: 0.839518441778451
Model improve: 0.8385 -> 0.8395
Epoch: 57/60
Train loss: 2.3109, val loss: 1.9293
Val cmap: 0.8378576588825211
Epoch: 58/60
Train loss: 2.3693, val loss: 1.9301
Val cmap: 0.8351765970220086
Epoch: 59/60
Train loss: 2.3238, val loss: 1.9464
Val cmap: 0.8371951610757398
Epoch: 60/60
Date :04/21/2023, 04:21:52
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnet_b0_ns
mix_up: 0.4
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
61125
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 7.7281, val loss: 4.5252
Val cmap: 0.5346535560756237
Model improve: 0.0000 -> 0.5347
Epoch: 2/60
Train loss: 5.2313, val loss: 3.6861
Val cmap: 0.6490429439682687
Model improve: 0.5347 -> 0.6490
Epoch: 3/60
Train loss: 4.7416, val loss: 3.3277
Val cmap: 0.6914402964940549
Model improve: 0.6490 -> 0.6914
Epoch: 4/60
Train loss: 4.4576, val loss: 3.1411
Val cmap: 0.716008063229785
Model improve: 0.6914 -> 0.7160
Epoch: 5/60
Train loss: 4.2399, val loss: 2.9105
Val cmap: 0.7368361329558826
Model improve: 0.7160 -> 0.7368
Epoch: 6/60
Train loss: 4.0880, val loss: 2.8887
Val cmap: 0.7413120494398951
Model improve: 0.7368 -> 0.7413
Epoch: 7/60
Train loss: 3.9675, val loss: 2.7455
Val cmap: 0.7573183169360626
Model improve: 0.7413 -> 0.7573
Epoch: 8/60
Train loss: 3.8509, val loss: 2.6270
Val cmap: 0.7684560643087408
Model improve: 0.7573 -> 0.7685
Epoch: 9/60
Train loss: 3.7998, val loss: 2.5930
Val cmap: 0.7748265862357965
Model improve: 0.7685 -> 0.7748
Epoch: 10/60
Train loss: 3.7033, val loss: 2.5746
Val cmap: 0.7783284748735961
Model improve: 0.7748 -> 0.7783
Epoch: 11/60
Train loss: 3.6342, val loss: 2.5750
Val cmap: 0.7768704342599596
Epoch: 12/60
Train loss: 3.6059, val loss: 2.5017
Val cmap: 0.7834429529700745
Model improve: 0.7783 -> 0.7834
Epoch: 13/60
Train loss: 3.5633, val loss: 2.4413
Val cmap: 0.794267219699043
Model improve: 0.7834 -> 0.7943
Epoch: 14/60
Train loss: 3.5105, val loss: 2.4430
Val cmap: 0.7886535444903185
Epoch: 15/60
Date :04/21/2023, 07:49:06
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnet_b0_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
61125
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :04/21/2023, 07:49:27
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnet_b0_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
61125
65256
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 7.7484, val loss: 4.5182
Val cmap: 0.5337551397249698
Model improve: 0.0000 -> 0.5338
Epoch: 2/60
Date :04/21/2023, 08:07:32
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.005
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnet_b0_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
61125
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.005
    lr: 0.005
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :04/21/2023, 08:19:33
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.0007
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnet_b0_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
61125
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.0007
    lr: 0.0007
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 7.6542, val loss: 4.4444
Val cmap: 0.541507471938163
Model improve: 0.0000 -> 0.5415
Epoch: 2/60
Train loss: 4.9671, val loss: 3.6742
Val cmap: 0.6483974954904786
Model improve: 0.5415 -> 0.6484
Epoch: 3/60
Date :04/22/2023, 05:13:04
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 0
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
61125
61125
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 7.9916, val loss: 4.5521
Val cmap: 0.5252299342397585
Model improve: 0.0000 -> 0.5252
Epoch: 2/60
Train loss: 5.4086, val loss: 3.7974
Val cmap: 0.6294863828895071
Model improve: 0.5252 -> 0.6295
Epoch: 3/60
Train loss: 4.9675, val loss: 3.4562
Val cmap: 0.6692138220035534
Model improve: 0.6295 -> 0.6692
Epoch: 4/60
Train loss: 4.7271, val loss: 3.1516
Val cmap: 0.7040368397185437
Model improve: 0.6692 -> 0.7040
Epoch: 5/60
Train loss: 4.5276, val loss: 2.9951
Val cmap: 0.7257249537687134
Model improve: 0.7040 -> 0.7257
Epoch: 6/60
Train loss: 4.4335, val loss: 2.8860
Val cmap: 0.7402205765505495
Model improve: 0.7257 -> 0.7402
Epoch: 7/60
Train loss: 4.3198, val loss: 2.8255
Val cmap: 0.7497909661219269
Model improve: 0.7402 -> 0.7498
Epoch: 8/60
Train loss: 4.1881, val loss: 2.7464
Val cmap: 0.757328841983549
Model improve: 0.7498 -> 0.7573
Epoch: 9/60
Train loss: 4.1320, val loss: 2.6787
Val cmap: 0.7642840504384713
Model improve: 0.7573 -> 0.7643
Epoch: 10/60
Train loss: 4.0781, val loss: 2.6008
Val cmap: 0.770764650763147
Model improve: 0.7643 -> 0.7708
Epoch: 11/60
Train loss: 4.0342, val loss: 2.5737
Val cmap: 0.7742775603095694
Model improve: 0.7708 -> 0.7743
Epoch: 12/60
Train loss: 3.9765, val loss: 2.6225
Val cmap: 0.7712170127341116
Epoch: 13/60
Train loss: 3.9101, val loss: 2.5057
Val cmap: 0.7800243882928247
Model improve: 0.7743 -> 0.7800
Epoch: 14/60
Train loss: 3.8581, val loss: 2.5179
Val cmap: 0.785378702072202
Model improve: 0.7800 -> 0.7854
Epoch: 15/60
Train loss: 3.8425, val loss: 2.4580
Val cmap: 0.7926579775429115
Model improve: 0.7854 -> 0.7927
Epoch: 16/60
Train loss: 3.7996, val loss: 2.4202
Val cmap: 0.7902701608000303
Epoch: 17/60
Train loss: 3.7854, val loss: 2.4613
Val cmap: 0.7901024473266467
Epoch: 18/60
Train loss: 3.6965, val loss: 2.3844
Val cmap: 0.800941745402706
Model improve: 0.7927 -> 0.8009
Epoch: 19/60
Train loss: 3.7143, val loss: 2.4057
Val cmap: 0.7985269496623932
Epoch: 20/60
Train loss: 3.6559, val loss: 2.3663
Val cmap: 0.7995425284918748
Epoch: 21/60
Train loss: 3.6419, val loss: 2.3517
Val cmap: 0.7999342487465102
Epoch: 22/60
Train loss: 3.6379, val loss: 2.3288
Val cmap: 0.8048749730828808
Model improve: 0.8009 -> 0.8049
Epoch: 23/60
Train loss: 3.5590, val loss: 2.2690
Val cmap: 0.8058148124331587
Model improve: 0.8049 -> 0.8058
Epoch: 24/60
Date :04/22/2023, 10:53:38
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 10
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
61125
61271
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 8.0531, val loss: 4.6928
Val cmap: 0.5191137379377584
Model improve: 0.0000 -> 0.5191
Epoch: 2/60
Train loss: 5.4140, val loss: 3.8135
Val cmap: 0.6306768960248251
Model improve: 0.5191 -> 0.6307
Epoch: 3/60
Train loss: 4.9657, val loss: 3.4063
Val cmap: 0.6798831648371503
Model improve: 0.6307 -> 0.6799
Epoch: 4/60
Date :04/22/2023, 11:39:48
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnet_b1_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
61125
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 7.8710, val loss: 4.5201
Val cmap: 0.5379464721803817
Model improve: 0.0000 -> 0.5379
Epoch: 2/60
Train loss: 4.9857, val loss: 3.6927
Val cmap: 0.6452829488593418
Model improve: 0.5379 -> 0.6453
Epoch: 3/60
Train loss: 4.4536, val loss: 3.2551
Val cmap: 0.7023415520356527
Model improve: 0.6453 -> 0.7023
Epoch: 4/60
Train loss: 4.1309, val loss: 2.9938
Val cmap: 0.7279481935835889
Model improve: 0.7023 -> 0.7279
Epoch: 5/60
Train loss: 3.8977, val loss: 2.8867
Val cmap: 0.742590771448885
Model improve: 0.7279 -> 0.7426
Epoch: 6/60
Train loss: 3.7536, val loss: 2.6986
Val cmap: 0.7546277460940494
Model improve: 0.7426 -> 0.7546
Epoch: 7/60
Train loss: 3.6345, val loss: 2.6851
Val cmap: 0.7617106340714859
Model improve: 0.7546 -> 0.7617
Epoch: 8/60
Train loss: 3.5436, val loss: 2.5928
Val cmap: 0.7726042614810308
Model improve: 0.7617 -> 0.7726
Epoch: 9/60
Train loss: 3.4490, val loss: 2.5017
Val cmap: 0.7787350278324597
Model improve: 0.7726 -> 0.7787
Epoch: 10/60
Train loss: 3.3974, val loss: 2.4801
Val cmap: 0.7851320533228283
Model improve: 0.7787 -> 0.7851
Epoch: 11/60
Train loss: 3.2928, val loss: 2.4152
Val cmap: 0.7938341401240375
Model improve: 0.7851 -> 0.7938
Epoch: 12/60
Train loss: 3.2323, val loss: 2.4247
Val cmap: 0.7878822315182045
Epoch: 13/60
Train loss: 3.2151, val loss: 2.3914
Val cmap: 0.7941737340948575
Model improve: 0.7938 -> 0.7942
Epoch: 14/60
Train loss: 3.1407, val loss: 2.3612
Val cmap: 0.7943993812618523
Model improve: 0.7942 -> 0.7944
Epoch: 15/60
Train loss: 3.1470, val loss: 2.3207
Val cmap: 0.8000694472743962
Model improve: 0.7944 -> 0.8001
Epoch: 16/60
Train loss: 3.0646, val loss: 2.3248
Val cmap: 0.8025148536380425
Model improve: 0.8001 -> 0.8025
Epoch: 17/60
Train loss: 3.0420, val loss: 2.2963
Val cmap: 0.8064486222350169
Model improve: 0.8025 -> 0.8064
Epoch: 18/60
Train loss: 3.0252, val loss: 2.2892
Val cmap: 0.8100141890211972
Model improve: 0.8064 -> 0.8100
Epoch: 19/60
Train loss: 3.0227, val loss: 2.2085
Val cmap: 0.8106551527533824
Model improve: 0.8100 -> 0.8107
Epoch: 20/60
Train loss: 2.9433, val loss: 2.1719
Val cmap: 0.8141314831071998
Model improve: 0.8107 -> 0.8141
Epoch: 21/60
Train loss: 2.8888, val loss: 2.2011
Val cmap: 0.8151334514331462
Model improve: 0.8141 -> 0.8151
Epoch: 22/60
Train loss: 2.8405, val loss: 2.1513
Val cmap: 0.8163328499143778
Model improve: 0.8151 -> 0.8163
Epoch: 23/60
Train loss: 2.8413, val loss: 2.1457
Val cmap: 0.8161480661523972
Epoch: 24/60
Train loss: 2.7993, val loss: 2.1961
Val cmap: 0.809852410911067
Epoch: 25/60
Train loss: 2.8093, val loss: 2.1105
Val cmap: 0.8204463331101446
Model improve: 0.8163 -> 0.8204
Epoch: 26/60
Train loss: 2.7459, val loss: 2.1256
Val cmap: 0.8229566939173656
Model improve: 0.8204 -> 0.8230
Epoch: 27/60
Train loss: 2.7449, val loss: 2.0790
Val cmap: 0.8219569600474183
Epoch: 28/60
Train loss: 2.7283, val loss: 2.2072
Val cmap: 0.8169290181092127
Epoch: 29/60
Train loss: 2.6585, val loss: 2.1034
Val cmap: 0.8245004156468831
Model improve: 0.8230 -> 0.8245
Epoch: 30/60
Train loss: 2.6548, val loss: 2.0965
Val cmap: 0.8231442539532463
Epoch: 31/60
Train loss: 2.6293, val loss: 2.1036
Val cmap: 0.8251577044638531
Model improve: 0.8245 -> 0.8252
Epoch: 32/60
Train loss: 2.6098, val loss: 2.1189
Val cmap: 0.8256037981279505
Model improve: 0.8252 -> 0.8256
Epoch: 33/60
Train loss: 2.5545, val loss: 2.0518
Val cmap: 0.8242382246420874
Epoch: 34/60
Train loss: 2.5681, val loss: 2.0108
Val cmap: 0.8311899952029795
Model improve: 0.8256 -> 0.8312
Epoch: 35/60
Train loss: 2.5660, val loss: 2.0307
Val cmap: 0.8294653062265765
Epoch: 36/60
Train loss: 2.5515, val loss: 1.9933
Val cmap: 0.8296671935577679
Epoch: 37/60
Train loss: 2.4550, val loss: 2.0054
Val cmap: 0.831402746126056
Model improve: 0.8312 -> 0.8314
Epoch: 38/60
Train loss: 2.4406, val loss: 1.9954
Val cmap: 0.8288790441105348
Epoch: 39/60
Train loss: 2.4659, val loss: 1.9682
Val cmap: 0.835462183600162
Model improve: 0.8314 -> 0.8355
Epoch: 40/60
Train loss: 2.4525, val loss: 1.9718
Val cmap: 0.8363874611790008
Model improve: 0.8355 -> 0.8364
Epoch: 41/60
Train loss: 2.4138, val loss: 1.9488
Val cmap: 0.8369000451240801
Model improve: 0.8364 -> 0.8369
Epoch: 42/60
Train loss: 2.4134, val loss: 1.9188
Val cmap: 0.8384364489463046
Model improve: 0.8369 -> 0.8384
Epoch: 43/60
Train loss: 2.3748, val loss: 1.9424
Val cmap: 0.8360805443130623
Epoch: 44/60
Train loss: 2.3771, val loss: 1.9880
Val cmap: 0.8338911820738437
Epoch: 45/60
Train loss: 2.4034, val loss: 1.9084
Val cmap: 0.8411090427747604
Model improve: 0.8384 -> 0.8411
Epoch: 46/60
Train loss: 2.2995, val loss: 1.9169
Val cmap: 0.8393271150007249
Epoch: 47/60
Train loss: 2.3152, val loss: 1.9061
Val cmap: 0.8414917025685046
Model improve: 0.8411 -> 0.8415
Epoch: 48/60
Train loss: 2.3271, val loss: 1.9156
Val cmap: 0.8364934496331615
Epoch: 49/60
Train loss: 2.3060, val loss: 1.9204
Val cmap: 0.8383561537406922
Epoch: 50/60
Train loss: 2.3188, val loss: 1.9117
Val cmap: 0.8402786652797117
Epoch: 51/60
Train loss: 2.2731, val loss: 1.9162
Val cmap: 0.8412327251528045
Epoch: 52/60
Train loss: 2.2906, val loss: 1.8844
Val cmap: 0.8421393418750586
Model improve: 0.8415 -> 0.8421
Epoch: 53/60
Train loss: 2.2697, val loss: 1.9068
Val cmap: 0.8403459215317493
Epoch: 54/60
Train loss: 2.2158, val loss: 1.9406
Val cmap: 0.840993775448886
Epoch: 55/60
Train loss: 2.2620, val loss: 1.9414
Val cmap: 0.8388512362033311
Epoch: 56/60
Train loss: 2.2374, val loss: 1.9134
Val cmap: 0.838809660972872
Epoch: 57/60
Train loss: 2.2404, val loss: 1.9250
Val cmap: 0.839817137101649
Epoch: 58/60
Train loss: 2.2747, val loss: 1.8921
Val cmap: 0.8407414185218731
Epoch: 59/60
Date :04/23/2023, 01:49:35
Duration: 5
Sample rate: 32000
nfft: 4096
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnet_b1_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
61125
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 7.7953, val loss: 4.5610
Val cmap: 0.5357509283579845
Model improve: 0.0000 -> 0.5358
Epoch: 2/60
Date :04/23/2023, 02:10:31
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.6
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
61125
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 7.7831, val loss: 4.4209
Val cmap: 0.5594284658243979
Model improve: 0.0000 -> 0.5594
Epoch: 2/60
Train loss: 5.2798, val loss: 3.7881
Val cmap: 0.650412562838626
Model improve: 0.5594 -> 0.6504
Epoch: 3/60
Train loss: 4.7967, val loss: 3.2687
Val cmap: 0.7051657995340996
Model improve: 0.6504 -> 0.7052
Epoch: 4/60
Train loss: 4.5126, val loss: 3.1283
Val cmap: 0.7251740133435945
Model improve: 0.7052 -> 0.7252
Epoch: 5/60
Train loss: 4.2936, val loss: 2.8402
Val cmap: 0.7511130051530513
Model improve: 0.7252 -> 0.7511
Epoch: 6/60
Train loss: 4.1503, val loss: 2.8292
Val cmap: 0.758123982119654
Model improve: 0.7511 -> 0.7581
Epoch: 7/60
Train loss: 4.0643, val loss: 2.6805
Val cmap: 0.7707330483801103
Model improve: 0.7581 -> 0.7707
Epoch: 8/60
Train loss: 3.9723, val loss: 2.6685
Val cmap: 0.7740285460350502
Model improve: 0.7707 -> 0.7740
Epoch: 9/60
Train loss: 3.8727, val loss: 2.5522
Val cmap: 0.7838263396564329
Model improve: 0.7740 -> 0.7838
Epoch: 10/60
Train loss: 3.7923, val loss: 2.5715
Val cmap: 0.7860594396911833
Model improve: 0.7838 -> 0.7861
Epoch: 11/60
Train loss: 3.7218, val loss: 2.5106
Val cmap: 0.7881852205255091
Model improve: 0.7861 -> 0.7882
Epoch: 12/60
Train loss: 3.7254, val loss: 2.5319
Val cmap: 0.7914534855532698
Model improve: 0.7882 -> 0.7915
Epoch: 13/60
Train loss: 3.6510, val loss: 2.5733
Val cmap: 0.7952667824250965
Model improve: 0.7915 -> 0.7953
Epoch: 14/60
Train loss: 3.6195, val loss: 2.4132
Val cmap: 0.7990102921582247
Model improve: 0.7953 -> 0.7990
Epoch: 15/60
Train loss: 3.5520, val loss: 2.4280
Val cmap: 0.799028415090827
Model improve: 0.7990 -> 0.7990
Epoch: 16/60
Train loss: 3.4730, val loss: 2.3442
Val cmap: 0.805722921605278
Model improve: 0.7990 -> 0.8057
Epoch: 17/60
Train loss: 3.4639, val loss: 2.3007
Val cmap: 0.813555333593781
Model improve: 0.8057 -> 0.8136
Epoch: 18/60
Train loss: 3.4791, val loss: 2.3160
Val cmap: 0.8102400371650925
Epoch: 19/60
Train loss: 3.4148, val loss: 2.3689
Val cmap: 0.8140397241838585
Model improve: 0.8136 -> 0.8140
Epoch: 20/60
Train loss: 3.4021, val loss: 2.2704
Val cmap: 0.812856407586151
Epoch: 21/60
Train loss: 3.3545, val loss: 2.2708
Val cmap: 0.8177510014207277
Model improve: 0.8140 -> 0.8178
Epoch: 22/60
Train loss: 3.3519, val loss: 2.2495
Val cmap: 0.8188253179028327
Model improve: 0.8178 -> 0.8188
Epoch: 23/60
Train loss: 3.3067, val loss: 2.2242
Val cmap: 0.8184701359059512
Epoch: 24/60
Train loss: 3.2886, val loss: 2.2892
Val cmap: 0.8174433785394294
Epoch: 25/60
Train loss: 3.2157, val loss: 2.2228
Val cmap: 0.8184981415022426
Epoch: 26/60
Train loss: 3.2213, val loss: 2.1839
Val cmap: 0.8221674779389062
Model improve: 0.8188 -> 0.8222
Epoch: 27/60
Train loss: 3.2087, val loss: 2.1949
Val cmap: 0.8245500977769478
Model improve: 0.8222 -> 0.8246
Epoch: 28/60
Train loss: 3.1249, val loss: 2.1595
Val cmap: 0.8262642314868602
Model improve: 0.8246 -> 0.8263
Epoch: 29/60
Train loss: 3.1696, val loss: 2.1667
Val cmap: 0.8253253068279439
Epoch: 30/60
Train loss: 3.1576, val loss: 2.1190
Val cmap: 0.8292864154651806
Model improve: 0.8263 -> 0.8293
Epoch: 31/60
Train loss: 3.0964, val loss: 2.1781
Val cmap: 0.8288627817708821
Epoch: 32/60
Train loss: 3.0834, val loss: 2.0594
Val cmap: 0.8320164057517418
Model improve: 0.8293 -> 0.8320
Epoch: 33/60
Train loss: 3.0338, val loss: 2.1700
Val cmap: 0.8311458526021753
Epoch: 34/60
Train loss: 3.0009, val loss: 2.0660
Val cmap: 0.8330345248390202
Model improve: 0.8320 -> 0.8330
Epoch: 35/60
Train loss: 3.0603, val loss: 2.1306
Val cmap: 0.8275251370259823
Epoch: 36/60
Train loss: 3.0022, val loss: 2.1245
Val cmap: 0.8316581058217206
Epoch: 37/60
Train loss: 2.9702, val loss: 2.0677
Val cmap: 0.8340109669111869
Model improve: 0.8330 -> 0.8340
Epoch: 38/60
Train loss: 2.9643, val loss: 2.0944
Val cmap: 0.8345465699043402
Model improve: 0.8340 -> 0.8345
Epoch: 39/60
Train loss: 2.9365, val loss: 2.1426
Val cmap: 0.8299253363664364
Epoch: 40/60
Train loss: 2.9227, val loss: 2.0659
Val cmap: 0.8341455158099702
Epoch: 41/60
Train loss: 2.8658, val loss: 2.0489
Val cmap: 0.8371914235976592
Model improve: 0.8345 -> 0.8372
Epoch: 42/60
Train loss: 2.9032, val loss: 2.0273
Val cmap: 0.8384368328027276
Model improve: 0.8372 -> 0.8384
Epoch: 43/60
Train loss: 2.8817, val loss: 2.0338
Val cmap: 0.8355632473084298
Epoch: 44/60
Train loss: 2.8578, val loss: 2.0142
Val cmap: 0.8396446215804031
Model improve: 0.8384 -> 0.8396
Epoch: 45/60
Train loss: 2.8218, val loss: 1.9987
Val cmap: 0.8389536645303698
Epoch: 46/60
Train loss: 2.7990, val loss: 1.9925
Val cmap: 0.8456154097543104
Model improve: 0.8396 -> 0.8456
Epoch: 47/60
Train loss: 2.8110, val loss: 2.0073
Val cmap: 0.8410909138248412
Epoch: 48/60
Train loss: 2.7978, val loss: 1.9999
Val cmap: 0.8421032813484355
Epoch: 49/60
Train loss: 2.7591, val loss: 2.0336
Val cmap: 0.8378928084647578
Epoch: 50/60
Train loss: 2.7847, val loss: 2.0116
Val cmap: 0.8428113561711388
Epoch: 51/60
Train loss: 2.7466, val loss: 1.9838
Val cmap: 0.8420600271944837
Epoch: 52/60
Train loss: 2.7407, val loss: 1.9990
Val cmap: 0.8396136660207538
Epoch: 53/60
Train loss: 2.7694, val loss: 2.0044
Val cmap: 0.8428918692463037
Epoch: 54/60
Train loss: 2.7585, val loss: 1.9836
Val cmap: 0.8429912694175107
Epoch: 55/60
Train loss: 2.7348, val loss: 1.9897
Val cmap: 0.8406896296371608
Epoch: 56/60
Train loss: 2.7173, val loss: 2.0201
Val cmap: 0.8414349245327543
Epoch: 57/60
Train loss: 2.7438, val loss: 2.0018
Val cmap: 0.8418749987588297
Epoch: 58/60
Train loss: 2.7560, val loss: 2.0337
Val cmap: 0.8416784347840685
Epoch: 59/60
Train loss: 2.7026, val loss: 1.9602
Val cmap: 0.8482140989424319
Model improve: 0.8456 -> 0.8482
Epoch: 60/60
Train loss: 2.7170, val loss: 1.9958
Val cmap: 0.8394582302051623
Date :04/25/2023, 15:42:54
Duration: 5
Sample rate: 32000
nfft: 1024
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.6
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
61125
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :04/25/2023, 05:09:26
Duration: 5
Sample rate: 32000
nfft: 1024
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.6
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
61125
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :04/25/2023, 05:11:42
Duration: 5
Sample rate: 32000
nfft: 1024
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.6
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
61125
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :04/25/2023, 05:12:43
Duration: 5
Sample rate: 32000
nfft: 1024
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.6
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
61125
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :04/25/2023, 05:14:24
Duration: 5
Sample rate: 32000
nfft: 1024
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.6
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
61125
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :04/25/2023, 05:15:21
Duration: 5
Sample rate: 32000
nfft: 1024
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.6
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
61125
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :04/25/2023, 16:42:23
Duration: 5
Sample rate: 32000
nfft: 1024
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.6
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
61125
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :04/25/2023, 16:43:23
Duration: 5
Sample rate: 32000
nfft: 1024
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.6
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
61125
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :04/25/2023, 16:43:51
Duration: 5
Sample rate: 32000
nfft: 1024
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.6
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
61125
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :04/25/2023, 16:45:17
Duration: 5
Sample rate: 32000
nfft: 1024
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.6
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
61125
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :04/25/2023, 16:46:36
Duration: 5
Sample rate: 32000
nfft: 1024
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.6
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
61125
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :04/25/2023, 17:48:16
Duration: 5
Sample rate: 32000
nfft: 1024
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.6
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
61125
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :04/25/2023, 19:32:14
Duration: 5
Sample rate: 32000
nfft: 1024
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.6
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
61125
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :04/25/2023, 19:40:44
Duration: 5
Sample rate: 32000
nfft: 1024
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.6
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
61125
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :04/25/2023, 19:42:23
Duration: 5
Sample rate: 32000
nfft: 1024
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.6
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
61125
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :04/25/2023, 19:46:12
Duration: 5
Sample rate: 32000
nfft: 1024
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.6
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
61125
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :04/25/2023, 19:46:56
Duration: 5
Sample rate: 32000
nfft: 1024
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.6
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
61125
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
f1: 0.3213136475487413
Model improve: 0.0000 -> 0.3213
Epoch: 2/60
f1: 0.45708905030938923
Model improve: 0.3213 -> 0.4571
Epoch: 3/60
f1: 0.5413378506796678
Model improve: 0.4571 -> 0.5413
Epoch: 4/60
Date :04/25/2023, 20:40:32
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
61125
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
f1: 0.2825829383886256
Model improve: 0.0000 -> 0.2826
Epoch: 2/60
f1: 0.44331670393944606
Model improve: 0.2826 -> 0.4433
Epoch: 3/60
f1: 0.5295748996233287
Model improve: 0.4433 -> 0.5296
Epoch: 4/60
f1: 0.5725022988046216
Model improve: 0.5296 -> 0.5725
Epoch: 5/60
f1: 0.6178893079824495
Model improve: 0.5725 -> 0.6179
Epoch: 6/60
f1: 0.6185583182994744
Model improve: 0.6179 -> 0.6186
Epoch: 7/60
f1: 0.6350373437560465
Model improve: 0.6186 -> 0.6350
Epoch: 8/60
f1: 0.6593729958124269
Model improve: 0.6350 -> 0.6594
Epoch: 9/60
f1: 0.6627250331627819
Model improve: 0.6594 -> 0.6627
Epoch: 10/60
f1: 0.6711379050489826
Model improve: 0.6627 -> 0.6711
Epoch: 11/60
f1: 0.6797269219352924
Model improve: 0.6711 -> 0.6797
Epoch: 12/60
f1: 0.6780136467020471
Epoch: 13/60
f1: 0.6886291968094973
Model improve: 0.6797 -> 0.6886
Epoch: 14/60
f1: 0.6980721736886726
Model improve: 0.6886 -> 0.6981
Epoch: 15/60
f1: 0.7022690979314444
Model improve: 0.6981 -> 0.7023
Epoch: 16/60
f1: 0.7030936118923262
Model improve: 0.7023 -> 0.7031
Epoch: 17/60
f1: 0.703158208732615
Model improve: 0.7031 -> 0.7032
Epoch: 18/60
f1: 0.7112768758227294
Model improve: 0.7032 -> 0.7113
Epoch: 19/60
f1: 0.7164430143698125
Model improve: 0.7113 -> 0.7164
Epoch: 20/60
f1: 0.7090204759269507
Epoch: 21/60
f1: 0.7123812276455075
Epoch: 22/60
f1: 0.722734005274376
Model improve: 0.7164 -> 0.7227
Epoch: 23/60
f1: 0.7183073207655984
Epoch: 24/60
f1: 0.7256198347107438
Model improve: 0.7227 -> 0.7256
Epoch: 25/60
f1: 0.7246545454545454
Epoch: 26/60
f1: 0.7218776072840716
Epoch: 27/60
f1: 0.7299463327370305
Model improve: 0.7256 -> 0.7299
Epoch: 28/60
f1: 0.7324167872648335
Model improve: 0.7299 -> 0.7324
Epoch: 29/60
f1: 0.7375937165298108
Model improve: 0.7324 -> 0.7376
Epoch: 30/60
f1: 0.7337426837588423
Epoch: 31/60
f1: 0.7338528877975873
Epoch: 32/60
f1: 0.7402251500408394
Model improve: 0.7376 -> 0.7402
Epoch: 33/60
f1: 0.7378682225700869
Epoch: 34/60
f1: 0.7416750035729598
Model improve: 0.7402 -> 0.7417
Epoch: 35/60
f1: 0.7399586216736819
Epoch: 36/60
f1: 0.7433710763938655
Model improve: 0.7417 -> 0.7434
Epoch: 37/60
f1: 0.7456779539934276
Model improve: 0.7434 -> 0.7457
Epoch: 38/60
f1: 0.7439620369714569
Epoch: 39/60
f1: 0.7509170428893905
Model improve: 0.7457 -> 0.7509
Epoch: 40/60
f1: 0.7509109562387236
Epoch: 41/60
f1: 0.754954187087151
Model improve: 0.7509 -> 0.7550
Epoch: 42/60
f1: 0.7550501483260347
Model improve: 0.7550 -> 0.7551
Epoch: 43/60
f1: 0.7517241379310345
Epoch: 44/60
f1: 0.7568405113216186
Model improve: 0.7551 -> 0.7568
Epoch: 45/60
f1: 0.7567014672686231
Epoch: 46/60
f1: 0.7580519018841096
Model improve: 0.7568 -> 0.7581
Epoch: 47/60
f1: 0.7566268466812752
Epoch: 48/60
f1: 0.7551647420277572
Epoch: 49/60
f1: 0.7561767612593533
Epoch: 50/60
f1: 0.7642940904729754
Model improve: 0.7581 -> 0.7643
Epoch: 51/60
f1: 0.7589585666293394
Epoch: 52/60
f1: 0.7615922083421554
Epoch: 53/60
f1: 0.7607327449808375
Epoch: 54/60
f1: 0.7614500442086648
Epoch: 55/60
f1: 0.7632857092211154
Epoch: 56/60
f1: 0.7615482412770165
Epoch: 57/60
f1: 0.764546637133917
Model improve: 0.7643 -> 0.7645
Epoch: 58/60
Date :04/26/2023, 08:06:19
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
61125
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :04/26/2023, 08:23:50
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnet_b0_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
61125
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Epoch: 2/100
Epoch: 3/100
Epoch: 4/100
Epoch: 5/100
Epoch: 6/100
Epoch: 7/100
Epoch: 8/100
Epoch: 9/100
Epoch: 10/100
Epoch: 11/100
Epoch: 12/100
Epoch: 13/100
Epoch: 14/100
Epoch: 15/100
Epoch: 16/100
Epoch: 17/100
Epoch: 18/100
Epoch: 19/100
Epoch: 20/100
Epoch: 21/100
Epoch: 22/100
Epoch: 23/100
Epoch: 24/100
Epoch: 25/100
Epoch: 26/100
Epoch: 27/100
Epoch: 28/100
Epoch: 29/100
Epoch: 30/100
Epoch: 31/100
Epoch: 32/100
Epoch: 33/100
Epoch: 34/100
Epoch: 35/100
Date :04/26/2023, 20:26:25
Duration: 5
Sample rate: 32000
nfft: 1024
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
61125
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :04/26/2023, 20:27:08
Duration: 5
Sample rate: 32000
nfft: 1024
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
61125
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :04/26/2023, 20:28:46
Duration: 5
Sample rate: 32000
nfft: 1024
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnet_b1_ns
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
61125
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Epoch: 2/100
Epoch: 3/100
Epoch: 4/100
Epoch: 5/100
Epoch: 6/100
Epoch: 7/100
Epoch: 8/100
Epoch: 9/100
Epoch: 10/100
Epoch: 11/100
Epoch: 12/100
Epoch: 13/100
Epoch: 14/100
Epoch: 15/100
Epoch: 16/100
Epoch: 17/100
Epoch: 18/100
Epoch: 19/100
Epoch: 20/100
Epoch: 21/100
Epoch: 22/100
Epoch: 23/100
Epoch: 24/100
Epoch: 25/100
Epoch: 26/100
Epoch: 27/100
Epoch: 28/100
Epoch: 29/100
Epoch: 30/100
Epoch: 31/100
Epoch: 32/100
Epoch: 33/100
Epoch: 34/100
Epoch: 35/100
Epoch: 36/100
Epoch: 37/100
Epoch: 38/100
Epoch: 39/100
Epoch: 40/100
Epoch: 41/100
Epoch: 42/100
Epoch: 43/100
Epoch: 44/100
Epoch: 45/100
Epoch: 46/100
Epoch: 47/100
Epoch: 48/100
Epoch: 49/100
Epoch: 50/100
Epoch: 51/100
Epoch: 52/100
f1: 0.7525612944252104
Model improve: 0.0000 -> 0.7526
Epoch: 53/100
f1: 0.7514983863531581
Epoch: 54/100
f1: 0.7500435828597328
Epoch: 55/100
f1: 0.7545310575400316
Model improve: 0.7526 -> 0.7545
Epoch: 56/100
f1: 0.7579412383423323
Model improve: 0.7545 -> 0.7579
Epoch: 57/100
f1: 0.7617675987073205
Model improve: 0.7579 -> 0.7618
Epoch: 58/100
f1: 0.7560751624752755
Epoch: 59/100
f1: 0.7624166695752329
Model improve: 0.7618 -> 0.7624
Epoch: 60/100
f1: 0.7582352324728403
Epoch: 61/100
f1: 0.7590012222804261
Epoch: 62/100
f1: 0.7599137331292612
Epoch: 63/100
f1: 0.7628094016501421
Model improve: 0.7624 -> 0.7628
Epoch: 64/100
f1: 0.7573460046818771
Epoch: 65/100
f1: 0.7661581137309291
Model improve: 0.7628 -> 0.7662
Epoch: 66/100
f1: 0.7653902084343188
Epoch: 67/100
f1: 0.762678329688639
Epoch: 68/100
f1: 0.7603191599599323
Epoch: 69/100
f1: 0.7623707017053396
Epoch: 70/100
f1: 0.7612641020250859
Epoch: 71/100
f1: 0.766335743194161
Model improve: 0.7662 -> 0.7663
Epoch: 72/100
f1: 0.7651104626927886
Epoch: 73/100
f1: 0.7651551975557254
Epoch: 74/100
f1: 0.7643800299620248
Epoch: 75/100
f1: 0.7661491544219572
Epoch: 76/100
f1: 0.7695671792023879
Model improve: 0.7663 -> 0.7696
Epoch: 77/100
f1: 0.7705939360376576
Model improve: 0.7696 -> 0.7706
Epoch: 78/100
f1: 0.7698063532753662
Epoch: 79/100
f1: 0.766749208969597
Epoch: 80/100
f1: 0.7725108673152556
Model improve: 0.7706 -> 0.7725
Epoch: 81/100
f1: 0.7725059134548491
Epoch: 82/100
f1: 0.7717439595365846
Epoch: 83/100
f1: 0.7753240518482958
Model improve: 0.7725 -> 0.7753
Epoch: 84/100
f1: 0.7748752366202031
Epoch: 85/100
f1: 0.7666356845330305
Epoch: 86/100
f1: 0.773295572287076
Epoch: 87/100
f1: 0.7684358210998187
Epoch: 88/100
f1: 0.7674982083745692
Epoch: 89/100
f1: 0.7739641733898537
Epoch: 90/100
f1: 0.768864835051191
Epoch: 91/100
f1: 0.772596669782255
Epoch: 92/100
f1: 0.774557667670787
Epoch: 93/100
f1: 0.7723605032822757
Epoch: 94/100
f1: 0.772833885209713
Epoch: 95/100
f1: 0.7739825080917293
Epoch: 96/100
f1: 0.7765198298339508
Model improve: 0.7753 -> 0.7765
Epoch: 97/100
f1: 0.7722969264544457
Epoch: 98/100
f1: 0.7755242115378016
Epoch: 99/100
f1: 0.7742752507102029
Epoch: 100/100
Date :04/27/2023, 18:04:54
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 256
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnet_b1_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Date :04/27/2023, 18:05:01
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 256
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnet_b1_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
61125
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Epoch: 2/100
Epoch: 3/100
Epoch: 4/100
Epoch: 5/100
Epoch: 6/100
Epoch: 7/100
Epoch: 8/100
Epoch: 9/100
Epoch: 10/100
Epoch: 11/100
Epoch: 12/100
Epoch: 13/100
Epoch: 14/100
Epoch: 15/100
Epoch: 16/100
Epoch: 17/100
Epoch: 18/100
Epoch: 19/100
Epoch: 20/100
Epoch: 21/100
Epoch: 22/100
Epoch: 23/100
Epoch: 24/100
Epoch: 25/100
Date :04/28/2023, 06:37:51
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.0
thrupsample: 0
model_name: tf_efficientnet_b1_ns
mix_up: 0.0
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
61125
61125
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Date :04/28/2023, 06:38:18
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.0
thrupsample: 0
model_name: tf_efficientnet_b1_ns
mix_up: 0.0
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
61125
61125
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Date :04/28/2023, 06:38:45
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.0
thrupsample: 0
model_name: tf_efficientnet_b1_ns
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
61125
61125
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Epoch: 2/100
Epoch: 3/100
Epoch: 4/100
Epoch: 5/100
Epoch: 6/100
Epoch: 7/100
Epoch: 8/100
Epoch: 9/100
Epoch: 10/100
Epoch: 11/100
Epoch: 12/100
Epoch: 13/100
Epoch: 14/100
Epoch: 15/100
Epoch: 16/100
Epoch: 17/100
Epoch: 18/100
Epoch: 19/100
Epoch: 20/100
Epoch: 21/100
Epoch: 22/100
Epoch: 23/100
Epoch: 24/100
Epoch: 25/100
Epoch: 26/100
Epoch: 27/100
Epoch: 28/100
Epoch: 29/100
Epoch: 30/100
Epoch: 31/100
Epoch: 32/100
Epoch: 33/100
Epoch: 34/100
Epoch: 35/100
Epoch: 36/100
Epoch: 37/100
Epoch: 38/100
Epoch: 39/100
Epoch: 40/100
Epoch: 41/100
Epoch: 42/100
Epoch: 43/100
Epoch: 44/100
Epoch: 45/100
Epoch: 46/100
Epoch: 47/100
Epoch: 48/100
Epoch: 49/100
Epoch: 50/100
Epoch: 51/100
Epoch: 52/100
f1: 0.7547327685388557
Model improve: 0.0000 -> 0.7547
Epoch: 53/100
f1: 0.7532476602877497
Epoch: 54/100
f1: 0.7606484535008852
Model improve: 0.7547 -> 0.7606
Epoch: 55/100
f1: 0.7567284897503835
Epoch: 56/100
f1: 0.7548673179519158
Epoch: 57/100
f1: 0.7582131551178333
Epoch: 59/100
f1: 0.7540892128483242
Epoch: 60/100
f1: 0.7538562407260431
Epoch: 61/100
f1: 0.7561953225750235
Epoch: 62/100
f1: 0.7601710791417766
Epoch: 63/100
f1: 0.7600568240878696
Epoch: 64/100
f1: 0.7644615116076103
Model improve: 0.7606 -> 0.7645
Epoch: 65/100
f1: 0.7583391003460208
Epoch: 66/100
f1: 0.7622472851705716
Epoch: 67/100
f1: 0.7620002073469953
Epoch: 68/100
f1: 0.7605525441510753
Epoch: 69/100
f1: 0.758008356545961
Epoch: 70/100
f1: 0.7590052646162372
Epoch: 71/100
f1: 0.7651614687780927
Model improve: 0.7645 -> 0.7652
Epoch: 72/100
f1: 0.7663362189332589
Model improve: 0.7652 -> 0.7663
Epoch: 73/100
f1: 0.7618360475503332
Epoch: 74/100
f1: 0.7584950606039376
Epoch: 75/100
f1: 0.764216942578098
Epoch: 76/100
f1: 0.7644537375849451
Epoch: 77/100
f1: 0.7679454153032097
Model improve: 0.7663 -> 0.7679
Epoch: 78/100
f1: 0.7623508187621425
Epoch: 79/100
Date :04/28/2023, 22:18:10
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 256
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnet_b1_ns
mix_up: 0.6
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
61125
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Date :04/28/2023, 22:19:13
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 256
fmax: 16000
trainbs: 32
validbs: 64
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnet_b1_ns
mix_up: 0.6
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
61125
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Epoch: 2/100
Epoch: 3/100
Epoch: 4/100
Epoch: 5/100
Epoch: 6/100
Epoch: 7/100
Epoch: 8/100
Epoch: 9/100
Epoch: 10/100
Epoch: 11/100
Epoch: 12/100
Epoch: 13/100
Epoch: 14/100
Epoch: 15/100
Epoch: 16/100
Epoch: 17/100
Epoch: 18/100
Epoch: 19/100
Epoch: 20/100
Epoch: 21/100
Epoch: 22/100
Epoch: 23/100
Epoch: 24/100
Epoch: 25/100
Epoch: 26/100
Epoch: 27/100
Epoch: 28/100
Epoch: 29/100
Epoch: 30/100
Epoch: 31/100
Epoch: 32/100
Epoch: 33/100
Epoch: 34/100
Epoch: 35/100
Epoch: 36/100
Epoch: 37/100
Epoch: 38/100
Epoch: 39/100
Epoch: 40/100
Epoch: 41/100
Epoch: 42/100
Epoch: 43/100
Epoch: 44/100
Epoch: 45/100
Epoch: 46/100
Epoch: 47/100
Epoch: 48/100
Epoch: 49/100
Epoch: 50/100
Epoch: 51/100
Epoch: 52/100
f1: 0.7586742986361388
Model improve: 0.0000 -> 0.7587
Epoch: 53/100
f1: 0.758683922840921
Model improve: 0.7587 -> 0.7587
Epoch: 54/100
f1: 0.7600042159997189
Model improve: 0.7587 -> 0.7600
Epoch: 55/100
f1: 0.7651592468766497
Model improve: 0.7600 -> 0.7652
Epoch: 56/100
f1: 0.7639744989609383
Epoch: 57/100
f1: 0.7642894875858426
Epoch: 58/100
f1: 0.7618711883526633
Epoch: 59/100
f1: 0.7678302580441861
Model improve: 0.7652 -> 0.7678
Epoch: 60/100
f1: 0.7641968821159572
Epoch: 61/100
f1: 0.7651926833599716
Epoch: 62/100
f1: 0.7648896220124243
Epoch: 63/100
f1: 0.7661663822822719
Epoch: 64/100
f1: 0.76804430890034
Model improve: 0.7678 -> 0.7680
Epoch: 65/100
f1: 0.7691080040245637
Model improve: 0.7680 -> 0.7691
Epoch: 66/100
f1: 0.7641595083455548
Epoch: 67/100
f1: 0.7678790212482934
Epoch: 68/100
f1: 0.7718832891246684
Model improve: 0.7691 -> 0.7719
Epoch: 69/100
f1: 0.769555282123396
Epoch: 70/100
f1: 0.7675773753445689
Epoch: 71/100
f1: 0.7672431902527834
Epoch: 72/100
f1: 0.7721059972105998
Model improve: 0.7719 -> 0.7721
Epoch: 73/100
f1: 0.7695109990191957
Epoch: 74/100
f1: 0.7749625344160597
Model improve: 0.7721 -> 0.7750
Epoch: 75/100
f1: 0.7685559114329481
Epoch: 76/100
f1: 0.7730027548209366
Epoch: 77/100
f1: 0.766488665700039
Epoch: 78/100
f1: 0.7669714406749745
Epoch: 79/100
f1: 0.7691503450198647
Epoch: 80/100
f1: 0.7698570012742461
Epoch: 81/100
f1: 0.7743114109050028
Epoch: 82/100
f1: 0.7691824338476575
Epoch: 83/100
f1: 0.7689106232441995
Epoch: 84/100
f1: 0.7720979020979021
Epoch: 85/100
f1: 0.7701073088888112
Epoch: 86/100
f1: 0.7737460350657046
Epoch: 87/100
f1: 0.7728877547453421
Epoch: 88/100
f1: 0.7692360847182393
Epoch: 89/100
f1: 0.7693436779198993
Epoch: 90/100
f1: 0.774934360231052
Epoch: 91/100
f1: 0.7769743965908694
Model improve: 0.7750 -> 0.7770
Epoch: 92/100
f1: 0.7755639488547768
Epoch: 93/100
f1: 0.7731513716752542
Epoch: 94/100
f1: 0.7769538729329852
Epoch: 95/100
f1: 0.775199748787551
Epoch: 96/100
f1: 0.7764234660405371
Epoch: 97/100
f1: 0.7733844278000349
Epoch: 98/100
f1: 0.7725656931762738
Epoch: 99/100
f1: 0.7746991712036618
Epoch: 100/100
f1: 0.7747822194676859
Date :05/05/2023, 14:39:32
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: eca_nfnet_l0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
78014
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/60
Date :05/05/2023, 14:40:09
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
78014
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/60
Date :05/05/2023, 14:40:30
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
78014
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/60
Date :05/05/2023, 14:40:56
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
Date :05/05/2023, 14:41:41
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
63682
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/60
Date :05/05/2023, 14:43:25
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
63682
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/60
Train loss: 7.6393, val loss: 4.39151
f1: 0.3427374301675978
Date :05/05/2023, 15:00:14
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
63682
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/60
Train loss: 7.6393, val loss: 4.39151
f1: 0.3427374301675978
Val cmap: 0.551189633532751
Model improve: 0.00000 -> 0.34274
Epoch: 2/60
Train loss: 4.8768, val loss: 3.56103
f1: 0.4943301133977321
Val cmap: 0.6590782863314248
Model improve: 0.34274 -> 0.49433
Epoch: 3/60
Train loss: 4.3731, val loss: 3.20714
f1: 0.564483728404982
Val cmap: 0.7032560593458701
Model improve: 0.49433 -> 0.56448
Epoch: 4/60
Train loss: 4.0605, val loss: 2.90704
f1: 0.6195933746761912
Val cmap: 0.7369144889912816
Model improve: 0.56448 -> 0.61959
Epoch: 5/60
Train loss: 3.8063, val loss: 2.85252
f1: 0.6328533781461254
Val cmap: 0.7466272235152907
Model improve: 0.61959 -> 0.63285
Epoch: 6/60
Date :05/05/2023, 16:04:26
Duration: 5
Sample rate: 32000
nfft: 1024
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
63682
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Date :05/05/2023, 16:05:01
Duration: 5
Sample rate: 32000
nfft: 1024
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
63682
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Epoch: 2/100
Epoch: 3/100
Epoch: 4/100
Epoch: 5/100
Epoch: 6/100
Epoch: 7/100
Epoch: 8/100
Epoch: 9/100
Epoch: 10/100
Epoch: 11/100
Epoch: 12/100
Epoch: 13/100
Epoch: 14/100
Epoch: 15/100
Epoch: 16/100
Epoch: 17/100
Epoch: 18/100
Epoch: 19/100
Epoch: 20/100
Epoch: 21/100
Epoch: 22/100
Epoch: 23/100
Epoch: 24/100
Epoch: 25/100
Epoch: 26/100
Epoch: 27/100
Epoch: 28/100
Epoch: 29/100
Epoch: 30/100
Epoch: 31/100
Epoch: 32/100
Epoch: 33/100
Epoch: 34/100
Epoch: 35/100
Epoch: 36/100
Epoch: 37/100
Epoch: 38/100
Epoch: 39/100
Epoch: 40/100
Epoch: 41/100
Epoch: 42/100
Epoch: 43/100
Epoch: 44/100
Epoch: 45/100
Epoch: 46/100
Epoch: 47/100
Epoch: 48/100
Epoch: 49/100
Epoch: 50/100
Epoch: 51/100
Epoch: 52/100
Epoch: 53/100
Epoch: 54/100
Epoch: 55/100
Epoch: 56/100
Epoch: 57/100
Epoch: 58/100
Epoch: 59/100
Epoch: 60/100
Epoch: 61/100
Epoch: 62/100
Train loss: 2.2956, val loss: 2.15206
f1: 0.7577460830464414
Val cmap: 0.83202254221298
Model improve: 0.00000 -> 0.75775
Epoch: 63/100
Train loss: 2.2606, val loss: 2.20213
f1: 0.7544624033731553
Val cmap: 0.8315629657434925
Epoch: 64/100
Train loss: 2.3086, val loss: 2.23439
f1: 0.7521941326619811
Val cmap: 0.8300105466738351
Epoch: 65/100
Train loss: 2.2874, val loss: 2.14443
f1: 0.7572584316884391
Val cmap: 0.8338662130861832
Epoch: 66/100
Train loss: 2.2655, val loss: 2.19327
f1: 0.7556361567456084
Val cmap: 0.8299231730785661
Epoch: 67/100
Train loss: 2.2042, val loss: 2.17010
f1: 0.7580087336244542
Val cmap: 0.8326973021508525
Model improve: 0.75775 -> 0.75801
Epoch: 68/100
Train loss: 2.2294, val loss: 2.20079
f1: 0.760528547201809
Val cmap: 0.8336854157470762
Model improve: 0.75801 -> 0.76053
Epoch: 69/100
Train loss: 2.2410, val loss: 2.17336
f1: 0.7582901645698981
Val cmap: 0.8351812060380334
Epoch: 70/100
Train loss: 2.1805, val loss: 2.20614
f1: 0.7583193512304252
Val cmap: 0.8324375052448267
Epoch: 71/100
Train loss: 2.1766, val loss: 2.18979
f1: 0.7589122164294774
Val cmap: 0.8356667561949744
Epoch: 72/100
Train loss: 2.1886, val loss: 2.12659
f1: 0.7658338582126111
Val cmap: 0.8376900559534034
Model improve: 0.76053 -> 0.76583
Epoch: 73/100
Train loss: 2.1886, val loss: 2.18627
f1: 0.7566356187408081
Val cmap: 0.8327661205953907
Epoch: 74/100
Train loss: 2.2050, val loss: 2.17519
f1: 0.7587478260869566
Val cmap: 0.8348332964010492
Epoch: 75/100
Train loss: 2.2027, val loss: 2.17118
f1: 0.7617610945448167
Val cmap: 0.8359434891287822
Epoch: 76/100
Train loss: 2.2033, val loss: 2.15245
f1: 0.7599159074982481
Val cmap: 0.8327390382942653
Epoch: 77/100
Train loss: 2.1983, val loss: 2.14536
f1: 0.7622644148396797
Val cmap: 0.8381317717250597
Epoch: 78/100
Train loss: 2.2217, val loss: 2.18400
f1: 0.7620707686944397
Val cmap: 0.8361633954544503
Epoch: 79/100
Train loss: 2.1214, val loss: 2.17075
f1: 0.7608273181565797
Val cmap: 0.8363872846051398
Epoch: 80/100
Train loss: 2.1528, val loss: 2.18343
f1: 0.7589707684228951
Val cmap: 0.8350208117515981
Epoch: 81/100
Train loss: 2.1232, val loss: 2.15311
f1: 0.763083108061274
Val cmap: 0.8381954941273918
Epoch: 82/100
Train loss: 2.1609, val loss: 2.21065
f1: 0.7614643545279384
Val cmap: 0.8344974807439283
Epoch: 83/100
Train loss: 2.0943, val loss: 2.14978
f1: 0.7638744543753898
Val cmap: 0.8378891328747793
Epoch: 84/100
Date :05/06/2023, 06:51:40
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
63682
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Epoch: 2/100
Epoch: 3/100
Epoch: 4/100
Epoch: 5/100
Epoch: 6/100
Epoch: 7/100
Epoch: 8/100
Epoch: 9/100
Epoch: 10/100
Epoch: 11/100
Epoch: 12/100
Epoch: 13/100
Epoch: 14/100
Epoch: 15/100
Epoch: 16/100
Epoch: 17/100
Epoch: 18/100
Epoch: 19/100
Epoch: 20/100
Epoch: 21/100
Epoch: 22/100
Epoch: 23/100
Epoch: 24/100
Epoch: 25/100
Epoch: 26/100
Epoch: 27/100
Epoch: 28/100
Epoch: 29/100
Epoch: 30/100
Epoch: 31/100
Epoch: 32/100
Epoch: 33/100
Epoch: 34/100
Epoch: 35/100
Epoch: 36/100
Epoch: 37/100
Epoch: 38/100
Epoch: 39/100
Epoch: 40/100
Epoch: 41/100
Epoch: 42/100
Epoch: 43/100
Epoch: 44/100
Epoch: 45/100
Epoch: 46/100
Epoch: 47/100
Epoch: 48/100
Epoch: 49/100
Epoch: 50/100
Epoch: 51/100
Epoch: 52/100
Epoch: 53/100
Epoch: 54/100
Epoch: 55/100
Epoch: 56/100
Epoch: 57/100
Epoch: 58/100
Epoch: 59/100
Epoch: 60/100
Epoch: 61/100
Epoch: 62/100
Train loss: 2.9241, val loss: 2.16934
f1: 0.7592176979801218
Val cmap: 0.831644756485673
Model improve: 0.00000 -> 0.75922
Epoch: 63/100
Train loss: 2.9055, val loss: 2.13928
f1: 0.7571076945029073
Val cmap: 0.8306930437701001
Epoch: 64/100
Train loss: 2.8948, val loss: 2.16718
f1: 0.7567374365572951
Val cmap: 0.8306090481650219
Epoch: 65/100
Train loss: 2.9064, val loss: 2.19160
f1: 0.7597965470305895
Val cmap: 0.830734675797516
Model improve: 0.75922 -> 0.75980
Epoch: 66/100
Train loss: 2.9058, val loss: 2.17154
f1: 0.7597721328508472
Val cmap: 0.8319312411953544
Epoch: 67/100
Train loss: 2.8785, val loss: 2.12623
f1: 0.7605803682287418
Val cmap: 0.8317490830585904
Model improve: 0.75980 -> 0.76058
Epoch: 68/100
Train loss: 2.8632, val loss: 2.16586
f1: 0.7610524814603536
Val cmap: 0.8345448302601435
Model improve: 0.76058 -> 0.76105
Epoch: 69/100
Train loss: 2.8760, val loss: 2.14699
f1: 0.7615427927927928
Val cmap: 0.8390718137583546
Model improve: 0.76105 -> 0.76154
Epoch: 70/100
Train loss: 2.8621, val loss: 2.14812
f1: 0.7616332929523266
Val cmap: 0.8322480047844099
Model improve: 0.76154 -> 0.76163
Epoch: 71/100
Date :05/06/2023, 19:12:59
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
63682
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Date :05/06/2023, 19:22:58
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
63682
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Date :05/06/2023, 19:23:13
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
63682
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Date :05/06/2023, 19:24:19
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
Date :05/06/2023, 19:25:03
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
Date :05/06/2023, 19:25:44
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
65256
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Epoch: 2/100
Epoch: 3/100
Date :05/06/2023, 19:54:53
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
65256
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Train loss: 7.8853
Epoch: 2/100
Train loss: 5.4311
Epoch: 3/100
Train loss: 4.9757
Epoch: 4/100
Train loss: 4.6919
Epoch: 5/100
Train loss: 4.4905
Epoch: 6/100
Train loss: 4.3404
Epoch: 7/100
Train loss: 4.2352
Epoch: 8/100
Train loss: 4.1265
Epoch: 9/100
Train loss: 4.0806
Epoch: 10/100
Train loss: 3.9928
Epoch: 11/100
Train loss: 3.9637
Epoch: 12/100
Train loss: 3.8922
Epoch: 13/100
Train loss: 3.8121
Epoch: 14/100
Train loss: 3.8206
Epoch: 15/100
Train loss: 3.7707
Epoch: 16/100
Train loss: 3.7358
Epoch: 17/100
Train loss: 3.6930
Epoch: 18/100
Train loss: 3.6920
Epoch: 19/100
Train loss: 3.6209
Epoch: 20/100
Train loss: 3.6464
Epoch: 21/100
Train loss: 3.5758
Epoch: 22/100
Train loss: 3.5645
Epoch: 23/100
Train loss: 3.5567
Epoch: 24/100
Train loss: 3.5258
Epoch: 25/100
Train loss: 3.5150
Epoch: 26/100
Train loss: 3.4873
Epoch: 27/100
Train loss: 3.4805
Epoch: 28/100
Train loss: 3.4487
Epoch: 29/100
Train loss: 3.4231
Epoch: 30/100
Train loss: 3.3996
Epoch: 31/100
Train loss: 3.3959
Epoch: 32/100
Train loss: 3.4051
Epoch: 33/100
Train loss: 3.3729
Epoch: 34/100
Train loss: 3.3516
Epoch: 35/100
Train loss: 3.3671
Epoch: 36/100
Train loss: 3.3406
Epoch: 37/100
Train loss: 3.2918
Epoch: 38/100
Train loss: 3.3014
Epoch: 39/100
Train loss: 3.2792
Epoch: 40/100
Train loss: 3.2735
Epoch: 41/100
Train loss: 3.2294
Epoch: 42/100
Train loss: 3.2439
Epoch: 43/100
Train loss: 3.2181
Epoch: 44/100
Train loss: 3.2411
Epoch: 45/100
Train loss: 3.1847
Epoch: 46/100
Train loss: 3.2178
Epoch: 47/100
Train loss: 3.1824
Epoch: 48/100
Train loss: 3.1925
Epoch: 49/100
Train loss: 3.1977
Epoch: 50/100
Train loss: 3.1701
Epoch: 51/100
Train loss: 3.1493
Epoch: 52/100
Train loss: 3.1160
Epoch: 53/100
Train loss: 3.0720
Epoch: 54/100
Train loss: 3.1379
Epoch: 55/100
Train loss: 3.0893
Epoch: 56/100
Train loss: 3.1129
Epoch: 57/100
Train loss: 3.1127
Epoch: 58/100
Train loss: 3.0708
Epoch: 59/100
Train loss: 3.0699
Epoch: 60/100
Train loss: 3.0229
Epoch: 61/100
Train loss: 3.0543
Epoch: 62/100
Train loss: 3.0042
Train loss: 3.0042, val loss: 2.16764
f1: 0.7553911205073995
Val cmap: 0.8336886464054445
Model improve: 0.00000 -> 0.75539
Epoch: 63/100
Train loss: 3.0462
Train loss: 3.0462, val loss: 2.16390
f1: 0.7590814783556634
Val cmap: 0.8332616569045781
Model improve: 0.75539 -> 0.75908
Epoch: 64/100
Train loss: 3.0238
Train loss: 3.0238, val loss: 2.16336
f1: 0.7599635777824473
Val cmap: 0.8327346318777736
Model improve: 0.75908 -> 0.75996
Epoch: 65/100
Train loss: 2.9815
Train loss: 2.9815, val loss: 2.21821
f1: 0.7546970284465438
Val cmap: 0.8257978960846434
Epoch: 66/100
Train loss: 3.0182
Train loss: 3.0182, val loss: 2.17388
f1: 0.7603551342364966
Val cmap: 0.8337348413185153
Model improve: 0.75996 -> 0.76036
Epoch: 67/100
Train loss: 3.0016
Train loss: 3.0016, val loss: 2.20615
f1: 0.7573884626314293
Val cmap: 0.8314685056962732
Epoch: 68/100
Train loss: 2.9778
Train loss: 2.9778, val loss: 2.17016
f1: 0.7551487414187643
Val cmap: 0.8302142567054961
Epoch: 69/100
Train loss: 2.9850
Train loss: 2.9850, val loss: 2.17894
f1: 0.7583940057962819
Val cmap: 0.8335137657469898
Epoch: 70/100
Train loss: 2.9763
Train loss: 2.9763, val loss: 2.16596
f1: 0.7606169448552714
Val cmap: 0.8345034553426681
Model improve: 0.76036 -> 0.76062
Epoch: 71/100
Train loss: 2.9554
Train loss: 2.9554, val loss: 2.09200
f1: 0.7644388574621133
Val cmap: 0.837301945973493
Model improve: 0.76062 -> 0.76444
Epoch: 72/100
Train loss: 2.9527
Train loss: 2.9527, val loss: 2.17540
f1: 0.7609800875582544
Val cmap: 0.8333524919025497
Epoch: 73/100
Train loss: 2.9384
Train loss: 2.9384, val loss: 2.06025
f1: 0.7650678226821422
Val cmap: 0.8376396414084792
Model improve: 0.76444 -> 0.76507
Epoch: 74/100
Train loss: 2.9415
Train loss: 2.9415, val loss: 2.15346
f1: 0.7602893664840567
Val cmap: 0.8356591817608001
Epoch: 75/100
Train loss: 2.9436
Train loss: 2.9436, val loss: 2.13769
f1: 0.7621897553247667
Val cmap: 0.8362916678150124
Epoch: 76/100
Train loss: 2.9425
Train loss: 2.9425, val loss: 2.14432
f1: 0.759597806215722
Val cmap: 0.8306820508272251
Epoch: 77/100
Train loss: 2.9420
Train loss: 2.9420, val loss: 2.18034
f1: 0.7595473833097596
Val cmap: 0.8317238468530467
Epoch: 78/100
Train loss: 2.9336
Train loss: 2.9336, val loss: 2.14415
f1: 0.763040968407795
Val cmap: 0.8341727111230757
Epoch: 79/100
Train loss: 2.9557
Train loss: 2.9557, val loss: 2.09300
f1: 0.7669408812046848
Val cmap: 0.8380551950631184
Model improve: 0.76507 -> 0.76694
Epoch: 80/100
Train loss: 2.9135
Train loss: 2.9135, val loss: 2.17501
f1: 0.761576597166063
Val cmap: 0.8342060013796958
Epoch: 81/100
Train loss: 2.9049
Train loss: 2.9049, val loss: 2.15261
f1: 0.763268033622903
Val cmap: 0.8366565796021078
Epoch: 82/100
Train loss: 2.9349
Train loss: 2.9349, val loss: 2.15267
f1: 0.761648620328327
Val cmap: 0.8339834736186071
Epoch: 83/100
Train loss: 2.8915
Train loss: 2.8915, val loss: 2.05257
f1: 0.7685304236021822
Val cmap: 0.8401787106359848
Model improve: 0.76694 -> 0.76853
Epoch: 84/100
Train loss: 2.8984
Train loss: 2.8984, val loss: 2.12100
f1: 0.7620741360672316
Val cmap: 0.8356707826492893
Epoch: 85/100
Train loss: 2.9230
Train loss: 2.9230, val loss: 2.09889
f1: 0.7660121633362295
Val cmap: 0.8376457959827521
Epoch: 86/100
Train loss: 2.8933
Train loss: 2.8933, val loss: 2.15687
f1: 0.7634160011209191
Val cmap: 0.8327911605324884
Epoch: 87/100
Train loss: 2.9144
Train loss: 2.9144, val loss: 2.14008
f1: 0.7602650864336057
Val cmap: 0.834211767966889
Epoch: 88/100
Train loss: 2.8960
Train loss: 2.8960, val loss: 2.15354
f1: 0.7617844231511706
Val cmap: 0.83221213537981
Epoch: 89/100
Train loss: 2.8733
Train loss: 2.8733, val loss: 2.09347
f1: 0.7674978204010463
Val cmap: 0.8378793309146236
Epoch: 90/100
Train loss: 2.9157
Train loss: 2.9157, val loss: 2.16516
f1: 0.7624777027736
Val cmap: 0.8346266092398251
Epoch: 91/100
Train loss: 2.8891
Train loss: 2.8891, val loss: 2.08357
f1: 0.7701975488664374
Val cmap: 0.8401667245824476
Model improve: 0.76853 -> 0.77020
Epoch: 92/100
Train loss: 2.8629
Train loss: 2.8629, val loss: 2.13477
f1: 0.7599663700693617
Val cmap: 0.8373905571840486
Epoch: 93/100
Train loss: 2.8793
Train loss: 2.8793, val loss: 2.10029
f1: 0.7640293269397824
Val cmap: 0.8398678492922467
Epoch: 94/100
Train loss: 2.8808
Train loss: 2.8808, val loss: 2.11501
f1: 0.7637517876451917
Val cmap: 0.8369352975248827
Epoch: 95/100
Train loss: 2.8785
Train loss: 2.8785, val loss: 2.13065
f1: 0.7660650326567876
Val cmap: 0.8359250482183449
Epoch: 96/100
Train loss: 2.8958
Train loss: 2.8958, val loss: 2.11573
f1: 0.7667975873193996
Val cmap: 0.8409593812875413
Epoch: 97/100
Train loss: 2.8695
Train loss: 2.8695, val loss: 2.10512
f1: 0.7656987105566622
Val cmap: 0.8364937444019341
Epoch: 98/100
Train loss: 2.8721
Train loss: 2.8721, val loss: 2.10651
f1: 0.7629196266889537
Val cmap: 0.8371261243599386
Epoch: 99/100
Train loss: 2.8635
Train loss: 2.8635, val loss: 2.15600
f1: 0.7613112214358649
Val cmap: 0.834627798143595
Epoch: 100/100
Train loss: 2.8983
Train loss: 2.8983, val loss: 2.09367
f1: 0.7687637391395373
Val cmap: 0.8396493473150547
Date :05/07/2023, 15:45:35
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
65256
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Train loss: 7.7764
Epoch: 2/100
Train loss: 5.4199
Epoch: 3/100
Train loss: 4.9456
Epoch: 4/100
Train loss: 4.6872
Epoch: 5/100
Train loss: 4.4848
Epoch: 6/100
Train loss: 4.3497
Epoch: 7/100
Train loss: 4.2594
Epoch: 8/100
Train loss: 4.1306
Epoch: 9/100
Train loss: 4.0676
Epoch: 10/100
Train loss: 4.0049
Epoch: 11/100
Train loss: 3.9681
Epoch: 12/100
Train loss: 3.9080
Epoch: 13/100
Train loss: 3.8403
Epoch: 14/100
Train loss: 3.8255
Epoch: 15/100
Train loss: 3.7866
Epoch: 16/100
Train loss: 3.7544
Epoch: 17/100
Train loss: 3.7037
Epoch: 18/100
Train loss: 3.7035
Epoch: 19/100
Train loss: 3.6483
Epoch: 20/100
Train loss: 3.6639
Epoch: 21/100
Train loss: 3.5956
Epoch: 22/100
Train loss: 3.5814
Epoch: 23/100
Train loss: 3.5821
Epoch: 24/100
Train loss: 3.5624
Epoch: 25/100
Train loss: 3.5309
Epoch: 26/100
Train loss: 3.4945
Epoch: 27/100
Train loss: 3.4984
Epoch: 28/100
Train loss: 3.4828
Epoch: 29/100
Train loss: 3.4545
Epoch: 30/100
Train loss: 3.4238
Epoch: 31/100
Train loss: 3.4172
Epoch: 32/100
Train loss: 3.4423
Epoch: 33/100
Train loss: 3.3965
Epoch: 34/100
Train loss: 3.3752
Epoch: 35/100
Train loss: 3.4004
Epoch: 36/100
Train loss: 3.3702
Epoch: 37/100
Train loss: 3.3184
Epoch: 38/100
Train loss: 3.3362
Epoch: 39/100
Train loss: 3.3007
Epoch: 40/100
Train loss: 3.3082
Epoch: 41/100
Train loss: 3.2668
Epoch: 42/100
Train loss: 3.2749
Epoch: 43/100
Train loss: 3.2479
Epoch: 44/100
Train loss: 3.2705
Epoch: 45/100
Train loss: 3.2195
Epoch: 46/100
Train loss: 3.2554
Epoch: 47/100
Train loss: 3.2302
Epoch: 48/100
Train loss: 3.2262
Epoch: 49/100
Train loss: 3.2328
Epoch: 50/100
Train loss: 3.1999
Epoch: 51/100
Train loss: 3.1792
Epoch: 52/100
Train loss: 3.1554
Epoch: 53/100
Train loss: 3.1184
Epoch: 54/100
Train loss: 3.1706
Epoch: 55/100
Train loss: 3.1142
Epoch: 56/100
Train loss: 3.1414
Epoch: 57/100
Train loss: 3.1483
Epoch: 58/100
Train loss: 3.1125
Epoch: 59/100
Train loss: 3.1207
Epoch: 60/100
Train loss: 3.0680
Epoch: 61/100
Train loss: 3.0835
Epoch: 62/100
Train loss: 3.0436
Train loss: 3.0436, val loss: 2.17768
f1: 0.7568160895120742
Val cmap: 0.8302006806424427
Model improve: 0.00000 -> 0.75682
Epoch: 63/100
Train loss: 3.0878
Train loss: 3.0878, val loss: 2.17561
f1: 0.755049343850589
Val cmap: 0.8279990465984922
Epoch: 64/100
Train loss: 3.0745
Train loss: 3.0745, val loss: 2.20249
f1: 0.7575394588500565
Val cmap: 0.8301774634296893
Model improve: 0.75682 -> 0.75754
Epoch: 65/100
Train loss: 3.0220
Train loss: 3.0220, val loss: 2.23577
f1: 0.7479006484073272
Val cmap: 0.826916786811531
Epoch: 66/100
Train loss: 3.0519
Train loss: 3.0519, val loss: 2.15943
f1: 0.7557268099547512
Val cmap: 0.8302203891771345
Epoch: 67/100
Train loss: 3.0377
Train loss: 3.0377, val loss: 2.19300
f1: 0.7547331265375976
Val cmap: 0.8273968935575404
Epoch: 68/100
Train loss: 3.0285
Train loss: 3.0285, val loss: 2.19511
f1: 0.7580702376729336
Val cmap: 0.8295597060092436
Model improve: 0.75754 -> 0.75807
Epoch: 69/100
Train loss: 3.0204
Train loss: 3.0204, val loss: 2.20345
f1: 0.7562062380649268
Val cmap: 0.8306548442869589
Epoch: 70/100
Train loss: 3.0132
Train loss: 3.0132, val loss: 2.19031
f1: 0.7586672830067916
Val cmap: 0.8320769675089541
Model improve: 0.75807 -> 0.75867
Epoch: 71/100
Train loss: 2.9988
Train loss: 2.9988, val loss: 2.13188
f1: 0.7573020280482233
Val cmap: 0.8321758870433013
Epoch: 72/100
Train loss: 2.9946
Train loss: 2.9946, val loss: 2.16340
f1: 0.7620362503540075
Val cmap: 0.8337851789405499
Model improve: 0.75867 -> 0.76204
Epoch: 73/100
Train loss: 2.9777
Train loss: 2.9777, val loss: 2.12759
f1: 0.7628139641385141
Val cmap: 0.8332992370750882
Model improve: 0.76204 -> 0.76281
Epoch: 74/100
Train loss: 2.9774
Train loss: 2.9774, val loss: 2.16635
f1: 0.7577459812753932
Val cmap: 0.8315834542873004
Epoch: 75/100
Train loss: 2.9846
Train loss: 2.9846, val loss: 2.18681
f1: 0.7555098129680236
Val cmap: 0.8297645865800461
Epoch: 76/100
Train loss: 2.9995
Train loss: 2.9995, val loss: 2.13602
f1: 0.7632007938195478
Val cmap: 0.8345794532970428
Model improve: 0.76281 -> 0.76320
Epoch: 77/100
Train loss: 2.9764
Train loss: 2.9764, val loss: 2.16749
f1: 0.760613374982252
Val cmap: 0.8324314775832147
Epoch: 78/100
Train loss: 2.9788
Train loss: 2.9788, val loss: 2.14556
f1: 0.7642345299869391
Val cmap: 0.8362607438610171
Model improve: 0.76320 -> 0.76423
Epoch: 79/100
Train loss: 3.0115
Train loss: 3.0115, val loss: 2.10474
f1: 0.7633507486237245
Val cmap: 0.835598081301187
Epoch: 80/100
Train loss: 2.9546
Train loss: 2.9546, val loss: 2.20041
f1: 0.7575059374003049
Val cmap: 0.8328816682569056
Epoch: 81/100
Train loss: 2.9518
Train loss: 2.9518, val loss: 2.17145
f1: 0.7611485288904644
Val cmap: 0.8327363119911884
Epoch: 82/100
Train loss: 2.9735
Train loss: 2.9735, val loss: 2.12248
f1: 0.7611997895106122
Val cmap: 0.8349492117618965
Epoch: 83/100
Train loss: 2.9256
Train loss: 2.9256, val loss: 2.10695
f1: 0.7631965321960428
Val cmap: 0.8335594605690817
Epoch: 84/100
Train loss: 2.9200
Train loss: 2.9200, val loss: 2.10000
f1: 0.7649756678220075
Val cmap: 0.8374824373294182
Model improve: 0.76423 -> 0.76498
Epoch: 85/100
Train loss: 2.9652
Train loss: 2.9652, val loss: 2.09665
f1: 0.7692361548694252
Val cmap: 0.8377689510259297
Model improve: 0.76498 -> 0.76924
Epoch: 86/100
Train loss: 2.9275
Train loss: 2.9275, val loss: 2.12588
f1: 0.7662894496380122
Val cmap: 0.8372185334672537
Epoch: 87/100
Train loss: 2.9578
Train loss: 2.9578, val loss: 2.15790
f1: 0.7630703926423771
Val cmap: 0.8312772601346125
Epoch: 88/100
Train loss: 2.9355
Train loss: 2.9355, val loss: 2.15941
f1: 0.7604573538811163
Val cmap: 0.8324081203338446
Epoch: 89/100
Train loss: 2.9074
Train loss: 2.9074, val loss: 2.12068
f1: 0.7602524771677421
Val cmap: 0.8339037136613929
Epoch: 90/100
Train loss: 2.9466
Train loss: 2.9466, val loss: 2.15330
f1: 0.7641526175687666
Val cmap: 0.8342265804806754
Epoch: 91/100
Train loss: 2.9273
Train loss: 2.9273, val loss: 2.09101
f1: 0.7662060521252406
Val cmap: 0.836700168689474
Epoch: 92/100
Train loss: 2.9038
Train loss: 2.9038, val loss: 2.12886
f1: 0.7621558907288262
Val cmap: 0.8366669130751462
Epoch: 93/100
Train loss: 2.9029
Train loss: 2.9029, val loss: 2.12266
f1: 0.7609250900948183
Val cmap: 0.834830758830871
Epoch: 94/100
Train loss: 2.9300
Train loss: 2.9300, val loss: 2.10627
f1: 0.7619778762605994
Val cmap: 0.8344964073536586
Epoch: 95/100
Train loss: 2.9394
Train loss: 2.9394, val loss: 2.16945
f1: 0.7583883095694118
Val cmap: 0.8336385225009337
Epoch: 96/100
Train loss: 2.9310
Train loss: 2.9310, val loss: 2.10957
f1: 0.7690199247988193
Val cmap: 0.8393491080011123
Epoch: 97/100
Train loss: 2.9069
Train loss: 2.9069, val loss: 2.16034
f1: 0.7601246105919002
Val cmap: 0.8320286677514187
Epoch: 98/100
Train loss: 2.9202
Train loss: 2.9202, val loss: 2.09231
f1: 0.7646894040198893
Val cmap: 0.8383307451660248
Epoch: 99/100
Train loss: 2.9044
Train loss: 2.9044, val loss: 2.15272
f1: 0.7593945167778131
Val cmap: 0.8331691758082096
Epoch: 100/100
Train loss: 2.9286
Train loss: 2.9286, val loss: 2.13382
f1: 0.7633111189056472
Val cmap: 0.8327851737143507
Date :05/08/2023, 07:58:54
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_b2
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
65256
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Train loss: 7.8870
Epoch: 2/100
Train loss: 5.4234
Epoch: 3/100
Train loss: 4.9657
Epoch: 4/100
Train loss: 4.6778
Epoch: 5/100
Train loss: 4.4715
Epoch: 6/100
Train loss: 4.3249
Epoch: 7/100
Date :05/08/2023, 09:14:24
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 0
nmels: 128
fmax: None
trainbs: 64
validbs: 128
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_b2
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 1
65256
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Train loss: 9.4671
Epoch: 2/100
Train loss: 5.4722
Epoch: 3/100
Train loss: 4.9330
Epoch: 4/100
Train loss: 4.6622
Epoch: 5/100
Train loss: 4.4715
Epoch: 6/100
Train loss: 4.2507
Epoch: 7/100
Train loss: 4.1649
Epoch: 8/100
Train loss: 4.0992
Epoch: 9/100
Train loss: 3.9426
Epoch: 10/100
Train loss: 3.9400
Epoch: 11/100
Train loss: 3.8417
Epoch: 12/100
Train loss: 3.7859
Epoch: 13/100
Train loss: 3.7439
Epoch: 14/100
Train loss: 3.6883
Epoch: 15/100
Train loss: 3.5953
Epoch: 16/100
Train loss: 3.6042
Epoch: 17/100
Train loss: 3.5338
Epoch: 18/100
Train loss: 3.5749
Epoch: 19/100
Train loss: 3.4695
Epoch: 20/100
Train loss: 3.4753
Epoch: 21/100
Train loss: 3.4837
Epoch: 22/100
Train loss: 3.3973
Epoch: 23/100
Train loss: 3.3788
Epoch: 24/100
Train loss: 3.3663
Epoch: 25/100
Train loss: 3.2807
Epoch: 26/100
Train loss: 3.2892
Epoch: 27/100
Train loss: 3.2699
Epoch: 28/100
Train loss: 3.2795
Epoch: 29/100
Train loss: 3.2279
Epoch: 30/100
Train loss: 3.2298
Epoch: 31/100
Train loss: 3.2624
Epoch: 32/100
Train loss: 3.1333
Epoch: 33/100
Train loss: 3.1478
Epoch: 34/100
Train loss: 3.1313
Epoch: 35/100
Train loss: 3.1363
Epoch: 36/100
Train loss: 3.1229
Epoch: 37/100
Train loss: 3.0800
Epoch: 38/100
Train loss: 3.0403
Epoch: 39/100
Train loss: 3.0631
Epoch: 40/100
Train loss: 3.0991
Epoch: 41/100
Train loss: 3.0393
Epoch: 42/100
Train loss: 2.9833
Epoch: 43/100
Train loss: 3.0073
Epoch: 44/100
Train loss: 2.9242
Epoch: 45/100
Train loss: 2.9820
Epoch: 46/100
Train loss: 2.9743
Epoch: 47/100
Train loss: 2.9299
Epoch: 48/100
Train loss: 2.9486
Epoch: 49/100
Train loss: 2.8859
Epoch: 50/100
Train loss: 2.9264
Epoch: 51/100
Train loss: 2.9069
Epoch: 52/100
Train loss: 2.8254
Epoch: 53/100
Train loss: 2.8858
Epoch: 54/100
Train loss: 2.8440
Epoch: 55/100
Train loss: 2.8181
Epoch: 56/100
Train loss: 2.8506
Epoch: 57/100
Train loss: 2.7909
Epoch: 58/100
Train loss: 2.8144
Epoch: 59/100
Train loss: 2.7646
Epoch: 60/100
Train loss: 2.7689
Epoch: 61/100
Train loss: 2.7652
Epoch: 62/100
Train loss: 2.7584
Train loss: 2.7584, val loss: 2.00836
f1: 0.7622543878050487
Val cmap: 0.8377209685026216
Model improve: 0.00000 -> 0.76225
Epoch: 63/100
Train loss: 2.7748
Train loss: 2.7748, val loss: 2.03027
f1: 0.7622202077958301
Val cmap: 0.8347018720514431
Epoch: 64/100
Train loss: 2.7733
Train loss: 2.7733, val loss: 2.06370
f1: 0.7658959537572254
Val cmap: 0.8383948085780343
Model improve: 0.76225 -> 0.76590
Epoch: 65/100
Train loss: 2.7495
Train loss: 2.7495, val loss: 2.03116
f1: 0.7617150644721669
Val cmap: 0.8361601335615885
Epoch: 66/100
Train loss: 2.7148
Train loss: 2.7148, val loss: 2.08168
f1: 0.7592553416543262
Val cmap: 0.8329542532040475
Epoch: 67/100
Train loss: 2.7159
Train loss: 2.7159, val loss: 2.07700
f1: 0.7624499543443141
Val cmap: 0.836649188217102
Epoch: 68/100
Train loss: 2.6983
Train loss: 2.6983, val loss: 2.00423
f1: 0.7661216756718827
Val cmap: 0.8408367619908944
Model improve: 0.76590 -> 0.76612
Epoch: 69/100
Train loss: 2.7137
Train loss: 2.7137, val loss: 2.03880
f1: 0.7655182132036921
Val cmap: 0.836744623958533
Epoch: 70/100
Train loss: 2.7262
Train loss: 2.7262, val loss: 2.04804
f1: 0.7653032637624317
Val cmap: 0.8370124366517109
Epoch: 71/100
Train loss: 2.6872
Train loss: 2.6872, val loss: 2.01599
f1: 0.7652100604187156
Val cmap: 0.8383379350189321
Epoch: 72/100
Train loss: 2.6852
Train loss: 2.6852, val loss: 2.03625
f1: 0.7644943977102168
Val cmap: 0.8354771623033068
Epoch: 73/100
Train loss: 2.6486
Train loss: 2.6486, val loss: 1.98161
f1: 0.7652011225444341
Val cmap: 0.8395519760102074
Epoch: 74/100
Train loss: 2.6284
Train loss: 2.6284, val loss: 2.02708
f1: 0.7636746143057502
Val cmap: 0.8389167801774233
Epoch: 75/100
Train loss: 2.6959
Train loss: 2.6959, val loss: 2.05625
f1: 0.7649410774410775
Val cmap: 0.8376380976743735
Epoch: 76/100
Train loss: 2.6128
Train loss: 2.6128, val loss: 2.02634
f1: 0.7649942630645665
Val cmap: 0.8347128950909106
Epoch: 77/100
Train loss: 2.6396
Train loss: 2.6396, val loss: 2.01082
f1: 0.7658788259081085
Val cmap: 0.8372331934525213
Epoch: 78/100
Train loss: 2.6295
Train loss: 2.6295, val loss: 2.00305
f1: 0.7647223192455465
Val cmap: 0.8383249319653093
Epoch: 79/100
Train loss: 2.6091
Train loss: 2.6091, val loss: 2.01252
f1: 0.7673460840429988
Val cmap: 0.8395996892388321
Model improve: 0.76612 -> 0.76735
Epoch: 80/100
Train loss: 2.6698
Train loss: 2.6698, val loss: 1.97612
f1: 0.7707354425533397
Val cmap: 0.8423241144025224
Model improve: 0.76735 -> 0.77074
Epoch: 81/100
Train loss: 2.5992
Train loss: 2.5992, val loss: 2.02705
f1: 0.7674255691768826
Val cmap: 0.8384631456412354
Epoch: 82/100
Train loss: 2.5971
Train loss: 2.5971, val loss: 2.02666
f1: 0.7657497472370394
Val cmap: 0.8381244893696419
Epoch: 83/100
Train loss: 2.6080
Train loss: 2.6080, val loss: 2.01804
f1: 0.7635106828655215
Val cmap: 0.8388083453676741
Epoch: 84/100
Train loss: 2.5870
Train loss: 2.5870, val loss: 2.04135
f1: 0.7642634764053591
Val cmap: 0.8367928143837552
Epoch: 85/100
Train loss: 2.5737
Train loss: 2.5737, val loss: 2.02570
f1: 0.7669797814016831
Val cmap: 0.840464839723061
Epoch: 86/100
Train loss: 2.5917
Train loss: 2.5917, val loss: 2.00863
f1: 0.7657801541559613
Val cmap: 0.8372065278960173
Epoch: 87/100
Train loss: 2.6160
Train loss: 2.6160, val loss: 2.06511
f1: 0.7626797614871974
Val cmap: 0.8351786431274306
Epoch: 88/100
Train loss: 2.6081
Train loss: 2.6081, val loss: 1.99046
f1: 0.7674102947830022
Val cmap: 0.8386635530445531
Epoch: 89/100
Train loss: 2.5883
Train loss: 2.5883, val loss: 1.95429
f1: 0.7682018753447325
Val cmap: 0.8398474549656256
Epoch: 90/100
Train loss: 2.5580
Train loss: 2.5580, val loss: 2.02117
f1: 0.7674712321077743
Val cmap: 0.8403403782876689
Epoch: 91/100
Train loss: 2.6215
Train loss: 2.6215, val loss: 2.00592
f1: 0.7690747604727602
Val cmap: 0.8374699447593952
Epoch: 92/100
Train loss: 2.6000
Train loss: 2.6000, val loss: 1.98211
f1: 0.7694123364226457
Val cmap: 0.8399429454720273
Epoch: 93/100
Train loss: 2.5942
Train loss: 2.5942, val loss: 1.99938
f1: 0.7665772326561902
Val cmap: 0.837016969227048
Epoch: 94/100
Train loss: 2.5789
Train loss: 2.5789, val loss: 1.96860
f1: 0.770873786407767
Val cmap: 0.8409434298458025
Model improve: 0.77074 -> 0.77087
Epoch: 95/100
Train loss: 2.5954
Train loss: 2.5954, val loss: 2.03751
f1: 0.7658463573392739
Val cmap: 0.8345803610440634
Epoch: 96/100
Train loss: 2.5889
Train loss: 2.5889, val loss: 2.01365
f1: 0.7699602538177255
Val cmap: 0.8383014176184678
Epoch: 97/100
Date :05/08/2023, 23:23:02
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Date :05/08/2023, 23:23:40
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.8
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Train loss: 11.3463
Epoch: 2/100
Train loss: 6.1224
Epoch: 3/100
Train loss: 5.5591
Epoch: 4/100
Train loss: 5.2225
Epoch: 5/100
Train loss: 4.9475
Epoch: 6/100
Train loss: 4.8111
Epoch: 7/100
Train loss: 4.6364
Epoch: 8/100
Train loss: 4.6024
Epoch: 9/100
Train loss: 4.5055
Epoch: 10/100
Train loss: 4.4087
Epoch: 11/100
Train loss: 4.2661
Epoch: 12/100
Train loss: 4.2812
Epoch: 13/100
Train loss: 4.2325
Epoch: 14/100
Train loss: 4.1545
Epoch: 15/100
Train loss: 4.1629
Epoch: 16/100
Train loss: 4.0940
Epoch: 17/100
Train loss: 3.9934
Epoch: 18/100
Train loss: 3.9641
Epoch: 19/100
Train loss: 3.9831
Epoch: 20/100
Train loss: 3.9644
Epoch: 21/100
Train loss: 3.9639
Epoch: 22/100
Train loss: 3.8510
Epoch: 23/100
Train loss: 3.8709
Epoch: 24/100
Train loss: 3.8255
Epoch: 25/100
Train loss: 3.7975
Epoch: 26/100
Train loss: 3.8320
Epoch: 27/100
Train loss: 3.8290
Epoch: 28/100
Train loss: 3.7032
Epoch: 29/100
Train loss: 3.6761
Epoch: 30/100
Train loss: 3.6747
Epoch: 31/100
Train loss: 3.7124
Epoch: 32/100
Train loss: 3.6620
Epoch: 33/100
Train loss: 3.5879
Epoch: 34/100
Train loss: 3.5750
Epoch: 35/100
Train loss: 3.6597
Epoch: 36/100
Train loss: 3.6265
Epoch: 37/100
Train loss: 3.5845
Epoch: 38/100
Train loss: 3.4978
Epoch: 39/100
Train loss: 3.5319
Epoch: 40/100
Train loss: 3.5455
Epoch: 41/100
Train loss: 3.5693
Epoch: 42/100
Train loss: 3.5235
Epoch: 43/100
Train loss: 3.5042
Epoch: 44/100
Train loss: 3.4309
Epoch: 45/100
Train loss: 3.4656
Epoch: 46/100
Train loss: 3.4044
Epoch: 47/100
Train loss: 3.4117
Epoch: 48/100
Train loss: 3.4290
Epoch: 49/100
Train loss: 3.3663
Epoch: 50/100
Train loss: 3.3096
Epoch: 51/100
Train loss: 3.3301
Epoch: 52/100
Train loss: 3.3790
Epoch: 53/100
Train loss: 3.3318
Epoch: 54/100
Train loss: 3.3208
Epoch: 55/100
Train loss: 3.3458
Epoch: 56/100
Train loss: 3.3316
Epoch: 57/100
Train loss: 3.2254
Epoch: 58/100
Train loss: 3.3414
Epoch: 59/100
Train loss: 3.3059
Epoch: 60/100
Train loss: 3.2583
Epoch: 61/100
Train loss: 3.3176
Epoch: 62/100
Train loss: 3.2995
Train loss: 3.2995, val loss: 2.21174
f1: 0.750026948366929
Val cmap: 0.8251010470256642
Model improve: 0.00000 -> 0.75003
Epoch: 63/100
Train loss: 3.2440
Train loss: 3.2440, val loss: 2.06617
f1: 0.7617428267800213
Val cmap: 0.8348482633595075
Model improve: 0.75003 -> 0.76174
Epoch: 64/100
Train loss: 3.1544
Train loss: 3.1544, val loss: 2.02882
f1: 0.756366962149993
Val cmap: 0.8318657967474928
Epoch: 65/100
Train loss: 3.2012
Train loss: 3.2012, val loss: 2.04688
f1: 0.759581881533101
Val cmap: 0.8336320590296006
Epoch: 66/100
Date :05/09/2023, 10:55:56
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/100
Date :05/09/2023, 11:04:33
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/60
Train loss: 7.6424
Train loss: 7.6424, val loss: 4.60640
f1: 0.28503208476346814
Val cmap: 0.5385388629164609
Model improve: 0.00000 -> 0.28503
Epoch: 2/60
Date :05/09/2023, 11:19:37
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/60
Train loss: 7.8184
Train loss: 7.8184, val loss: 4.33045
f1: 0.3302743569017165
Val cmap: 0.5698246051327109
Model improve: 0.00000 -> 0.33027
Epoch: 2/60
Train loss: 4.8864
Train loss: 4.8864, val loss: 3.65034
f1: 0.4817493971896566
Val cmap: 0.6590144201976265
Model improve: 0.33027 -> 0.48175
Epoch: 3/60
Train loss: 4.3631
Train loss: 4.3631, val loss: 3.12442
f1: 0.5880967846703474
Val cmap: 0.7174652795841614
Model improve: 0.48175 -> 0.58810
Epoch: 4/60
Train loss: 4.0571
Train loss: 4.0571, val loss: 2.97688
f1: 0.6085691312172149
Val cmap: 0.7356745080686778
Model improve: 0.58810 -> 0.60857
Epoch: 5/60
Train loss: 3.8066
Train loss: 3.8066, val loss: 2.84288
f1: 0.6227696558075285
Val cmap: 0.7485824040608245
Model improve: 0.60857 -> 0.62277
Epoch: 6/60
Train loss: 3.6637
Train loss: 3.6637, val loss: 2.67598
f1: 0.6572919364985383
Val cmap: 0.7640744579318759
Model improve: 0.62277 -> 0.65729
Epoch: 7/60
Train loss: 3.5495
Train loss: 3.5495, val loss: 2.61306
f1: 0.6665180396091108
Val cmap: 0.7708634604446865
Model improve: 0.65729 -> 0.66652
Epoch: 8/60
Train loss: 3.4604
Train loss: 3.4604, val loss: 2.57469
f1: 0.6755770662695458
Val cmap: 0.7789257003955037
Model improve: 0.66652 -> 0.67558
Epoch: 9/60
Train loss: 3.3704
Train loss: 3.3704, val loss: 2.47645
f1: 0.6892675984381272
Val cmap: 0.7882410319827159
Model improve: 0.67558 -> 0.68927
Epoch: 10/60
Train loss: 3.3084
Train loss: 3.3084, val loss: 2.51224
f1: 0.6848776223776224
Val cmap: 0.7870860074805914
Epoch: 11/60
Train loss: 3.2161
Train loss: 3.2161, val loss: 2.41753
f1: 0.7040678337780052
Val cmap: 0.7943823302372962
Model improve: 0.68927 -> 0.70407
Epoch: 12/60
Train loss: 3.1517
Train loss: 3.1517, val loss: 2.44605
f1: 0.6979307324550198
Val cmap: 0.7903270095412579
Epoch: 13/60
Train loss: 3.1259
Train loss: 3.1259, val loss: 2.37248
f1: 0.7075168177829775
Val cmap: 0.8002377721256042
Model improve: 0.70407 -> 0.70752
Epoch: 14/60
Train loss: 3.0582
Train loss: 3.0582, val loss: 2.35351
f1: 0.7094057089377633
Val cmap: 0.8004444561173422
Model improve: 0.70752 -> 0.70941
Epoch: 15/60
Train loss: 3.0701
Train loss: 3.0701, val loss: 2.38909
f1: 0.7085123425325858
Val cmap: 0.8005008449235095
Epoch: 16/60
Train loss: 2.9960
Train loss: 2.9960, val loss: 2.32243
f1: 0.7238294495346655
Val cmap: 0.8059094858835006
Model improve: 0.70941 -> 0.72383
Epoch: 17/60
Train loss: 2.9661
Train loss: 2.9661, val loss: 2.31436
f1: 0.7195434680535991
Val cmap: 0.8080991122010882
Epoch: 18/60
Train loss: 2.9398
Train loss: 2.9398, val loss: 2.40728
f1: 0.7190046245320414
Val cmap: 0.8058262964563828
Epoch: 19/60
Train loss: 2.9428
Train loss: 2.9428, val loss: 2.24399
f1: 0.7244955953395851
Val cmap: 0.812235759559072
Model improve: 0.72383 -> 0.72450
Epoch: 20/60
Train loss: 2.8606
Train loss: 2.8606, val loss: 2.29455
f1: 0.7263000213995292
Val cmap: 0.8094325131278539
Model improve: 0.72450 -> 0.72630
Epoch: 21/60
Train loss: 2.8257
Train loss: 2.8257, val loss: 2.29001
f1: 0.724808696274047
Val cmap: 0.8095469250964005
Epoch: 22/60
Date :05/09/2023, 15:06:27
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.0
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
Epoch: 1/60
Train loss: 7.3682
Train loss: 7.3682, val loss: 4.56280
f1: 0.3003253448196857
Val cmap: 0.5328527866947073
Model improve: 0.00000 -> 0.30033
Epoch: 2/60
Date :05/09/2023, 15:28:05
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 7.3455
Train loss: 7.3455, val loss: 4.55208
f1: 0.31050185873605946
Val cmap: 0.5305744887606042
Model improve: 0.00000 -> 0.31050
Epoch: 2/60
Date :05/09/2023, 15:52:36
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 7.2687
Train loss: 7.2687, val loss: 4.54533
f1: 0.3049480525641634
Val cmap: 0.5372102026433065
Model improve: 0.00000 -> 0.30495
Epoch: 2/60
Date :05/09/2023, 16:15:35
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 7.9112
Date :05/09/2023, 16:33:31
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Date :05/09/2023, 16:34:21
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
60309
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :05/09/2023, 16:36:35
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
Date :05/09/2023, 16:37:01
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
60309
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 8.2732
Train loss: 8.2732, val loss: 5.78058
f1: 0.10704944954686454
Val cmap: 0.4509404507648059
Model improve: 0.00000 -> 0.10705
Epoch: 2/60
Date :05/09/2023, 16:55:09
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
Fold: 0
60309
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 7.4106
Train loss: 7.4106, val loss: 4.47364
f1: 0.33117672812180765
Val cmap: 0.5986949853113928
Model improve: 0.00000 -> 0.33118
Epoch: 2/60
Train loss: 4.9703
Train loss: 4.9703, val loss: 3.65124
f1: 0.47763743426079924
Val cmap: 0.686434055315002
Model improve: 0.33118 -> 0.47764
Epoch: 3/60
Train loss: 4.4449
Train loss: 4.4449, val loss: 3.31013
f1: 0.5491833781269382
Val cmap: 0.7206489468671411
Model improve: 0.47764 -> 0.54918
Epoch: 4/60
Train loss: 4.1646
Train loss: 4.1646, val loss: 3.08150
f1: 0.5863603611240622
Val cmap: 0.7495806057386758
Model improve: 0.54918 -> 0.58636
Epoch: 5/60
Date :05/09/2023, 22:17:54
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
70943
Fold: 0
60309
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :05/09/2023, 22:45:11
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 1
65256
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :05/09/2023, 22:46:55
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 1
65256
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :05/09/2023, 22:48:34
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 1
65256
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :05/09/2023, 22:48:49
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 256
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_s
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 1
65256
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 7.5771
Train loss: 7.5771, val loss: 4.37762
f1: 0.28377839776803504
Val cmap: 0.5583802742334117
Model improve: 0.00000 -> 0.28378
Epoch: 2/60
Train loss: 5.1294
Train loss: 5.1294, val loss: 3.37322
f1: 0.48341662880508857
Val cmap: 0.6856576935627046
Model improve: 0.28378 -> 0.48342
Epoch: 3/60
Date :05/09/2023, 16:25:30
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 40
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 1
65256
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/40
Date :05/09/2023, 16:26:55
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 40
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 1
65256
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/40
Date :05/09/2023, 16:28:27
Duration: 5
Sample rate: 32000
nfft: 2048
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 128
epochwarmup: 0
totalepoch: 40
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 1
65256
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/40
Train loss: 7.6741, val loss: 4.1470
0.6031026557481364
f1: 0.3408972476194988
Model improve: 0.0000 -> 0.3409
Epoch: 2/40
Train loss: 4.9528, val loss: 3.1429
0.718531172918752
f1: 0.5426643098184155
Model improve: 0.3409 -> 0.5427
Epoch: 3/40
Train loss: 4.4499, val loss: 2.7409
0.7687942835580008
f1: 0.6196177757310615
Model improve: 0.5427 -> 0.6196
Epoch: 4/40
Train loss: 4.1325, val loss: 2.4798
0.7968750854113109
f1: 0.6657691013126894
Model improve: 0.6196 -> 0.6658
Epoch: 5/40
Train loss: 3.9083, val loss: 2.3739
0.8127618642595017
f1: 0.6833339543202056
Model improve: 0.6658 -> 0.6833
Epoch: 6/40
Train loss: 3.7591, val loss: 2.0971
0.8308235299475293
f1: 0.7217213692069306
Model improve: 0.6833 -> 0.7217
Epoch: 7/40
Train loss: 3.6462, val loss: 2.0471
0.8398016731209395
f1: 0.7356534748690913
Model improve: 0.7217 -> 0.7357
Epoch: 8/40
Train loss: 3.5643, val loss: 2.0807
0.8430490777937352
f1: 0.7444870219073098
Model improve: 0.7357 -> 0.7445
Epoch: 9/40
Train loss: 3.4728, val loss: 1.9498
0.8503015548323759
f1: 0.7507677275265215
Model improve: 0.7445 -> 0.7508
Epoch: 10/40
Train loss: 3.4108, val loss: 2.0612
0.8509797214743682
f1: 0.7503994190268701
Epoch: 11/40
Train loss: 3.3075, val loss: 1.9424
0.8606278129283773
f1: 0.7629050119672774
Model improve: 0.7508 -> 0.7629
Epoch: 12/40
Train loss: 3.2518, val loss: 1.8031
0.8650124162431247
f1: 0.7758878569689381
Model improve: 0.7629 -> 0.7759
Epoch: 13/40
Train loss: 3.2178, val loss: 1.7636
0.8673367267264334
f1: 0.7766100410047897
Model improve: 0.7759 -> 0.7766
Epoch: 14/40
Train loss: 3.1521, val loss: 1.8399
0.8667432841787535
f1: 0.7736469929286565
Epoch: 15/40
Train loss: 3.1350, val loss: 1.7443
0.8734691236100498
f1: 0.7869658563612438
Model improve: 0.7766 -> 0.7870
Epoch: 16/40
Train loss: 3.0696, val loss: 1.8007
0.8728974145059285
f1: 0.7865631991051455
Epoch: 17/40
Train loss: 3.0266, val loss: 1.7076
0.8783341565356111
f1: 0.7966529511427681
Model improve: 0.7870 -> 0.7967
Epoch: 18/40
Train loss: 3.0207, val loss: 1.7931
0.8784959683648212
f1: 0.7972359879946953
Model improve: 0.7967 -> 0.7972
Epoch: 19/40
Train loss: 2.9795, val loss: 1.6788
0.8809771194798685
f1: 0.8003279472551499
Model improve: 0.7972 -> 0.8003
Epoch: 20/40
Train loss: 2.9190, val loss: 1.6519
0.8831967693561981
f1: 0.8009922822491731
Model improve: 0.8003 -> 0.8010
Epoch: 21/40
Train loss: 2.8610, val loss: 1.6589
0.8837101124040567
f1: 0.8064682253579154
Model improve: 0.8010 -> 0.8065
Epoch: 22/40
Train loss: 2.7859, val loss: 1.5918
0.8877383619245711
f1: 0.8085839810746873
Model improve: 0.8065 -> 0.8086
Epoch: 23/40
Train loss: 2.7992, val loss: 1.5494
0.8899085953375301
f1: 0.8127100519401161
Model improve: 0.8086 -> 0.8127
Epoch: 24/40
Train loss: 2.7552, val loss: 1.6449
0.8875063839482437
f1: 0.8140821690413611
Model improve: 0.8127 -> 0.8141
Epoch: 25/40
Train loss: 2.7582, val loss: 1.5692
0.8908618112036369
f1: 0.8161462403588052
Model improve: 0.8141 -> 0.8161
Epoch: 26/40
Train loss: 2.6950, val loss: 1.5478
0.8922234359065464
f1: 0.8185496209159215
Model improve: 0.8161 -> 0.8185
Epoch: 27/40
Train loss: 2.6881, val loss: 1.6105
0.8908335611484564
f1: 0.8184868195823347
Epoch: 28/40
Train loss: 2.6653, val loss: 1.5268
0.8951030969067052
f1: 0.823337856173677
Epoch: 29/40
Train loss: 2.6081, val loss: 1.5011
0.8958546119674761
f1: 0.8231085914827564
Epoch: 30/40
Train loss: 2.5981, val loss: 1.4860
0.8969478337697178
f1: 0.8279000775376733
Epoch: 31/40
Train loss: 2.5743, val loss: 1.5059
0.8974927800788076
f1: 0.8277681924287019
Epoch: 32/40
Train loss: 2.5603, val loss: 1.5206
0.896814233455879
f1: 0.8273133462942119
Epoch: 33/40
Train loss: 2.5274, val loss: 1.5513
0.8968929797211985
f1: 0.8294757055172651
Epoch: 34/40
Train loss: 2.5257, val loss: 1.4387
0.9001813531298983
f1: 0.8304851435286219
Epoch: 35/40
Train loss: 2.5494, val loss: 1.4931
0.8989983442817921
f1: 0.8310266263134628
Epoch: 36/40
Train loss: 2.5349, val loss: 1.4929
0.8995846672809734
f1: 0.8313685636856368
Epoch: 37/40
Train loss: 2.4495, val loss: 1.4993
0.8992387793963313
f1: 0.8308131434979102
Epoch: 38/40
Train loss: 2.4525, val loss: 1.4624
0.9008676697409652
f1: 0.8313297639492204
Epoch: 39/40
Train loss: 2.4864, val loss: 1.4508
0.9007639748621712
f1: 0.8309754783077339
Epoch: 40/40
Train loss: 2.5065, val loss: 1.5078
0.8989243604205407
f1: 0.8311183806992436
Date :05/10/2023, 13:45:49
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Val cmap: 0.5539
Model improve: 0.0000 -> 0.5539
Epoch: 2/60
Val cmap: 0.6615
Model improve: 0.5539 -> 0.6615
Epoch: 3/60
Val cmap: 0.7071
Model improve: 0.6615 -> 0.7071
Epoch: 4/60
Val cmap: 0.7294
Model improve: 0.7071 -> 0.7294
Epoch: 5/60
Val cmap: 0.7494
Model improve: 0.7294 -> 0.7494
Epoch: 6/60
Val cmap: 0.7649
Model improve: 0.7494 -> 0.7649
Epoch: 7/60
Val cmap: 0.7718
Model improve: 0.7649 -> 0.7718
Epoch: 8/60
Val cmap: 0.7808
Model improve: 0.7718 -> 0.7808
Epoch: 9/60
Val cmap: 0.7818
Model improve: 0.7808 -> 0.7818
Epoch: 10/60
Val cmap: 0.7916
Model improve: 0.7818 -> 0.7916
Epoch: 11/60
Val cmap: 0.7914
Epoch: 12/60
Val cmap: 0.7941
Model improve: 0.7916 -> 0.7941
Epoch: 13/60
Val cmap: 0.8017
Model improve: 0.7941 -> 0.8017
Epoch: 14/60
Val cmap: 0.8018
Model improve: 0.8017 -> 0.8018
Epoch: 15/60
Val cmap: 0.7981
Epoch: 16/60
Val cmap: 0.8043
Model improve: 0.8018 -> 0.8043
Epoch: 17/60
Val cmap: 0.8089
Model improve: 0.8043 -> 0.8089
Epoch: 18/60
Val cmap: 0.8065
Epoch: 19/60
Val cmap: 0.8146
Model improve: 0.8089 -> 0.8146
Epoch: 20/60
Val cmap: 0.8130
Epoch: 21/60
Val cmap: 0.8145
Epoch: 22/60
Val cmap: 0.8154
Model improve: 0.8146 -> 0.8154
Epoch: 23/60
Val cmap: 0.8188
Model improve: 0.8154 -> 0.8188
Epoch: 24/60
Val cmap: 0.8182
Epoch: 25/60
Val cmap: 0.8207
Model improve: 0.8188 -> 0.8207
Epoch: 26/60
Val cmap: 0.8200
Epoch: 27/60
Val cmap: 0.8263
Model improve: 0.8207 -> 0.8263
Epoch: 28/60
Val cmap: 0.8230
Epoch: 29/60
Val cmap: 0.8264
Model improve: 0.8263 -> 0.8264
Epoch: 30/60
Val cmap: 0.8256
Epoch: 31/60
Val cmap: 0.8304
Model improve: 0.8264 -> 0.8304
Epoch: 32/60
Val cmap: 0.8239
Epoch: 33/60
Val cmap: 0.8268
Epoch: 34/60
Val cmap: 0.8324
Model improve: 0.8304 -> 0.8324
Epoch: 35/60
Val cmap: 0.8328
Model improve: 0.8324 -> 0.8328
Epoch: 36/60
Val cmap: 0.8319
Epoch: 37/60
Val cmap: 0.8342
Model improve: 0.8328 -> 0.8342
Epoch: 38/60
Val cmap: 0.8350
Model improve: 0.8342 -> 0.8350
Epoch: 39/60
Val cmap: 0.8341
Epoch: 40/60
Val cmap: 0.8352
Model improve: 0.8350 -> 0.8352
Epoch: 41/60
Val cmap: 0.8393
Model improve: 0.8352 -> 0.8393
Epoch: 42/60
Val cmap: 0.8395
Model improve: 0.8393 -> 0.8395
Epoch: 43/60
Val cmap: 0.8361
Epoch: 44/60
Val cmap: 0.8366
Epoch: 45/60
Val cmap: 0.8377
Epoch: 46/60
Val cmap: 0.8432
Model improve: 0.8395 -> 0.8432
Epoch: 47/60
Val cmap: 0.8415
Epoch: 48/60
Val cmap: 0.8388
Epoch: 49/60
Val cmap: 0.8393
Epoch: 50/60
Val cmap: 0.8400
Epoch: 51/60
Val cmap: 0.8438
Model improve: 0.8432 -> 0.8438
Epoch: 52/60
Val cmap: 0.8430
Epoch: 53/60
Val cmap: 0.8431
Epoch: 54/60
Val cmap: 0.8392
Epoch: 55/60
Val cmap: 0.8431
Epoch: 56/60
Val cmap: 0.8414
Epoch: 57/60
Val cmap: 0.8420
Epoch: 58/60
Val cmap: 0.8446
Model improve: 0.8438 -> 0.8446
Epoch: 59/60
Val cmap: 0.8417
Epoch: 60/60
Val cmap: 0.8437
Date :05/11/2023, 04:31:39
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 128
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Epoch: 2/60
Epoch: 3/60
Epoch: 4/60
Epoch: 5/60
Epoch: 6/60
Epoch: 7/60
Val cmap: 0.6598
Model improve: 0.0000 -> 0.6598
Epoch: 8/60
Date :05/11/2023, 05:35:07
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Epoch: 2/60
Epoch: 3/60
Epoch: 4/60
Epoch: 5/60
Epoch: 6/60
Epoch: 7/60
Val cmap: 0.6673
Model improve: 0.0000 -> 0.6673
Epoch: 8/60
Val cmap: 0.6728
Model improve: 0.6673 -> 0.6728
Epoch: 9/60
Val cmap: 0.6764
Model improve: 0.6728 -> 0.6764
Epoch: 10/60
Val cmap: 0.6776
Model improve: 0.6764 -> 0.6776
Epoch: 11/60
Date :05/11/2023, 07:36:56
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Epoch: 2/60
Epoch: 3/60
Epoch: 4/60
Epoch: 5/60
Epoch: 6/60
Epoch: 7/60
Val cmap: 0.6670
Model improve: 0.0000 -> 0.6670
Epoch: 8/60
Date :05/11/2023, 08:53:31
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Epoch: 2/60
Epoch: 3/60
Date :05/11/2023, 09:23:44
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 7.6324
Train loss: 7.6324, val loss: 4.5915
Val cmap: 0.5432
Model improve: 0.0000 -> 0.5432
Epoch: 2/60
Train loss: 5.0342
Train loss: 5.0342, val loss: 3.8850
Val cmap: 0.6202
Model improve: 0.5432 -> 0.6202
Epoch: 3/60
Date :05/11/2023, 09:55:15
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 7.5286
Train loss: 7.5286, val loss: 4.6384
Val cmap: 0.5398
Model improve: 0.0000 -> 0.5398
Epoch: 2/60
Train loss: 5.0287
Train loss: 5.0287, val loss: 3.8949
Val cmap: 0.6192
Model improve: 0.5398 -> 0.6192
Epoch: 3/60
Date :05/11/2023, 10:25:30
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 7.7227
Train loss: 7.7227, val loss: 4.5855
Date :05/12/2023, 11:26:47
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 0
65200
Date :05/12/2023, 11:28:25
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Val cmap: 0.8436
Epoch: 2/100
Val cmap: 0.8436
Epoch: 3/100
Date :05/12/2023, 11:36:38
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :05/12/2023, 11:39:09
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Val cmap: 0.8436
Epoch: 2/100
Val cmap: 0.8436
Epoch: 3/100
Date :05/12/2023, 11:43:27
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :05/12/2023, 11:46:58
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Val cmap: 0.8436
f1: 0.7701782150966766
Epoch: 2/100
Date :05/12/2023, 11:49:51
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Val cmap: 0.9891
f1: 0.7839195979899497
Epoch: 2/100
Date :05/12/2023, 11:52:33
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Val cmap: 0.9891
f1: 0.7839195979899497
Epoch: 2/100
Date :05/12/2023, 11:54:09
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :05/12/2023, 12:16:19
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 0
1127
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 85.1624
Val cmap: 0.9259
f1: 0.0018939393939393938
Epoch: 2/100
Date :05/12/2023, 12:17:18
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 0
1127
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 85.1559
Val cmap: 0.9257
f1: 0.0036968576709796672
Epoch: 2/100
Date :05/12/2023, 12:21:43
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 0
1127
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 85.1539
Train loss: 85.1539, val loss: 11.3000
Val cmap: 0.9256
Model improve: 0.0000 -> 0.9256
Epoch: 2/100
Date :05/12/2023, 12:22:37
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 0
1127
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 85.1617
Train loss: 85.1617, val loss: 11.1497
Val cmap: 0.9257
Model improve: 0.0000 -> 0.9257
Epoch: 2/100
Date :05/12/2023, 12:24:06
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 20
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/20
Train loss: 7.5784
Train loss: 7.5784, val loss: 4.5695
Val cmap: 0.5466
Model improve: 0.0000 -> 0.5466
Epoch: 2/20
Train loss: 5.0163
Train loss: 5.0163, val loss: 3.8621
Val cmap: 0.6248
Model improve: 0.5466 -> 0.6248
Epoch: 3/20
Train loss: 4.2427
Train loss: 4.2427, val loss: 3.6747
Val cmap: 0.6434
Model improve: 0.6248 -> 0.6434
Epoch: 4/20
Date :05/12/2023, 07:27:37
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 7.7606
Train loss: 7.7606, val loss: 4.5312
Val cmap: 0.5596
Model improve: 0.0000 -> 0.5596
Epoch: 2/100
Date :05/12/2023, 07:41:14
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 7.7685
Epoch: 2/100
Train loss: 4.8804
Epoch: 3/100
Train loss: 4.1241
Epoch: 4/100
Train loss: 3.5538
Epoch: 5/100
Train loss: 3.0777
Epoch: 6/100
Train loss: 2.6775
Epoch: 7/100
Train loss: 2.3556
Epoch: 8/100
Date :05/12/2023, 09:00:35
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 50
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/50
Train loss: 7.7680
Train loss: 7.7680, val loss: 4.5608
Val cmap: 0.5573
Model improve: 0.0000 -> 0.5573
Epoch: 2/50
Train loss: 4.8844
Train loss: 4.8844, val loss: 3.7288
Val cmap: 0.6471
Model improve: 0.5573 -> 0.6471
Epoch: 3/50
Train loss: 4.1261
Train loss: 4.1261, val loss: 3.5267
Val cmap: 0.6667
Model improve: 0.6471 -> 0.6667
Epoch: 4/50
Date :05/12/2023, 09:40:26
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 50
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 0
1127
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/50
Train loss: 76.9474
Train loss: 76.9474, val loss: 9.8369
Val cmap: 0.9256
Model improve: 0.0000 -> 0.9256
Epoch: 2/50
Train loss: 3.7284
Train loss: 3.7284, val loss: 9.9792
Val cmap: 0.9288
Model improve: 0.9256 -> 0.9288
Epoch: 3/50
Date :05/12/2023, 09:41:37
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 50
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 0
1127
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/50
Train loss: 76.9050
Train loss: 76.9050, val loss: 9.9642
Val cmap: 0.9255
Model improve: 0.0000 -> 0.9255
Epoch: 2/50
Train loss: 3.7408
Train loss: 3.7408, val loss: 9.8769
Val cmap: 0.9292
Model improve: 0.9255 -> 0.9292
Epoch: 3/50
Date :05/12/2023, 09:42:36
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 50
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 0
1127
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/50
Train loss: 76.9181
Train loss: 76.9181, val loss: 9.8317
Val cmap: 0.9256
Model improve: 0.0000 -> 0.9256
Epoch: 2/50
Date :05/12/2023, 09:43:51
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 50
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 0
1127
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/50
Train loss: 76.9414
Train loss: 76.9414, val loss: 9.9003
Val cmap: 0.9256
Model improve: 0.0000 -> 0.9256
Epoch: 2/50
Date :05/12/2023, 09:44:34
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 50
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 0
1127
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/50
Train loss: 76.9284
Train loss: 76.9284, val loss: 9.8745
Val cmap: 0.9257
Model improve: 0.0000 -> 0.9257
Epoch: 2/50
Date :05/12/2023, 09:45:26
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 50
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 0
1127
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/50
Train loss: 76.9438
Train loss: 76.9438, val loss: 9.8932
Val cmap: 0.9256
Model improve: 0.0000 -> 0.9256
Epoch: 2/50
Date :05/12/2023, 09:46:07
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 50
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 0
1127
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/50
Train loss: 76.9374
Train loss: 76.9374, val loss: 9.9332
Val cmap: 0.9256
Model improve: 0.0000 -> 0.9256
Epoch: 2/50
Date :05/12/2023, 09:47:20
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 50
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
normalized: True
76407
Fold: 0
1127
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/50
Train loss: 76.9379
Train loss: 76.9379, val loss: 9.7900
Val cmap: 0.9255
Model improve: 0.0000 -> 0.9255
Epoch: 2/50
Date :05/12/2023, 09:49:13
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :05/12/2023, 09:49:58
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 420.1073
Epoch: 2/60
Train loss: 420.1855
Epoch: 3/60
Train loss: 420.5303
Epoch: 4/60
Date :05/12/2023, 09:50:39
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 420.1073
Epoch: 2/60
Train loss: 420.1855
Epoch: 3/60
Train loss: 420.5303
Epoch: 4/60
Train loss: 420.1786
Epoch: 5/60
Train loss: 420.1335
Epoch: 6/60
Train loss: 420.2146
Epoch: 7/60
Train loss: 420.1734
Epoch: 8/60
Train loss: 420.1289
Epoch: 9/60
Train loss: 420.2378
Epoch: 10/60
Train loss: 420.2045
Epoch: 11/60
Train loss: 419.9762
Epoch: 12/60
Train loss: 420.3070
Epoch: 13/60
Date :05/12/2023, 09:53:33
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 420.1073
Epoch: 2/60
Train loss: 420.1855
Epoch: 3/60
Train loss: 420.5303
Epoch: 4/60
Train loss: 420.1786
Epoch: 5/60
Train loss: 420.1335
Epoch: 6/60
Train loss: 420.2146
Epoch: 7/60
Date :05/12/2023, 09:55:09
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 420.1073
Epoch: 2/60
Date :05/12/2023, 09:55:54
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
1127
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 9.8899
Epoch: 2/60
Train loss: 9.9354
Epoch: 3/60
Train loss: 10.9503
Epoch: 4/60
Date :05/12/2023, 09:57:24
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
1127
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :05/12/2023, 09:57:44
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
1127
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 9.8682
Train loss: 76.9362, val loss: 9.8682
Epoch: 2/60
Date :05/12/2023, 09:58:49
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
1127
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :05/12/2023, 09:59:31
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
1127
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 9.8614
0.9255980906092974
Epoch: 2/60
Date :05/12/2023, 10:00:09
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
1127
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Valid loss: 9.7487
0.9253932294404446
Epoch: 2/60
Date :05/12/2023, 10:01:07
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
1127
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :05/12/2023, 10:01:39
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
1127
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Valid loss: 9.7537
0.9253851384699774
Epoch: 2/60
Valid loss: 9.7995
0.9252752959680552
Epoch: 3/60
Date :05/12/2023, 10:03:14
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
1127
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Valid loss: 9.8352
0.9253361066445208
Epoch: 2/60
Valid loss: 9.9679
0.9291873067122882
Epoch: 3/60
Date :05/12/2023, 10:04:25
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
1127
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Valid loss: 9.9772
0.928858976158547
Epoch: 2/60
Valid loss: 9.9983
0.9287480458484687
Epoch: 3/60
Date :05/12/2023, 10:06:05
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
1127
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :05/12/2023, 10:06:18
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
1127
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Valid loss: 420.1073
0.924435209263383
Epoch: 2/60
Valid loss: 420.1855
0.9242397826429165
Date :05/12/2023, 10:08:00
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
1127
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Valid loss: 420.1073
0.924435209263383
Epoch: 2/60
Valid loss: 420.1855
0.9242397826429165
Date :05/12/2023, 10:08:55
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
1127
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Valid loss: 420.1073
0.924435209263383
Date :05/12/2023, 10:10:32
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
1127
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Valid loss: 420.1073
0.924435209263383
Date :05/12/2023, 10:11:37
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
1127
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Valid loss: 420.1073
0.924435209263383
Epoch: 2/60
Valid loss: 420.1855
0.9242397826429165
Date :05/12/2023, 10:12:56
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
1127
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Valid loss: 421.1043
0.9864239889472117
Epoch: 2/60
Valid loss: 420.9238
0.9864858398148585
Date :05/12/2023, 10:13:53
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
1127
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Valid loss: 421.1043
0.9864239889472117
Epoch: 2/60
Valid loss: 420.9238
0.9864858398148585
Date :05/12/2023, 10:19:31
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
1127
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Valid loss: 420.9238
0.9864858398148585
Epoch: 2/60
Valid loss: 421.5574
0.9864095688831183
Date :05/12/2023, 10:20:42
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
1127
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Valid loss: 420.9238
0.9864858398148585
Epoch: 2/60
Valid loss: 421.5574
0.9864095688831183
Date :05/12/2023, 10:21:37
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
1127
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Valid loss: 420.9044
0.9865212261714064
Epoch: 2/60
Valid loss: 421.2274
0.986440511954162
Date :05/12/2023, 10:22:10
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
1127
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Valid loss: 420.9044
0.9865212261714064
Epoch: 2/60
Valid loss: 421.2274
0.986440511954162
Date :05/12/2023, 10:23:23
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
1127
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Valid loss: 420.9044
0.9865212261714064
Epoch: 2/60
Date :05/12/2023, 10:23:44
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
1127
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Valid loss: 420.9044
0.9865212261714064
Epoch: 2/60
Date :05/12/2023, 10:24:11
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
1127
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Valid loss: 420.9044
0.9865212261714064
Epoch: 2/60
Date :05/12/2023, 10:24:35
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
1127
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Valid loss: 420.9044
0.9865212261714064
Epoch: 2/60
Valid loss: 421.2274
0.986440511954162
Date :05/12/2023, 10:27:06
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
1127
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Valid loss: 420.9044
0.9865212261714064
Epoch: 2/60
Valid loss: 421.2274
0.986440511954162
Date :05/12/2023, 10:27:49
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
1127
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Valid loss: 420.9044
0.9865212261714064
Epoch: 2/60
Valid loss: 421.2274
0.986440511954162
Date :05/12/2023, 10:30:36
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
1127
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Valid loss: 421.1960
0.9866850520843247
Epoch: 2/60
Valid loss: 421.2608
0.9865705600785091
Date :05/12/2023, 10:31:17
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
1127
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Valid loss: 420.9044
0.9865212261714064
Epoch: 2/60
Valid loss: 421.2274
0.986440511954162
Date :05/12/2023, 10:31:43
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
1127
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Valid loss: 420.9044
0.9865212261714064
Epoch: 2/60
Valid loss: 421.2274
0.986440511954162
Date :05/12/2023, 10:35:27
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
1127
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Valid loss: 421.6660
0.9865556050762812
Epoch: 2/60
Valid loss: 420.6751
0.986575125149639
Date :05/12/2023, 10:36:12
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
1127
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :05/12/2023, 10:36:26
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
1127
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Valid loss: 421.6660
0.9865556050762812
Epoch: 2/60
Valid loss: 420.6751
0.986575125149639
Date :05/12/2023, 10:36:53
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
1127
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Valid loss: 421.6660
0.9865556050762812
Epoch: 2/60
Date :05/12/2023, 10:37:08
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
1127
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Valid loss: 421.6660
0.9865556050762812
Epoch: 2/60
Valid loss: 420.6751
0.986575125149639
Date :05/12/2023, 10:37:34
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
1127
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Valid loss: 421.6660
0.9865556050762812
Epoch: 2/60
Valid loss: 420.6751
0.986575125149639
Date :05/12/2023, 10:39:27
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
1127
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Valid loss: 421.6660
0.9865556050762812
Epoch: 2/60
Valid loss: 420.6751
0.986575125149639
Date :05/12/2023, 10:40:08
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
1127
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Valid loss: 421.6660
0.9865556050762812
Epoch: 2/60
Valid loss: 420.6751
0.986575125149639
Date :05/12/2023, 10:40:49
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
1127
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :05/12/2023, 10:41:19
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
1127
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Valid loss: 421.6660
0.9865556050762812
Epoch: 2/60
Valid loss: 420.6751
0.986575125149639
Date :05/12/2023, 10:41:55
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
1127
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Valid loss: 421.6660
0.9865556050762812
Epoch: 2/60
Valid loss: 420.6751
0.986575125149639
Date :05/12/2023, 10:42:24
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
1127
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Valid loss: 421.6660
0.9865556050762812
Epoch: 2/60
Valid loss: 420.6751
0.986575125149639
Date :05/12/2023, 10:42:50
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
1127
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Valid loss: 421.6660
0.9865556050762812
Epoch: 2/60
Valid loss: 420.6751
0.986575125149639
Date :05/12/2023, 10:43:53
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
1127
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Valid loss: 421.6660
0.9865556050762812
Epoch: 2/60
Valid loss: 420.6751
0.986575125149639
Date :05/12/2023, 10:44:38
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
1127
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Valid loss: 421.3111
0.9865531214461785
Epoch: 2/60
Valid loss: 421.3111
0.9865531214461785
Date :05/12/2023, 10:45:19
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
1127
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Valid loss: 421.3111
0.9865531214461785
Epoch: 2/60
Valid loss: 421.3111
0.9865531214461785
Date :05/12/2023, 10:47:02
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
1127
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Valid loss: 421.6660
0.9865556050762812
Epoch: 2/60
Valid loss: 420.6751
0.986575125149639
Date :05/12/2023, 10:49:28
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
1127
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Valid loss: 421.6660
0.9865556050762812
Epoch: 2/60
Valid loss: 420.6751
0.986575125149639
Date :05/12/2023, 10:50:39
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
1127
Date :05/12/2023, 10:52:07
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
1127
Date :05/12/2023, 10:52:52
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
1127
Date :05/12/2023, 10:53:36
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
1127
Date :05/12/2023, 10:54:04
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
1127
Date :05/12/2023, 10:54:16
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
1127
Date :05/12/2023, 18:02:00
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
1127
Date :05/12/2023, 18:05:58
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
1127
Date :05/12/2023, 11:52:55
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
1127
Date :05/12/2023, 11:57:23
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
1127
Date :05/12/2023, 11:58:07
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
1127
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :05/12/2023, 11:58:42
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
1127
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Valid loss: 421.9866
0.9864554910936008
Model improve: 0.0000 -> 0.9865
Epoch: 2/60
Date :05/12/2023, 11:59:10
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
1127
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Valid loss: 421.9866
0.9864554910936008
Model improve: 0.0000 -> 0.9865
Epoch: 2/60
Valid loss: 421.9866
0.9864554910936008
Date :05/12/2023, 11:59:43
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
1127
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Valid loss: 421.9866
0.9864554910936008
Model improve: 0.0000 -> 0.9865
Epoch: 2/60
Valid loss: 421.9866
0.9864554910936008
Date :05/12/2023, 12:01:18
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :05/12/2023, 12:02:27
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 7.6677, val loss: 4.1220
0.6057130157284125
Model improve: 0.0000 -> 0.6057
Epoch: 2/60
Date :05/12/2023, 12:14:01
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :05/12/2023, 12:14:34
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 7.6640, val loss: 4.4522
Val cmap: 0.55717
Model improve: 0.0000 -> 0.5572
Epoch: 2/60
Date :05/12/2023, 12:33:25
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Date :05/12/2023, 12:33:46
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 60
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/60
Train loss: 7.6491, val loss: 4.5284
Val cmap: 0.55575
Model improve: 0.0000 -> 0.5558
Epoch: 2/60
Train loss: 4.9525, val loss: 3.5655
Val cmap: 0.66422
Model improve: 0.5558 -> 0.6642
Epoch: 3/60
Date :05/12/2023, 13:00:47
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :05/12/2023, 13:02:35
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Date :05/12/2023, 13:03:19
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b1
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 7.6478
Epoch: 2/100
Train loss: 4.9742
Epoch: 3/100
Train loss: 4.4696
Epoch: 4/100
Train loss: 4.1592
Epoch: 5/100
Train loss: 3.9104
Epoch: 6/100
Train loss: 3.7725
Epoch: 7/100
Train loss: 3.6812
Epoch: 8/100
Train loss: 3.5803
Epoch: 9/100
Train loss: 3.4864
Epoch: 10/100
Train loss: 3.4466
Epoch: 11/100
Train loss: 3.3400
Epoch: 12/100
Train loss: 3.2973
Epoch: 13/100
Train loss: 3.2694
Epoch: 14/100
Train loss: 3.2236
Epoch: 15/100
Train loss: 3.2054
Epoch: 16/100
Train loss: 3.1445
Epoch: 17/100
Train loss: 3.1286
Epoch: 18/100
Train loss: 3.1104
Epoch: 19/100
Train loss: 3.1050
Epoch: 20/100
Train loss: 3.0429
Epoch: 21/100
Train loss: 3.0034
Epoch: 22/100
Train loss: 2.9591
Epoch: 23/100
Train loss: 2.9583
Epoch: 24/100
Train loss: 2.9312
Epoch: 25/100
Train loss: 2.9398
Epoch: 26/100
Train loss: 2.8803
Epoch: 27/100
Train loss: 2.8990
Epoch: 28/100
Train loss: 2.8846
Epoch: 29/100
Train loss: 2.8356
Epoch: 30/100
Train loss: 2.8249
Epoch: 31/100
Train loss: 2.8151
Epoch: 32/100
Train loss: 2.7882
Epoch: 33/100
Train loss: 2.7509
Epoch: 34/100
Train loss: 2.7669
Epoch: 35/100
Train loss: 2.7746
Epoch: 36/100
Train loss: 2.7659
Epoch: 37/100
Train loss: 2.6810
Epoch: 38/100
Train loss: 2.6722
Epoch: 39/100
Train loss: 2.7007
Epoch: 40/100
Train loss: 2.6744
Epoch: 41/100
Train loss: 2.6591
Epoch: 42/100
Train loss: 2.6681
Epoch: 43/100
Train loss: 2.6307
Epoch: 44/100
Train loss: 2.6389
Epoch: 45/100
Train loss: 2.6589
Epoch: 46/100
Train loss: 2.5563
Epoch: 47/100
Train loss: 2.5743
Epoch: 48/100
Train loss: 2.5812
Epoch: 49/100
Train loss: 2.5771
Epoch: 50/100
Train loss: 2.5712
Epoch: 51/100
Train loss: 2.5176
Train loss: 2.5176, val loss: 1.5647
Val cmap: 0.90045
Saved models
Model improve: 0.0000 -> 0.9005
Epoch: 52/100
Train loss: 2.5260
Train loss: 2.5260, val loss: 1.5462
Val cmap: 0.90074
Saved models
Model improve: 0.9005 -> 0.9007
Epoch: 53/100
Train loss: 2.4918
Train loss: 2.4918, val loss: 1.4997
Val cmap: 0.90025
Saved models
Epoch: 54/100
Train loss: 2.4436
Train loss: 2.4436, val loss: 1.5598
Val cmap: 0.90141
Saved models
Model improve: 0.9007 -> 0.9014
Epoch: 55/100
Train loss: 2.4820
Train loss: 2.4820, val loss: 1.6087
Val cmap: 0.90167
Saved models
Model improve: 0.9014 -> 0.9017
Epoch: 56/100
Train loss: 2.4663
Train loss: 2.4663, val loss: 1.5374
Val cmap: 0.90176
Saved models
Model improve: 0.9017 -> 0.9018
Epoch: 57/100
Train loss: 2.4465
Train loss: 2.4465, val loss: 1.6121
Val cmap: 0.90306
Saved models
Model improve: 0.9018 -> 0.9031
Epoch: 58/100
Train loss: 2.4752
Train loss: 2.4752, val loss: 1.5115
Val cmap: 0.90386
Saved models
Model improve: 0.9031 -> 0.9039
Epoch: 59/100
Train loss: 2.4210
Train loss: 2.4210, val loss: 1.4330
Val cmap: 0.90482
Saved models
Model improve: 0.9039 -> 0.9048
Epoch: 60/100
Train loss: 2.4358
Train loss: 2.4358, val loss: 1.5216
Val cmap: 0.90275
Saved models
Epoch: 61/100
Train loss: 2.3985
Train loss: 2.3985, val loss: 1.4818
Val cmap: 0.90585
Saved models
Model improve: 0.9048 -> 0.9058
Epoch: 62/100
Train loss: 2.4449
Train loss: 2.4449, val loss: 1.4889
Val cmap: 0.90602
Saved models
Model improve: 0.9058 -> 0.9060
Epoch: 63/100
Train loss: 2.4147
Train loss: 2.4147, val loss: 1.4093
Val cmap: 0.90628
Saved models
Model improve: 0.9060 -> 0.9063
Epoch: 64/100
Train loss: 2.4028
Train loss: 2.4028, val loss: 1.3990
Val cmap: 0.90709
Saved models
Model improve: 0.9063 -> 0.9071
Epoch: 65/100
Train loss: 2.3754
Train loss: 2.3754, val loss: 1.4524
Val cmap: 0.90688
Saved models
Epoch: 66/100
Train loss: 2.3573
Train loss: 2.3573, val loss: 1.5275
Val cmap: 0.90672
Saved models
Epoch: 67/100
Train loss: 2.3402
Train loss: 2.3402, val loss: 1.4519
Val cmap: 0.90674
Saved models
Epoch: 68/100
Train loss: 2.3252
Train loss: 2.3252, val loss: 1.5679
Val cmap: 0.90661
Saved models
Epoch: 69/100
Train loss: 2.2821
Train loss: 2.2821, val loss: 1.4815
Val cmap: 0.90727
Saved models
Model improve: 0.9071 -> 0.9073
Epoch: 70/100
Train loss: 2.2797
Train loss: 2.2797, val loss: 1.4772
Val cmap: 0.90780
Saved models
Model improve: 0.9073 -> 0.9078
Epoch: 71/100
Train loss: 2.3008
Train loss: 2.3008, val loss: 1.4027
Val cmap: 0.90932
Saved models
Model improve: 0.9078 -> 0.9093
Epoch: 72/100
Train loss: 2.2791
Train loss: 2.2791, val loss: 1.4838
Val cmap: 0.90825
Saved models
Epoch: 73/100
Train loss: 2.3423
Train loss: 2.3423, val loss: 1.4288
Val cmap: 0.90991
Saved models
Model improve: 0.9093 -> 0.9099
Epoch: 74/100
Train loss: 2.2753
Train loss: 2.2753, val loss: 1.5103
Val cmap: 0.90823
Saved models
Epoch: 75/100
Train loss: 2.3063
Train loss: 2.3063, val loss: 1.4525
Val cmap: 0.90962
Saved models
Epoch: 76/100
Train loss: 2.3024
Train loss: 2.3024, val loss: 1.4115
Val cmap: 0.90990
Saved models
Epoch: 77/100
Train loss: 2.2297
Train loss: 2.2297, val loss: 1.4721
Val cmap: 0.90914
Saved models
Epoch: 78/100
Train loss: 2.2441
Train loss: 2.2441, val loss: 1.4027
Val cmap: 0.91043
Saved models
Model improve: 0.9099 -> 0.9104
Epoch: 79/100
Train loss: 2.1952
Train loss: 2.1952, val loss: 1.4474
Val cmap: 0.91136
Saved models
Model improve: 0.9104 -> 0.9114
Epoch: 80/100
Train loss: 2.2252
Train loss: 2.2252, val loss: 1.3994
Val cmap: 0.91139
Saved models
Model improve: 0.9114 -> 0.9114
Epoch: 81/100
Train loss: 2.1966
Train loss: 2.1966, val loss: 1.4861
Val cmap: 0.90997
Saved models
Epoch: 82/100
Train loss: 2.1642
Train loss: 2.1642, val loss: 1.4152
Val cmap: 0.91079
Saved models
Epoch: 83/100
Train loss: 2.1773
Train loss: 2.1773, val loss: 1.4121
Val cmap: 0.91118
Saved models
Epoch: 84/100
Train loss: 2.1639
Train loss: 2.1639, val loss: 1.4138
Val cmap: 0.91085
Saved models
Epoch: 85/100
Train loss: 2.2104
Train loss: 2.2104, val loss: 1.5077
Val cmap: 0.91020
Saved models
Epoch: 86/100
Train loss: 2.1748
Train loss: 2.1748, val loss: 1.3805
Val cmap: 0.91219
Saved models
Model improve: 0.9114 -> 0.9122
Epoch: 87/100
Train loss: 2.2690
Train loss: 2.2690, val loss: 1.4512
Val cmap: 0.91107
Saved models
Epoch: 88/100
Train loss: 2.1793
Train loss: 2.1793, val loss: 1.3991
Val cmap: 0.91161
Saved models
Epoch: 89/100
Train loss: 2.1354
Train loss: 2.1354, val loss: 1.4105
Val cmap: 0.91175
Saved models
Epoch: 90/100
Train loss: 2.1360
Train loss: 2.1360, val loss: 1.3889
Val cmap: 0.91242
Saved models
Model improve: 0.9122 -> 0.9124
Epoch: 91/100
Train loss: 2.1981
Train loss: 2.1981, val loss: 1.4355
Val cmap: 0.91096
Saved models
Epoch: 92/100
Train loss: 2.1434
Train loss: 2.1434, val loss: 1.4181
Val cmap: 0.91230
Saved models
Epoch: 93/100
Train loss: 2.1268
Train loss: 2.1268, val loss: 1.3897
Val cmap: 0.91253
Saved models
Model improve: 0.9124 -> 0.9125
Epoch: 94/100
Train loss: 2.1589
Train loss: 2.1589, val loss: 1.3866
Val cmap: 0.91264
Saved models
Model improve: 0.9125 -> 0.9126
Epoch: 95/100
Train loss: 2.1264
Train loss: 2.1264, val loss: 1.3772
Val cmap: 0.91246
Saved models
Epoch: 96/100
Train loss: 2.1536
Train loss: 2.1536, val loss: 1.4004
Val cmap: 0.91232
Saved models
Epoch: 97/100
Train loss: 2.1338
Train loss: 2.1338, val loss: 1.4436
Val cmap: 0.91174
Saved models
Epoch: 98/100
Train loss: 2.1278
Train loss: 2.1278, val loss: 1.4049
Val cmap: 0.91183
Saved models
Epoch: 99/100
Train loss: 2.1534
Train loss: 2.1534, val loss: 1.4087
Val cmap: 0.91175
Saved models
Epoch: 100/100
Date :05/13/2023, 18:07:11
Duration: 5
Sample rate: 32000
nfft: 768
fmin: 20
nmels: 128
fmax: 16000
trainbs: 32
validbs: 192
epochwarmup: 0
totalepoch: 100
learningrate: 0.001
weightdecay: 0.01
thrupsample: 50
model_name: tf_efficientnetv2_b0
mix_up: 0.2
hop_length: 256
train_with_mixup: True
num_channels: 1
use_spec_augmenter: False
use_drop_path: True
76407
Fold: 0
65200
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.01
)
Epoch: 1/100
Train loss: 7.6413
Epoch: 2/100
Train loss: 4.9515
Epoch: 3/100
Train loss: 4.4431
Epoch: 4/100
Train loss: 4.1439
Epoch: 5/100
Train loss: 3.8961
Epoch: 6/100
Train loss: 3.7803
Epoch: 7/100
Train loss: 3.6696
Epoch: 8/100
Train loss: 3.5855
Epoch: 9/100
Train loss: 3.4955
Epoch: 10/100
Train loss: 3.4415
Epoch: 11/100
Train loss: 3.3518
Epoch: 12/100
Train loss: 3.2991
Epoch: 13/100
Train loss: 3.2720
Epoch: 14/100
Train loss: 3.2264
Epoch: 15/100
Train loss: 3.2231
Epoch: 16/100
Train loss: 3.1694
Epoch: 17/100
Train loss: 3.1409
Epoch: 18/100
Train loss: 3.1267
Epoch: 19/100
Train loss: 3.1240
Epoch: 20/100
Train loss: 3.0557
Epoch: 21/100
Train loss: 3.0106
Epoch: 22/100
Train loss: 2.9564
Epoch: 23/100
Train loss: 2.9786
Epoch: 24/100
Train loss: 2.9482
Epoch: 25/100
Train loss: 2.9537
Epoch: 26/100
Train loss: 2.9087
Epoch: 27/100
Train loss: 2.9135
Epoch: 28/100
Train loss: 2.9103
Epoch: 29/100
Train loss: 2.8425
Epoch: 30/100
Train loss: 2.8364
Epoch: 31/100
Train loss: 2.8339
Epoch: 32/100
Train loss: 2.8094
Epoch: 33/100
Train loss: 2.7729
Epoch: 34/100
Train loss: 2.7930
Epoch: 35/100
Train loss: 2.7923
Epoch: 36/100
Train loss: 2.7808
Epoch: 37/100
Train loss: 2.6973
Epoch: 38/100
Train loss: 2.6984
Epoch: 39/100
Train loss: 2.7155
Epoch: 40/100
Train loss: 2.7054
Epoch: 41/100
Train loss: 2.6729
Epoch: 42/100
Train loss: 2.6895
Epoch: 43/100
Train loss: 2.6491
Epoch: 44/100
Train loss: 2.6559
Epoch: 45/100
Train loss: 2.6794
Epoch: 46/100
Train loss: 2.5838
Epoch: 47/100
Train loss: 2.6043
Epoch: 48/100
Train loss: 2.6072
Epoch: 49/100
Train loss: 2.5885
Epoch: 50/100
Train loss: 2.6016
Epoch: 51/100
Train loss: 2.5348
Epoch: 52/100
Train loss: 2.5682
Epoch: 53/100
Train loss: 2.5367
Epoch: 54/100
Train loss: 2.4724
Epoch: 55/100
Train loss: 2.5076
Epoch: 56/100
Train loss: 2.4910
Epoch: 57/100
Train loss: 2.4749
Epoch: 58/100
Train loss: 2.5025
Epoch: 59/100
Train loss: 2.4513
Epoch: 60/100
Train loss: 2.4623
Epoch: 61/100
Train loss: 2.4335
Epoch: 62/100
Train loss: 2.4804
Train loss: 2.4804, val loss: 1.4886
Val cmap: 0.90371
Saved models
Model improve: 0.0000 -> 0.9037
Epoch: 63/100
Train loss: 2.4484
Train loss: 2.4484, val loss: 1.3919
Val cmap: 0.90446
Saved models
Model improve: 0.9037 -> 0.9045
Epoch: 64/100
Train loss: 2.4352
Train loss: 2.4352, val loss: 1.3812
Val cmap: 0.90640
Saved models
Model improve: 0.9045 -> 0.9064
Epoch: 65/100
Train loss: 2.4075
Train loss: 2.4075, val loss: 1.4575
Val cmap: 0.90407
Saved models
Epoch: 66/100
Train loss: 2.3953
Train loss: 2.3953, val loss: 1.4945
Val cmap: 0.90417
Saved models
Epoch: 67/100
Train loss: 2.3702
Train loss: 2.3702, val loss: 1.4105
Val cmap: 0.90561
Saved models
Epoch: 68/100
Train loss: 2.3605
Train loss: 2.3605, val loss: 1.4246
Val cmap: 0.90638
Saved models
Epoch: 69/100
Train loss: 2.3268
Train loss: 2.3268, val loss: 1.3934
Val cmap: 0.90578
Saved models
Epoch: 70/100
Train loss: 2.3205
Train loss: 2.3205, val loss: 1.4228
Val cmap: 0.90711
Saved models
Model improve: 0.9064 -> 0.9071
Epoch: 71/100
Train loss: 2.3254
Train loss: 2.3254, val loss: 1.3923
Val cmap: 0.90684
Saved models
Epoch: 72/100
Train loss: 2.3233
Train loss: 2.3233, val loss: 1.4335
Val cmap: 0.90762
Saved models
Model improve: 0.9071 -> 0.9076
Epoch: 73/100
Train loss: 2.3808
Train loss: 2.3808, val loss: 1.3827
Val cmap: 0.90915
Saved models
Model improve: 0.9076 -> 0.9092
Epoch: 74/100
Train loss: 2.3021
Train loss: 2.3021, val loss: 1.4337
Val cmap: 0.90784
Saved models
Epoch: 75/100
Train loss: 2.3445
Train loss: 2.3445, val loss: 1.4114
Val cmap: 0.90827
Saved models
Epoch: 76/100
Train loss: 2.3250
Train loss: 2.3250, val loss: 1.3802
Val cmap: 0.90789
Saved models
Epoch: 77/100
Train loss: 2.2646
Train loss: 2.2646, val loss: 1.4208
Val cmap: 0.90708
Saved models
Epoch: 78/100
Train loss: 2.2847
Train loss: 2.2847, val loss: 1.3718
Val cmap: 0.90799
Saved models
Epoch: 79/100
Train loss: 2.2239
Train loss: 2.2239, val loss: 1.3896
Val cmap: 0.90870
Saved models
Epoch: 80/100
Train loss: 2.2678
Train loss: 2.2678, val loss: 1.3717
Val cmap: 0.90917
Saved models
Model improve: 0.9092 -> 0.9092
Epoch: 81/100
Train loss: 2.2388
Train loss: 2.2388, val loss: 1.4336
Val cmap: 0.90905
Saved models
Epoch: 82/100
Train loss: 2.1906
Train loss: 2.1906, val loss: 1.3745
Val cmap: 0.90924
Saved models
Model improve: 0.9092 -> 0.9092
Epoch: 83/100
Train loss: 2.2101
Train loss: 2.2101, val loss: 1.3628
Val cmap: 0.90938
Saved models
Model improve: 0.9092 -> 0.9094
Epoch: 84/100
Train loss: 2.2022
Train loss: 2.2022, val loss: 1.3972
Val cmap: 0.90870
Saved models
Epoch: 85/100
Train loss: 2.2470
Train loss: 2.2470, val loss: 1.4318
Val cmap: 0.90890
Saved models
Epoch: 86/100
Train loss: 2.2069
Train loss: 2.2069, val loss: 1.3599
Val cmap: 0.91025
Saved models
Model improve: 0.9094 -> 0.9102
Epoch: 87/100
Train loss: 2.3002
Train loss: 2.3002, val loss: 1.4241
Val cmap: 0.90850
Saved models
Epoch: 88/100
Train loss: 2.2109
Train loss: 2.2109, val loss: 1.3560
Val cmap: 0.90984
Saved models
Epoch: 89/100
Train loss: 2.1831
Train loss: 2.1831, val loss: 1.3751
Val cmap: 0.90978
Saved models
Epoch: 90/100
Train loss: 2.1827
Train loss: 2.1827, val loss: 1.3638
Val cmap: 0.91038
Saved models
Model improve: 0.9102 -> 0.9104
Epoch: 91/100
Train loss: 2.2273
Train loss: 2.2273, val loss: 1.3917
Val cmap: 0.90959
Saved models
Epoch: 92/100
Train loss: 2.1711
Train loss: 2.1711, val loss: 1.3716
Val cmap: 0.91051
Saved models
Model improve: 0.9104 -> 0.9105
Epoch: 93/100
Train loss: 2.1601
Train loss: 2.1601, val loss: 1.3518
Val cmap: 0.91080
Saved models
Model improve: 0.9105 -> 0.9108
Epoch: 94/100
Train loss: 2.1915
Train loss: 2.1915, val loss: 1.3424
Val cmap: 0.91100
Saved models
Model improve: 0.9108 -> 0.9110
Epoch: 95/100
Train loss: 2.1622
Train loss: 2.1622, val loss: 1.3531
Val cmap: 0.91104
Saved models
Model improve: 0.9110 -> 0.9110
Epoch: 96/100
Train loss: 2.1970
Train loss: 2.1970, val loss: 1.3642
Val cmap: 0.91057
Saved models
Epoch: 97/100
Train loss: 2.1718
Train loss: 2.1718, val loss: 1.3834
Val cmap: 0.91091
Saved models
Epoch: 98/100
Train loss: 2.1630
Train loss: 2.1630, val loss: 1.3768
Val cmap: 0.91040
Saved models
Epoch: 99/100
Train loss: 2.1963
Train loss: 2.1963, val loss: 1.3624
Val cmap: 0.91078
Saved models
Epoch: 100/100
Train loss: 2.1510
Train loss: 2.1510, val loss: 1.4178
Val cmap: 0.91006
Saved models
