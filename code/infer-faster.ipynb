{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n\n!export OMP_NUM_THREADS=N\n\n!export OMP_SCHEDULE=STATIC\n!export OMP_PROC_BIND=CLOSE\n!export GOMP_CPU_AFFINITY=\"N-M\"","metadata":{"execution":{"iopub.status.busy":"2023-05-24T10:18:45.276452Z","iopub.execute_input":"2023-05-24T10:18:45.277159Z","iopub.status.idle":"2023-05-24T10:18:49.783371Z","shell.execute_reply.started":"2023-05-24T10:18:45.277116Z","shell.execute_reply":"2023-05-24T10:18:49.781730Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport random\nimport glob\nimport os, shutil\nfrom tqdm import tqdm\ntqdm.pandas()\nimport copy\nimport joblib\nfrom collections import defaultdict\nimport gc\nimport math\nimport cv2\nimport time\nimport sys\nsys.path.append('../input/timm-pytorch-image-models/pytorch-image-models-master')\nimport timm\n# PyTorch \nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F\nfrom torch.cuda.amp import autocast, GradScaler\nimport librosa\nimport librosa as lb\nimport torchaudio.transforms as T\nimport soundfile as sf\nimport albumentations as A\nfrom  soundfile import SoundFile\nimport warnings\nfrom pathlib import Path\nwarnings.filterwarnings(\"ignore\")\nimport concurrent.futures","metadata":{"execution":{"iopub.status.busy":"2023-05-24T10:18:49.786209Z","iopub.execute_input":"2023-05-24T10:18:49.786944Z","iopub.status.idle":"2023-05-24T10:18:57.410343Z","shell.execute_reply.started":"2023-05-24T10:18:49.786891Z","shell.execute_reply":"2023-05-24T10:18:57.408921Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    seed = 1\n    \n    # Audio duration, sample rate, and length\n    duration = 5 # second\n    sample_rate = 32000\n    audio_len = duration*sample_rate\n    \n    # STFT parameters\n    nfft = 768\n    n_mels = 128\n    fmin = 20\n    fmax = 16000\n#     model_name = \"eca_nfnet_l0\"\n    model_name = \"tf_efficientnetv2_b1\"\n    train_bs = 64\n    valid_bs = train_bs * 4\n    num_fold = 5\n    epoch_warm_up = 0\n    total_epoch = 100\n    learning_rate = 4e-4\n    weight_decay = 0.01\n    thr_upsample = 50\n    mix_up = 0.1\n    hop_length = 256\n    train_with_mixup=True\n    num_channels = 1\n    use_drop_path=True\n    \n    num_classes = 264\n    device = 'cpu'\n    target_columns = \"abethr1 abhori1 abythr1 afbfly1 afdfly1 afecuc1 affeag1 afgfly1 afghor1 afmdov1 afpfly1 afpkin1 afpwag1 afrgos1 afrgrp1 afrjac1 afrthr1 amesun2 augbuz1 bagwea1 barswa bawhor2 bawman1 bcbeat1 beasun2 bkctch1 bkfruw1 blacra1 blacuc1 blakit1 blaplo1 blbpuf2 blcapa2 blfbus1 blhgon1 blhher1 blksaw1 blnmou1 blnwea1 bltapa1 bltbar1 bltori1 blwlap1 brcale1 brcsta1 brctch1 brcwea1 brican1 brobab1 broman1 brosun1 brrwhe3 brtcha1 brubru1 brwwar1 bswdov1 btweye2 bubwar2 butapa1 cabgre1 carcha1 carwoo1 categr ccbeat1 chespa1 chewea1 chibat1 chtapa3 chucis1 cibwar1 cohmar1 colsun2 combul2 combuz1 comsan crefra2 crheag1 crohor1 darbar1 darter3 didcuc1 dotbar1 dutdov1 easmog1 eaywag1 edcsun3 egygoo equaka1 eswdov1 eubeat1 fatrav1 fatwid1 fislov1 fotdro5 gabgos2 gargan gbesta1 gnbcam2 gnhsun1 gobbun1 gobsta5 gobwea1 golher1 grbcam1 grccra1 grecor greegr grewoo2 grwpyt1 gryapa1 grywrw1 gybfis1 gycwar3 gyhbus1 gyhkin1 gyhneg1 gyhspa1 gytbar1 hadibi1 hamerk1 hartur1 helgui hipbab1 hoopoe huncis1 hunsun2 joygre1 kerspa2 klacuc1 kvbsun1 laudov1 lawgol lesmaw1 lessts1 libeat1 litegr litswi1 litwea1 loceag1 lotcor1 lotlap1 luebus1 mabeat1 macshr1 malkin1 marsto1 marsun2 mcptit1 meypar1 moccha1 mouwag1 ndcsun2 nobfly1 norbro1 norcro1 norfis1 norpuf1 nubwoo1 pabspa1 palfly2 palpri1 piecro1 piekin1 pitwhy purgre2 pygbat1 quailf1 ratcis1 raybar1 rbsrob1 rebfir2 rebhor1 reboxp1 reccor reccuc1 reedov1 refbar2 refcro1 reftin1 refwar2 rehblu1 rehwea1 reisee2 rerswa1 rewsta1 rindov rocmar2 rostur1 ruegls1 rufcha2 sacibi2 sccsun2 scrcha1 scthon1 shesta1 sichor1 sincis1 slbgre1 slcbou1 sltnig1 sobfly1 somgre1 somtit4 soucit1 soufis1 spemou2 spepig1 spewea1 spfbar1 spfwea1 spmthr1 spwlap1 squher1 strher strsee1 stusta1 subbus1 supsta1 tacsun1 tafpri1 tamdov1 thrnig1 trobou1 varsun2 vibsta2 vilwea1 vimwea1 walsta1 wbgbir1 wbrcha2 wbswea1 wfbeat1 whbcan1 whbcou1 whbcro2 whbtit5 whbwea1 whbwhe3 whcpri2 whctur2 wheslf1 whhsaw1 whihel1 whrshr1 witswa1 wlwwar wookin1 woosan wtbeat1 yebapa1 yebbar1 yebduc1 yebere1 yebgre1 yebsto1 yeccan1 yefcan yelbis1 yenspu1 yertin1 yesbar1 yespet1 yetgre1 yewgre1\".split()\n\n#     # Class Labels for BirdCLEF 23\n#     class_names = sorted(os.listdir('birdclef-2023/train_audio/'))\n#     num_classes = len(class_names)\n#     class_labels = list(range(num_classes))\n#     label2name = dict(zip(class_labels, class_names))\n#     name2label = {v:k for k,v in label2name.items()}\n\n","metadata":{"execution":{"iopub.status.busy":"2023-05-24T10:18:57.412320Z","iopub.execute_input":"2023-05-24T10:18:57.412701Z","iopub.status.idle":"2023-05-24T10:18:57.425248Z","shell.execute_reply.started":"2023-05-24T10:18:57.412663Z","shell.execute_reply":"2023-05-24T10:18:57.423780Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def compute_melspec(y, sr, n_mels, fmin, fmax):\n    \"\"\"\n    Computes a mel-spectrogram and puts it at decibel scale\n    Arguments:\n        y {np array} -- signal\n        params {AudioParams} -- Parameters to use for the spectrogram. Expected to have the attributes sr, n_mels, f_min, f_max\n    Returns:\n        np array -- Mel-spectrogram\n    \"\"\"\n    melspec = lb.feature.melspectrogram(\n        y=y, sr=sr, n_mels=n_mels, fmin=fmin, fmax=fmax,n_fft = CFG.nfft\n    )\n\n    melspec = lb.power_to_db(melspec, ref=1.0).astype(np.float32)\n    return melspec\n\ndef mono_to_color(X, eps=1e-6, mean=None, std=None):\n    mean = mean or X.mean()\n    std = std or X.std()\n    X = (X - mean) / (std + eps)\n    \n    _min, _max = X.min(), X.max()\n\n    if (_max - _min) > eps:\n        V = np.clip(X, _min, _max)\n        V = 255 * (V - _min) / (_max - _min)\n        V = V.astype(np.uint8)\n    else:\n        V = np.zeros_like(X, dtype=np.uint8)\n\n    return V\n\ndef crop_or_pad(y, length, is_train=True, start=None):\n    if len(y) < length:\n        y = np.concatenate([y, np.zeros(length - len(y))])\n        \n        n_repeats = length // len(y)\n        epsilon = length % len(y)\n        \n        y = np.concatenate([y]*n_repeats + [y[:epsilon]])\n        \n    elif len(y) > length:\n        if not is_train:\n            start = start or 0\n        else:\n            start = start or np.random.randint(len(y) - length)\n\n        y = y[start:start + length]\n\n    return y\n\nmean = (0.485) # R only for RGB\nstd = (0.229) # R only for RGB\nimg_transforms = {\n    'valid' : A.Compose([\n            A.Normalize(mean, std),\n    ], p=1.0),\n}","metadata":{"execution":{"iopub.status.busy":"2023-05-24T10:18:57.428464Z","iopub.execute_input":"2023-05-24T10:18:57.428887Z","iopub.status.idle":"2023-05-24T10:18:57.451412Z","shell.execute_reply.started":"2023-05-24T10:18:57.428846Z","shell.execute_reply":"2023-05-24T10:18:57.449521Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def gem(x, p=3, eps=1e-6):\n    return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1.0 / p)\n\n\nclass GeM(nn.Module):\n    def __init__(self, p=3, eps=1e-6):\n        super(GeM, self).__init__()\n        self.p = nn.Parameter(torch.ones(1) * p)\n        self.eps = eps\n\n    def forward(self, x):\n        ret = gem(x, p=self.p, eps=self.eps)\n        return ret\n\n    def __repr__(self):\n        return (\n                self.__class__.__name__\n                + \"(\"\n                + \"p=\"\n                + \"{:.4f}\".format(self.p.data.tolist()[0])\n                + \", \"\n                + \"eps=\"\n                + str(self.eps)\n                + \")\"\n        )\nclass TimmClassifier(nn.Module):\n    def __init__(self, base_model_name, pretrained=True, num_classes=CFG.num_classes, in_channels=CFG.num_channels):\n        super().__init__()\n\n        self.bn0 = nn.BatchNorm2d(CFG.n_mels)\n        if CFG.use_drop_path:\n            self.base_model = timm.create_model(\n                base_model_name, pretrained=pretrained, in_chans=in_channels, drop_rate=0.5, drop_path_rate = 0.2)\n        else:\n            self.base_model = timm.create_model(\n                base_model_name, pretrained=pretrained, in_chans=in_channels)\n        self.gem = GeM(p=3, eps=1e-6)\n        if 'efficientnet' in base_model_name:\n            in_features = self.base_model.classifier.in_features\n        elif 'nfnet' in base_model_name:\n            in_features = 2304\n        self.head1 = nn.Linear(in_features, num_classes)\n        \n\n    def forward(self, x):\n        x = self.base_model.forward_features(x)\n        x = self.gem(x)\n        x = x[:, :, 0, 0]\n        logit = self.head1(x)\n\n        output_dict = {\n            'logit': logit,\n        }\n\n        return output_dict\n    \nclass FineTuneTimmClassifier(nn.Module):\n    def __init__(self, base_model_name = CFG.model_name, num_classes=CFG.num_classes):\n        super().__init__()\n        self.backbone = TimmClassifier(base_model_name=base_model_name, num_classes = 572, pretrained=False)\n        if 'v2_b0' in base_model_name or 'v2_b1' in base_model_name or 'v2_s' in base_model_name:\n            in_features = 1280\n        elif 'v2_b2' in base_model_name:\n            in_features = 1408\n        elif 'nfnet' in base_model_name:\n            in_features = 2304\n        self.backbone.head1 = nn.Linear(in_features, num_classes)\n    \n    def forward(self, x):\n        output_dict = self.backbone(x)\n        return output_dict\nmodel = FineTuneTimmClassifier().to(CFG.device)\n\ncheckpoint = torch.load(\"/kaggle/input/birdclef2023final/swa_tf_efficientnetv2_b1_fold_1_model_0.820418.pth\",\n                        map_location=torch.device('cpu'))\nmodel.load_state_dict(checkpoint['state_dict'])  \n# model1 = FineTuneTimmClassifier().to(CFG.device)\n\n# checkpoint1 = torch.load(\"/kaggle/input/birdclef2023final/finetune_tf_efficientnetv2_b1_fold_4_model_epoch_176_0.810505.pth\",\n#                         map_location=torch.device('cpu'))\n# model1.load_state_dict(checkpoint1['state_dict'])\n\nmodel2 = FineTuneTimmClassifier(base_model_name = 'tf_efficientnetv2_b2').to(CFG.device)\n\ncheckpoint2 = torch.load(\"/kaggle/input/birdclef2023final/swa_tf_efficientnetv2_b2_fold_1_model_0.819913.pth\",\n                        map_location=torch.device('cpu'))\nmodel2.load_state_dict(checkpoint2['state_dict'])\n\n# model4 = FineTuneTimmClassifier(base_model_name = 'tf_efficientnetv2_b0').to(CFG.device)\n\n# checkpoint4 = torch.load(\"/kaggle/input/birdclef2023final/finetune_tf_efficientnetv2_b0_fold_1_model_epoch_173_0.813764.pth\",\n#                         map_location=torch.device('cpu'))\n# model4.load_state_dict(checkpoint4['state_dict'])\n\nmodel3 = FineTuneTimmClassifier(base_model_name = 'tf_efficientnetv2_s').to(CFG.device)\n\ncheckpoint3 = torch.load(\"/kaggle/input/birdclef2023final/swa_tf_efficientnetv2_s_fold_1_model_0.816482.pth\",\n                        map_location=torch.device('cpu'))\nmodel3.load_state_dict(checkpoint3['state_dict'])\n\n\nmodel5 = FineTuneTimmClassifier(base_model_name = 'tf_efficientnetv2_b2')\n\ncheckpoint5 = torch.load(\"/kaggle/input/birdclef2023final/swa_tf_efficientnetv2_b2_fold_4_model_0.813245.pth\",\n                        map_location=torch.device('cpu'))\nmodel5.load_state_dict(checkpoint5['state_dict'])\n\n# model2 = TimmSED(base_model_name='tf_efficientnetv2_b1', pretrained=False)\n# checkpoint2 = torch.load(\"/kaggle/input/birdclef2023/tf_efficientnetv2_b1_fold_1_model_epoch_68_0.8750.pth\",map_location=torch.device('cpu'))\n\n# model2.load_state_dict(checkpoint2['state_dict'])","metadata":{"execution":{"iopub.status.busy":"2023-05-24T10:18:57.453209Z","iopub.execute_input":"2023-05-24T10:18:57.453722Z","iopub.status.idle":"2023-05-24T10:19:03.068025Z","shell.execute_reply.started":"2023-05-24T10:18:57.453680Z","shell.execute_reply":"2023-05-24T10:19:03.066833Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"class TestDataset(Dataset):\n    def __init__(self, \n                 df: pd.DataFrame, \n                 clip: np.ndarray\n                ):\n        \n        self.df = df\n        self.clip = clip\n        \n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx: int):\n\n        sample = self.df.loc[idx, :]\n        row_id = sample.row_id\n\n        end_seconds = int(sample.seconds)\n        start_seconds = int(end_seconds - 5)\n#         print(f\"start: {start_seconds}\")\n        y = self.clip[CFG.sample_rate * start_seconds : CFG.sample_rate  * end_seconds].astype(np.float32)\n#         print(f\"Audio now: {idx}, {y}\")\n        waveform=torch.Tensor(y)\n        torchaudio_melspec = T.MelSpectrogram(\n            sample_rate=CFG.sample_rate,\n            n_fft=CFG.nfft,\n            win_length=None,\n            hop_length=CFG.hop_length,\n            center=True,\n            pad_mode=\"constant\",\n            power=2.0,\n            norm='slaney',\n            mel_scale='slaney',\n            n_mels=CFG.n_mels,\n            f_min = CFG.fmin,\n            f_max = CFG.fmax,\n        )(waveform)\n        torchaudio_melspec = T.AmplitudeToDB(stype=\"power\",top_db=80.00)(torchaudio_melspec)\n        image = mono_to_color(torchaudio_melspec.numpy())\n        image = image.astype(np.uint8)\n        image = img_transforms['valid'](image=image)['image']\n#         print(f\"Image: {idx}{image}\")\n        image = np.stack([image])\n        image = torch.tensor(image).float()\n            \n        return {\n            \"image\": image,\n            \"row_id\": row_id,\n        }\n    \n# class TestDatasetB1(Dataset):\n#     def __init__(self, \n#                  df: pd.DataFrame, \n#                  clip: np.ndarray\n#                 ):\n        \n#         self.df = df\n#         self.clip = clip\n        \n\n#     def __len__(self):\n#         return len(self.df)\n\n#     def __getitem__(self, idx: int):\n\n#         sample = self.df.loc[idx, :]\n#         row_id = sample.row_id\n\n#         end_seconds = int(sample.seconds)\n#         start_seconds = int(end_seconds - 5)\n# #         print(f\"start: {start_seconds}\")\n#         y = self.clip[CFG.sample_rate * start_seconds : CFG.sample_rate  * end_seconds].astype(np.float32)\n# #         print(f\"Audio now: {idx}, {y}\")\n#         waveform=torch.Tensor(y)\n#         torchaudio_melspec = T.MelSpectrogram(\n#             sample_rate=CFG.sample_rate,\n#             n_fft=CFG.nfft,\n#             win_length=None,\n#             hop_length=CFG.hop_length,\n#             center=True,\n#             pad_mode=\"constant\",\n#             power=2.0,\n#             norm='slaney',\n#             mel_scale='slaney',\n#             n_mels=CFG.n_mels,\n#             f_min = CFG.fmin,\n#             f_max = CFG.fmax,\n# #             normalized=True,\n#         )(waveform)\n#         torchaudio_melspec = T.AmplitudeToDB(stype=\"power\",top_db=80.00)(torchaudio_melspec)\n#         image = mono_to_color(torchaudio_melspec.numpy())\n#         image = image.astype(np.uint8)\n# #         image = img_transforms['valid'](image=image)['image']\n# #         print(f\"Image: {idx}{image}\")\n#         image = np.stack([image])\n#         image = torch.tensor(image).float()\n            \n#         return {\n#             \"image\": image,\n#             \"row_id\": row_id,\n#         }","metadata":{"execution":{"iopub.status.busy":"2023-05-24T10:19:03.070109Z","iopub.execute_input":"2023-05-24T10:19:03.070643Z","iopub.status.idle":"2023-05-24T10:19:03.088139Z","shell.execute_reply.started":"2023-05-24T10:19:03.070589Z","shell.execute_reply":"2023-05-24T10:19:03.087126Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"all_audios = list(Path(\"../input/birdclef-2023/test_soundscapes/\").glob(\"*.ogg\"))\nprint(all_audios)","metadata":{"execution":{"iopub.status.busy":"2023-05-24T10:19:03.089400Z","iopub.execute_input":"2023-05-24T10:19:03.090177Z","iopub.status.idle":"2023-05-24T10:19:03.117706Z","shell.execute_reply.started":"2023-05-24T10:19:03.090135Z","shell.execute_reply":"2023-05-24T10:19:03.115973Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"[PosixPath('../input/birdclef-2023/test_soundscapes/soundscape_29201.ogg')]\n","output_type":"stream"}]},{"cell_type":"code","source":"def set_seed(seed = 42):\n    '''Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    # When running on the CuDNN backend, two further options must be set\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    print('> SEEDING DONE')\nset_seed(1)   \nseconds = [i for i in range(5, 605, 5)]\nmodels_ensemble = []\n# os.environ[\"OMP_NUM_THREADS\"]=\"2\"\n# os.environ[\"OMP_SCHEDULE\"]=\"STATIC\"\n# os.environ[\"OMP_PROC_BIND\"]=\"CLOSE\"\n# torch.set_num_threads(4)\nmodel.eval()\n# model1.eval()\nmodel2.eval()\nmodel3.eval()\n# model4.eval()\nmodel5.eval()\n# models_ensemble.append(model)\nmodels_ensemble.append(model)\n# models_ensemble.append(model1)\nmodels_ensemble.append(model2)\nmodels_ensemble.append(model3)\n# models_ensemble.append(model4)\nmodels_ensemble.append(model5)\n# models_ensemble.append(model4)\n\n# models_ensemble.append(model2)\n# models_ensemble.append(model3)\nweights = np.array([0.35, 0.35, 0.15, 0.15])\ndef prediction_for_clip(\n    audio_path\n):\n    \n    device = torch.device(\"cpu\")\n    \n    # inference\n    prediction_dict = {}\n\n    clip, _ = librosa.load(audio_path, sr=32000)\n    name_ = \"_\".join(audio_path.name.split(\".\")[:-1])\n    row_ids = [name_+f\"_{second}\" for second in seconds]\n\n    test_df = pd.DataFrame({\n        \"row_id\": row_ids,\n        \"seconds\": seconds\n    })\n    \n    for i in range(len(models_ensemble)):\n#         model.eval()\n#         if i==0 or i==1:\n        dataset = TestDataset(\n            df=test_df, \n            clip=clip,\n        ) \n#         else:\n       \n#             dataset = TestDataset1(\n#                 df=test_df, \n#                 clip=clip,\n#             ) \n#         dataset = TestDatasetB1(\n#             df=test_df, \n#             clip=clip,\n#         ) \n        \n        loader = DataLoader(\n            dataset,\n            batch_size=4, \n            num_workers=4,\n            drop_last=False,\n            shuffle=False,\n            pin_memory=True\n        )\n        \n        for data in loader:\n            \n            row_ids = data['row_id']\n            \n            for row_id in row_ids:\n                if row_id not in prediction_dict:\n                    prediction_dict[str(row_id)] = []\n            \n            image = data['image'].to(device)\n                \n            probas = []\n            \n\n            with torch.no_grad():\n                with autocast(enabled=True):\n                    output = models_ensemble[i](image)\n#                 print(output['logit'])\n                \n#                     \n            for row_id_idx, row_id in enumerate(row_ids):\n                prediction_dict[str(row_id)].append(F.softmax(output['logit'][[row_id_idx]], dim=1).numpy().reshape(-1))\n        gc.collect()                                                \n    for row_id in list(prediction_dict.keys()):\n#         print(\"Shape here\",np.array(prediction_dict[row_id]).shape)\n#         print(\"Here\",prediction_dict[row_id])        \n#         weighted_avg = np.average(np.array(prediction_dict[row_id])[:3], axis=0, weights=weights)\n#         print(\"After weights avg: \", weighted_avg.shape)\n#         print(\"Result: \", weighted_avg)\n#         logits = np.average((weighted_avg, np.array(prediction_dict[row_id])[3]), axis = 0)\n#         logits = np.array(prediction_dict[row_id]).mean(0)\n        logits = np.average(np.array(prediction_dict[row_id]), axis=0, weights=weights)\n#         print(\"Logits here: \", logits)\n        prediction_dict[row_id] = {}\n        for label in range(len(CFG.target_columns)):\n            prediction_dict[row_id][CFG.target_columns[label]] = logits[label]\n\n    return prediction_dict","metadata":{"execution":{"iopub.status.busy":"2023-05-24T10:19:03.119924Z","iopub.execute_input":"2023-05-24T10:19:03.120359Z","iopub.status.idle":"2023-05-24T10:19:03.167704Z","shell.execute_reply.started":"2023-05-24T10:19:03.120288Z","shell.execute_reply":"2023-05-24T10:19:03.166341Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"> SEEDING DONE\n","output_type":"stream"}]},{"cell_type":"code","source":"# os.environ[\"OMP_NUM_THREADS\"]=\"2\"\n# os.environ[\"OMP_SCHEDULE\"]=\"STATIC\"\n# os.environ[\"OMP_PROC_BIND\"]=\"CLOSE\"\n# torch.set_num_threads(4)\n\nstart = time.time()\nwith concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n    dicts = list(executor.map(prediction_for_clip, all_audios))\nprint(f\"With concurrent ThreadPoolExecutor, time cost reduced to {time.time()-start} for processing 1 audios\")","metadata":{"execution":{"iopub.status.busy":"2023-05-24T10:19:03.168980Z","iopub.execute_input":"2023-05-24T10:19:03.169951Z","iopub.status.idle":"2023-05-24T10:20:04.971695Z","shell.execute_reply.started":"2023-05-24T10:19:03.169910Z","shell.execute_reply":"2023-05-24T10:20:04.969975Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"With concurrent ThreadPoolExecutor, time cost reduced to 61.79345464706421 for processing 1 audios\n","output_type":"stream"}]},{"cell_type":"code","source":"prediction_dicts = {}\nfor d in dicts:\n    prediction_dicts.update(d)","metadata":{"execution":{"iopub.status.busy":"2023-05-24T10:20:04.976026Z","iopub.execute_input":"2023-05-24T10:20:04.976943Z","iopub.status.idle":"2023-05-24T10:20:04.985019Z","shell.execute_reply.started":"2023-05-24T10:20:04.976892Z","shell.execute_reply":"2023-05-24T10:20:04.983126Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame.from_dict(prediction_dicts, \"index\").rename_axis(\"row_id\").reset_index()\nsubmission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-05-24T10:20:04.986740Z","iopub.execute_input":"2023-05-24T10:20:04.987139Z","iopub.status.idle":"2023-05-24T10:20:05.106958Z","shell.execute_reply.started":"2023-05-24T10:20:04.987102Z","shell.execute_reply":"2023-05-24T10:20:05.105405Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# submission.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-24T10:20:05.108805Z","iopub.execute_input":"2023-05-24T10:20:05.109986Z","iopub.status.idle":"2023-05-24T10:20:05.115745Z","shell.execute_reply.started":"2023-05-24T10:20:05.109936Z","shell.execute_reply":"2023-05-24T10:20:05.114012Z"},"trusted":true},"execution_count":12,"outputs":[]}]}